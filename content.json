{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Yuan Cheng","url":"http://hellcy.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-02-26T12:08:47.158Z","updated":"2021-02-26T12:08:47.158Z","comments":false,"path":"/404.html","permalink":"http://hellcy.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-02-26T12:08:47.159Z","updated":"2021-02-26T12:08:47.159Z","comments":false,"path":"books/index.html","permalink":"http://hellcy.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-02-26T12:08:47.159Z","updated":"2021-02-26T12:08:47.159Z","comments":false,"path":"categories/index.html","permalink":"http://hellcy.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-02-26T12:08:47.457Z","updated":"2021-02-26T12:08:47.457Z","comments":true,"path":"links/index.html","permalink":"http://hellcy.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-02-26T12:08:47.457Z","updated":"2021-02-26T12:08:47.457Z","comments":false,"path":"repository/index.html","permalink":"http://hellcy.github.io/repository/index.html","excerpt":"","text":""},{"title":"About","date":"2021-02-26T12:08:47.159Z","updated":"2021-02-26T12:08:47.159Z","comments":false,"path":"about/index.html","permalink":"http://hellcy.github.io/about/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-02-26T12:08:47.457Z","updated":"2021-02-26T12:08:47.457Z","comments":false,"path":"tags/index.html","permalink":"http://hellcy.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Java Concurrency","slug":"Java-Concurrency","date":"2022-02-12T10:23:56.000Z","updated":"2022-02-12T12:11:41.304Z","comments":true,"path":"2022/02/12/Java-Concurrency/","link":"","permalink":"http://hellcy.github.io/2022/02/12/Java-Concurrency/","excerpt":"","text":"什么是进程 进程是指可执行程序并存放在计算机存储器的一个指令序列，他是一个动态执行的过程 什么是线程 线程是比进程还要小的运行单位，一个进程包含多个线程 线程可以看做一个子程序 线程的创建 创建一个Thread类，或者一个Thread子类的对象 创建一个实现了Runnable接口的类的对象 创建一个实现了Callable接口的类的对象 继承Thread类 12345public class CustomThread extends Thread &#123; public void run() &#123; System.out.println(getName() + &quot; thread is running&quot;); &#125;&#125; Runnable 接口 只有一个方法run() Runnable是Java中用已实现线程的接口 任何实现线程功能的类都必须实现该接口 为什么要实现runnable接口？ Java不支持多重继承，如果一个类已经继承了一个父类，那么他只能通过实现runnable接口变成线程 可以不重写Thread类的其他方法，只需要重写Run()方法 12345678910public class PrintRunnable implements Runnable&#123; int i &#x3D; 1; @Override public void run() &#123; while (i &lt;&#x3D; 10) &#123; System.out.println(Thread.currentThread().getName() + &quot; is running &quot; + (i++)); &#125; &#125;&#125; 123456789101112131415class PrintRunnableTest &#123; @Test public void testRunnableThread() &#123; PrintRunnable runnable1 &#x3D; new PrintRunnable(); &#x2F;&#x2F; use runnable object to create thread &#x2F;&#x2F; 2 threads are sharing the same variable Thread t1 &#x3D; new Thread(runnable1); t1.start(); Thread t2 &#x3D; new Thread(runnable1); t2.start(); &#125;&#125; 实现callable接口 重写call()方法，作为线程的主体，具有返回值，并且可以对异常进行声明和抛出， 使用start()方法来启动线程 创建callable接口的实现类，并实现call()方法 创建callable实现类的实例，使用FutureTask类来包装callable对象，该FutureTask对象封装了callable对象的call()方法的返回值 使用FutureTask对象作为Thread对象的target，创建并启动线程 调用FutureTask对象的get()方法来获得子线程执行结束后的返回值 实现callable接口，创建线程 1234567public class CustomThread implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; String str &#x3D; &quot;thread message&quot;; return str; &#125;&#125; test thread 12345678910111213141516@Testpublic void testCustomThread() &#123; Callable&lt;String&gt; callObj &#x3D; new CustomThread(); FutureTask&lt;String&gt; ft &#x3D; new FutureTask&lt;&gt;(callObj); Thread thread &#x3D; new Thread(ft); &#x2F;&#x2F; start thread thread.start(); &#x2F;&#x2F; the return value can only be got after thread has been started try &#123; System.out.println(ft.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 线程的状态 new runnable running blocked dead sleep方法的使用 public static void sleep(long millis) 作用：在指定的毫秒数内让正在执行的线程休眠（暂停执行） 参数为休眠的时间，单位是毫秒 12345678while (i &lt;&#x3D; 30) &#123; System.out.println(Thread.currentThread().getName() + &quot; is running &quot; + (i++)); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; join方法应用 public final void join() 等待调用该方法的线程结束后才能执行 （调用者抢占CPU使用权） public final void join(long millis) 等待该线程终止的最长时间为millis毫秒 线程优先级 Java为线程类提供了10个优先级 优先级可以用整数1-10表示，超过范围会抛出异常 主线程默认优先级为5 getPriority() setPriority() 优先级常量 MAX_PRIORITY 10 MIN_PRIORITY 1 NORM_PRIORITY 5 (默认) 多线程运行问题 各个线程是通过竞争CPU时间获得运行机会的 各线程什么时候得到CPU时间，占用多久，是不可预测的 一个正在运行的线程在什么地方被暂停是不确定的 同步 synchronized 可以被用在 成员方法 静态方法 语句块 线程间通信 生产者 - 消费者 模型 wait()方法，终端方法的执行，是线程等待 notify()方法，唤醒处于等待的某一个线程，使其结束等待 notifyAll()方法，唤醒处于等待的所有线程，使它们结束等待","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"}]},{"title":"Java Collections","slug":"Java-Collections","date":"2022-02-11T15:17:40.000Z","updated":"2022-02-12T09:22:52.944Z","comments":true,"path":"2022/02/12/Java-Collections/","link":"","permalink":"http://hellcy.github.io/2022/02/12/Java-Collections/","excerpt":"","text":"应用场景 无法预测存储数据的数量 同时存储具有一对一关系的数据 需要进行数据的增删 数据重复的问题 集合框架的体系结构 Collection - 类的对象 List ArrayList Queue LinkedList Set HashSet Map - key value pairs HashMap List List是元素有序并且可以重复的集合，成为序列 List可以精确的控制每个元素的插入位置，或删除某个位置的元素 List的两个主要实现是ArrayList和LinkedList ArrayList ArrayList底层是由数组实现的 动态增长（倍增)，以满足应用程序的需求 在列表尾部插入或删除数据非常有效，增删中间部分的元素则不是非常高效 更适合查找和更新元素 ArrayList中的元素可以为null Set Set是元素无序并且不可以重复的集合 HashSet HashSet是Set的一个重要的实现类， HashSet中的元素无序并且不可以重复 HashSet中只允许一个null元素 具有良好的存取和查找性能 迭代器 Iterator Iterator接口可以以统一的方法对各种集合元素进行遍历 hasNext()方法检测集合中是否还有下一个元素 next()方法返回集合中的下一个元素 Map Map中的数据是以key value的形式存储的 key value是以Entry类型的对象实例存在 可以通过key值快速得查找value 一个映射不能包含重复的key， value可以重复 每个key最多映射到一个value HashMap 给予哈希表得Map实现 允许使用null值和null key key值不允许重复 HashMap中的entry对象是无序排列得 集合排序 使用Collections类的sort()方法 使用Comparator和Comparable接口进行排序 Comparator接口 强行对某个对象进行整体排序的比较函数 可以将comparator传递给sort方法 （如Collections.sort或者Arrays.sort） int compare(T o1, T o2) 比较用来排序的两个参数 if o1 &lt; o2, return negative integer if o1 == o2, return 0 if o1 &gt; o2, return positive integer Comparable接口 此接口强行对实现他的每个类的对象进行整体排序 这种排序被称为类的自然排序，类的compareTo方法被称为他的自然比较方法 对于集合，通过调用Collections.sort方法进行排序 对于数组，通过调用Arrays.sort方法进行排序 int compareTo(T o) 该对象小于，等于或大于指定对象，则分别返回负整数，零或者正整数 （与之前的int compare(T o1, T o2)相同） Comparator 和 Comparable 的区别 Comparator Comparable 位于java.util包 位于java.lang包 在要比较的类的外部实现该接口，并且可以实现多个不同的Comparator 在要比较的类上实现该接口 调用sort方法时，要指定comparator的实现类 调用sort方法时，只需指定集合名即可 应用场景 如果一个类实现了comparable接口，还希望通过不同的方式进行排序，我们还可以定义额外的Comparator Comparable会作为默认的排序方式，Comparator接口则作为一个扩展的排序方式 TreeSet TreeSet 是一个有序的集合，它支持自然排序个根据实现Comparable和Comparator接口进行排序 当TreeSet用来储存String或者Integer对象时，会按照他们的升序排列 TreeSet无法排列自定义类，元素需要实现Comparator或者Comparable来排序 泛型 在Java中增加泛型之前，泛型程序设计使用继承来实现 坏处 需要强制转换 可向集合中添加任意类型的元素，存在风险 泛型的使用 1List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;(); Java SE7及以后的版本中，构造方法中的泛型可以省略 1List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); 多态与泛型 12345class Cat extends Animal &#123;&#125;List&lt;Animal&gt; list &#x3D; new ArrayList&lt;Cat&gt;(); &#x2F;&#x2F; not allowedList&lt;Number&gt; numbers &#x3D; new ArrayList&lt;Integer&gt;(); &#x2F;&#x2F; not allowed 以上代码是不允许的， 变量声明的类型必须匹配传递给实际对象的类型 自定义泛型类 123456789101112131415161718192021222324252627282930313233343536373839public class NumGeneric&lt;T&gt; &#123; private T num; public T getNum() &#123; return num; &#125; public void setNum(T num) &#123; this.num &#x3D; num; &#125;&#125;&#x2F;&#x2F; generic class with two generic typespublic class TwoNumGeneric&lt;A, B&gt; &#123; private A num1; private B num2; public TwoNumGeneric(A num1, B num2) &#123; this.num1 &#x3D; num1; this.num2 &#x3D; num2; &#125; public A getNum1() &#123; return num1; &#125; public void setNum1(A num1) &#123; this.num1 &#x3D; num1; &#125; public B getNum2() &#123; return num2; &#125; public void setNum2(B num2) &#123; this.num2 &#x3D; num2; &#125;&#125; 自定义泛型方法 12345678public class GenericMethod &#123; &#x2F;&#x2F; &lt;T&gt; between public and void declare that this is a generic method public &lt;T&gt; void printValue(T t) &#123; System.out.println(t); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"}]},{"title":"Java Exceptions","slug":"Java-Exceptions","date":"2022-02-10T23:51:36.000Z","updated":"2022-02-12T12:21:13.328Z","comments":true,"path":"2022/02/11/Java-Exceptions/","link":"","permalink":"http://hellcy.github.io/2022/02/11/Java-Exceptions/","excerpt":"","text":"什么是异常 错误在我们编写程序的过程中会经常发生，包括编译期间和运行期间的错误 在程序运行过程中，意外发生的情况，背离我们程序本身意图的表现，都可以理解为异常 异常分类 Throwable Error OutOfMemoryError ThreadDeath … Exception 程序本身可以处理的异常 unchecked exception （runtime exception） NullPointerException ArrayIndexOutOfBoundsException … checked exception IOException SQLException … 捕获异常 对于运行时异常，错误或可查异常，Java技术所要求的异常处理方式有所不同 对于可查异常必须捕捉，或者声明抛出 允许忽略不可查的RuntimeException 和 Error 异常处理 通过5个关键字来实现： try, catch, finally, throw, throws 捕获异常 try: 执行可能产生异常的代码 catch: 捕获异常 finally: 无论是否发生异常，总是执行的代码 声明异常 throws: 声明可能要抛出的异常 抛出异常 手动抛出异常 try catch finally try之后可以接零个或多个catch， 如果没有catch，则必须接一个finally throw throws 可以通过throws声明将要抛出何种类型的异常， 通过throw将产生的异常抛出 throws 如果一个方法可能会出现异常，但没有能力处理这种异常，可以在方法声明处用throws子句来声明抛出异常 throws语句用在方法定义时，生命该方法要抛出的异常类型 当方法抛出异常列表中的异常时，方法将不对这些类型及其子类类型的异常做处理，而抛向方法的调用者，由它去处理 1234567891011121314151617public int divideIntegers(int a, int b) throws Exception &#123; return a &#x2F; b;&#125;public static void main(String[] args) &#123; try &#123; int answer &#x3D; divideIntegers(5, 10); &#125; catch(ArithmeticException e) &#123; &#125; catch(InputMismatchException e) &#123; &#125; catch(Exception e) &#123; &#125; finally &#123; &#125;&#125; throw throw用来抛出一个异常 抛出的只能够是可抛出类throwable或者其子类的实例对象 自己抛出的异常，自己处理 抛出异常使用throws由调用者处理 自定义异常 使用Java内置的异常类可以描述在编程时出现的大部分异常情况 也可以通过自定义异常描述特定业务产生的异常类型 自定义异常就是定义一个类，去继承Throwable类或者它的子类 12345public class CustomException extends Exception &#123; public CustomExpcetion () &#123; super(&quot;Custom exception message&quot;)； &#125;&#125; 异常链 有时候我们会捕获一个异常后再抛出另一个异常 顾名思义就是：将异常发生的原因一个穿一个穿起来，把底层的异常信息传给上层，这样逐层抛出 新的异常可以保留原有异常的信息 构造方法的定义如下 1Exception(String mesasge, Throwable cause) 123456789101112131415161718192021public void methodOne() throws FirstException &#123; throw new FirstException(&quot;first exception&quot;);&#125;public void methodTwo() throws SecondException &#123; try &#123; methodOne(); &#125; catch (FirstException e) &#123; throw new SecondException(&quot;Second Exception&quot;, e); &#125;&#125;public void methodThree() throws ThirdException &#123; try &#123; methodTwo(); &#125; catch (SecondException e) &#123; ThridException exception &#x3D; new ThirdException(&quot;third exception&quot;); exception.initCause(e); &#x2F;&#x2F; initCause是Exception中的另外一个成员方法，用于添加cause，原有异常的信息 throw exception; &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"}]},{"title":"Java Polymorphism","slug":"Java-Polymorphism","date":"2022-02-10T05:16:18.000Z","updated":"2022-02-12T12:20:50.095Z","comments":true,"path":"2022/02/10/Java-Polymorphism/","link":"","permalink":"http://hellcy.github.io/2022/02/10/Java-Polymorphism/","excerpt":"","text":"编译时多态 设计时多态方法重载 运行时多态 程序运行时动态决定调用那个方法 必要条件 满足继承关系 父类引用指向子类对象 向上转型 父类引用指向子类实例，可以调用子类重写父类的方法以及父类派生的方法，无法调用子类独有的方法 父类中的静态方法无法被子类重写，所以向上转型之后，只能调用到父类原有的静态方法 向下转型 子类引用指向父类对象，此处可以使用instanceof进行检查，避免类型转换时的安全性问题 可以调用子类独有的方法 抽象类 abstract class 限制实例化 只能被继承 应用场景： 某个父类只是知道其子类应该包含怎样的方法，但无法准确知道这些子类如何实现这些方法 抽象方法 abstract method 不能有方法体 必须由子类实现 子类如果没有重写父类的所有抽象方法，则也要定义为抽象类 接口 Interface 当多个类具有相同能力的时候，可以使用接口抽象出相同的能力 接口定义了某一批类所需要遵守的规范 接口不关心这些类的内部数据，也不关心这些类里的方法的实现细节，它只规定这些类里必须提供某些方法 接口方法可以不写abstract关键字，并且默认为public的访问权限 当类实现接口时，需要去实现接口中的所有抽象方法，否则需要将该类设置为抽象类 接口中可以定义常量，默认为public static final 默认方法 自JDK1.8之后，接口中可以存在默认方法，使用default关键字定义 默认方法可以带方法体，子类实现接口时可以不用实现默认方法 子类可以重写默认方法，并可以通过接口的引用调用 静态方法 自JDK1.8之后，接口中可以存在静态方法，使用static关键字定义 静态方法可以带方法体，子类可以通过使用接口名访问接口的静态方法 多重实现 子类可以继承一个父类，但是可以实现多个接口 当多个接口中具有相同签名的方法时，子类需要重写方法 当父类和接口具有相同签名的方法时，父类方法具有优先权 当父类和接口具有相同名字的变量时，子类需要重新定义该变量，父类中的变量不具有优先权 接口的继承 接口也可以实现继承关系 接口可以继承多个父接口 1234567891011public interface ParentOne &#123;&#125;public interface ParentTwo &#123;&#125;public interface Child extends ParentOne, ParentTwo &#123;&#125; 内部类 内部类提供了更好的封装，不允许其他外部类访问内部类的信息 成员内部类 最常见的内部类，也称为普通内部类 内部类在外部使用时，无法直接实例化，需要借由外部类信息才能完成实例化 内部类的访问修饰符，可以是任意的，但是访问权限会受到修饰符的影响 内部类可以直接访问外部类的成员（包括成员属性和成员方法），如果出现同名属性，优先访问内部类中定义的 外部类访问内部类的信息需要通过内部类的实例，无法直接访问 内部类编译后得class文件名：外部类$内部类.class 获取内部类对象实例 new 外部类.new 内部类 1Person.Heart myHeart &#x3D; new Person().new Heart(); 外部类对象.new 内部类 1myHeart &#x3D; myPerson.new Heart(); 外部类对象.获取方法 1myHeart &#x3D; myPerson.getHeart(); 静态内部类 静态内部类中，只能直接访问外部类的静态成员 需要使用外部类的实例对象来访问非静态成员 访问静态内部类对象实例时，可以不依赖于外部类对象 获取静态内部类实例 1Person.Heart myHeart &#x3D; new Person.Heart(); 方法内部类 定义在外部类方法中的内部类，也成为局部内部类 方法内部类中无法定义静态成员 类中可以使用final，abstract成员 和方法内部成员使用规则一样，class前面不可以添加public，private，protected，static等关键字 匿名内部类 将类的定义和类的创建放在一起完成，程序只会用到一次类的实例，所以类名无关紧要 对于抽象类Person来说，如果我们想调用其中的抽象方法，一种做法是创建一个实现read方法的子类 但是如果这个子类只会被用到一次，那这个子类的名字就不重要，就可以使用匿名内部类来解决 无法使用private，public，protected，static修饰 无法编写构造方法，但是可以添加初始化代码块 不能出现静态成员 可以实现接口也可以继承父类，但是不能同时 123456789101112131415161718192021public abstract class Person &#123; public abstract void read();&#125;public class PersonTest &#123; public void getRead(Person person) &#123; person.read(); &#125; public static void main(String[] args) &#123; PersonTest personTest &#x3D; new PersonTest(); personTest.getRead(new Person() &#123; @Override public void read() &#123; System.out.println(&quot;implement read method in Person parent abstract class&quot;); &#125; &#125;) &#125;&#125; 匿名类的例子 在我们使用comparator对Collections进行排序的时候可以使用匿名类来省去创建子类的过程 不使用匿名类对List排序 1234567891011121314&#x2F;&#x2F; create a child class implements parent Comparatorpublic class CustomComparator implements Comparator&lt;String&gt; &#123; @Override public int compare(String o1, String o2) &#123; return o1.compareTo(o2); &#125;&#125;public class testComparator() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F; use CustomComparator to sort list Collections.sort(list, new CustomComparator());&#125; 使用匿名类对list排序 123456789public class testComparator() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); Collections.sort(list, new Comparator&lt;String&gt;() &#123; public int compare(String o1, String o2) &#123; return o1.compareTo(o2); &#125; &#125;)&#125; 自Java1.8以后，可以使用lambda expression来省去方法名，匿名方法 12345public class testComparator() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); Collections.sort(list, (x, y) -&gt; x.compareTo(y));&#125; 因为上面的匿名方法和String里面定义的compareTo方法一样，我们可以使用method reference来更加简化代码 12345public class testComparator() &#123; List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;(); Collections.sort(list, String::compareTo);&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"}]},{"title":"Design Pattern - Singleton in Java","slug":"Design-Pattern-Singleton-in-Java","date":"2022-02-09T06:26:19.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2022/02/09/Design-Pattern-Singleton-in-Java/","link":"","permalink":"http://hellcy.github.io/2022/02/09/Design-Pattern-Singleton-in-Java/","excerpt":"","text":"目的 使得类的一个对象成为该类系统中唯一的实例 定义 一个类有且仅有一个实例，并且自行实例化向整个系统提供 要点 某个类只能有一个实例 必须自行创建实例 必须自行向这个系统提供这个实例 实现 只提供私有的构造方法 含有一个该类的静态私有对象 提供一个静态的公有方法用于创建，获取静态私有对象 Create class instance when class is loaded 12345678910111213141516public class SingletonOne &#123; &#x2F;&#x2F; private class constructor private SingletonOne () &#123; &#125; &#x2F;&#x2F; private static instance private static SingletonOne INSTANCE &#x3D; new SingletonOne(); &#x2F;&#x2F; return instance in public method public static SingletonOne getInstance() &#123; return INSTANCE; &#125;&#125;&#x2F;&#x2F; SingletonOne instance &#x3D; SingletonOne.getInstance(); Only create instance when the method is being called 123456789101112public class SingletonTwo &#123; private SingletonTwo () &#123; &#125; private static SingletonTwo INSTANCE &#x3D; null; public static SingletonTwo getInstance() &#123; if (INSTANCE &#x3D;&#x3D; null) INSTANCE &#x3D; new SingletonTwo(); return INSTANCE; &#125;&#125; 注意 lazy loading 存在线程风险 1 同步锁 双重校验锁 静态内部类 枚举 优点 在内存中只有一个对象，节省内存空间 避免频繁地创建对象，提高性能 避免对共享资源的多重占用 缺点 扩展比较困难 如果实例化后的对象长期不利用，系统将默认为垃圾进行回收，造成对象状态丢失 场景 创建对象时占用资源过多，但同时有需要用到该类对象 对系统内资源要求统一读写，如读写配置信息 当多个实例存在可能引起程序逻辑错误，如号码生成器 （ID）","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Elastic Search","slug":"Elastic-Search","date":"2022-01-02T06:41:13.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2022/01/02/Elastic-Search/","link":"","permalink":"http://hellcy.github.io/2022/01/02/Elastic-Search/","excerpt":"","text":"Basics Nodes store the data that we add to ElasticSearch A cluster is a collection of nodes Data is stored as document which are JSON objects Documents are grouped together with indices The purpose of sharding mainly to be able to store more documents to easier fit large indices onto nodes improved performance parallelization of queries increases the throughput of an index Configuring the number of shards an index contains a single shard by default increase the number of shards with the Split API reduce the number of shards with the Shrink API when changing the number of shards, new index will be created and documents in the old index will be migrated to the new index Replication replication is configured at the index level replication works by creating copies of shards, referred to as replica shards a shard that has been replicated is called a primary shard a primary shard and its replica shards are referred as a replication group replica shards are a complete copy of a shard a replica shard can serve search requests, exactly like its primary shard the number of replicas can be configured at index creation node can store multiple shards, and primary shard and replica shards will never be stored in the same node. So the data will NOT be lost if the node fails snapshots ElasticSearch supports taking snapshots as backups snapshots can be used to restore to a given point in time snapshots can be taken at the index level, or for the entire cluster use snapshots for backups, and replication for high availability and performance increasing query throughput with replication replica shards of a replication group can serve different search requests simultaneously this increases the number of requests that can be handled at the same time ElasticSearch intelligently routes requests to the best shard CPU parallelization (CPU has multiple cores now and can run queries on different threads at the same time) improves performance if multiple replica shards are stored on the same node Master-eligible node the node may be elected as the cluster’s master node a master node is responsible for creating and deleting indices, among others may be used for having dedicated master nodes useful for large clusters meaning that this master node will not be serving requests, only focusing on its own tasks Data node enables a node to store data storing data includes performing queries related to that data, such as search queries for relatively small clusters, this role is almost always enabled Managing Documents Delete index 1DELETE &#x2F;index_name create index 123456PUT &#x2F;index_name&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 2 &#125;&#125; indexing documents 123456POST &#x2F;products&#x2F;_doc&#123; &quot;name&quot;: &quot;Coffee Maker&quot;, &quot;price&quot;: 53, &quot;in_stack&quot;: 10&#125; Retrieving document by ID 1GET &#x2F;products&#x2F;_doc&#x2F;document_ID Updating document 1234567POST &#x2F;products&#x2F;_update&#x2F;document_ID&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;new name&quot;, &quot;new field&quot;: &quot;this is a new field&quot; &#125;&#125; Upserts insert the new document if not exists, and run the script if the document exists 1234567891011POST &#x2F;products&#x2F;_update&#x2F;101&#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.in_stock++ &#125;, &quot;upsert&quot;: &#123; &quot;name&quot;: &quot;name&quot;, &quot;price&quot;: 399, &quot;in_stock&quot;: 5 &#125;&#125; Delete document 1DELETE &#x2F;products&#x2F;_doc&#x2F;101 Routing routing is the process of resolving a shard for a document the default routing strategy ensures that documents are distributed evenly Optimistic concurrency control prevent overwriting documents inadvertently dur to concurrent operations primary terms a way to distinguish between old and new primary shards essentially a counter for how many times the primary shard has changed the primary term is appended to write operations sequence numbers appended to write operations together with the primary term essentially a counter that is incremented for each write operation the primary shard increases the sequence number enables ElasticSearch to order write operations sending write requests to ElasticSearch concurrently may overwrite changes made by other concurrent processes we use the primary terms and sequence number fields ElasticSearch will reject a write operation if it contains the wrong primary term or sequence number Updating multiple document the query creates a snapshot to do optimistic concurrency control search queries and bulk requests are sent to replication groups sequentially ElasticSearch retries these queries up to ten times if the queries still fail, the whole query is aborted any changes already made to documents, are NOT rolled back the API returns information about failures Mapping and Analysis a field’s values are stored in one of several data structures the data structure depends on the field’s data type ensures efficient data access Inverted indices mapping between terms (tags) and which documents contain them outside the context of analyzers, we use the terminology ‘terms’ an inverted index is created for EACH text field values for a text field are analyzed and the results are stored within an inverted index each field has a dedicated inverted index an inverted index is a mapping between terms and which documents contain them terms are sorted alphabetically for performance reasons created and maintained by Apache Lucene inverted indices enable fast searches Note: for array of strings In ElasticSearch, there is no dedicated array data type, any field can contain zero or more values by default, however, all values in the array must be of the same data type when adding a field dynamically, the first value in the array determines the field type meaning for array of strings, ElasticSearch would have created a inverted index mapping table for it keyword data type keyword fields are analyzed with the keyword analyzer the keyword analyzer is an no-op analyzer it outputs the unmodified string as a single token this token is then placed into the inverted index used for exact matching of values typically used for filtering, aggregations, and sorting for full-text searches, use the text data type instead Arrays there is no such thing as an array data type any field may contain zero or more values no configuration or mapping needed simply supply an array when indexing a document how is the array stored in the ElasticSearch internally? e.g. if it is an array of strings the strings are simply concatenated before being analyzed and the resulting tokens are stored within an inverted index as normal String data type Dates specified in one of the three ways specially formatted strings milliseconds since the epoch seconds since the epoch epoch refers to the 1st of Jan 1970 custom formats are supported How date fields are stored stored internally as milliseconds since the epoch any valid value that you supply at index time is converted to a long value internally dates are converted to the UTC timezone the same date conversion happens for search queries too Missing fields all fields are ElasticSearch are optional you can leave out a field when indexing documents unlike relational databases when you need to allow NULL values some integrity checks need to be done at the application level e.g. having required fields adding a field mapping does not make a field required searches automatically handle missing fields Stemming and stop words Stemming reduces words to their root form e.g. loved -&gt; love drinking -&gt; drink stop words words that are filtered out during the text analysis common words such as ‘a’, ‘the’, ‘at’, ‘of’, ‘on’ etc… they provide little to no value to the relevance scoring fairly common to remove such words less common in ElasticSearch today than in the past the relevance algorithm has been improved significantly not removed by default Analyzer Standard analyzer splits text at word boundaries and removes punctuation done by the standard tokenizer lowercase letter with the lowercase token filter contains the stop token filter (for removing stop words, disabled by default) Simple analyzer whitespace analyzer keyword analyzer pattern analyzer a regular expression is used to match token separators it should match whatever should split the text into tokens this analyzer is very flexible the default pattern matches all non-word characters lowercase letters by default Introduction to Searching 123456GET &#x2F;product&#x2F;default&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; search queries will hit the coordinating node first, and this node will broadcast the query to all the other nodes, they will fetch the result and combine them together and return it. term level queries search for exact matches, case sensitive, searching the inverted index, not the original document term level queries are more suited for searching static values, like enums","categories":[],"tags":[{"name":"Elastic Search","slug":"Elastic-Search","permalink":"http://hellcy.github.io/tags/Elastic-Search/"}]},{"title":"Dot net + React","slug":"Dot-net-React","date":"2021-12-09T12:17:52.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2021/12/09/Dot-net-React/","link":"","permalink":"http://hellcy.github.io/2021/12/09/Dot-net-React/","excerpt":"","text":"","categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"http://hellcy.github.io/tags/NET/"},{"name":"React","slug":"React","permalink":"http://hellcy.github.io/tags/React/"}]},{"title":"Spring Framework","slug":"Spring-Framework","date":"2021-11-04T13:06:37.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/11/05/Spring-Framework/","link":"","permalink":"http://hellcy.github.io/2021/11/05/Spring-Framework/","excerpt":"","text":"Spring Framework Stereotypes Stereotype - a fixed general image or ser of characteristics which represent a particular type of person or thing Spring Stereotypes are class level annotations used to define Spring Beans when classes annotated with Spring Stereotypes are detected via the component scan, an instance of the class will be added to the Spring context Available Spring Stereotypes @Component @Controller @RestController @Repository @Service Annotation Description @Component Indicates that an annotated class is a component and it will be created as a bean @Controller indicates that an annotated class has the role of a Spring MVC controller @RestController convenience annotation which extends @Controller and @ResponseBody @Repository indicates class is a Repository, a mechanism for encapsulating storage, retrieval, and search behavior which emulates a collection of objects @Service indicates that an annotated class is a Service, an operation offered as an interface that stands alone in the model, with no encapsulated state Spring Component Scan Spring Beans defined with Spring Stereotypes are detected with Spring component scan On startup, Spring is told to scan packages for classes with Spring Stereotype annotations This configuration is Spring Framework specific, not Spring Boot Spring Boot’s auto configuration will tell Spring to perform a component scan of the package of the main class this includes all sub packages of the main class package when using Spring Boot, if class is outside of the main class package tree, you must declare the package scan manually Spring Bean Scopes Singleton - (default) only one instance of the bean is created in the IoC container Prototype - A new instance is created each time the bean is requested Request - For web app, a single instance per http request, only valid in the context of a web aware Spring applicationContext Session - for web app, a single instance per http session, only valid in the context of a web aware Spring applicationContext Global-session - a single instance per global session, typically only used in a Portlet context, only valid in the context of a web aware Spring applicationContext Application - bean is scoped to the lifecycle of a ServletContext, only valid in the context of web aware WebSocket - scopes a single bean definition to the lifecycle of a WebSocket, only valid in the context of a web aware Spring applicationContext Custom Scope - Spring Scopes are extensible, and you can define your own scope by implementing Spring’s scope interface Declaring Bean Scope No declaration needed for singleton scope in Java configuration use @Scope annotation in XML configuration file scope is an XML attribute of the bean tag 99% of the time singleton scope is fine External Properties Why use External Properties? hard coding values which can change is considered a bad practice makes your application rigid and hard to change you want your application to be portable deployment artifact (jar, war) should be deployable to different environments what can change? usernames, passwords, urls, API keys, paths, queue names etc… which property files to use using application.properties or application.yml in packaged JAR or WAR using profile specific properties or YAML files for profile specific properties for deployments, override properties that change with environment variables typically 70 - 80% of values do not change, only override what is needed environment variables offer a secure way of seeing sensitive values such as passwords Properties Setting Hierarchy (from low to high) application.properties environment variables command line variables Thymeleaf Thymeleaf is a Java template engine producing XML, XHTML and HTML5 Thymeleaf is a replacement of JSPs Thymeleaf is a natural template engine (the templates are viewable in a web browser) is not tied to web environment (can be used for producing HTML for emails) Thymeleaf is not a web framework Request Methods GET is a request for a resource (html file, javascript file, image, etc…) is used when you visit a website HEAD is like GET, but only asks for meta information without the body POST is used to post data to the server typical use case for POST is to post form data to the server (like a checkout form) POST is a create request PUT is a request for the enclosed entity be stored at the supplied URI, if the entity exists, it is expected to be updated PUT is a create OR update request DELETE is a request to delete the specified resource TRACE will echo the received request, can be used to see if request was altered by intermediate servers OPTIONS returns the HTTP methods supported by the server for the specified URL Safe methods safe methods are considered safe to use because they only fetch information and do not cause changes on the server the safe methods are: GET, HEAD, OPTIONS, and TRACE Idempotent methods a quality of an action such that repetitions of the action have no further effect on the outcome PUT and DELETE are Idempotent methods safe methods (GET, HEAD, OPTIONS and TRACE) are also Idempotent being truly idempotent is not enforced by the protocol Non-Idempotent methods POST is NOT idempotent multiple POSTs are likely to create multiple resources Ever seen websites asking you to click submit only once? HTTP Status Codes 100 series are information in nature 200 series indicate successful request 200 Okay, 201 Created, 204 Accepted 300 series are redirections 301 Moved Permanently 400 series are client errors 400 Bad Request, 401 Not Authorized, 404 Not Found 500 series are server side errors 500 Internel Server Error, 503 Service Unavailable Developer Tools added to project via artifact ‘spring-boot-devtools’ developer tools are automatically disabled when running a packaged application by default, not included in repackaged archives Features by default you need to select Build -&gt; Make project there is an advanced setting you can change to make this more seamless Template caching by default templates are cached for performance but caching will require a container restart to refresh the cache developer tools will disable template caching so the restart is not required to see changes LiveReload LiveReload is a technology to automatically trigger a browser refresh when resources are changed Spring boot developer tools includes a LiveReload server Entity Relationships One to One One entity is related to one other entity One to Many One entity is related to many entities (List, Set, Map, SortedSet, SortedMap) Many to One the inverse relationship of one to many Many to Many many entities are related to many entities each has a list or set reference to the other a join table us used to define the relationships (mapping table) Unidirectional vs Bidirectional Unidirectional is one way mapping is only done one way, one side of the relationship will not know about the other Bidirectional is two way both sides know about each other generally recommended to use bidirectional, since you can navigate the object graph in either direction Owning side the Owning side in the relationship will hold the foreign key in the database one to one is the side where the foreign key is specified one to many and many to one is the Many side mappedBy is used to define the field with owns the reference of the relationship Fetch type Lazy - data is not required until referenced Eager - data is queried up front JPA Cascade Types JPA Cascade types control how state changes are cascaded from parent objects to child objects Types PERSIS MERGE REFRESH REMOVE DETACH ALL by default, no operations are cascaded Embeddable Types JPA / Hibernate support embeddable types these are used to define a common set of properties for example, an order might have a billing address and a shipping address an embeddable type could be used for the address properties for reuse Inheritance mappedSuperclass - entities inherit from a super class, a database table IS NOT created for the super class Single Table - default - one table is used for all subclasses Joined Table - base class and subclasses have their own tables, fetching subclass entities require a join to the parent table (subclasses will not have the common properties from the parent class) Table Per Class - each subclass has its own table (no table for parent class. common properties will exist in all subclasses) Create and Update Timestamps often a best practice to use create and update timestamps on your entities for audit purposes JPA supports @PrePersist and @PreUpdate which can be used for support audit timestamps via JPA lifecycle callbacks hibernate provides @CreationTimestamp and @UpdateTimestamp Hibernate DDL Auto DDL - Data Definition Language DML - Data Manipulation Language Hibernate property is set by the Spring property options are none validate - check if there is any table or columns that are missing update - find missing table and columns and update the existing DB, not good for PROD environment create - create the DB create-drop - create the DB and drop the DB when application instance stops running. Spring boot will use create-drop for embedded databases (HSQL, H2, Derby) or none DDL vs DML DDL is used to define database structures such as tables and indexes, while DML is used with data operations such as inserts and updates Initialize with Hibernate data can be loaded from import.sql, this is a file that can be created in the root path Hibernate feature, not Spring specific must be on root of class path only executed if Hinernate’s DDL auto property is set to create or create-drop Spring JDBC Spring’s datasource initializer via Spring Boot will by default load schema.sql and data.sql from the root of the class path Spring Boot will also load from schema-$&#123;platform&#125;.sql and data-$&#123;platform&#125;.sql must set spring.datasource.platform property may conflict with Hinernate DDL auto property if using Spring datasource initializer, should set DDL auto property to none or validate Spring Data JPA Query Method define method in interface method name rules: findBy + &lt;PROPERTY_NAME&gt; it will create the query based on method name and query the database to find the data and return to us. no manual implementation needed. Repository layer and Service layer All your business logic should be in the Service Layer. Any access to the Database (any storage) should go to the Repository Layer. Lets take an Example. You have to save an entity(Person). But before saving the Person you want to make sure that the Person’s FirstName does not exist already. So the validation part should go to the business layer. In the service Layer 123456789101112PersonRepository repository; public Person save(Person p)&#123; Person p &#x3D; findByName(p.getName(); if (p !&#x3D; null)&#123; return some customException(); &#125; return repository.save(p); &#125;public Person findByName(String name)&#123; return repository.findByName(name);&#125; And in your Repository Layer just concentrate on DB Operation. View&lt;--&gt;Controller( create instance of Model class)&lt;--&gt;Service&lt;---&gt;Repository Lombok hooks in via the annotation processor API the raw source code is passed to Lombok for code generation before the Java complier continues thus, produces properly compiled Java code in conjunction with the Java compiler under the classes you can view the compiled class files IntelliJ will decompile to show you the source code IDEs since compiled code is change, and source files are not, IDE’s can get confused by this more of an issue for IDEs several years old modern IDEs such as IntelliJ, Eclipse support Project Lombok Plug in Installation may be necessary IntelliJ verify you have enabled annotation processing under compiler settings features val - local variables declared final var - mutable local variables @NonNull - Null check, will throw NPE if parameter is null @Cleanup - will call close() on resource/connection in finally block @Getter - creates getter methods for all properties @Setter - creates setter for all non-final properties @ToString generates String of classname, and each field separated by commas optional parameter to include field names optional parameter to include call to the super toString method @EqualsAndHashCode generates implementations of equals(Object other) and hashCode() by default will use all non-static, non-transient properties can optionally exclude specific properties @NoArgsConstructor generates no args constructor will cause compiler error if there are final fields can optionally force, which will initialize final fields with 0 / false / null @RequiredArgsConstructor generates a constructor for all fields that are final or marked @NonNull constructor will throw a NullPointerException if any @NonNull fields are null @AllArgsConstructor generates a constructor for all properties of class any @NotNull properties will have null check @Data generates typical boilerplate code for POJOs combines - @Getter, @Setter, @ToString, @EqualsAndHashCode, @RequiredArgsConstructor no constructor is generated if constructors have been explicitly declared @Value the immutable variant of @Data all fields are made private and final by default @NonNull set on parameter of method or constructor and a NullPointerException will be thrown if parameter is null @Builder implements the builder pattern for object creation @SneakyThrows throw checked exceptions without declaring in calling method’s throws clause @Synchronized a safer implementation of Java’s synchronized @Getter(lazy = true) for expensive getters will calculate value first time and cache additional gets will read from cache @Log creates a Java util logger @Slf4j creates a SLF4J logger recommended - SLF4J is a generic logging facade spring boot’s default logger is LogBack Testing Terminology Code under test this is the code you are testing Test fixture a test fixture is a fixed state of a set of objects used as a baseline for running tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable includes: input data, mock objects, loading database with known data etc… Unit Tests code written to test code under test designed to test specific sections of code percentage of lines of code tested is code coverage ideal coverage is in the 70-80% range should be unity and execute very fast should have no external dependencies no databases, no Spring Context etc… Integration tests designed to test behaviors between objects and parts of the overall system much larger scope can include Spring Context, database, and message brokers will run much slower than unit tests Functional tests typically means you are testing the running application application is live, likely deployed in a known environment functional touch points are tested i.e. using a web driver, calling web services, sending / receiving messages etc…(Selenium) TDD test driven development write tests first, which will fail, then code to fix the tests BDD behavior driven development builds on TDD and specifies that tests of any unit of software should be specified in terms of desired behavior of the unit often implemented with DSLs to create natural language tests JBehave, Cucumber, Spock Mock a fake implementation of a class used for testing, like a test double Spy a partial mock, allowing you to override select methods of a real class Testing goals generally, you will want the majority of your tests to be unit tests bringing up the Spring Context makes your tests exponentially slower try to test specific business logic in unit tests use Integration tests to test interactions think of a pyramid, base is unit tests, middle is integration tests, top is functional tests Test scope dependencies using spring-boot-starter-test JUnit - the default standard for unit testing Java applications Spring test and Spring boot Test - utilities and integration test support for Spring Boot applications AssertJ - a fluent assertion library Hamcrest - a library of matcher objects Mockito - a Java mocking framework JSONassert - an assertion library for JSON JSONPath - XPath for JSON JUnit Annotations Annotation Description @Test identifies a method as a test method @Before executed before each test, it is used to prepare the test environment (e.g. read input data, initialize the class) @After executed after each test, it is used to cleanup the test environment, it can also save memory by cleaning up expensive memory structures @BeforeClass executed once, before the start of all tests, methods marked with this annotation need to be defined as static to work with JUnit @AfterClass executed once, after all tests have been finished, methods annotated with this annotation need to be defined as static to work with JUnit @Ignore marks that the test should be ignored @Test(expected = Exception.class) fails if the method does not throw the named exception @Test(timeout = 10) fails if the method takes longer than 100 milliseconds Spring Boot Annotations Annotation Description @RunWith(SpringRunner.class) run test with Spring Context @SpringBootTest search for Spring boot application for configuration @TestConfiguration specify a Spring configuration for you test @MockBean injects Mockito mock @SpyBean injects Mockito Spy @JsonTest creates a Jackson or Gson object mapper via Spring boot @WebMvcTest used to test web context without a full http server @DataJpaTest used to test data layer with embedded database … ArgumentCaptor ArgumentCaptor allows us to capture an argument passed to a method in order to inspect it. This is especially useful when we can’t access the argument outside of the method we’d like to test. For example, consider an EmailService class with a send method that we’d like to test: 1234567891011121314151617181920public class EmailService &#123; private DeliveryPlatform platform; public EmailService(DeliveryPlatform platform) &#123; this.platform &#x3D; platform; &#125; public void send(String to, String subject, String body, boolean html) &#123; Format format &#x3D; Format.TEXT_ONLY; if (html) &#123; format &#x3D; Format.HTML; &#125; Email email &#x3D; new Email(to, subject, body); email.setFormat(format); platform.deliver(email); &#125; ...&#125; In EmailService.send, notice how platform.deliver takes a new Email as an argument. As part of our test, we’d like to check that the format field of the new Email is set to Format.HTML. In order to do this, we need to capture and inspect the argument that is passed to platform.deliver. Set up the unit test First, let’s create our unit test class: 1234567891011RunWith(MockitoJUnitRunner.class)public class EmailServiceUnitTest &#123; @Mock DeliveryPlatform platform; @InjectMocks EmailService emailService; ...&#125; We’re using the @Mock annotation to mock DeliveryPlatform, which is automatically injected into our EmailService with the @InjectMocks annotation. Add an ArgumentCaptor field Second, let’s add a new ArgumentCaptor field of type Email to store our captured argument: 12@CaptorArgumentCaptor&lt;Email&gt; emailCaptor; Capture the argument 1Mockito.verify(platform).deliver(emailCaptor.capture()); We can then get the captured value and store it as a new Email object: 1Email emailCaptorValue &#x3D; emailCaptor.getValue(); Inspect the captured value 123456789101112@Testpublic void whenDoesSupportHtml_expectHTMLEmailFormat() &#123; String to &#x3D; &quot;info@baeldung.com&quot;; String subject &#x3D; &quot;Using ArgumentCaptor&quot;; String body &#x3D; &quot;Hey, let&#39;use ArgumentCaptor&quot;; emailService.send(to, subject, body, true); Mockito.verify(platform).deliver(emailCaptor.capture()); Email value &#x3D; emailCaptor.getValue(); assertEquals(Format.HTML, value.getFormat());&#125; Exception Handling in Spring MVC HTTP status code HTTP 5XX server error HTTP 500 - internal server error generally, any unhandled exception other 500 errors are generally not used with Spring MVC HTTP 4XX client errors - generally checked exceptions 400 bad request - cannot process due to client error 401 unauthorized - authentication required 404 not found - resource not found 405 method not allowed, HTTP method not allowed @ResponseStatus allows you to annotate custom exception classes to indicate to the framework the HTTP status you want returned when that exception is thrown global to the application @ExceptionHandler works at the controller level allows you to define custom exception handling can be used with @ResponseStatus for just printing a http status can be used to return a specific view also can take total control and work with the Model and View Model cannot be a parameter of an ExceptionHandler method @HandlerExceptionResolver is an interface you can implement for custom exception handling used internally by Spring MVC note Model is not passed Docker What is Docker? Docker is a standard for Linux containers a Container is an isolated runtime inside of Linux a Container provides a private machine like space under Linux Containers will run under any modern Linux Kernel Containers can Have their own process space their own network interface run processes as Root (inside the container) have their own disk space can share with host too Container is not a VM Docker Terminology Docker Image - the representation of a Docker container, kind of like a JAR or WAR file in Java Docker Container - the standard runtime of Docker, effectively a deployed and running Docker image, like a Spring Boot Executable JAR Docker Engine - the code which manages Docker stuff, creates and runs Docker containers Notes about Docker Images and Containers Containers are like snapshots of Docker images Docker images are built by Docker files which contains multiple layers, each layer is a Command and will generate a file for the image. The layers will be re-created everytime when we run a new container for an image. e.g. when you docker run -d mongo, a new container for the image mongo will be created, and there is a layer for this container where you can store data in it. But when you docker stop &lt;Container_ID&gt; and re-start using docker run -d mongo, a new container with different Container ID will be created, so your old data in the previous container will not show in your new container. You can map a storage path to the container, so the current container and all future containers will save data into the directory you setup. Which will make the data persistence. Docker housekeeping Cleaning up after Docker with development use docker can leave behind a lot of files these files will grow and consume a lot of disk space this is less of an issue on production systems where containers aren’t being built and restarted all the time there are 3 key areas of house keeping containers images volumes MySQL MySQL features Stored procedures a piece of code inside of the database that execute against the database, runs locally on the database Triggers when something happens in the database e.g. insert a record, the trigger will run before or after that transaction Cursors point a place in a large set of data so you can scroll through it and look into the next record or previous record. Updated views a virtual table, stored inside the database Query cache database is going to remember in memory the result of your query, when you ask for that data again, it doesn’t have to go back to the file system to get the data. Subselects nested queries. ACID compliance atomicity - all or nothing consistency - transactions are valid to rules of the DB isolation - results of transactions are as if they are done end to end durability - once a transaction is committed, it remains so (DB will not losing data) RDBMS deployment architectures typically is driven by needs of scalability and availability can be done on a single non-dedicated server or many dedicated servers communication is typically over a network socket (MySQL: 3306) the client will need software called a driver to talk to the database over the network socket. MySQL data types a data type defines the data type of a column MySQL does support the standard ANSI SQL data types data types are broken down into the following categories numeric date and time string spatial (location, places) JSON MySQL installation options Native installation meaning install on your opearting system Running MySQL in a Container MySQL can also be run inside a technology called containers Docker is a highly popular container technology through Docker, you can run MySQL locally using pre built image from Docker hub Connecting to MySQL Local connection connecting to MySQL from the command line on the machine running MySQL to login to docker: docker exec -it yuan-mysql bash to connect to mysql server in Docker: mysql --user=root -p remote / client connection using some type of client software on the same machine running MySQL or connecting to the MySQL server from a different machine over the network Mongo DB About Mongo DB Mongo DB is a document oriented database developed in C++ MongoDB is a NoSQL database MongoDB documents are stored in BSON binary JSON Why use Mongo DB? MongoDB is great for high insert systems such as sensor readings, social media systems, advertising systems good when you need schema flexibility can also support a high number of reads per second Why avoid MongoDB? MongoDB has no concept of transactions No ACID no locking for transactional support, hence faster inserts not good for concurrent updates Reactive Manifesto Responsive the system responds in a timely manner responsiveness is the cornerstone of useability and utility responsiveness also means problems may be detected quickly and dealt with effectively responsive systems provide rapid and consistent response times consistent behavior simplifies error handling, builds end user confidence, and encourages further interaction Resilient system stays responsive in the face of failure resilience is achieved by replication, containment, isolation and delegation failures are contained within each component parts of the system can fail, without compromising the system as a whole recovery of each component is delegated to another high availability is ensured by replication where necessary Elastic the system stays responsive under varying workload reactive systems can react to changes in the input rate by increasing or decreasing resources allocated to service inputs reactive systems achieve elasticity ina cost effective way on commodity hardware and software platforms Message Driven reactive systems rely on asynchronous message passing to establish a boundary between components this ensures loose coupling, isolation, and location transparency message passing enables load management, elasticity, and flow control location transparent messaging makes management of failure possible non blocking communication allows recipients to only consume resources while active, leading to less system overhead. reactive programming with reactive systems reactive programming is a useful implementation technique reactive programming focuses on non-blocking, asynchronous execution - a key characteristic of reactive systems reactive programming is just one tool in building reactive systems Reactive Programming reactive programming is an asynchronous programming paradigm focused on streams of data reactive programs also maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself. Interactive programs work at their own pace and mostly deal with communication, while reactive programs only work in response to external demands and mostly deal with accurate interrupt handling, real time programs are usually reactive. Common use cases external service calls highly concurrent message consumers spreadsheets abstraction over asynchronous processing abstract whether or not your program is synchronous or asynchronous features opf reactive programming data streams asynchronous non-blocking backpressure failures as messages data streams data streams can be just about anything mouse clicks, or other user interactions JMS messages, RESTful service calls, Twitter feed, Stock trades, list of data from a database a stream is a sequence of events ordered in time events you want to listen to Asynchronous events are captured asynchronously a function is defined to execute when an event is emitted another function is defined if an error is emitted another function is defined when complete is emitted Observer pattern you have a subject and an observer when the subject is going to change, it will notify the observer Non-blocking the concept of using non blocking is important in blocking, the code will stop and wait for more dta (reading from disk, network etc…) non blocking in contrast, will process available data, ask to be notified when more is available, then continue back pressure the ability of the subscriber to throttle data failures as messages exceptions are not thrown in a traditional sense would break processing of stream exceptions are processed by a handler function Reactive Streams Spring Reactive Types two new reactive types are introduced with Spring framework 5 Mono is a publisher with zero or one elements in data stream Flux is a publisher with zero or MANY elements in the data stream both types implement the reactive streams publisher interface WebFlux web MVC webFlux @Controller, @RequestMapping Router functions spring-webmvc spring-webflux Servlet API HTTP / Reactive Streams Servlet Container Tomcat, Jetty, Netty, Undertow RESTful web services because of their simplicity and versatility, RESTful web services have become the de facto standard for web services REST - representational state transfer representational - typically JSON or XML state transfer - typically via HTTP terminology verbs - HTTP methods: GET, PUT, POST, DELETE messages - the payload of the action(JSON / XML) URI - uniform resource identifier a unique string identifying a resource URL - uniform resource locator a URI with network information Idempotence the property of certain operations in mathematics and computer science that they can be applied multiple times without changing the result beyond the initial application in other words, you can exercise the operation multiple times, without changing the result example: refreshing a web page (HTTP GET operation) Stateless - service does not maintain any client state HATEOAS - hypermedia as the engine of application state a REST client should then be able to use server-provided links dynamically to discover all the available actions and resources it needs, as access proceeds, the server responds with text that includes hyperlinks to other actions that are currently available GET use: to read data from resource read only idempotent safe operation - does not change state of resource PUT use: to insert or update idempotent - multiple PUT will not change result like saving a file multiple times not safe operation - does change state of resource POST use: to create new object non-idempotent - multiple POSTs is expected to create multiple objects not safe operation - does change state of resource only non-idempotent, non-safe HTTP verb DELETE use: to delete an object idempotent - multiple DELETEs will have same effect / behavior not safe operation, does change the state of resource MapStruct MapStruct is a code generator for Java bean mapping helps reduce coding for type conversions when dealing with Rest services, a common use case is to expose API data using DTOs (Data Transfer Object) as project grows bigger, it is not good to expose POJO directly to Rest services. MapStruct is helping us to convert POJO to DTOs, then export DTOs to Rest service to be consumed by public Content Negotiation Content Negotiating view resolver used by Spring MVC to determine view handler to use auto configured by Spring boot the content negotiating view resolver will determine the view to use to render the data of the model to the client Content type view to use is determined by Content Type in HTTP header application/json, application/xml, text/html if view for requested Content type is not found, HTTP status 406 not acceptable is returned. Helper Library and Classes Structure the project should follow a structure from database to frontend: Database -&gt; Repository -&gt; Service -&gt; Controller -&gt; View JPA H2 in memory database Thymeleaf frontend template engine, model can be added to view dynamically Dependency Injection Repositories and Services can be injected when needed, use Annotations @Service, @Component classes marked as @Component, @Service and @Controller will be managed by Spring Application Context, it will inject necessary class to the right place when needed. (Beans) Configuration config files can be Java, YAML, or XML application.properties can define project profiles JPA entity relationships One to Many Many to One Many to Many (needs mapping table) JPA Query methods e.g. findByDescription this is done by the library, findBy + property name library will do the implementations for you Project Lombok will do the constructors, getters, setters and even builder patterns for you JUnit Unit Test framework Mockito for unit tests, we do not want to bring in Spring Application Context so to manage Repositories and Services, we need Mockito to create some Mock component for us when(), thenReturn(), verify(), ArgumentCaptor, MockMvc… WebJars bring in bootstrap library Web Data Binder binds HTTP variables to Java object specifically handling form posts and binding form variables to Java data objects whereas Rest service we could use RestTemplate Validations @NotBlank, @NotNull, @Max, @Min… annotations to data models, for form validations Exceptions @ResponseStatus @ControllerAdvice dealing with exceptions and custom error messages Reactive Programming Mono, Flux WebFlux Reactive vs Servlet RestTemplate bind Rest Response to Java objects WebClient performing web request MapStruct auto mapper","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://hellcy.github.io/tags/Spring/"}]},{"title":"git advanced","slug":"git-advanced","date":"2021-10-14T12:04:12.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/10/14/git-advanced/","link":"","permalink":"http://hellcy.github.io/2021/10/14/git-advanced/","excerpt":"","text":"HEAD and detached HEAD HEAD when you checkout to a branch, git is pointing to the latest commit in that branch, which is the HEAD of that branch Detached HEAD you can also checkout to a specific commit in any branch, when you do that, that commit becomes a detached HEAD, it doesn’t belong to any branch anymore. if you made some changes in the detached HEAD and commit it, it is better to create a new branch (which contains all the changes you made), before switching back to other branches, then you could merge the new branch into the master branch. Otherwise if you don’t create a new branch, the changes you made in detached mode will be lost. Undo changes git restore you could use git restore &lt;file-name&gt; or git restore . to revert all unstaged changes git clean you could use git clean -d to delete new files (unstaged) Undo staged changes git restore --staged &lt;file-name&gt; this will copy the latest commited file to the staging area, which basically means it will revert all changes that currently are in the staging area now Deleting commits soft git reset --soft HEAD~1 commit will be deleted, changes will still be in the staging area mixed (default) git reset HEAD~1 commit will be deleted, changes will be removed from the staging area, but the changes will still be in the working directory hard git reset --hard HEAD~1 commit will be deleted, all changes will be removed from the staging area, the all changes will be removed from the working directory too Stash add current changes to Stash with a comment git stash push -m &quot;message&quot; see the list of stashed changes git stash list add changes back to unstaged state and remove from Stash git stash pop &lt;index&gt; add changes back to unstaged state but also keep it at Stash git stash apply &lt;index&gt; remove a particular stash from the list git stash drop &lt;index&gt; clear the entire stash list git stash clear Reflog allows us to bring back lost information, it could be commits or branches use git reflog you will see a list of commits for the last 30 days, even the commits you deleted, which can’t be found in git log. Then you could use git reset --hard &lt;commit hash code&gt; to retrieve lost commits Merge fast forward can only be used when no additional commit in master (after feature branch was created) MERGE moves HEAD forward to the latest commit but does not create new commit fast forward with squash git merge --squash &lt;branch name&gt; all commits you made in the feature branch will be combined into one new commit when you MERGE it to the master branch more specifically, when you use --squash, all the changes you made across all commits will be move to the staging area in the master branch. recursive additional commits in both master and feature branch after feature branch was created additional commit is created in master branch when MERGE NOTE: when you use recursive MERGE, all commits either originated from the feature branch or the master branch, will be showing in the log history. But to revert the commit, you only need to revert for 1 step: git reset --hard HEAD~1 if you don’t want to see the commits from the feature branch to be display in your master git log history, you could use --squash with recursive MERGE rebase when you have new commits for both master branch and feature branch (after you created the feature branch), you could use rebase to let the latest commit in master branch becomes the new base commit for commits created in feature branch after rebase in the feature branch, all commits you made in the feature branch will have new commit code, even though the changes for these commits are the same. These might raise issues when working with others outside the repo because you will have different commit history when to use rebase? you could use rebase when there is new commits in the master branch why use rebase? feature branch relies on additional commits in the master branch, then you could rebase the master into feature branch feature branch is finished, you want to merge it into master without creating new merge commit you could first rebase master into feature branch then you could use fast-forward merge because now your master branch don’t have any new commits to your feature branch (because you rebased) cherry-pick add a specific commit from feature branch to your master branch a new commit ID will be created, even though the commit content is the same useful when you want just a commit to be in your master branch, but you don’t want to merge the entire feature branch Github deleteing commits in github we could use git reset --hard HEAD~1 to delete local commits but when we use git push to push to remote git repo, it will fail because our local branch is behind the remote branch we could use git push --force origin master to force push changes to remote repo, in this case the remote commits will be deleted.","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://hellcy.github.io/tags/git/"}]},{"title":"Wishlist 2022","slug":"Wishlist-2022","date":"2021-10-14T03:22:02.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/10/14/Wishlist-2022/","link":"","permalink":"http://hellcy.github.io/2021/10/14/Wishlist-2022/","excerpt":"","text":"Get AWS SAP Certificate Learn Spring Boot Join a Hackathon project Create a personal project using AWS Find a new job by the end of 2022 - Done","categories":[],"tags":[{"name":"Wishlist","slug":"Wishlist","permalink":"http://hellcy.github.io/tags/Wishlist/"}]},{"title":"AWS DVA review","slug":"AWS-DVA-review","date":"2021-09-11T16:06:00.000Z","updated":"2022-02-10T14:39:24.299Z","comments":true,"path":"2021/09/12/AWS-DVA-review/","link":"","permalink":"http://hellcy.github.io/2021/09/12/AWS-DVA-review/","excerpt":"","text":"I PASSED! View My Certificate ELB + ASG Why use a load balancer spread load across multiple downstream instances expose a single point of access (DNS) to your application seamlessly handle faliures of downstream instances do regular health checks to your instances provide SSL termination (HTTPS) for your websites enforce stickness with cookies high availability across zones separate public traffic from private traffic ELB integrated with many AWS services EC2, ASG, ECS ACM, CloudWatch Route 53, WAF, Global Accelerator ALB Layer 7 (HTTP) load balancing to multiple HTTP applications across machines (target groups) load balancing to multiple applications on the same machine (containers) support for HTTP and WebSockets support redirects (from HTTP to HTTPS) routing tables to different target groups based on path in URL based on hostname in URL routing based on query string, headers ALB are a great fit for micro services and container based application has a port mapping feature to redirect to a dynamic port in ECS in comparison, we need multiple CLB per application the application doesn’t see the client IP directly Target groups EC2 instances ECS tasks lambda functions (HTTP request is translated into a JSON event) IP addresses - must be private IPs ALB can route to multiple target groups, each target group could have multiple instances health checks are at the target group level you can set rules to decide which target groups to redirect the traffic NLB layer 4 (TCP and UDP) handle millions of request per second, lower latency NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Sticky Sessions it is possible to implement stickness so that the same client is always redirected to the same instance behind a load balancer this works for CLB and ALB the cookie used for stickness has an expiration date you control use case: make sure the user doesn’t lose his session data enabling stickness may bring imbalance to the load over the backend EC2 instnaces SSL - server name indication SNL solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) it is a newer protocol, and requires the client to indicate the hostname and the target server in the initial SSL handshake the server will then find the correct certificate, or return the default one only works for ALB and NLB, CloudFront Connection Draining (De-registration delay) time to complete the in-flight requests while the instance is de-registering or unhealthy stops sending new requests to the EC2 instances which is de-registering between 1 to 3600 seconds (default 300 seconds) can be disabled (set value to 0) set to a low value if your requests are short instances will be terminated after the draining time is over X-Forwarded-For and X-Forwarded-Proto X-Forwarded-For The X-Forwarded-For (XFF) header is a de-facto standard header for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or a load balancer. When traffic is intercepted between clients and servers, server access logs contain the IP address of the proxy or load balancer only. To see the original IP address of the client, the X-Forwarded-For request header is used. X-Forwarded-Proto The X-Forwarded-Proto (XFP) header is a de-facto standard header for identifying the protocol (HTTP or HTTPS) that a client used to connect to your proxy or load balancer. Your server access logs contain the protocol used between the server and the load balancer, but not the protocol used between the client and the load balancer. To determine the protocol used between the client and the load balancer, the X-Forwarded-Proto request header can be used. ASG A launch configuration (launch template is the newer version) AMI + instance type EC2 user data EBS volume security groups SSH key pair min size, max size, initial capacity network + subnets information load balancer information (so ASG knows which target group to launch the instnace), ASG and ELB can be linked scaling policies it is possible to scale an ASG based on CloudWatch alarms an alarm monitors a metric (such as average CPU) metrics are computed for the overall ASG instnaces to update an ASG, you must provide a new launch configuration or new launch template IAM roles attached to an ASG will get assigned to EC2 instances launched ASG is free, you need to pay for the underlying resources launched Scaling Policies Target tracking scaling most simple and easy to setup example: I want the average ASG CPU to stay at around 40% Simple / Step scaling when a CloudWatch alarm is triggered, then add 2 units when a CloudWatch alarm is triggered, then remove 1 unit the difference between simple and step scaling policies are: for step policy, you can create step adjustments, and ASG will change the number of instances based on the size of the alarm breach. scheduled actions anticipate a scaling based on known usage patterns example: increase the min capacity to 10 at 5pm on Fridays predictive scaling continuously forecast load and schedule scaling ahead Good metrics to scale on CPU utilization request count per target average network in / out Scaling cooldowns after a scaling policy happens, you are in the cooldown period (default is 300 seconds) during the cooldown period, the ASG will not launch or terminate additional instance (to allow for metrics to stablize) advice: use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period RDS managed DB service for DB use SQL as a language it allows you to create databases in the cloud that are managed by AWS Postgres MySQL MariaDB Oracle SQL server Aurora RDS is a managed service automated provisioning, OS patching continuous backups and restore to specific timestamp (point in time restore) monitoring dashboards read replicas for improved read performance Multi AZ setup for DR maintenance windows for upgrades scaling capability storage backup by EBS RDS DB will be launched in a VPC in an AZ you can’t SSH into your instances RDS backups Automated backups daily full backup of the database transaction logs are backuped by RDS every 5 mins ability to restore to any point in time (from oldest to 5 mins ago) 7 days retention (can be increased to 35 days) DB snapshots manually triggered by the user retention of backup for as long as you want RDS storage auto scaling helps you increase storage on your RDS DB instnace dynamically when RDS detects you are running out of free database storage, it scales automatically avoid manually scaling your database storage you have to set Maximum storage threshold automatically modify storage if free storage is less than 10% of allocated storage low storage lasts at least 5 mins 6 hours have passed since last modification useful for applications with unpredictable workloads RDS read replicas for read scalability up to 5 read replicas within AZ, cross AZ or cross region replication is async, so reads are eventually consistent replicas can be promoted to their own DB applications must update the connection string to leverage read replicas RDS read replicas - use case you have a production database, that is taking on normal load you want to run a reporting application to run some analytics you create a read replica to run the new workload there the production application is unaffected read replicas are used for SELECT only kind of statements RDS read replicas - network cost in AWS there is a network cost when data goes from one AZ to another for RDS read replicas, within the same region, you don’t pay that fee, but you do need to pay if data goes to another region RDS multi AZ (DR) SYNC replication one DNS name - automatic app failover to standby increase availability failover in case of loss of AZ, loss of network, instance or storage failure no manual intervention in apps not used for scaling (standby instance can’t be read or write) NOTE: the read replicas can be setup as Multi AZ for DR RDS from single AZ to Multi AZ zero downtime operation (no need to stop the DB) just click on modify for the database the following happens internally a snapshot is taken a new DB is restored from the snapshot in a new AZ synchronization is established between the two databases RDS security - encryption at rest encryption possibility to encrypt the master and read replicas with AWS KMS - AES-256 encryption encryption has to be defined at launch time if the master is not encrypted, the read replicas cannot be encrypted in flight encryption SSL certificates to encrypt data to RDS in flight provide SSL options with trust certificate when connecting to database RDS encryption operations encrypting RDS backups snapshots of unencrypted RDS databases are un-encrypted snapshots of encrypted RDS databases are encrypted can copy a snapshot into an encrypted one to encrypt an un-encrypted RDS database create a snapshot of the un-encrypted database copy the snapshot and enable encryption for the snapshot restore the database from the encrypted snapshot migrate applications to the new database, and delete the old database RDS security - Network and IAM network security RDS databases are usually deployed within a private subnet, not in a public one RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS Access management IAM policies help control who can manage AWS RDS traditional username and password can be used to login into the database IAM based authentication can be used to login into RDS MySQL and postgreSQL Aurora Aurora is a proprietary technology from AWS postgres and MySQL are both supported as Aurora DB Aurora is AWS cloud optimized and claims 5x performance improvement over MySQL on RDS, over 3x performance of Postgres on RDS Aurora storage automatically grows in increments of 10GB, up to 64TB Aurora can have 15 replicas while MySQL has 5, and the replication process is faster failover in Aurora is instantaneous, it is HA native user to connect to write endpoint or read endpoint, these endpoints will redirect traffic to the correct instances. Security is the same as RDS Aurora has 4 features one writer, multiple reader one writer, multiple readers - parallel query multiple writers severless ElastiCache the same way RDS is to get managed relational databases ElatiCache is to get managed Redis and Memcachced Caches are in memory databases with really high performance, low latency helps reduce load off of databases for read intensive workloads helps make your application stateless AWS takes care of OS maintenance and patching, optimizations, setup, configuration, monitoring, failure recovery and backups using ElastiCache involves heavy application code changes DB cache Application queries ElastiCache, if not avaialble, get from RDS and store in ElastiCache helps relieve load in RDS cache must have an invalidation strategy to make sure only the most current data is used in there User session store user logs into any of the application the application writes the session data into ElastiCache the user hits another instance of our application the instance retrieves the data and the user is already logged in Redis vs Memcached Redis Memcached Multi AZ with auto failover multi node for partitioning of data (sharding) read replicas to scale reads and have HA no HA data durability using AOF persistence non persistent backup and restore features no back and restore - Multi threaded architecture Redis Auth if you enable encryption in transit, you can enable Redis Auth, you need to setup a token for your application to connect to Redis. Caching implementation considerations is it safe to cache data data may be out of data, eventaully consistent is caching effective for that data pattern: data changing slowly, few keys are frequently needed, good to use caching anti pattern: data changing rapidly, all large keys space frequently needed, not good to use caching is data structured well for caching? key value caching, or caching of aggregations results? caching is good for well structured data lazy loading / cache aside / lazy population 1234567891011121314# pythondef get_user(user_id): &#x2F;&#x2F; check the cache record &#x3D; cache.get(user_id) if record is None: &#x2F;&#x2F; run a DB query record &#x3D; db.query(&quot;select * from users where id &#x3D; ?&quot;, user_id) &#x2F;&#x2F; populate the cache cache.set(user_id, record) return record else: return record write through - add or update cache when database is updated when there is a write call write to DB write to cache pros data in cache is never stale, reads are quick wirte penalty vs read penalty (each write requires 2 calls) cons missing data until it is added/ updated in the DB, mitigation is to implement lazy loading strategy as well, combine 2 strategies together cache churn - a lot of the data will never be read 1234567891011# pythondef save_user(user_id, values): # save to DB record &#x3D; db.query(&quot;update users ... where id &#x3D; ?&quot;, user_id, values) # push into cache cache.set(user_id, record) return record cache evictions and TTL (time to live) cache eviction can occur in 3 ways you delete item explicity in the cache item is evicted because the memory is full and it is not recently used (LRU) you set an item TTL TTL are helpful for any kind of data leaderboard comments activity streams TTL can range from few seconds to hours or days if too many evictions happen due to memory, you should scale up or out Final words lazy loading is easy to implement and works for many situations as a foundation, especially on the read side write through is usually combined with lazy loading as targeted for the queries or workloads that benefit from this optimization setting a TTL is usually not a bad idea, except when you are using write through, set it to a sensible value for your application only cache data that makes sense ElastiCache replication - cluster mode disabled one primary node, up to 5 replicas asynchronous replication the primary node is used for read and write the other nodes are read only we have only one shard, and all nodes are in the shard, each node has all the data guard against data loss if node failure multi AZ enabled by default for failover helpful to scale read performance ElastiCache replication - cluster mode enabled data is partitioned across shards (helpful to scale writes) each shard has a primary and up to 5 replica nodes, each shard has part of the data multi AZ capability up to 500 nodes per cluster Route 53 DNS Domain Name System which translates the human friendly hostnames into the machine IP addresses Route 53 A highly available, scalable, fully managed and authoritive DNS route 53 is also a Domain Registrar ability to check the health of your resources the only AWS service which provides 100% availability SLA Hosted zones a container for records that define how to route traffic to a domain and its subdomains public hosted zones contains records that specify how to route traffic on the internet (public domain names) private hosted zones contain records that specify how you route traffic within one or more VPCs (private domain names) CNAME vs alias CNAME points a hostname to any other hostname only for non root domain Alias points a hostname to an AWS resource works for root domain and non root domain free of charge native health check automatically recognizes changes in the resource’s IP addresses Alias targets ELB CloudFront distributions API gateway Elastic Beanstalk environments S3 websites VPC interface endpoints Global accelerator route 53 record in the same hosted zone You cannot set an Alias record for an EC2 DNS name Route 53 - routing policies define how route 53 responds to DNS queries route 53 supports the following routing policies simple weighted failover latency based geolocation multi value answer geoproximity (using route 53 traffic flow feature) Simple typically, route traffic to a single resource can specify multiple values in the same record if multiple values are returned, a random one is chosen by the client when Alias enabled, sepecify only one AWS resource can’t be associated with health checks Weighted control the percentage of the requests that go to each specific resource assign each record a relative weight DNS records must have the same name and type use cases: load balancing between regions, testing new application versions… assign a weight of 0 to a record to stop sending traffic to a resource if all records have weight of 0, then all records will be returned equally latency redirect to the resource that has the least latency close to us super helpful when latency for users is a priority latency is based on traffic between users and AWS regions Germany users may be directed to the US can be associated with health checks (has a failover capability) health checks HTTP health checks are only for public resources health check =&gt; automated DNS failover health checks that monitor an endpoint health checks that monitor other health checks (calculated health checks) health checks that monitor cloudwatch alarms health checks are integrated with CW metrics monitor an endpoint about 15 global health checks will check the endpoint health healthy / unhealthy threshold = 3 interval - 30 seconds supported protocol: HTTP, HTTPS, TCP if &gt; 18% of health checks report the endpoint is healthy, route 53 consider it is healthy, otherwise, it is unhealthy ability to choose which locations you want route 53 to use health checks pass only when the endpoint responds with the 2xx and 3xx status codes health checks can be setup to pass or fail based on the text in the first 5120 bytes of the response your ELB must allow the incoming requests from the route 53 health checkers IP address range calculated health checks combine the results of multiple health checks into a single health check you can use OR, AND, or NOT can monitor up to 256 child health checks specify how many of the health checks need to pass to make the parent pass usage: perform maintenance to your website without causing all health checks to fail private hosted zones route 53 health checks are outside the VPC they can’t access private endpoint you can create a CloudWatch metric and associate a CloudWatch alarm, then create a health check that checks the alarm itself, if the CloudWatch alarm status becomes ALARM, the health checker will become to unhealthy failover create two records associate with 2 resources primary and secondary records primary record must associated with a health checker if the primary record is unhealthy, DNS will return IP address of the secondary resource Geolocation different from latency based this routing is based on user location specify location by Continent, Country, or by US state should create a default record (in case there is no match on location) use cases: website localization, restrict content distribution, load balancing… can be associated with health checks Geoproximity route traffic to your resources based on the geographic location of users and resources ability to shift more to resources based on the defined bias to change the size of the geographic region, specify bias values to expand (1 to 99), more traffic to the resource to shrink (-1 to -99), less traffic to the resource resource can be AWS resources (AWS region) non AWS resources (latitude and longitude) you must use route 53 traffic flow (advanced) to use this feature Traffic flow simplify the process of creating and maintaing records in large and complex configurations visual editor to manage complex routing decision trees configurations can be saved as traffic flow policy can be applied to different route 53 hosted zones supports versioning multi value use when routing traffic to multiple resources route 53 return multiple values / resources can be associated with health checks (return only values for healthy resources) up to 8 healthy records are returned for each multi value query multi value is not a substitude for having an ELB (it is more like a client side load balancing) VPC VPC: private network to deploy your resource subnet: allow you to partition your network inside your VPC (AZ resource) a public subnet is a subnet that is accessible from the internet a private subnet is a subnet that is not accessible from the internet to define access to the internet and between subnets, we use route tables Internet gateway and NAT gateways internet gateways helps our VPC instances connect with the internet public subnets have a route to the internet gateway NAT gateways (AWS managed) and NAT instances (self managed) allow your instances in your private subnets to access the internet while remaining private NACL and security groups NACL (network ACL) a firewall which controls traffic from and to subnet can have ALLOW and DENY rules are attached at the subnet level rules only include IP addresses security groups a firewall that controls traffic to and from an ENI / an EC2 instance can have only ALLOW rules rules include IP addresses and other security groups Security group Network ACL operates at the instance level operates at the subnet level supports allow rules only supports allow rules and deny rules is stateful: return traffic is automatically allowed, regardless of any rules is stateless: return traffic must be explicitly allowed by rules we evaluate all rules before deciding whether to allow traffic we process rules in number order when deciding whether to allow traffic applies to an instance only if someone specifies the security group when launching the instance, or associate the security group with the instance later on automatically applies to all instances in the subnets it’s associated with (therefore, you don’t have to rely on users to specify the security group) VPC Flow logs capture information about IP traffic going into your interfaces VPC flow logs subnet flow logs ENI (elastic network interface) flow logs helps to monitor and troubleshoot connectivity issues subnets to internet subnets to subnets internet to subnets captures network information from AWS managed interfaces too: Elastic load balancers, ElastiCache, RDS, Aurora, etc… VPC flow logs data can go to S3 / CloudWatch logs VPC Peering connect two VPCs, privately using AWS network make them behave as if they were in the same network must not have overlapping CIDR VPC peering connection is not transitive (must be established for each VPC that need to communicate with one another) VPC endpoints endpoints allow you to connect to AWS services using a private network instead of the public www network this gives you enhanced security and lower latency to access AWS services VPC endpoint gateway: S3 and DynamoDB VPC endpoint interface: the rest AWS rervices only used within your VPC Site to site VPN and Direct connect site to site VPN connect an on premises VPN to AWS the connection is automatically encrypted goes over the public internet direct connect establish a physical connection between on premises and AWS the connection is private, secure, and fast goes over a private network takes at least a month to establish NOTE: site to site VPN and direct connect cannot access VPC endpoints S3 buckets S3 allows people to store objects in buckets buckets must have a globally unique name buckets are defined at the region level objects objects have a key the key is the FULL path the key is composed of prefix + object name there is no concept of directories within buckets just keys with very long names that contain slashes object values are the content of the body max object size is 5TB if uploading more than 5GB, must use multi-part upload metadata (list of text key / value pairs - system or user metadata) tags (unicode key / value pair, up to 10) - useful for security / lifecycle version ID (if versioning is enabled) versioning you can version your files in S3 it is enabled at the bucket level same key overwrite will increment the version: 1,2,3… it is best practice to version your buckets protect against unintended deletes easy roll back to previous version note: any file that is not versioned prior to enabling versioning will have version null suspending versioning does not delete the previous versions Encryption for objects SSE-S3 encryption using keys handled and managed by S3 object is encrypted server side AES-256 encryption type SSE-KMS encryption using keys handled and managed by KMS KMS advantages: user control + audit trail object is encrypted server side SSE-C server side encryption using data keys fully managed by the customer outside of AWS S3 does not store the encryption key you provide HTTPS must be used, because you need to send the encryption key in the header encryption key must be provided in HTTP headers, for every HTTP request made client side encryption client library such as the Amazon S3 encryption client clients must encrypt data themselves before sending to S3 clients must decrypt the data themselves when retrieving from S3 customer fully manages the keys and encryption cycle Encryption in transit (SSL/TLS) Amazon S3 exposes HTTP endpoint: non encrypted HTTPS endpoint: encryption in flight you are free to use the endpoint you want, but HTTPS is recommended most clients would use the HTTPS endpoint by default HTTPS is mandatory for SSE-C security user based IAM policies - which API calls should be allowed for a specific user from IAM console resource based bucket policies - bucket wide rules from the S3 console - allows cross account object ACL - finer grain bucket ACL - less common NOTE: an IAM principal can access an S3 object if the user IAM permissions allow it OR the resource policy alloow it AND there is no explicit DENY bucket settings for block public access block public access to buckets and objects granted through new access control lists any access control lists new public bucket or access point policies block public and cross account access to buckets and objects through any public bucket or access point policies these settings were created to prevent company data leaks if you know your bucket should never be public, leave these on can be set at the account level others networking supports VPC endpoints (for instances in VPC without www internet) logging and audit S3 access logs can be stored in other S3 buckets API calls can be logged in AWS cloudtrail user security MFA delete: MFA can be required in versioned buckets to delete objects pre-signed URLs: URLs that are valid for a limited time (premium videos service for logged in users) CORS an origin is a scheme, host, and port CORS means cross origin resource sharing web browser based mechanism to allow requests to other origins while visiting the main origin the requests won’t be fulfilled unless the other origin allows for the requests, using CORS headers if a client does a cross origin request on our S3 bucket, we need to enable the correct CORS headers you can allow for a specific origin or for * (for all origins) consistency model strong consistency as of Dec 2020 AWS CLI, SDK, IAM Roles and policies AWS CLI Dry run tells you if your command would have succeed or not without actually executing it AWS CLI STS decode erros decode API error messgaes using the STS command line AWS EC2 instance metadata it allows AWS EC2 instance to learn about themselves without using an IAM role for that purpose the URL is http://169.254.169.254/latest/meta-data/ you can retrieve the IAM role name from the metadata, but you cannot retrieve the IAM policy metadata = info about the EC2 instance user data = launch script of the EC2 instance MFA with CLI to use MFA with the CLI, you must create a temporary session to do so, you must run the STS GetSessionToken API call AWS SDK what if you want to perform actions on AWS directly from your applications code? you can use an SDK we have to use the AWS SDK when coding against AWS services such as DynamoDB if you don’t specify or configure a default region, then us-east-1 will be chosen by default AWS limit API rate limits DescribeInstances API for EC2 has a limit of 100 calls per seconds GetObject on S3 has a limit of 5500 GET per second per prefix for intermittent errors: implement exponential backoff for consistent errors: request an API throttling limit increase service quotas running on-demand standard instances: 1152 vCPU you can request a service limit increase by opening a ticket you can request a service quota increase by using the service quotas API Exponential Backoff if you get ThrottlingException intermittently, use exponential backoff retry mechanism already included in AWS SDK API calls must implement yourself if using the AWS API as-is or in specific cases must only implement the retries on 5xx server errors and throttling do not implement on the 4xx client errors AWS CLI credentials provider chain the CLI will look for credentials in this order command line options environment variables CLI credentials file CLI configuration file container credentials instance profile credentials AWS SDK default credentials provider chain the java SDK will look for credentials in this order java system properties environment variables the default credential profiles file Amazon ECS container credentials instance profile credentials Credentials Scenario an application deployed on an EC2 instance is using environment variables with credentials from an IAM user to call the Amazon S3 API The IAM user has S3FullAccess permissions the application only uses one S3 bucket, so according to best practices an IAM role and EC2 instance profile was created for the EC2 instance the role was assigned the minimum permissions to access that one S3 bucket the IAM instance profile was assigned to the EC2 instance, but it still had access to all S3 buckets, why? the credentials provider chain is still giving priorities to the environment variables credentials best practice never store AWS credentials in your code best practice is for credentials to be inherited from the credentials chain if working within AWS, use IAM roles EC2 instance roles for EC2 instances ECS roles for ECS tasks lambda roles for lambda functions if working outside AWS, use environment variables / named profiles signing AWS API requests when you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key and secret key) note: some requests to Amazon S3 don’t need to be signed if you use the SDK or CLI, the HTTP requests are signed for you you should sign an AWS HTTP request using Signature v4 (SigV4) sigV4 options HTTP header query string in URL S3 and Athena Advanced S3 MFA delete MFA forces user to generate a code on a device before doing important operations on S3 to use MFA delete, we need to enable versioning on the S3 bucket you will need MFA to permanently delete an object version suspend versioning on the bucket you won’t need MFA for enabling versioning listing deleted versions only the bucket owner can enable / disable MFA delete MFA delete can only be enabled using the CLI S3 default encryption vs bucket policies one way to force encryption is to use a bucket policy and refuse any API call to PUT an S3 object without encryption headers another way is to use the default encryption option in S3 note: bucket policies are evaluated before default encryption S3 access logs for audit purpose, you may want to log all access to S3 buckets any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket that data can be analyzed using data analysis tools… or Amazon Athena do not set your logging bucket to be the monitored bucket it will create a logging loop, and your bucket will grow in size exponentially S3 replication (CRR or SRR) must enable versioning in source and destination cross region replication - CRR same region replication - SRR buckets can be in different accounts copying is asynchronous must give proper IAM permissions to S3 CRR use cases: compliance, lower latency access for users in another region, replicatioin across accounts SRR use cases: log aggregation, live replication between production and test accounts after activating, only new objects are replicated (not retroactive), existing objects will not be replicated for DELETE operations can replicate delete markers from source to target (optional setting) deletions with a version ID are not replicated (to avoid malicious deletes), it means if you delete an object using its version ID, this operation will not be replicated there is no chaining of replication if bucket 1 has replication into bucket 2, which has replication into bucket 3 then objects created in bucket 1 are not replicated to bucket 3 S3 pre signed URLs can generate per signed URLs using SDK or CLI for downloads (easy, can use the CLI) for uploads (harder, must use the SDK) valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument users given a pre signed URL inherit the permissions of the person who generated the URL for GET / PUT examples allow only logged in users to download a permium video on your S3 buckets allow an ever changing list of users to download files by generating URLs dynamically allow temporarily a user to upload a file to a precise location in our bucket Amazon Glacier and Glacier Deep Archive Amazon Glacier - 3 retrival options expedited - 1 to 5 mins standard - 3 to 5 hours bulk - 5 to 12 hours minimum storage duration of 90 days Amazon Glacier Deep Archive - for long term storage - cheaper standard - 12 hours bulk - 48 hours minimum storage duration of 180 days S3 lifecycle rules transition actions: it defines when objects are transitioned to another storage class move objects to standard IA class 60 days after creation move to Glacier for archiving after 6 months expiration actions: configure objects to expire (delete) after some time access log files can be set to delete after a year can be used to delete old versions of files (if versioning is enabled) can be used to delete incomplete multi part uploads rules can be created for a certain prefix rules can be created for certain object tags S3 performance Amazon S3 automatically scales to high request rates, latency 100-200ms your application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix in a bucket there are no limits to the number of prefixes (prefix is a folder in S3) in a bucket KMS limitation if you SSE-KMS, S3 performance may be impacted by the KMS limits when you upload, it calls the GenerateDataKey KMS API when you download it, it calls the Decrypt KMS API count towards the KMS quota per second (5500, 10000, 30000 based on region) you can request a quota increase using the service quotas console multi part upload recommended for files &gt; 100MB, must be used for files &gt; 5GB can help parallelize uploads (speed up transfers) S3 transfer acceleration increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region compatible with multi part upload S3 byte range fetches parallelize GETs by requesting specific byte ranges better resilience in case of failures can be used to speed up downloads can be used to retrieve only partial data (for example the head of a file) S3 select and Glacier select retrieve less data using SQL by performing servide side filtering can filter by rows and columns (complex query not supported) less network transfer, less CPU cost client side S3 event notifications use case: generate thumbnails of images uploaded to S3 can create as many S3 events as desired S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer it two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent if you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket compared to CloudWatch event or EventBridge, S3 event notifications have lower latency and lower costs, it works better with S3 AWS Athena serverless service to perform analytics directly against S3 files uses SQL language to query the files has a JDBC / ODBC driver charged per query and amount of data scanned supports CSV, JSON, ORC, Avro and Parquet (built on Presto) use cases: business intelligence, analytics, reporting, analyze and query, VPC flow logs, ELB logs, CloudTrail trails, etc… exam tip: analyze data directly on S3 =&gt; use Athena AWS CloudFront CDN improves read performance, content is cached at the edge 216 point of presence globally (edge locations) DDoS protection, integration, with Shield, AWS WAF can expose external HTTPS and can talk to internal HTTPS backends Origins S3 bucket for distributing files and caching them at the edge enhanced security with CloudFront OAI CloudFront can be used as ingress (to upload files to S3) Custom Origin (HTTP) ALB EC2 instance S3 website (must first enable the bucket as a static S3 website) any HTTP backend you want CloudFront Geo Restriction you can restrict who can access your distribution whitelist: allow your users to access your content only if they are in one of the countries on a list of approved countries blacklist: prevent your users from accessing your content if they are in one of the countries on a blacklist of banned countries the country is determined using a third party geo IP database use case: copyright laws to control access to content CloudFront vs S3 CRR CloudFront global edge network files are cached for a TTL great for static content that must be available everywhere S3 CRR must be setup for each region you want to replication to happen files are updated in near real time read only great for dynamic content that needs to be available at low latency in few regions CloudFront caching cache based on headers session cookies query string parameters the cache lives at each CloudFront edge location you want to maximize the cache hit rate to minimize request on the origin control the TTL, can be set by the origin usign the cache-control header, expires header… you can invalidate part of the cache using the CreateInvalidation API CloudFront signed URL / signed cookies you want to distrbute paid shared content to premium users over the world we can use CloudFront signed URL / signed cookie, we attach a policy with includes URL expiration includes IP ranges to access the data from trusted signers (which AWS accounts can create signed URLs) how long should the URL be valid for shared content (movie, music): make it short (a few minutes) private content (private to the user): you can make it for years signed URL: access to individual files (one signed URL per file) signed cookies: access to multiple files (one signed cookie for many files) CloudFront signed URL vs S3 pre signed URL Signed URL allow access to a path, no matter the origin account wide key pair, only the root can manage it can filter by IP, path, date, expiration can leverage caching features S3 pre signed URL issue a request as the person who pre signed the URL uses the IAM key of the signing IAM principal (has the same access as the IAM user who create the URL) limited lifetime CloudFront signed URL process two types of signers either a trusted key group (recommended) can leverage APIs to create and rotate keys (and IAM for API security) an AWS account that contains a CloudFront key pair need to manage keys using the root account and the AWS console not recommended because you shouldn’t use the root account for this in your CloudFront distribution, create one or more trusted key groups you generate your own public / private key the private key is used by your applications to sign URLs the public key is used by cloudfront to verify URLs Price classes you can reduce the number of edge locations for cost reduction three price classes price class All: all regions - best performance price class 200: most regions, but excludes the most expensive regions price class 100: only the least expensive regions Multiple origin to route to different kind of origins based on the content type based on path pattern /images/* /api/* /* origin groups to increase high availability and do failover origin group: one primary and one secondary origin if the primary origin fails, the second one is used (works for both EC2 instance and S3 buckets) field level encryption protect user sensitive information throught application stack adds an additional layer of security along with HTTPS sensitive information encrypted at the edge close to user uses asymmetric encryption usage: specify set of fields in POST request that you want to be encrypted (up to 10 fields) specify the public key to encrypt them fields will be encrypted using the public key at edge locations and will be decrypted when the request reached the web servers ECS Docker Docker is a software development platform to deploy apps apps are packaged in containers that can be run on any OS apps run the same, regardless of where they are run any machine no compatibility issues predictable behavior less work easier to maintain and deploy works with any language, any OS, any technology Docker images are stored in Docker repositories public: Docker hub private: Amazon ECR Docker vs VM docker is sort of a virtualization technology, but not exactly resources are shared with the host =&gt; many containers on one server Docker containers management to manage containers, we need a container management platform 3 choices ECS: Amazon’s own platform Fargate: Amazon’s own serverless platform EKS: Amazon’s managed Kubernates (open source) ECS clusters overview ECS clusters are logical grouping of EC2 instances EC2 instances run the ECS agent (Docker container) the ECS agents registers the instance to the ECS cluster the EC2 instances run a special AMI, made specifically for ECS ECS task definitions tasks definintions are metadata in JSON form to tell ECS how to run a Docker Container it contains crucial information around Image name port binding for container and host (80 -&gt; 8080) memory and CPU required environment variables networking information ECS service ECS service help define how many tasks should run and how they should be run they ensure that the number of tasks desired is running across our fleet of EC2 instances they can be linked to ELB / NLB / ALB if needed ECS service with load balancer ALB has the dynamic port forwarding feature when you create ECS tasks it assign random port numbers to tasks multiple ECS tasks can be run on a single EC2 instances with different port numbers ALB can use the dynamic port forwarding feature to route traffic to these tasks based on their port number ECR ECR is a private Docker image reporsitory access is controlled through IAM (if you have permission errors, check the policy) how to login to ECR using AWS CLI if you have AWS CLI version 1 $(aws ecr get-login --no-include-email --region eu-west-1) then you need to execute the output of the above command if you have AWS CLI version 2 aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 12334556790.dkr.ecr.eu-west-1.amazonaws.com you could just execute the above command which is using the pipe feature Docker push and pull Fargate when launching an ECS cluster, we have to create our EC2 instances if we need to scale, we need to add EC2 instances so we need to manage infrastructure… with Fargate, it is all serverless we don’t provision EC2 instances we just create task definitions, and AWS will run our containers for us ECS IAM roles deep dive EC2 instance profile for EC2 instance to run ECS task, we need to install ECS agent on the EC2 instance ECS will do these things make API calls to ECS service send container logs to CloudWatch logs pull docker image from ECR so ECS agent will use the EC2 instance profile role to do these things ECS task role when we run ECS tasks on EC2 instance, each task will have its own role we use different roles for the different ECS services run task role in defined in the task definition ECS tasks placement when a task of type EC2 is launched, ECS must determine where to place it, with the constraints of CPU, memory, and available port similarly, when a service scales in, ECS needs to determine which task to terminate to assist with this, you can define a task placement strategy and task placement constraints NOTE: this is only for ECS with EC2, not for Fargate ECS task placement process task placement strategies are a best effort when Amazon ECS places tasks, it uses the following process to select container instances identify the instances that satisfy the CPU, memory, and port requirements in the task definition identify the instances that satisfy the task placement constraints identify the instances that satisfy the placement strategies ECS task placement strategies Binpack place tasks based on the least available amount of CPU or memory this minimize the number of instances in use (cost savings) random place the task randomly spread place the task evenly based on the specified value example: instanceID, availability zone you can also mix the placement strategies together, e.g. use Spread for the AZ and Binpack for memory ECS task placement constraints distinctInstance: place each task on a different container instance memberOf: places task on instances that satisfy an expression uses the Cluster Query language e.g. place tasks only on t2 instances ECS service auto scaling CPU and RAM is tracked in CloudWatch at the ECS service level target tracking: target a specific average CloudWatch metric step scaling: scale based on CloudWatch alarms scheduled scaling: based on predictable changes ECS service scaling (task level) != EC2 auto scaling (instance level) Fargate auto scaling is much easier to setup (because of serverless) ECS cluster capacity provider a capacity provider is used in association with a cluster to determine the infrastructure that a task runs on for ECS and Fargate users, the FARGATE and FARGATE_SPOT capacity providers are added automatically for Amazon ECS on EC2, you need to associate the capacity provider with an auto scaling group when you run a task or a service, you define a capacity provider strategy, to prioritize in which provider to run this allows the capacity provider to automatically provision infrastructure for you if you set the average CPU to be at most 70%, then cluster capacity provider will create a new EC2 instance for you when you create a new task to run ECS data volumes EC2 task strategies the EBS volume is already mounted onto the EC2 instances this allows your Docker containers to mount the EBS volume and extend the storage capacity of your task Problem: if your task moves from one EC2 instance to another one, it won’t be the same EBS volume and data, because EBS volume is mounted to the old EC2 instance use cases: mount a data volume between different containers on the same instance extend the temporary storage of a task EFS file systems works for both EC2 tasks and Fargate tasks ability to mount EFS volumes onto tasks tasks launched in any AZ will be able to share the same data in the EFS volume Fargate + EFS = serverless + data storage without managing servers use case: persistent multi AZ shared storage for your containers Bind Mounts sharing data between containers works for both EC2 tasks (using local EC2 instance storage) and Fargate tasks (get 4GB for volume mounts) useful to share an ephemeral storage between multiple containers part of the same ECS task great for sidecar container pattern where the sidecar can be used to send metrics / logs to other destinations Beanstalk developer problems on AWS managing infrastructure deploying code configuring all the databases, load balancers, etc… scaling concerns most web apps have the same architecture (ALB + ASG) all the developers want is for their code to run possibly, consistently across different applications and environments Elastic Beanstalk overview a developer centric view of deploying an application on AWS it uses all components’ we have seen before: EC2, ASG, ELB, RDS… managed services automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration… just the application code is the responsiblity of the developer we still have full control over the configuration Beanstalk is free but you pay for the underlying instances Components application: collection of Elastic Beanstalk components (environments, versions, configurations) application version: an iteration of your application code environment collection of AWS resources running an application version (only one application version at a time) tiers: web server environment tier and worker environment tier you can create multiple environmnets (dev, test, prod…) web environment use ELB with multiple EC2 instances running on different AZs and ASG to scale worker environment use SQS queue with multiple EC2 instances running on different AZs and ASG will scale based on SQS’s length Beanstalk deployment options All at once fastest deployment application has downtime great for quick iterations in development environment no additional cost Rolling application is running below capacity can set the bucket size, bucket size is the number of new instances we launched each time application is running both versions simultaneously no additional cost long deployment Rolling with additional batches application is running at capacity can set the bucket size application is running both versions simultaneously small additional cost additional batch is removed at the end of the deployment longer deployment good for production environment Immutable zero downtime new code is deployed to new instances on a temporary ASG high cost, double capacity longest deployment quick rollback in case of failures (just terminate new ASG) great for production Blue / Green not a direct feature of Elastic Beanstalk zero downtime and release facility create a new stage environment and deploy version 2 there the new environment (green) can be validated independently and roll back if issue happens route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment using Beanstalk, swap URLs when done with the environment test Traffic splitting (Canary Testing) new application version is deployed to a temporary ASG with the same capacity a small percetage of traffic is sent to the temporary ASG for a configuraion amount of time deployment health is monitored if there is a deployment failure, this triggers an automated roll back (very quick) no application downtime new instances are migrated from the temporary to the original ASG old application version is then terminated Deploy using CLI describe dependencies package code as zip, and describe dependencies console: upload zip file (creates new app version), and then deploy CLI: create new app version using CLI (uploads zip), and then deploy Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application Beanstalk lifecycle policy Elastic Beanstalk can store at most 1000 application versions if you don’t remove old versions, you won’t be able to deploy anymore to phase out old application versions, use a lifecycle policy based on time (old versions are removed) based on space (when you have too many versions) versions that are currently used won’t be deleted option not to delete the source bundle in S3 to prevent data loss Beanstalk extensions a zip file containing our code must be deployed to Elastic Beanstalk all the parameters set in the UI can be configured with code using files requriements in the .ebextensions/ directory in the root of source code YAML / JSON format .config extensions (example: logging.config) able to modify some default settings using: option_settings ability to add resources such as RDS, ElastiCache, DynamoDB, etc… resources managed by .ebextensions get deleted if the environment goes away Beanstalk vs CloudFormation under the hood, Elastic Beanstalk relies on CloudFormation CloudFormation is used to provision other AWS services use case: you can define CloudFormation resources in your .ebextensions to provision ElastiCache, S3 bucket, or anything you want. Elastic Beanstalk cloning clone an environment with the exact same configuration useful for deploying a test version of your application all resources and configuration are preserved load balancer type and configuration RDS database type (but data is not preserved) environment variables after cloning an environment, you can change settings Beanstalk migration load balancer after creating an Elastic Beanstalk environmnet, you cannot change the ELB type to migrate to a different ELB create a new env with the same configuration except LB, create your new LB here deploy your application onto the new env perform a CNAME swap or Route 53 update so all your traffic can be direct to the new env RDS RDS can be provisioned with Beanstalk, which is greate for dev / test this is not great for production as database lifecycle is tied to the Beanstalk environment lifecycle the best for prod is to separately create an RDS database and provide our Beanstalk application with the connection string but what if you have already created Beanstalk application with the RDS in production? How to migrate it to a new environment without RDS? create a snapshot of RDS DB (as a safeguard) go to the RDS console and protect the RDS database from deletion create a new environment, without RDS, point your application to the existing RDS in the old env perform a CNAME swap or Route 53 update, confirm it is working terminate the old env (RDS will not be deleted because you prevent it in the console) delete the CloudFormation stack manually (it will be in DELETE_FAILED state because it can’t delete RDS) single Docker run your application as a single Docker container either provide Dockerfile: Elastic Beanstalk will build and run the Docker container Dockerrun.aws.json (v1): describe where the Docker image is (already built) Beanstalk in single Docker container does not use ECS Multi Docker containers multi docker helps run multiple containers per EC2 instance in EB this will create for you ECS cluster EC2 instances, configured to use the ECS cluster load balancer (in HA mode) task definitions and execution requries a config Dockerrun.aws.json (v2) at the root of the source code Dockerrun.aws.json is used to generate the ECS task definition Elastic Beanstalk and HTTPS Beanstalk with HTTPS idea: load the SSL certificate onto the load balancer can be done from the console (EB console, load balancer configuration) can be done from the code: .ebextensions/securelistener-alb.config SSL certificate can be provisioned using ACM or CLI must configure a security group rule to allow incoming port 443 (HTTPS port) Beanstalk redirect HTTP to HTTPS configure your instances to redirect HTTP to HTTPS configure the application load balancer with a rule make sure health checks are not redirected (so they keep giving 200 OK, otherwise they will receive 301 and 302…) Web server vs worker environment if your application performs tasks that are long to complete, offload these tasks to a dedicated worker environment decoupling your application into two tiers is common example: processing a video, generating a zip file, etc… you can define periodic tasks in a file cron.yaml custom platform (advanced) custom platforms are very advanced, they allow to define from scratch the OS additional software scripts that Beanstalk runs on these platforms use case: app language is incompatible with Beanstalk and doesn’t use Docker to create your own platform define an AMI using Platform.yaml file build that platform using the Packer software (open source tool to create AMIs) custom platform vs Custom image custom image is to tweak an existing Beanstalk platform custom platform is to create an entirely new Beanstalk platform CICD Introduction we now know how to create resources in AWS manually we know how to interact with AWS CLI we have seen how to deploy code to AWS using Elastic Beanstalk all these manual steps make it very likely for us to do mistakes what we would like is to push our code in a repository and have it deployed onto the AWS automatically the right way making sure it is tested before deploying with possibility to go into different stages with manual approval where needed to be a proper AWS developer, we need to learn AWS CICD Continuous integration developers push the code to a code repository often (github, codecommit, bitbucket, etc…) a testing / build server checks the code as soon as it is pushed (codebuild, Jenkins CI, etc…) the developer gets feedback about the tests and checks that have passed / failed find bugs early, fix bugs deliver faster as the code is tested deploy often Continuous delivery ensure that the software can be released reliably whenever needed ensures deployments happen often and are quick automated deployment CodeCommit version control is the ability to understand the various changes that happened to the code over time all these are enabled by using a version control system such as git a git repository can live on one’s machine, but it usually lives on a central online repository benefits are collaborate with other developers make sure the code is backed up somewhere make sure it is fully viewable and auditable git repositories can be expensive the industry incldues github bitbucket AWS CodeCommit private git repositories no size limit on repositories fully managed, HA code only in AWS, increased security and compliance secure integrated with Jenkins / CodeBuild / other CI tools security interactions are done using git authentication in git SSH keys: AWS users can configure SSH keys in their IAM console HTTPS: done through the AWS CLI Authentication helper ot generating HTTPS credentials MFA can be enabled for extra security Authorization in git IAM policies manage user / roles rights to repositories encryption repositories are automatically encrypted at rest using KMS encrypted in transit (can only use HTTPS and SSH - both secure) cross account access do not share your SSH keys do not share your AWS credentials use IAM role in your AWS account and use AWS STS (with AssumeRole API) CodeCommit vs Github Similarities both are git repositories both support code review github and CodeCommit can be integrated with AWS CodeBuild both support HTTPS and SSH method of authentication differences security github: github users codecommit: AWS IAM users / roles hosted: github: hosted by github github enterprise: self hosted on your servers codecommit: managed and hosted by AWS UI github UI is fully featured notifications you can trigger notifications in CodeCommit using AWS SNS or AWS lambda or CloudWatch event rules use cases for notifications SNS / lambda deletion of branches trigger for pushes that happens in master branch notify external build system trigger AWS lambda function to perform codebase analysis use cases for CloudWatch event rules trigger for pull request updates commit comment event CloudWatch event rules goes into an SNS topic CodePipeline Continuous delivery visual workflow source: github / codecommit / S3 build: codebuild / Jenkins load testing: third party tools deploy: AWS code deploy / Beanstalk / CloudFormation / ECS made of stages each stage can have sequential actions or parallel actions stages examples: build / test / deploy / load test manual approval can be defined at any stage artifacts each pipeline stage can create artifacts artifacts are passed stored in S3 and passed on to the next stage troubleshooting codepipeline state changes happen in CloudWatch events, which can in return create SNS notifications you can create events for failed pipelines you can create events for cancelled stages if codepipeline fails a stage, your pipeline stops and you can get information in the console CloudTrail can be used to audit AWS API calls if pipeline can’t perform an action, make sure the IAM service role attached does have enough permissions CodeBuild fully managed build service alternative to other build tools such as Jenkins continuous scaling (no servers to manage or provision - no build queue) pay for usage: the time it takes to complete the builds leverages Docker under the hood for reproducible builds possibility to extend capabilities leveraging our own base Docker images secure: integration with KMS for encryption of build artifacts, IAM for build permissions, and VPC for network security, CloudTrail for API calls logging source code from github / codecommit / codepipeline / S3 build instructions can be defined in code (buildspec.yml file) output logs to S3 and AWS cloudwatch logs metrics to monitor codebuild statistics use cloudwatch alarms to detect failed builds and trigger notifications cloudwatch events / lambda as a Glue SNS notifications ability to reproduce codebuild locally to troubleshoot in case of errors builds can be defined within CodePipeline or Codebuild itself BuildSpec buildspec.yml file must be at the root of your code define environment variables plaintext variables secure secrets: use SSM parameter store phases install: install dependencies you may need for your build pre build: final commands to execute before build build: actual build commands post build: finishing touches (zip output) artifacts: what to upload to S3 cache: files to cache to S3 for future build speedup local build in case of need of deep troubleshooting beyond logs you can run CodeBuild on your laptop (after installing Docker) for this, leverage CodeBuild agent CodeBuild in VPC by default, your Codebuild containers are launched outside your VPC therefore, by default, it cannot access resources in a VPC you can specify a VPC configuration VPC ID subnet ID security group ID they your build can access resources in your VPC use case: integration tests, data query, internal load balancers CodeDeploy we want to deploy our application automatically to many EC2 instances these instances are not managed by Elastic Beanstalk there are several ways to handle deployments using open source tools (Ansible, Terraform, Chef, Pupper, etc…) Steps Each EC2 machine (or on premises machine) must be running the CodeDeploy agent the agent is continuously polling AWS codeDeploy for work to do CodeDeploy sends appspec.yml file application is pulled from github or S3 EC2 will run the deployment instructions CodeDeploy agent will report of success / faliure of deployment on the instance other information EC2 instances are grouped by deployment group (dev / test / prod) lots of flexibility to define any kind of deployments CodeDeploy can be chained into CodePipeline and use artifacts from there CodeDeploy can reuse existing setup tools, works with any application, auto scaling integraion Note: Blue / Green only works with EC2 instances (not on premises) support for AWS lambda deployments CodeDeploy does not provision resources primary components application: unique name compute platform: EC2 or on premises or lambda deployment configuration: deployment rules for success / failures EC2 or on premises: you can specify the minimum number of healthy instances for the deployment lambda: specify how traffic ;is routed to your updated lambda function versions deployment group: group of tagged instances (allows to deploy gradually) deployment type: in place deployment or Blue/Green deployment IAM instance profile: need to give EC2 the permissions to pull from S3 / github application revision: application code + appspec.yml file service role: role for CodeDeploy to perform what it needs target revision: target deployment application version Appspec file section: how to source and copy from S3 / github to filesystem hooks: set of instructions to do to deploy the new version (hooks can have timeouts) applicationStop DownloadBundle BeforeInstall AfterInstall ApplicationStart ValidateService: really important Deployment config Configs: one a time: one instance at a time, one instance fails =&gt; deployment stops half at a time: 50% all at once: quick but no healthy host, downtime, good for dev custom: min healthy host = 75% failures: instances stay in failed state new deployments will first be deployed to failed state instances to rollback: re-deploy old deployment or enable automated rollback for failures deployment targets set of EC2 instances with tags directly to an ASG mix of ASG / tags so you can build deployment segments customization in scripts with DEPLOYMENT_GROUP_NAME environment variables CodeDeploy for EC2 and ASG code deploy to EC2 define how to deploy the application using appspec.yml + deployment strategy will do in place update to your fleet of EC2 instances can use hooks to verify the deployment after each deployment phase code deploy to ASG in place updates updates current existing EC2 instances instances newly created by an ASG will also get automated deployments Blue / Green deployment a new auto scaling group is created (settings are copied) choose how long to keep the old instances must be using an ELB (for directing traffic to new ASG group) CodeStar CodeStar is an integrated solution that regroups: github, codecommit, codebuild, codeDeploy, CloudFormation, codepipeline, cloudwatch helps quickly create CICD ready projects for EC2, lambda, Beanstalk supported language: C#, Go, HTML5, Java, Node.js, PHP, Python, Ruby issue tracking integration with JIRA, Github issues ability to integrate with Cloud9 to obtain a web IDE one dashboard to view all your components free services, pay only for the underlying usage of other services limited customization CloudFormation infrastructure as code manual work will be very tough to reproduce in another region in another AWS account within the same region if everything was deleted CloudFormation would be the code to create / update / delete our infrastructure CloudFormation is a declarative way of outlining your AWS infrastructure, for any resources CloudFormation creates the resources for you in the right order, with the exact configuration that you sepcify benefits infrastructure as code no resources are manually created, which is excellent for control the code can be version controlled for example using git changes to the infrastructure are reviewed through code cost each resources within the stack is stagged with an identifier so you can easily see how much a stack costs you you can estimate the costs of your resources using the CloudFormation template savings strategy: in dev, you could automation deletion of templates at 5pm and recreate anything at 8am safely productivity ability to destroy and recreate an infrastructure on the cloud on the fly automated generation of diagram for your templates declarative programming (no need to figure out ordering and orchestration) separation of concern: create many stacks for many apps, and many layers don’t reinvent the wheel leverage existing templates on the web leverage the documentation how cloudformation works templates have to be uplaoded in S3 and then referenced in cloudformation to update a template, we can’t edit previous ones, we have to reupload a new version of the template to AWS stacks are identified by a name deleting a stack deletes every single artifact that was created by CloudFormation deploying cloudformation template manual way editing templates in the CloudFormation designer using the console to input parameters automated way editing templates in a YAML file using the AWS CLI to deploy the templates recommended way when you fully want to automate your flow building blocks templates components resources: your AWS resources declared in the template (mandatory) parameters: the dynamic inputs for your template mappings: the static variables for your templates outputs: references to what has been created conditionals: list of conditions to perform resource creation metadata template helpers references functions resources resources are the core of your CloudFormation template they repreesent the different AWS components that will be created and configured resources are declared and can reference each other AWS figures out creation, updates and deletes of resources for us can I create a dynamic amount of resources no you can’t is every AWS services suported almost, only a few are not parameters parameters are a way to provide inputs to your AWS CloudFormation template they are important to know about if you want to resue your templates across the company some inputs can not be determined ahead of time parameters are extremely powerful, controlled, and can precent errors from happening in your templates thanks you types how to reference a parameter the Fn::Ref function can be leveraged to reference parameters parameters can be used anywhere in a template the shorthand for this in YAML is !Ref the function can also reference other elements within the template Pseudo parameters AWS offers us pseudo parameters in any CloudFormation template these can be used at any time and are enabled by default mappings mappings are fixed variables within your CloudFormation template they are very handy to differentiate between different environments (dev vs prod), regions, AMI types, etc… all the values are hardcoded within the template when would you use mappings vs parameters mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as region AZ AWS account environment they allow safer control over the template use parameters when the values are really user specific accessing mapping values we use Fn::FindInMap to return a named value from a specific key !FindInMap [MapName, TopLevelKey, SecondLevelKey] outputs the outputs section declares optional outputs values that we can import into other stacks (if you export them first) you can also view the outputs in the AWS console or using the AWS CLI they are very useful for example if you define a network CloudFormation, and output the variables such as VPC ID, and your subnet IDs it is the best way to perform some collaboration cross stack, as you let export handle their own part of the stack you can’t delete a CloudFormation stack if its outputs are being referenced by another CloudFormation stack outputs example create a SSH security group as part of one template we create an output that references that security group 123456Outputs: StackSSHSecurityGroup: Description: The SSH security group for our company Value: !Ref MyCompanyWideSSHSecurityGroup Export: Name: SSHSecurityGroup Cross stack reference we then create a second template that leverages that security group for this, we use the Fn::ImportValue function you can’t delete the underlying stack until all the references are deleted too 123456789Resources: MySecureInstance: Type: AWS::EC2::Instance Properties: AvailabilityZone: us-east-1a ImageId: ami-xxxxxxxx InstanceType: t2.micro SecurityGroups: - !ImportValue SSHSecurityGroup conditions conditions are used to control the creation of resources or outputs based on a condition conditions can be whatever you want them to be, but common ones are environment region parameter value each condition can reference another condition, parameter value or mapping define a conditon 12Conditions: CreateProdResources: !Equals [ !Ref EnvType, prod] the logical ID is for you to choose, it is how you name condition the intrinsic function can by any of the following Fn::And Fn::Equals Fn::If Fn::Not Fn::Or using a condition conditions can be applied to resources / outputs 1234Resources: Mountpoint: Type: &quot;AWS::EC2::VolumeAttachment&quot; Condition: CreateProdResources Intrinsic functions Fn::Ref can be leveraged to reference parameters resources Fn::GetAtt attributes are attached to any resources you create to know the attributes of your resources, the best place to look at is the documentation example: AZ of an EC2 machine 1234567891011121314Resources: EC2Instance: Type: &quot;AWS::EC2::Instance&quot; Properties: ImageId: ami-xxxxx InstanceType: t2.microNewVolume: Type: &quot;AWS::EC2::Instance&quot; Condition: CreateProdResources Properties: Size: 100 AvailabilityZone: !GetAtt EC2Instance.AvailabilityZone Fn::FindInMap we use Fn::FindInMap to return a named value from a specific key !FindInMap [MapName, TopLevelKey, SecondLevelKey] Fn::ImportValue import values that are exported in other templates Fn::Join join values with a delimiter !Join [delimiter, [a, b, c]] Fn::Sub used to substitute variables from a text, it is a very handy function that will allow you to fully customize your templates for example: you can combine Fn::Sub with references or AWS Pseudo variables String must contain ${VariableName} and will substitute them Condition Functions the logical ID is for you to choose, it is how you name condition CloudFormation rollbacks stack creation fails default: everything rolls back, we can look at the log option to disable rollback and troubleshoot what happened stack update fails: the stack automatically rolls back to the previous known working state ability to see in the log what happened and error messages ChangeSets when you update a stack, you need to know what changes before it happens for greater confidence ChangeSets won’t say if the update will be successful Nested Stacks stacks as part of other stacks they allow you to isolate repeated patterns / common components in separate stacks and call them from other stacks Example: load balancer configuration that is re used security group that is re used nested stacks are considered best practice to update a nested stack, always update the parent (root stack) Cross stack vs nested stack Cross stacks helpful when stacks have different lifecycles use outputs export and Fn::ImportValue when you need to pass export values to many stacks nested stacks helpful when components must be re used example: re use how to properly configure an application load balancer the nested stack only is important to the higher level stack StackSets create, update, or delete stacks across multiple accounts and regions with a single operation administrator account to create StackSets trusted accounts to create, update, delete stack instances from StackSets when you update a stack set, all associated stack instances are updated throughout all accounts and regions CloudFormation drift CloudFormation allows you to create infrastructure but it doesn’t protect you against manual configuration changes how do we know if our resources have drifted? we can use CloudFormation drift Monitoring AWS CloudWatch metrics: collect and track key metrics log: collect, monitor, analyze, and store log files events: send notifications when certain events happen in your AWS alarms: react in real-time to metrics / events X-Ray troubleshooting application performance and errors distrbuted tracing of microservices CloudTrail internal monitoring of API calls being made audit changes to AWS resources by your users CLoudWatch metrics CloudWatch provides metrics for every services in AWS metric is a variable to monitor metric belong to namespaces dimension is an attribute of a metric up to 10 dimensions per metric metrics have timestamps can create CloudWatch dashboards of metrics Detailed monitoring EC2 instance metrics have metrics every 5 minutes with detailed monitoring, you get data every 1 minute use detailed monitoring if you want to scale faster for your ASG the AWS free tier allows us to have 10 detailed monitoring metrics NOTE: EC2 memory usage is by default not pushed (must be pushed from inside the instance as a custom metric) CloudWatch Custom metrics possibility to define and send your own custom metrics to CloudWatch example: RAM usage, disk space, number of logged in users use API call PutMetricData ability to use dimensions (attribute) to segment metrics instance id environment name matric resolution standard: 60 seconds high resolution: 1 / 5 / 10 / 30 seconds - higher cost important: the API accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly) CloudWatch logs applicatoins can send logs to CloudWatch using the SDK CLoudWatch can collect log from Elastic Beanstalk: collection of logs from application ECS: colletion from containers AWS lambda: collection from function logs VPC flow logs: VPC specific logs API gateway CLoudTrail based on filter CloudWatch log agents: for example on EC2 machines route 53: log DNS queries cloudWatch logs can go to batch exporter to S3 for archival stream to elasticSearch cluster for further analysis ClouWatch logs can use filter expressions logs storage architecture log groups: arbitrary name, usually representing an application log stream: instances within application / log files / containers can define log expiration policies (never, 30 days, etc…) using the AWS CLI we can tail CloudWatch logs to send logs to CloudWatch, make sure IAM permissions are correct security: encryption of logs using KMS at the group level CloudWatch logs for EC2 by default, no logs from EC2 machine will go to CloudWatch you need to run a CloudWatch agent on EC2 to push the log files you want make sure IAM permissions are correct the CLoudWatch log agent can be setup on premises too CloudWatch logs agent vs Unified agent CloudWatch logs agent old version of the agent can only send to CloudWatch logs CloudWatch Unified agent collect additional system level metrics such as RAM, processes, etc… collect logs to send to CloudWatch logs centralized configuration using SSM parameters store CloudWatch logs metric filter CloudWatch logs can use filter expressions for exampe: find a specific IP inside of a log or count occurrences of ERROR in your logs metric filters can be used to trigger alarms filter do not retroactively filter data. (it doesn’t not filter historical data, only data since filter is created is counted). filters only publish the metric data points for events that happen after the filter was created can be integrated with CloudWatch alarms, SNS, etc… CloudWatch alarms alarms are used to trigger notifications for any metric various options alarm states OK INSUFFICIETN_DATA ALARM period length of time in seconds to evaluate the metric high resolution custom metrics: 10 sec, 30 sec or multiple of 60 sec CloudWatch alarm targets stop, terminate, reboot or recover EC2 instances trigger auto scaling action send notification to SNS good to know alarms can be created based on CloudWatch logs metrics filters to test alarms and notifications, you could set the alarm state using CLI CloudWatch Events event pattern: intercept events from AWS service EC2 instance state change, code build failure, S3… can intercept any API call with CloudTrail integration schedule or Cron A json payload is created from the event and passed to a target CloudWatch event bridge EventBridge is the next evolution of CloudWatch events default event bus: generated by AWS service partner event bus: receive events from SaaS service or applications custom event bus: for you own application event buses can be accessed by other AWS accounts rules: how to process the events (similar to CloudWatch event rules) Schema registry eventBridge can analyze the events in your bus and infer the schema the schema registry allows you to generate code for your application that will know in advance how data is structured in the event bus schema can be versioned Amazon EventBridge vs CloudWatch events EventBridge builds upon and extends CloudWatch events it uses the same service API and endpoint, and the same underlying service infrastructure EventBridge allows extension to add event buses for your custom applications and third party SaaS apps EventBridge has the schema registry capability EventBridge has a different name to mark the new capabilities over time, the CloudWatch events name will be replaced with EventBridge X-Ray debugging in Production, the old way test locally add log statements everywhere re deploy in production log formats differ across applications using CLoudWatch and analytics is hard X-ray advantages troubleshooting performance understand dependencies in a microservices architecture pinpoint service issues review request behavior find errors and exceptions tracing tracing is an end to end way to following a request each component dealing with the request adds its own trace tracing is made of segments annotations can be added to traces to provide extra information ability to trace every request sample request (as a percentage for example or a rate per minute) X-Ray security IAM for authorization How to enable? Your code must import the AWS X-Ray SDK very little code modification needed the application SDK will then capture calls to AWS service HTTP / HTTPS requests database calls queue calls install X-Ray daemon or enable X-Ray AWS integration X-Ray daemon works as a low level UDP packet interceptor lambda / other AWS services already run the X-Ray daemon for you each application must have the IAM rights to write data to X-Ray X-Ray magic X-Ray service collects data from all the different services service map is computed from all the segments and traces X-Ray is graphical, so even non technical people can help troubleshoot X-Ray troubleshooting if X-Ray is not working on EC2 ensure the EC2 IAM role has the proper permissions ensure the EC2 instance is running the X-Ray daemon to enable on AWS lambda ensure it has an IAM execution role with proper policy ensure that X-Ray is imported in the code X-Ray instrumentation and concepts instrumentation means the measure of product’s performance, diagnose errors and to write trace information to instrument your application code, you use the X-Ray SDK many SDK require only configuration changes you can modify your application code to customize and annotation the data that the SDK sends to X-Ray, using interceptors, filters, handlers, middleware… X-Ray concepts segments: each application / service will send them subsegments: if you need more details in your segment trace: segments collected together to form an end to end trace sampling: decrease the amount of requests sent to X-Ray, reduce cost annotations: key value pairs used to index traces and use with filters metadata: key value pairs, not indexed, not used for searching the X-Ray daemon / agent has a config to send traces cross account make sure the IAM permissions are correct - the agent will assume the role this allows to have a central account for all your application tracing X-Ray sampling rules with sampling rules, you control the amount of data that you record you can modify sampling rules without changing your code by default, the X-Ray SDK records the first request each second, and five percent of any additional requests one request per second is the reservior, which requests that at least one trace is recorded each second as long the service is serving requests Five percent is the rate, at which additional requests beyond the reservior size are sampled X-Ray with Beanstalk Elastic Beanstalk platforms include the X-Ray daemon you can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextension/xray-daemon.config) make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly then make sure your application code is intrumentated with the X-Ray SDK note: the X-Ray daemon is not provided for multicontainer Docker CloudTrail provides governance, compliance, and audit for your AWS account CloudTrail is enabled by default get an history of events / API calls made within your AWS accounts by console SDK CLI AWS services can put logs from CloudTrail into CLoudWatch logs or S3 a trail can be applied to All regions (default), or a single region if a resource is deleted in AWS, investigate CloudTrail first CloudTrail events management events operations that are performed on resources in you account examples configuring security configuring rules for routing data setting up logging by default, trails are configured to log management events can separate read events (that don’t modify resources) and write events (that may modify resources) data events by default, data events are not logged (because high volume operations) S3 object-level activity can separate read and write events lambda function execution activity CloudTrail insights enable CloudTrail insights to detect unusual activity in your account inaccurate resource provisioning hitting service limits bursts of IAM actions gaps in periodic maintenance activity CloudTrail insights analyzes normal management events to create a baseline and then continuously analyzes write events to detect usuaual patterns anomalies appear in the CloudTrail console event is sent to S3 eventBridge is generated CloudTrail events retention events are stored for 90 days in CloudTrail to keep events beyond this period, log them to S3 and use Athena CloudTrail vs CloudWatch vs X-Ray CloudTrail audit API calls made by users / services / AWS console useful to detect unauthorized calls or root cause of changes CloudWatch metrics over time for monitoring logs for storing application log alarms to send notifications in case of unexpected metrics X-Ray automated trace analysis and central service map visualization latency, errors and fault analysis request tracking across distributed systems SQS Communications between applications Synchronous synchronous between applications can be problematic if there are sudden spikes of traffic what if you need to suddenly encode 1000 videos but usually it is 10? asynchronous it is better to decouple your applications SQS: queue model SNS: pub/sub model Kinesis: real time streaming model these services can scale independently from our application SQS - standard queue fully managed service, used to decouple applications attributes unlimited throughput, unlimited number of messages in queue default retention of messages: 4 to 14 days law latency limitation of 256 KB per message sent can have duplicate messages (at least once delivery, occasionally) can have out of order messages (best effort ordering) Producing messages produced to SQS using the SDK (SendMessage API) the message is persisted in SQS until a consumer deletes it comsuming messages consumers (running on EC2 instances, servers, or lambda) poll SQS for messages (receive up to 10 messages at a time) process the messages (example: insert the message into an RDS database) delete the messages using the DeleteMessage API multiple EC2 instances consumers consumers receive and process messages in parallel at least once delivery best effort message ordering consumers delete messages after processing them we can scale consumers horizontally to improve throughput of processing (using ASG) security encryption in flight encryption using HTTPS API at rest encryption using KMS keys client side encryption if the client wants to perform encryption / decryption itself access controls: IAM policies to regulate access to SQS API SQS access policies: (similar to S3 bucket policies) useful for cross account access to SQS queues useful for allowing other services (SNS, S3…) to write to an SQS queue message visibility timeout after a message is polled by a consumer, it becomes invisible to other consumers by default, the message visibility timeout is 30 seconds that means the message has 30 seconds to be processed after the message visibility timeout is over, the message is visible again in SQS if a message is not processed within the visibility timeout, it will be received by consumer again so it will be processed twice a consumer could call the ChangeMessageVisibility API to get more time if visibility timeout is high, and consumer crashes, it will take longer time for the message to become visible in the queue and being consumed by others if the visibility timeout is low, we may get duplicates Dead letter queue if a consumer fails to process a message within the visibility timeout, the message goes back to the queue we can set a threshold of how many times a message can go back to the queue after the MaximumRecevies threshold is exceeded, the message goes into a dead letter queue useful for debugging make sure to process the messages in the DLQ before they expire good to set a retention of 14 days in the DLQ delay queue delay a message (consumers don’t see it immediately) up to 15 minutes default is 0 seconds (message is available right away) can set a default at queue level can override the default on send using the DelaySeconds parameter long polling when a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue this is called long polling long polling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application the wait time can be between 1 to 20 seconds long polling is preferable to short polling long polling can be enabled at the queue level or at the API level using WaitTimeSeconds SQS extended client message size limit is 256 KB, how to send large messages? using the SQS extended client (Java Library) it can be implemented using any language, it first uploads the large object to S3 then send the metadata of that object to SQS, once the consumer received the metadata, it will fetch the real object from S3. FIFO queue First in first out limited throughput: 300 messages / second, without batching, 3000 m/s with batching exactly once send capability (by removing duplicates) messages are processed in order by the consumer FIFO deduplication deduplication interval is 5 minutes two deduplication methods content based deduplication: will do a SHA-256 hash of the message body explicitly provide a message deduplication ID if the queue receives messages with the same hash key or the same deduplication ID, it will refuse to receive the message message grouping if you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order to get ordering at the level of a subset of messages, specify different values for MessageGroupID messages that share a common message group ID will be in order within the group each group ID can have a different consumer (parallel processing) ordering across groups is not guaranteed SNS what if you want to send one message to many receivers? the event producer only sends message to one SNS topic as many event receivers as we want to listen to the SNS topic notifications each subscriber to the topic will get all the messages (note: new feature to filter messages) up to 10 million subscriptions per topic 100k topics limit subscribers can be SQS HTTP / HTTPS lambda emails SMS messages mobile notifications SNS integrates with a lot of AWS services many AWS services can send data directly to SNS for notifications CloudWatch alarms ASG notifications S3 CloudFormation (upon state changes =&gt; failed to build etc…) How to publish topic publish (using the SDK) create a topic create a subscription publish to the topic direct publish (for mobile apps SDK) create a platform application create a platform endpoint publish to the platform endpoint works with Google GCM, Apple APNS, Amazon ADM… security encryption in flight encryption using HTTPS API at rest encryption using KMS keys client side encryption if the client wants to perform encryption / decryption itself access controls: IAM policies to regulate access to the SNS API SNS access policies (similar to S3 bucket policies) useful for cross account to SNS topic useful for allowing other services to write to an SNS topic SNS + SQS: Fan out push once in SNS, receive in all SQS queues that are subscribers fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work ability to add more SQS subscribers over time make sure your SQS queue access policy allows for SNS to write S3 events to multiple queues for the same combination of: event type and prefix, you can only have one S3 event rule if you want to send the same S3 event to many SQS queues, use fanout (SNS + SQS) SNS - FIFO similar features as SQS FIFO ordering by message group ID deduplication using a deduplication ID or Content based deduplication can only have SQS FIFO queues as subscribers limited throughput (same throughput as SQS FIFO) message filtering JSON policy used to filter messages sent to SNS topic’s subscriptions if a subscription doesn’t have a filter policy, it receives every message Kinesis Kinesis data streams billing is per shard provisioned, can have as many shards as you want retention between 1 to 365 days ability to reprocess data (because data will not be deleted by consumer, it stays in Kinesis data streams until retention period is over) once data is inserted in Kinesis, it can’t be deleted (immutability) data that shares the same partition goes to the same shard (shard level ordering) producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis agent consumers write your own: Kinesis Client Library (KCL), AWS SDK managed: AWS lambda, Kinesis data firehose, Kinesis data analytics Kinesis data streams security control access / authorization using IAM policies encryption in flight using HTTPS encryption at rest using KMS you can implement encryption / decryption of data on client side VPC endpoints available for Kinesis to access within VPC (e.g. EC2 instance in private subnet access Kinesis data stream using VPC endpoint) monitor API calls using CloudTrail Kinesis consumers Kinesis consumer types Shared fanout consumer - pull enhanced fanout consumer - push low number of consuming applications multiple consuming applications for the same stream read throughput 2MB/ second per shard across all consumers 2 MB / second per consumer per shard max 5 GetRecords API calls / sec - latency ~200ms latency ~ 70ms minimize cost higher cost consumers poll data from Kinesis using GetRecords API call Kinesis push data to consumers over HTTP returns up to 10MB or up to 10000 records soft limit of 5 consumer applications per data stream Kinesis Client library (KCL) a Java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload each shard is to be read by only one KCL instance e.g. 4 shards =&gt; max 4 KCL instances progress is checkpointed into DynamoDB (needs IAM access from KCL instance to DynamoDB), this means if one KCL instance is down, DynamoDB will save the checkpoint and knows where to resume when KCL instance goes backup track other workers and share the work amongst shards using DynamoDB KCL can run on EC2, elastic Beanstalk and on premises records are read in order at the shard level versions KCL 1.x (supports shared consumer) KCL 2.x (supports shared and enhanced fanout consumer) Kinesis operations Shard splitting used to increase the Stream capacity used to divide a hot shard the old shard is closed and will be deleted once the data is expired (until the retention period is over) no automatic scaling (manually increase / decrease capacity) can’t split into more than two shards in a single operation merging shards decrease the Stream capacity and save costs can be used to group two shards with low traffic old shards are closed and will be deleted once the data is expired can’t merge more than two shards in a single operation Kinesis data firehose fully managed service, no administration, automatic scaling, serverless target: redshift, S3, ElasticSearch third party custom HTTP endpoint pay for data going through firehose near real time 60 seconds latency minimum for non full batches or minimum 32 MB of data at a time it is not real time because it will batch the data into 60 seconds of data or 32MB of data supports many data formats, conversions, transformations, compression supports custom data transformations using AWS lambda can send failed or all data to a backup S3 bucket Kinesis data streams vs Firehose Kinesis data streams Kinesis data firehose streaming service for ingest at scale load streaming data into S3 / redshift / ElasticSearch / Thrid party / custom HTTP write custom code (producer / consumer) fully managed real time (~200ms) near real time (60 seconds or 32MB) manage scaling (shard spliting / shard merging) automatic scaling data storage for 1 to 365 days no data storage supports replay capability doesn’t support replay capability Kinesis data analytics (SQL application) perform real time analytics on Kinesis streams using SQL fully managed, no server to provision automatic scaling real time analytics pay for actual consumption rate can create streams out of the real time queries use cases time series analytics real time dashboards real time metrics SQS vs SNS vs Kinesis SQS consumer pull data data is deleted after being consumed can have as many as workers as we want no need to provision throughput ordering guarantees only on FIFO queues individual message delay capability SNS push data to many subscribers data is not persisted (lost if not delivered) pub/sub no need to provision throughput integrates with SQS for fanout architecture pattern FIFO capability for SQS FIFO Kinesis standard: pull data, 2 MB per shard enhanced fanout: push data, 2 MB per shard per consumer possibility to replay data meant for real time big data, analytics and ETL ordering at the shard level data expires after X days must provision throughput Kinesis vs SQS ordering let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO Kinesis data streams on average you will have 20 trucks per shard trucks will have their data ordered within each shard the maximum amount of consumer in parallel we can have is 5 SQS FIFO you only have one SQS FIFO queue you will have 100 group ID you can have up to 100 consumers (due to the 100 group ID) you have up to 300 message per second (or 3000 if using batching, because one GetRecords API call can receive up to 10 messages) Lambda what is serverless serverless is a new paradigm in which the developers don’t have to manage servers anymore they just deploy code serverless was pioneered by AWS lambda but now also includes anything that is managed: databases, messaging, storage, etc… serverless does not mean there are no servers, it means you just don’t manage / provision / see them serverless in AWS lambda DynamoDB Cognito API Gateway S3 SNS and SQS Kinesis data firehose Aurora serverless Step functions Fargate Lambda synchronous invocations synchronous: CLI, SDK, API Gateway, ALB results is returned right away error handling must happen client side lambda integration with ALB to expose a lambda function as an HTTP endpoint you can use the ALB or an API gateway the lambda function must be registered in a target group ALB will convert the request HTTP to JSON and convert the response JSON to HTTP ALB multi healer values ALB can support multi header values when you enable multi value headers, HTTP headers and query string parametersthat are sent with multiple values are shown as arrays within the AWS lambda event and response objects 12345HTTPhttp:&#x2F;&#x2F;example.com&#x2F;path?name&#x3D;foo&amp;name&#x3D;barJSON&quot;queryStringParameters&quot;:&#123;&quot;name&quot;:[&quot;foo&quot;, &quot;bar&quot;]&#125; lambda@Edge you have deployed a CDN using CloudFront what if you wanted to run a global lambda alongside? or how to implement request filtering before reaching your application? for this, you can use Lambda@edge, deploy lambda functions alongside your CloudFront CDN build more responsive applications you don’t manage servers, lambda is deployed globally customize the CDN content pay only for what you use you can use lambda to change CloudFront requests and responses after CloudFront receives a request from a viewer before CloudFront forwards the request to the origin after CloudFront receives the response from the origin before CloudFront forwards the response to the viewer you can also generate responses to viewers without ever sending the request to the origin lambda - asynchronous invocations S3, SNS, CloudWatch events the events are placed in an event queue lambda attempts to retry on errors 3 tries total 1 minute after first, then 2 minutes wait make sure the processing is idempotent (result is the same after retry) if the function is retried, you will see duplicate logs entries in CloudWatch logs can define a DLQ - SNS or SQS - for failed processing (need correct IAM permissions for lambda to write to SQS) asynchronous invocations allow you to speed up the processing if you don’t need to wait for the result lambda event source mapping Kinesis data Streams and DynamoDB Streams SQS and SQS FIFO queue common denominator: records need to be pulled from the source your lambda function is invoked synchronously Streams and lambda (Kinesis and DynamoDB) an event source mapping creates an iterator for each shard, processes items in order start with new items, from the beginning or from timestamp processed items aren’t removed from the stream (other consumers can read them again) if traffic is low, we can use batch window to accumulate records before processing you can process multiple batches in parallel up to 10 batches per shard in order processing is still guaranteed for each partition key Streams and lambda - error handling by default, if your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire to ensure in order processing, processing for the affected shard is paused until the error is resolved you can configure the event source mapping to discard old events restrict the number of retries split the batch on error (to work around lambda timeout issue, maybe there is not enough time to process the whole batch, so we split the batch to make it small and faster to process) discarded events can go to a Destination SQS and SQS FIFO with lambda event source mapping will pull SQS (long polling) specify batch size (1 to 10 messages) recommended: set the queue visibility timeout to 6x the timeout of your lambda function to use a DLQ: setup on the SQS queue, not lambda (DLQ for lambda is only for async invocations) or use a lambda Destination for failures lambda also supports in order processing for FIFO queues, scaling up to the number of active message groups for standard queues, items aren’t necessarily processed in order lambda scales up to process a standard queue as quickly as possible when an error occurs, batches are returned to the queue as individual items and might be processed in a different grouping than the original batch occasionally, the event source mapping receive the same item from the queue twice, even if no function error occurred lambda deletes items from the queue after they are processed successfully you can configure the source queue to send items to a DLQ if they can’t be processed lambda event mapper scaling Kinesis data streams and DynamoDB streams one lambda invocation per stream shard if you use parallelization, up to 10 batches processed per shard simultaneously SQS standard lambda adds 60 more instances per minute to scale up up to 1000 batches of messages processed simultaneously SQS FIFO messages with the same group ID will be processed in order the lambda function scales to the number of active message groups lambda - Destinations for asynchronous invocations, we can define destinations for successful and failed event SQS SNS lambda EventBridge bus note: AWS recommends you use Destinations instead of DLQ now (but both can be used at the same time) lambda permissions - IAM roles and resource policies lambda execution role grants the lambda function permissions to AWS services / resources when you use an event source mapping to invoke your function, lambda uses the execution role to read event data (e.g. lambda need permission to pull messages from SQS) lambda resource based policies use resource based policies to give other accounts and AWS services permission to use your lambda resources similar to S3 bucket policies for S3 bucket an IAM principal can access lambda if the IAM policy attached to the principal authorizes it (user access) or if the resource based policy authorizes (service access) when an AWS service like S3 calls your lambda function, the resource based policy gives it access lambda environment variables environment variable = key / value pair in string form adjust the function behavior without updating code the environment variable are available to your code lambda service adds its own system environment variables as well helpful to store secrets (encrypted by KMS) secrets can be encrypted by the lambda service key, or your own CMK lambda logging and monitoring CLoudWatch logs lambda execution logs are stored in AWS CloudWatch logs make sure your AWS lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch logs CLoudWatch metrics lambda metrics are displayed in AWS CloudWatch metrics invocations, Durations, concurrent executions error count, success rates, throttles async delivery failures iterator age (lagging for Kinesis and DynamoDB streams) lambda tracing with X-Ray enable in lambda configuration (active tracing) runs the X-Ray daemon for you use AWS X-Ray SDK in code ensure lambda function has a correct IAM execution role to write to X-Ray the managed policy is called: AWSXRayDaemonWriteAccess lambda in VPC lambda by default by default, your lambda function is launched outside your own VPC (in an AWS owned VPC) therefore it cannot access resources in your VPC lambda in VPC you must define the VPC ID, the subnets and the security groups lambda will create an ENI in your subnets lambda needs AWSLambdaVPCAccessExecutionRole internet access a lambda function in your VPC does not have internet access deploying a lambda function in a public subnet does not give it internet access or a public IP deploying a lambda function in a private subnet gives it internet access if you have a NAT gateway / NAT instance you can use VPC endpoints to privately access AWS services without a NAT lambda function performance configuration RAM from 128MB to 3008MB in 64MB increments the more RAM you add, the more vCPU credits you get at 1792MB, a function has the equivalent of one full vCPU after 1792MB, you get more than one CPU, and need to use multi threading in your code to benefit from it if your application is CPU-bound (computation heavy), increase RAM timeout: default 3 seconds, maximum is 900 seconds lambda execution context the execution context is a temporary runtime environment that initialize any external dependencies of your lambda code great for database connections, HTTP clients, SDK clients… the execution context is maintained for some time in anticipation of another lambda function invocation the next function invocation can reuse the context to execution time and save time in initializing connections objects (e.g. establish database connection outside of function handler) the execution context includes the /tmp directory lambda function /tmp space if your lambda function needs to download a big file to work if your lambda function needs disk space to perform operations you can use the /tmp directory max size is 512 MB the directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work) for permanent persistence of object, use S3 lambda concurrency concurrency limit: up to 1000 concurrent executions across entire account, so if one of your lambda function takes up all the concurrencies (if you didn’t setup reserved concurrency limit), the other lambda functions will be throttled. can set a reserved concurrency at the function level each invocation over the concurrency limit will trigger a throttle throttle behavior if synchronous invocation = return throttle error 429 if asynchronous invocation = retry automatically and then go to DLQ if you need a higher limit, open a support ticket lambda concurrency and asynchronous invocations if the function doesn’t have enough concurrency available to process all events, additional requests are throttled for throttling errors and system errors, lambda returns the event to the queue and attempts to run the funtion again for up to 6 hours the retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes Cold start and provisioned concurrency cold start new instance =&gt; code is loaded and code outside the handler run (init) if the init is large, this process can take some time first request served by new instances has higher latency than the rest provisioned concurrency concurrency is allocated before the function is invoked (in advance) so the cold start never happens and all invocations have low latency application auto scaling can manage concurrency lambda external dependencies if your lambda function depends on external libraries for example AWS X-Ray SDK, database client, etc… you need to install the packages alongside your code and zip it together upload the zip straight to lambda if less than 50MB, else to S3 first and reference from S3 native libraries work: they need to be complied on Amazon Linux AWS SDK comes by default with every lambda function lambda and CloudFormation inline inline functions are very simple use the code.zipfile property you cannot include function dependencies with inline functions through S3 you must store the lambda zip in S3 you must refer the S3 zip location in the CloudFormation code S3 bucket S3 key: full path to zip S3 object version: if versioned bucket if you update the code in S3, but don’t update S3 bucket, S3 key or S3 object version, CloudFormation won’t update your function because it will not detect the change lambda layers externalize dependencies to re use them lambda container images deploy lambda function as container images of up to 10GB from ECR pack complex dependencies, large dependencies in a container base images are available can create your own image as long as it implements the lambda runtime API test the containers locally using the lambda runtime interface emulator unified workflow to build apps lambda versions and aliases lambda versions when you work on a lambda function, we work on $LATEST, which is an unpublished mutable version when we are ready to publish a lambda function, we create a version versions are immutable versions have increasing version numbers versions get their own ARN version = code + configuration each version of the lambda function can be accessed lambda aliases aliases are pointers to lambda function versions we can define a dev, test, prod aliases and have them point at different lambda versions aliases are mutable aliases enable Blue / Green deployment by assigning weights to lambda functions aliases enable stable configuration of our event triggers / destinations aliases have their own ARNs aliases cannot reference other aliases lambda and CodeDeploy CodeDeploy can help you automate traffic shift for lambda aliases feature is integrated within the SAM framework linear grow traffic every N minutes until 100% canary try X percent then 100% AllAtOnce immediate can create pre and post traffic hooks to check the health of the lambda function lambda limits good to know - per region memory allocation: 128MB - 10 GB maximum execution time: 15 minutes environment variables: 4KB disk capacity in the function container in /tmp: 512 MB concurrency executions: 1000 lambda function deployment size(zipped): 50MB size of uncompressed deployment(code + dependencies): 250MB can use the /tmp directory to load other files at startup lambda best practices perform heavy duty work outside of your function handler connect to databases initilize the SDK pull in dependencies use environment variables for database connection sttrings, S3 buckets, etc… passwords, sensitive values minimize your deployment package size to its runtime necessities break down the function remember lambda limits use Layers where necessary aviod using recursive code, never have a lambda function call itself DynamoDB NoSQL database non-relational databases and are distributed include MongoDB, DynamoDB… do not support query joins (or just limited support) all the data that is needed for a query is present in one row don’t perform aggregations such as SUM, AVG… scale horizontally there is no right or wrong for NoSQL or SQL, they just require to model the data differently and think about user queries differently Amazon DynamoDB fully managed, highly available with replication across multiple AZ NoSQL database scales to massive workloads, distributed database millions of requests per second, trillions of row, 100s of TB of storage fast and consistent in performance (low latency on retrieval) integrated with IAM for security, authorization and administration enables event driven programming with DynamoDB streams low cost and auto scaling capabilities basics DynamoDB is made of Tables each table has a Primary Key (must be decided at creation time) each table can have an infinite number of items each item has attributes (can be added over time - can be null) maximum size of an item is 400KB data types supported are: scalar types: String, Number, Binary, Boolean, Null Document types: List, Map Set Types: String Set, Number Set, Binary Set Primary keys Partition Key (HASH) partition key must be unique for each item partition key must be diverse so that the data is distributed Partition Key + Sort Key (HASH + RANGE) the combination must be unique for each item data is grouped by partition key Read / Write capacity modes control how you manage your table’s capacity provisioned mode (default) you specify the number of reads/ writes per second you need to plan capacity beforehand pay for provisioned read / write capacity units on demand mode read / writes automatically scale up / down with your workloads no capacity planning needed pay for what you use, more expensive you can switch between different modes once every 24 hours R/W capacity modes - provisioned table must have provisioned read an dwrite capacity units read capacity units (RCU) write capacity units option to setup auto scaling of throughput to meet demand throughput can be exceeded temporarily using brust capacity if burst capacity has been consumed, you will get a ProvisionedThroughpuutExceededException it is then advised to do an exponential backoff retry Write Capacity units (WCU) one WCU represents one write per second for an item up to 1KB in size if the items are larget then 1 KB, more WCUs are consumed Strongly consistent read vs Eventually consistent read Eventually consistent read (default) if we read just after a write, it is possible we will get some stale data because of replication Strongly consistent read if we read just after a write, we will get the correct data set ConsistentRead parameter to True in API calls consumes twice the RCU Read capacity units (RCU) one RCU represents one Strongly Consistent Read per second, or two Eventually consistent reads per second, for an item up to 4KB if the items are larger than 4KB, more RCUs are consumed Paritions Internal data is stored in partitions partition keys go through a hashing algorithm to know to which partition they go to WCUs and RCUs are spread evenly across partitions Throttling if we exceed provisioned RCUs or WCUs, we get ProvisionedThroughputExceededException reasons hot keys: one partition key is being read too many times (popular item) hot partitions very large items, remember RCU and WCU depends on size of items solutions exponential backoff when exception is encountered distribute partition keys as much as possible if RCU issue, we can use DynamoDB Accelerator (DAX) on demand Read and writes automatically scale up and down with your workloads no capacity planning needed unlimited WCU and RCU, no throttle, more expensive you are charged for reads and writes that you use in terms of RRU and WRU read request units (RRU) - throughput for reads (same as RCU) write request units (WRU) - throughput for writes (same as WCU) 2.5x more expensive than provisioned capacity use cases: unknown workloads, unpredictable application traffic… writing data PutItem creates a new item or fully replace an old item consumers WCUs UpdateItem edits an existing item’s attributes or adds a new item if it doesn’t exist can be used to implement Atomic Counters - a numeric attribute that is unconditionally incremented conditional writes accept a write / update / delete only if conditions are met, otherwise returns an error helps with concurrent access to items no performance impact reading data GetItem read based on primary key primary key can be HASH or HASH + RANGE eventually consistent read option to use strongly consistent reads (more RCU - might take longer) ProjectionExpression can be specified to retrieve only certain attributes reading data - query query returns items based on KeyConditionExpression partition key value - required sort key value = optional FilterExpression additional filtering after the query operation (before data returned to you) use only with non key attributes returns the number of items specified in limit or up to 1 MB of data ability to do pagination on the results can query table, a local secondary index, or a global secondary index reading data - scan scan the entire table and then filter out data (inefficient) returns up to 1 MB of data - use pagination to keep on reading consumes a lot of RCU limit impact using Limit or reduce the size of the result and pause for faster performance, use parallel scan multiple workers scan multiple data segments at the same time increases the throughput and RCU consumed limit the impact of parallel scans just like you would for Scans can use ProjectionExpression and FilterExpression filtering will be done at the client side (e.g. in the browser) deleting data DeleteItem delete an individual item ability to perform a conditional delete DeleteTable delete a whole table and all its items much quicker deletion than calling DeleteItem on all items batch operations allows you to save in latency by reducing the number of API calls operations are done in parallel for better efficiency part of a batch can fail, in which case we need to try again for the failed items BatchWriteItem up to 25 PutItem and DeleteItem in one call up to 16 MB of data written, up to 400KB of data per item can’t update items BatchGetItem return items from one or more tables up to 100 items, up to 16 MB of data items are retrieved in parallel to minimize latency Local Secondary Index (LSI) alternative sort key for your table (use the same partition key) the sort key consists of one scalar attribute up to 5 local secondary indexes per table must be defined at table creation time attribute projections - can contain some or all the attributes of the base table Global secondary index (GSI) alternative Primary key (HASH + HASH + RANGE) from the base table speed up queries on non key attributes the index key consists of scalar attributes attribute projections - some or all the attributes of the base table must provision RCUs and WCUs for the index can be added / modified after table creation indexes and throttling GSI if the writes are throttled on the GSI, then the main table will be throttled even if the WCU on the main tables are fine choose your GSI partition key carefully assign your WCU capacity carefully LSI uses the WCUs and RCUs of the main table no special throttling considerations Optimistic locking DynamoDB has a feature called Conditional Writes a strategy to ensure an item hasn’t changed before you update / delete it each item has an attribute that acts as a version number, and each update / delete request will change the value of the item, and also update the version number if two request send at the same time, only one will succeed because the second request will not try to change the item because the version is different already. DynamoDB DAX fully managed, highly available, seamless in memory cache for DynamoDB microseconds latency for cached reads and queries doesn’t require application logic modification solves the hot key problem (too many reads) 5 minutes TTL for cache (default) up to 10 nodes in the cluster multi AZ secure DAX vs ElastiCache DAX is for individual object cache and simple query and scan ElastiCache can store aggregation result and complex intermediate results DynamoDB Streams ordered stream of item level modifications in a table stream records can be sent to Kinesis Data Streams read by AWS lambda read by Kinesis Client Linrary applications data retention for up to 24 hours use case react to changes in real time analytics insert into derivative tables insert into ElasticSearch implement cross region replication ability to choose the information that will be written to the stream KEYS_ONLY - only the key attributes of the modified item NEW_IMAGE - the entire item, as it appears after it was modified OLD_IMGAE - the entire item, as it appeared before it was modified NEW_AND_OLD_IMAGES - both the new and old images of the item DynamoDB streams are made of shards, just like Kinesis Data Streams, so Kinesis KCL can be the consumer for DynamoDB Streams you don’t need to provision shards, this is automated by AWS records are not retroactively populated in a stream after enabling it Streams and lambda you need to define an Event Source Mapping to read from DynamoDB streams you need to ensure the lambda function has the appropriate permissions your lambda function is invoked synchronously DynamoDB TTL automatically delete items after an expiry timestamp doesn’t consume any WCUs the TTL attribute must be a number data type with Unix Epoch timestamp value expired items deleted within 48 hours of expiration expired items that haven’t been deleted, appears in reads/queries/scans (if you don’t want them, filter them out) expired items are deleted from both LSIs and GSIs a delete operation for each expired item enters the DynamoDB streams (can help recover expired items) use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, user sessions… DynamoDB CLI --projection-expression: one or more attributes to retrieve --filter-expression: filter items before returned to you general CLI pagination options --page-size: specify that CLI retrieves the full list of items but with a larger number of API calls instead of one API call --max-items: max number of items to show in the CLI (returns NextToken) --starting-token: specify the last NextToken to retrieve the next set of items DynamoDB transactions coordinated, all or nothing opeartions to multiple items across one or more tables provides Atomicity, Consistency, Isolation, and Durability (ACID) read modes - Eventual consistency, strong consistency, transactional write modes - standard, transactional consumers 2x WCUs and 2x RCUs two operations TransactGetItems - one or more GetItem operations TransactWriteItems - one or more PutItem, UpdateItem, DeleteItem operations use cases: financial transactions, managing orders, multi player games… DynamoDB Session State Cache it is common to use DynamoDB to store session state vs ElastiCache ElastiCache is in memory, but DynamoDB is serverless with auto scaling both are key value pairs vs EFS EFS must be attached to EC2 instances as a network drive vs EBS and Instance store EBS and Instance store can only be used for local caching, not shared caching vs S3 S3 is higher latency, and not meant for small objects DynamoDB Security and other features security VPC endpoints available to access DynamoDB without using the internet access fully controled by IAM encryption at rest using KMS and in transit using SSL/TLS backup and restore feature available point in time recovery (PITR) like RDS no performance impact global tables multi region, multi active, fully replicated, high performance, need to enable DynamoDB streams first DynamoDB local develop and test apps locally without accessing the DynamoDB web service (without internet) AWS database migration service can be used to migrate to DynamoDB Fine-Grained access control using web identity federation or cognito identity pools, each user gets AWS credentials you can assign an IAM role to these users with a condition to limit their API access to DynamoDB Leading Keys - limit row level access for users on the primary key Attributes - limit specific attributes the user can see API Gateway Integrations high level lambda function invoke lambda function easy way to expose REST API backed by lambda HTTP expose HTTP endpoints in the backend why? add rate limiting, caching, user authentications, API keys, etc… AWS service expose any API through the API Gateway example: Step function workflow, post a message to SQS why? add authentication, deploy publicly, rate control… endpoint types Edge-Optimized (default): for global clients requests are routed through the CloudFront Edge locations the API Gateway still lives in only one region regional for clients within the same region could manually combine with CloudFront (more control over the caching strategies and the distribution) private can only be accessed from your VPC using an interface VPC endpoint (ENI) use a resource policy to define access Deployment stages making changes in the API Gateway does not mean they are effective you need to make a deployment for them to be in effect changes are deployed to Stages (as many as you want) use the naming you like for stages (dev, test, prod) each stage has its own configuration parameters stages can be rolled back as a history of deployments is kept stage variables stage variables are like environment variables for API Gateway use them to change often changing configuration values they can be used in lambda function ARN HTTP endpoint parameter mapping templates use cases: configure HTTP endpoints your stages talk to pass configuration parameters to lambda through mapping templates stage variables are passed to the context object in lambda stage variables with lambda aliases we can create a stage variable to indicate the corresponding lambda alias our API gateway will automatically invoke the right lambda function canary deployment possibility to enable canary deployments for any stage choose the percentage of traffic the canary channel receives metrics and logs are separate (for better monitoring) possiblity to override stage variables for canary this is Blue / Green deployment with lambda and API gateway API Gateway - Integration types MOCK API Gateway returns a response without sending the request to the backend (for testing and dev purpose) HTTP / AWS you must configure both the integration request and integration response setup data mapping using mapping templates for the request and response AWS_PROXY (lambda proxy) incoming request from the client is the input to lambda the function is responsible for the logic of request / response no mapping template, headers, query string parameters HTTP_PROXY no mapping template the HTTP request is passed to the backend the HTTP response from the backend is forwarded by API gateway Mapping template mapping templates can be used to modify request / response rename and modify query string parameters modify body content add headers uses Velocity template language filter output results Mapping template: JSON to XML with SOAP SOAP API are XML based, whereas REST API are JSON based in this case, API gateway should extract data from the request: either path, payload or header build SOAP message based on request data (mapping template) call SOAP service and receive XML response transform XML response to desired format and respond to the user API Gateway Swagger / Open API spec common way of defining REST APIs, using API defintion as code import existing Swagger / OpenAPI 3.0 spec to API Gateway method method request integration request method response extensions for API Gateway and setup every single option can export current API as Swagger / OpenAPI spec swagger can be written in YAML or JSON Caching API response caching reduces the number of calls made to the backend default TTL is 300 seconds caches are defined per stage possible to override cache settings per method cache encryption option cache capacity between 0.5 to 237 GB cache is expensive, makes sense in production, may not make sense in dev and test API Gateway cache invalidation able to flush the entire cache immediately clients can invalidate the cache with header: Cache-Control: max-age=0 (with proper IAM authorization) if you don’t impose an InvalidateCache policy or choose the require authorization check box in the console, any client can invalidate the API cache, which is not good. Usage plan and API keys if you want to make an API available as an offering to your customers usage plan who can access one or more deployed API stages and methods how much and how fast they can access them uses API keys to identify API clients and meter access configure throttling limits and quota limits that are enforced on individual client API keys alphanumberic string values to distribute to your customers can use with usage plans to control access throttling limits are applied to API keys quotas limits is the overall number of maximum requests logging and tracing CloudWatch logs enable CloudWatch logging at the stage level can override settings on a per API basis log contains information about request / response body X-Ray enable tracing to get extra information about requests in API gateway X-Ray API Gateway + Lambda gives you the full picture CloudWatch metrics metrics are by stage, possiblity to enable detailed metrics CacheHitCount and CacheMissCount: efficiency of the cache Count: the total number of API requests in a given period IntegrationLatency: the time between when API Gateway relays a request to the backend and when receives a response from the backend Latency: the time between when API gateway receives a request from a client and when it returns a response to the client, the latency includes the integration latency and other API gateway overhead 4xx Error (client side) and 5xx error (server side) throttling account limit API gateway throttles requests at 10000 rps across all API soft limit that can be increased upon request in case of throttling = 429 too many requests can set stage limit and method limits to improve performance or you can define usage plans to throttle per customer just like lambda concurrency, one API that is overloaded, if not limited, can cause the other APIs to be throttled too. CORS CORS must be enabled when you receive API calls from another domain the OPTIONS pre flight request must contain the following headers Access-Control-Allow-Methods Access-Control-Allow-Headers Access-Control-Allow-Origin CORS can be enabled through the console Authentication and Authorization IAM great for users already within your AWS accounts + resource policy for cross account Custom Authorizer great for third party tokens very flexible in terms of what IAM policy is returned Cognito User Pool you manage your own user pool no need to write any custom code must implement authorization in the backend WebSocket API what is WebSocket two way interactive communication between a user’s browser and a server server can push information to the client this enables stateful application use cases WebSocket APIs are often used in real time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms works with AWS services (lambda, DynamoDB) or HTTP endpoints Routing incoming JSON messages are routed to different backend if no routes =&gt; send to default you request a route selection expression to select the field on JSON to route from the result is evaluated against the route keys available in your API gateway the route is then connceted to the backend you have setup through API gateway Architecture create a single interface for all the microservices in your company use API endpoints with various resources apply a simple domain name and SSL certificates can apply forwarding and transformation rules at the API gateway level SAM (serverless application model) framework for developing and deploying serverless applications all the configurations is YAML code generate complex CloudFormation from simple SAM YAML file supports anything from CLoudFormation only two commmands to deploy to AWS SAM can use CodeDeploy to deploy lambda functions SAM can help you to run lambda, API gateway, DynamoDB locally Recipe transform header indicates its SAM template Transform: write code AWS::Serverless::Function AWS::Serverless::Api AWS::Serverless::SimpleTable package and deploy aws cloudformation package / sam package aws cloudformation deploy / sam deploy SAM policy templates list of templates to apply permissions to your lambda functions important examples S3ReadPolicy: give read only permissions to objects in S3 SQSPollerPolicy: allows to poll an SQS queue DynamoDBCrudPolicy: CRUD = create read update delete 12345678910MyFunction: Type: &#39;AWS::Serverless::Function&#39; Properties: CodeUri: xxxxx Handler: xxxxxx Runtime: xxxxxx Policies: - SQSPollerPolicy: QueueName: !GetAtt MyQueue.QueueName SAM Sumary SAM is built on CloudFormation SAM requires the Transform and Resources sections commands to know sam build: fetch dependencies and create local deployment artifacts sam package: package and upload to Amazon S3, generate CloudFormation template sam deploy: deploy to CloudFormation SAM policy templates for easy IAM policy definition SAM is integrated with CodeDeploy to do deploy to lambda aliases Serverless Application Repository (SAR) managed repository for serverless applications the applications are packaged using SAM build and publish applications that can be re used by organizations can share publicly can share with specific accounts this prevents duplicate work, and just go straight to publishing application settings and behavior can be customized using Environment variables Cloud Development Kit (CDK) define your cloud infrastructure using a familiar language contains high level components called constructs the code is complied into a CloudFormation template (YAML / JSON) you can therefore deploy infrastructure and application runtime code togther great for lambda functions great for Docker Containers in ECS / EKS CDK vs SAM SAM serverless focused write your template declaratively in JSON or YAML great for quickly getting started with lambda leverages CloudFormation CDK all aws services write infra in a programming language leverages CloudFormation Cognito we want to give our users an identity so that they can interact with our application Cognito user pools sign in functionality for app users integrate with API gateway and ALB Cognito Identity Pool (federated identity) provide AWS credentials to users so they can access AWS resources directly integrate with Cognito user pools as an identity provider Cognito Sync Synchronize data from device to Cognito is deprecated and replaced by AppSync Cognito User Pools create a serverless database of user for your web and mobile apps simple login: username and password combination password reset email and phone number verification federated identities: users from Facebook, Google, SAML… feature: block users if their credentials are compromised elsewhere login send back a JSON web token (JWT) Cognito has a hosted authentication UI that you can add to your app to handle signup and signin workflows using the hosted UI, you have a foundation for integration with social logins, OIDC or SAML can customize with a custom logo and custom CSS Cognito Identity Pools get identities for users so they obtain temporary AWS credentials your identity pool can include public providers (login with Amazon, Facebook, Google, Apple) users in an Amazon Cognito user pool OpenID Connect Providers and SAML identity providers developer authenticated identities Cognito identity pools allow for unauthenticated (guest) access users can then access AWS service directly or through API gateway the IAM policies applied to the credentials are defined in Cognito they can be customized based on the user_id for fine grained control IAM roles default IAM roles for authenticated and guest users define rules to choose the role for each user based on the user’s ID you can partition your users’ access using policy variables IAM credentials are obtained by Cognito identity pools through STS the roles must have a trust policy of Cognito identity pools Cognito User Pools vs Cognito Identity Pools Cognito User Pool database of users for your web and mobile application allows to federate logins through public social identity provider, OIDC, SAML… can customize the hosted UI for authentication has triggers with AWS lamdba during the authentication flow Cognito identity pools obtain AWS credentials for your users users can login through public social, OIDC, SAML and Cognito User Pools users can be unauthenticated users are mapped to IAM roles and policies, can leverage policy variables CUP + CIP = manage users / password + access AWS services Cognito Sync Deprecated - use AWS AppSync now store preferences, configuration, state of app cross device synchronization offline capability store data in datasets push sync: silently notify across all devices when identity data changes Cognito Stream: stream data from Cognito into Kinesis Cognito Events: execute lambda functions in response to events Step Functions model your workflows as state machines (one per workflow) order fulfillment, data processing web applications, any workflow written in JSON visualization of the workflow and the execution of the workflow, as well as history start workflow with SDK call, API gateway, eventbridge task states do some work in your state machine invoke one service can invoke a lambda function run an batch job run an ECS task and wait for it to complete insert an item from DynamoDB publish message to SNS, SQS launch another step function workflow run an activity EC2, Amazon ECS, on premises activities poll the step functions for work activities send result back to step functions states choice state: test for a condition to send to a branch fail or succeed state: stop execution with failure or success pass state: simply pass its input to its output or inject some fixed data, without performing work wait state: provide a delay for a certain amount of time or until a specified time/date Map state: dynamically iterate steps parallel state: begin parallel branches of execution Error handling any state can encounter runtime errors for various reasons state machine definition issues task failtures transient issues use retry and catch in the state machine to handle the errors instead of inside the application code the state may report its own errors Retry evaluated from top to bottom ErrorEquals: match a specific kind of error IntervalSeconds: initial delay before retrying BackOffRate: multiple the delay after each retry MaxAttempts: default to 3, set to 0 for never retried When max attempts are reached, the catch kicks in Catch evaluated from top to bottom ErrorEquals: match a specific kind of error Next: state to send to ResultPath: a path that determines what input is sent to the state specified in the Next field ResultPath include the error in the input AppSync AppSync is a managed service that uses GraphQL GraphQL makes it easy for applications to get exactly the data they needed this includes combining data from one or more sources retrieve data in real time with WebSocket or MQTT on WebSocket for mobile apps: local data access and data Synchronization it all starts with uploading one GraphQL schema Security there are four ways you can authorize applications to interact with your AppSync GraphQL API API KEY IAM OPENID_CONNECT COGNITO USER POOLS for custom domain and HTTPS, use CloudFront in front of AppSync STS (security Token service) Allows to grant limited and temporary access to AWS resources AssumeRole: assume roles within your account or cross account AssumeRoleWithSAML: return credentials for users logged in with SAML AssumeRoleWithWebIdentity return credentials for users logged with an IDP AWS recommends against using this, and using Cognito User Pools instead GetSessionToken: For MFA, from a user or account root user GetFederationToken: obtain temporary credentials for a federated user GetCallerIdentity: return details about the IAM user or role used in the API call DecodeAuthorizationMessage: decode error message when an AWS API is called using STS to assume a role define an IAM role within your account or cross account define which principals can access this IAM role user STS to retrieve credentials and impersonate the IAM role you have access to temporary credentials can be valid between 15 minutes to 1 hour STS with MFA use GetSessionToken from STS appropriate IAM policy using IAM conditions aws:MultiFactorAuthPresent:true GetSessionToken returns access ID secret key session token expiration date Advanced IAM IAM policies and S3 Bucket policies IAM policies are attached to users, roles and groups S3 bucket policies are attached to buckets when evaluating if an IAM principal can perform an operation X on a bucket, the union of its assigned IAM policies and S3 bucket policies will be evaluated at the same time. Dynamic policies with IAM how do you assign each user access to their own foler in S3 bucket? create one dynamic policy with IAM leverage the special policy variable $&#123;aws:username&#125; inline vs managed policies AWS managed policy maintained by AWS good for power users and administrators updated in case of new services and new APIs customer managed policy best practice, re usable, can be applied to many principals version controlled + rollback, central change management inline strict one to one relationship between policy and principal policy is deleted if you delete the IAM principal granting a user permissions to pass a role to an AWS service to configure many services, you must pass an IAM role to the service the service will later assume the role and perform actions for this, you need the IAM permission iam:PassRole it often comes with iam:GetRole to view the role being passed can a role be passed to any service? no: roles can only be passed to what their trust allows a trust policy for the role that allows the service to assume the role Directory service - overview AWS managed Microsoft AD create your own AD in AWS, manage users locally, supports MFA establish trust connections with your on permise AD AD connector directory gateway to redirect to on premises AD users are managed on the on premises AD only Simple AD AD compatible managed directory on AWS cannot be joined with on premises AD KMS Encryption Encryption in flight data is encrypted before sending and decrypted after receiving SSL certificate help with encryption encryption in flight ensures no MITM can happen server side encryption at rest data is encrypted after being received by the server data is decrypted before being sent it is stored in an encrypted form thanks to a key the encryption / decryption keys must be managed somewhere and the server must have access to it Client side encryption data is encrypted by the client and never decrypted by the server data will be decrypted by a receiving client the server should not be able to decrypt the data could leverage Envelop encryption AWS KMS fully integrated with IAM for authorization seamlessly integrated into EBS S3 RedShift RDS SSM but you can also use CLI / SDK the value in KMS is the CMK used to encrypt data can never be retrieved by user, and the CMK can be rotated for extra security KMS can only help in encryping up to 4KB of data per call, if data &gt; 4KB, we need to use Envelope encryption to give access to KMS to someone make sure the key policy allows the user make sure the IAM policy allows the API calls CMK Types Symmetric first offering of KMS, single encryption key that is used to encrypt and decrypt AWS services that are integrated with KMS use Symmetric CMKs necessary for envelope encryption you never get access to the key uncrypted (must call KMS API to use) Asymmetric public and private key pair used for encrypt and decrypt the public key is downloadable, but you can’t access the private key unencrypted use case: encryption outside of AWS by users who can’t call the KMS API KMS key policies control access to KMS keys, similar to S3 bucket policies difference: you cannot control access without them default KMS key policy created if you don’t provide a specific key policy compelete access to the key to the root user, which means all IAM users can access the key gives access to the IAM policies to the KMS key custom KMS key policy define users, roles that can access the KMS key define who can administer the key helpful for cross account access of your KMS key copying snapshots across accounts create a snapshot, encrypted with your own CMK attach a KMS key policy to authorize cross account access share the encrypted snapshot create a copy of the snapshot, encrypt it with a KMS key in your account create a volume from the snapshot Envelope encryption KMS encrypt API call has limit of 4kb if you want to encrypt &gt; 4KB, we need to user envelope encryption the main API that will help us is the GenerateDataKey API steps Encryption call GenerateDataKey API to get the plaintext date key and encrypted data key (encrypted using your CMK) encrypt the big file using the plaintext data key on your local machine (client side) create an envelope includes the encrypted date key and the encrypted big file decryption call Decrypt API, send the encrypted data key to KMS to decrypt using your own CMK plaintext data key will be returned use the plaintext data key to decrypt your encrypted big file. Encryption SDK the Encryption SDK implemented envelope encryption for us the encryption SDK also exists as a CLI tool we can install feature - data key caching re use data keys instead of creating new data keys for each encryption helps with reducing the number of API calls to KMS with a security trade off KMS symmetric - API summary encrypt: up to 4KB GenerateDataKey: generates a unique symmetric data key returns a plaintext copy of the data key and a copy that is encrypted under the CMK that you specify decrypt: decrypt up to 4KB of data (including data encryption keys) GenerateRamdom: returns a random byte string Quota limits when you exceed a request quota, you get a ThrottlingException to respond, use exponential backoff for crytographic operations, they share the same quota this includes requests made by AWS on your behalf for GenerateDataKey, consider using DEK caching from the encryption SDK you can also request quotas increase through AWS support SSE-KMS deep dive SSE-KMS leverages the GenerateDataKey and Decrypt KMS API calls these KMS API calls will show up in CloudTrail, helpful for logging to perform SSE-KMS, you need a KMS key policy that authorize the user / role (so we could use the key) an IAM policy that authorizes access to KMS (so we could access the AWS KMS service) otherwise you will get an access denied error S3 calls to KMS for SSE-KMS count against your KMS limits if throttling, try exponential backoff or request an increase in KMS limits S3 bucket policies - force SSL to force SSL, create an S3 bucket policy with a DENY on the condition aws:SecureTransport=false S3 bucket policy - force encryption of SSE-KMS deny incorrect encryption header: make sure it includes aws:kms deny no encryption header to ensure objects are not uploaded un encrypted we could also use S3 default encryption of SSE-KMS, in this case, we don’t need the second policy. S3 bucket key for SSE-KMS encryption we could enable S3 bucket key to reduce the API calls to KMS directly the key is used to encrypt kMS objects with new data keys using envelope encryption you will see less KMS cloudtrail events SSM Parameter Store secure storage for configuration and secrets optional seamless encryption using KMS serverless, scalable, durable, easy sdk version tracking of configurations / secrets configuration management using path and IAM notifications with CloudWatch events integration with CloudFormation Parameter policies allow to assign a TTL to a parameter to force updating or deleting sensitive data can assign multiple policies at a time Secrets Manager Newer service, meant for storing secrets capability to force rotation of secrets every X days automate generation of secrets on rotation using lambda function integration with RDS secrets are encrypted using KMS mostly meant for RDS integration SSM Parameter store vs secrets manager secrets manager automatic rotation of secrets with lambda lambda function is provided for RDS, Redshift… KMS encryption is mandatory SSM parameter store simple API no secret rotation (can be implemented using CloudWatch events and lambda) KMS encryption is optional can pull a secrets manager secrets using the SSM parameter Store API CloudWatch logs - encryption you can encrypt CloudWatch logs with KMS keys encryption is enabled at the log group level, by associating a CMK with a log group, either when you create the log group or after it exists you cannot associate a CMK with a log group using the CloudWatch console, have to use CLI you must use the CloudWatch logs API associate-kms-key: if the log group already exists create-log-group: if the log group doesn’t exist yet ACM (AWS certificate manager) provision, manage, and deploy SSL / TLS certificates used to provide in flight encryption for websites supports both public and private TLS certificates free of charge for public TLS certificates automatic TLS certificate renewal integration with ELB CloudFront APIs on API Gateway","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"AWS SAA Review","slug":"AWS-SAA-Review","date":"2021-08-09T06:32:59.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2021/08/09/AWS-SAA-Review/","link":"","permalink":"http://hellcy.github.io/2021/08/09/AWS-SAA-Review/","excerpt":"","text":"I PASSED! View My Certificate Getting Started AWS Regions AWS has regions all aroung the world Names can be us-east-1, eu-west-3… A region is a cluster of data centers Most AWS services are regoin-scoped How to choose an AWS Region? Compliance: with data governance and legal requirements, data never leaves a region without your explicit permission Proximity to customers: reduced latency Available services within a region: new services and new features aren’t available in every region Pricing: pricing varies region to region and is transparent in the service pricing page AWS Availability Zones Each region has many AZs example: ap-southeast-2a ap-southeast-2b ap-southeast-2c Each AZ is one or more discrete data centers with redundant power, networking and connectivity they are separate from each other, so that they are isolated from disasters they are connected with high bandwidth, ultra low latency networking AWS Points of Presence (Edge locations) Content is delivered to end users with lower latency IAM and AWS CLI IAM Identity and Access Management, Global service Root Account: created by default, shouldn’t be used or shared Users are people within your organization, and can be grouped Groups only contain users, not other groups Users don’t have to belong to a group, and user can belong to multiple groups IAM Permissions Users or Groups can be assigned JSON documents called policies These policies define the permissions of the users In AWS you apply the least privilege principle, don’t give more permissions than a user needs IAM Policies Structure Consist of Version: policy language version, always include ‘2012-10-17’ Id: an identifier for the policy (optional) Statement: one or more individual statements (required) Statements consists of Sid: an identifier for the statement (optional) Effect: whether the statement allows or denies access (Allow, Deny) Principal: account/user/role to which this policy applied to Action: list of actions this policy allows or denies Resource: list of resources to which the actions applied to Condition: conditions for when this policy is in effect (optional) How can users access AWS? AWS Management Console: protected by password + MFA AWS Command Line Interface: protected by access keys AWS SDK: for code, protected by access keys Access key ID = username Secret access key = password IAM Roles for services Some AWS service will need to perform actions on your behalf we will assign permissions to AWS services with IAM Roles Common roles: EC2 instance roles lambda function roles roles for CloudFormation IAM Security Tools IAM credentials report (account-level) a report that lists all your account’s users and the status of their various credentials IAM Access Advisor (user-level) Access Advisor shows the service permissions granted to a user and when those services were last accessed you can use this information to revise your policies IAM Guidelines and Best practices Don’t user root account except for AWS account setup One physical user = one AWS user assign users to groups and assign permissions to groups create a strong password policy use and enforce the use of MFA create and use roles for giving permissions to AWS services use access keys for CLI and SDK Audit permissions of your account with the IAM Credential Report Never share IAM users and access keys EC2 EC2 is one of the most popular of AWS offering EC2 = Elastic Compute Cloud = Infrastructure as a Service It mainly consists in the capability of Renting virtual machines (EC2) Storing data on virtual drives (EBS) Distributing load across machines (ELB) Scaling the services using an auto-scaling group (ASG) Knowing EC2 is fundamental to understand how to Cloud works EC2 Instance Types - Overview you can use different types of EC2 instances that are optimised for different use cases e.g. m5.2xlarge m: instance class 5: generation 2xlarge: size within the instance class General Purpose Great for a diversity of workloads such as web servers or code repositories Balance between Compute Memory Networking Compute Optimized Great for compute intensive tasks that require high performance processors Batch processing workloads media transcoding high performance web servers high performance computing scientific modeling and machine learning dedicated gaming servers Memory Optimized Fast performance for workloads that process large data sets in memory high performance, relational/non-relational databases distributed web scale cache stores in memory databases optimized for BI applications performing real time processing of big unstructured data Storage Optimized great for storage intensive tasks that require high, sequential read and write access to large data sets on local storage high frequency online transaction processing systems relational and NoSQL databases cache for in memory databases data warehousing applications distributed file systems Security Groups the fundamental of network security in AWS they control how traffic is allowed into or out of our EC2 instance security groups only contain allow rules security groups rules can reference by IP or by security group (inbound/outbound rules) Good to know security groups can be attached to multiple instances and one instance can have multiple security groups attach to it security group are locked down to a region and VPC security group live outside the EC2, if traffic is blocked, the EC2 instacne won’t see it (doesn’t know it tried to get in) if your application is not accessible (time out), then its a security group issue if your application gives a connection failed error, then its an application error or its not launched all inbound traffic is blocked by default all outbound traffic is authorized by default Classic Ports to know 22 = SSH (Secure Shell) - log into a Linux instance 21 = FTP (File Transfer Protocol) - upload files into a file share 22 = SFTP (Secure File Transfer Protocol) - upload files using SSH 80 = HTTP - access unsecured websites 443 = HTTPS - access secured websites 3389 = RDP (Remote Desktop Protocol) - log into a Windows instance EC2 Instances purchasing options on-demand instance: short workload, predictable pricing reserved, minimum 1 year: reserved instances: long workloads convertible reserved instances: long workloads with flexible instances scheduled reserved instances: example - every Thursday between 3 and 6 pm Spot instance: short workloads, cheap and can lose instances (less reliable) dedicated hosts: book an entire physical server, control instance placement EC2 on demand pay for what you use linux - billing per second, after the first minute all other os - billing per hour has the highest cost but no upfront payment no long-term commitment recommended for short term and un-interrupted workloads, where you can’t predict how the application will behave. EC2 reserved instances up to 75% discount compared to on demand reservation period: 1 year or 3 year2 purchasing options: no upfront / partial upfront / all upfront reserve a specific instance type recommended for steady state usage applications (think database) Convertible reserved instance can change the EC2 instance type up to 54% discount scheduled reserved instances launch within time window you reserve when you require a fraction of day / week / month still commitment over 1 to 3 years EC2 Spot instance can get a discount of up to 90% compared to on demand instances that you can lose at any point of time if your max price is less than the current spot price the MOST cost efficient instances in AWS useful for workloads that are resilient to failure not suitable for critical jobs or databases Spot instance requests define max spot price and get the instance while current spot price &lt; max the hourly spot price varies based on offer and capacity if the current spot price &gt; your max price you can choose to stop or terminate your instance with a 2 minutes grace period other strategy: spot block block spot instance during a specified time frame (1 to 6 hours) without interruptions in rare situations, the instance may be reclaimed cancel the spot instance request before terminate the spot instances Spot fleets spot fleets = set of spot instances + (optional) on demand instances the spot fleet will try to meet the target capacity with price constraints define possible launch pools: instance type, OS, AZ can have multiple launch pools, so that the fleet can choose spot fleet stops launching instnaces when reaching capacity or max cost strategies to allocate spot instances lowest price: from the pool with the lowest price (cost optimization, short workload) diversified: distributed across all pools (great for availability, long workloads) capacity optimized: pool with the optimal capacity for the number of instances spot fleets allow us to automatically request spot instance with the lowest price EC2 dedicated hosts an Amazon EC2 dedicated host is a physical server with EC2 instance capacity fully dedicated to your use, dedicated hosts can help you address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses allocated for your account for a 3 year period reservation more expensive useful for software that have complicated licensing model (BYOL - bring your own license) or for companies that have strong regulatory or compliance needs Elastic IP when you stop and then start an EC2 instance, it can change its public IP if you need to have a fixed public IP for your instance, you need an Elastic IP an Elastic IP is a public IPv4 IP you own as long as you don’t delete it you can attach it to one instance at a time with an Elastic IP, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account (not common) you can only have 5 Elastic IP in your account Overall, try to avoid using Elastic IP they often reflect poor architectrual decisions instead, use a random public IP and register a DNS name to it use a load balancer and don’t use a public IP Placement Groups Sometimes you want control over the EC2 instance placement strategy when you create a placement group, you specify one of the following strategies for the group Cluster: clusters instances into a low latency group in a single AZ Spread: spreads instances across underlying hardware (max 7 instances per group per AZ) - critical applications partition: spreads instances across many different partitions (which rely on different sets of racks) within an AZ, Scales to 100 of EC2 instances per group (Hadoop, Cassandra, Kafka), the instances in a partition do not share racks with the instances in the other partitions, a partition failure can affect many EC2 but won’t affect other partitions Elastic Network Interfaces (ENI) logical component in a VPC that represents a virtual network card the ENI can have the following attributes Primary private IPv4, one or more secondary IPv4 One Elastic IP per private IPv4 one public IPv4 one or more security groups a MAC address you can create ENI independently and attach them on the fly on EC2 instances for failover bound to a specific AZ EC2 Hibernate Stop: the data on disk (EBS) is kept intact in the next start Terminate: any EBS volums (root) also setup to be destroyed is lost First start: the OS boots and the EC2 user data script is run Following starts: the OS boots up the your application starts, caches get warmed up and that can take time Introducing EC2 Hibernate RAM state is preserved the instance boot is much faster (the OS is not stopped / restarted) under the hood: the RAM state is written to a file in the root EBS volume the root EBS volume must be encrypted Use cases long running process saving the RAM state servcies that take time to initialize EC2 Nitro underlying platform for the next generation of EC2 instances new virtualization technology allows for better performance better networking options Higher Speed EBS better underlying security EC2 vCPU EC2 instance comes with a combination of RAM and vCPU in some cases, you may want to change the vCPU options change the number of CPU cores chagne the number of vCPUs (threads) per core only specified during instance launch EC2 capacity reservations ensure you have EC2 capacity when needed manual or planned end date for the reservation no need for 1 or 3 year commitment capacity access is immediate, you get billed as soon as it starts combine with reserved instances and savings plans to do cost saving EC2 EBS an EBS volume is a network drive you can attach to your instances while they run it allows your instances to persist data, even after their termination they can only be mounted to one instance at a time (in some cases, some EBS can be attached to multiple EC2 instances at same time) they are bound to a specific AZ think of them as a network USB stick EBS volume its a network drive it uses the network to communicate the instance, which means there might be a bit of latency it can be detached from an EC2 instance and attached to another one quickly its locked down to an AZ an EBS volume in us-east-1a cannot be attached to an instance in us-east-1b to move a volume across, you first need to snapshot it have a provisioned capacity you get billed for all the provisioned capacity you can increase the capacity of the drive over time EBS volume types gp2 / gp3: general purpose SSD gp3: newer generation, IOPS and throughput are independet gp2: IOPS and throughput are linked io1 / io2: highest performance SSD st1: low cost HDD, for throughput intensive workloads sc1: lowest cost HDD EBS volumes are characterized in Size / Throughput / IOPS EBS multi-attach - io1/io2 family attach the same EBS volume to multiple EC2 instances in the same AZ each instance has full read and write permissions to the volume applications must manage concurrent write operations must use a file system thta’s cluster-aware EBS encryption when you create an encrypted EBS volume, you get the following data at rest is encrypted inside the volume all the data in flight mobing between the instance and the volume is encrypted all snapshots are encrypted all volumes created from the snapshot are encryped encryption and decryption are handled transparently encryption has a minimal impact on latency EBS encryption leverages keys from KMS (AES-256) copying an unencrypted snapshot allows encryption EBS RAID RAID 0 increase performance combining 2 or more volumes and getting the total disk space and I/O but if one disk fails, all the data is failed using this, we can have a very big disk with a lot of IOPS RAID 1 increase fault tolerance mirroring a volume to another we have to send the data to two EBS volume at the same time (2 * network) EBS delete on termination controls the EBS behaviour when an EC2 instance terminates by default, the root EBS volume is delete (attribute enabled) by default, any other attached EBS volume is not deleted (attribute disabled) this can be controlled by the AWS console / CLI use case: preseve root volume when instance is terminated (disable the attribute) EBS Snapshots make a backup (snapshot) of your EBS volume at a point of time not necessary to detach volume to do snapshot, but recommended can copy snapshots across AZ or Region can create volume from snapshot EFS - Elastic File System Managed NFS (network file system) that can be mounted on many EC2 EFS works with EC2 instances in multi-AZ highly avialble, scalable, expensive, pay per use uses security group to control access to EFS compatible with Linux based AMI (not Windows) encryption at rest using KMS to mount EFS to EC2, you need to add EC2 security group as a inbound rule in EFS security group Performance and Storage Class Performance mode general purpose MAX I/O Throughput mode bursting provisioned Storage tiers standard infrequent access, cost to retrieve files, lower price to store AMI Overview Amazon Machien Image a customization of an EC2 instance you add your own software, configuration, operating system etc… faster boot / configuration time because all your software is pre-packaged AMI are built for a specific region and can be copied across regions you can launch EC2 instance from public AMI: provided by AWS your own AMI: you make and maintain them yourself AWS marketplace AMI: an AMI someone else made EC2 instance store EBS volumes are network drives with good but limited performance if you need a high performance hardware disk, use EC2 instance store better I/O performance EC2 instance store lose their storage if they are stopped (ephemeral) good for buffer / cache / scratch data / temporary content risk of data loss if hardware fails backups and replication are your responsibility EC2 Metadata AWS EC2 instance metadata is powerful but one of the least known features to developers it allows EC2 instance to learn about themselves without using an IAM role for that purpose the URL is http://169.254.169.254/latest/meta-data you can retrieve the IAM role name from the metadata, but you CANNOT retrieve the IAM policy Elastic Load Balancer What is load balancing? load balancers are servers that forward internet traffic to multiple servers (EC2 instances) downstream Why use a load balancer? Spread load across multiple downstream instances expose a single point of access (DNS) to your application seamlessly handle failures of downstream instances do regular health checks to your instances provide SSL termination (HTTPS) for your websites enforce stickness with cookies high availability across zones separate public traffic from private traffic An ELB is a managed load balancer AWS guarantees that it will be working AWS takes care of upgrades, maintenance, high availability AWS provides only a few configuration knobs it costs less to setup your own load balancer but it will be a lot more effort on your end it is integrated with many AWS offering / services Health Checks Health Checks are crucial for load balancers they enable the load balancer to know if instances it forwards traffic to are available to reply to requests the health check is done on a port and a route (/health is common) if the response is not 200, then the instance is unhealthy Classic Load Balanceers (v1) supports TCP (layer 4), HTTP and HTTPS (layer 7) health checks are TCP or HTTP based Application Load Balancer (v2) Application load balancer is layer 7 (HTTP) load balancing to multiple HTTP applications across machines (target groups) load balancing to multiple applications on the same machine (containers) support for HTTP/2 and WebSocket Support redirects (from HTTP to HTTPS) Routing tables to differnt target groups routing based on path in URL (example.com/users &amp; example.com/posts) routing based on hostname in URL (one.example.com &amp; other.example.com) routing based on query string, headers (example.com/users?id=123&amp;other=false) ALB are a great fit for micro services and container based application (Docker and Amazon ECS) Has a port mapping feature to redirect to a dynamic port in ECS in comparison, we would need multiple CLB, one for each application Target Groups EC2 instances can be managed by an Auto Scaling Group - HTTP ECS tasks (managed by ECS itself) - HTTP Lambda function - HTTP request is translated into a JSON event IP addresses - must be private IPs ALB can route to multiple target groups health checks are at the target group level Network Load Balancer (v2) network load balancer (layer 4) forward TCP and UDP traffic to your instance handle millions of request per second less latency ~ 100 ms (vs 400 ms for ALB) NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in AWS free tier Sticky Sessions (Session Affinity) it is possible to implement stickness so that the same client is always redirected to the same instance behind a load balancer this works for CLB and ALB the cookie used for stickness has an expiration date you control use case: make sure the user doesn’t lost his session data enabling stickness may bring imbalance to the load over the backend EC2 instances Application based cookies custom cookie generated by the target can include any custom attributes required by the application cookie name must be specified individually for each target group don’t use AWSALB, AWSALBAPP, AWSALBTG (reserved for use by the ELB) application cookie generated by the load balancer cookie name is AWSALBAPP Duration based cookie cookie generated by the load balancer cookie name is AWSALB for ALB, AWSELB for CLB Cross Zone Load Balancing each load balancer instance distribute evenly across all registered instances in all AZ ALB always on (can’t be disabled) no charges for inter AZ data NLB disabled by default you pay charges for inter AZ data if enabled CLB Through console =&gt; enabled by default through CLI / API =&gt; disabled by default no charges SSL/TLS an SSL certificate allows traffic between your clients and your load balancer to be encrypted in transit SSL refers to Secure Sockets Layer, used to encrypt connections TLS refers to Transport Layer Security, which is a newer version TLS certificate are mainly used, but people still refer as SSL public SSL certificates are issued by Certificate Authorities (CA) SSL certificates have an expiration date and must be renewed the load balancer uses an X.509 certificate (SSL/TLS server certificate) you can manage certificates using ACM (AWS Certificate Manager) You can create upload your own certificate HTTPS listner you must specify a default certificate you can add an optional list of certs to support multiple domains clients can use SNI (Server Name Indication) to specify the host name they reach ability to specify a security policy to support older version of SSL/TLS SSL - Server Name Indication SNI solves the problem of loading multiple SSL certificate onto one web server (to serve multiple website) its newer protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake the server will then find the correct certificate, or return the default one Only works for ALB and NLB, CloudFront doesn’t work for CLB ELB Connection Draining Time to complete in-flight requests while the instance is de-registering or unhealthy stops sending new requests to the instance which is de-registering between 1 to 3600 seconds, default is 300 seconds can be disabled (set to zero) set to a low value if your requests are short Auto Scaling Group in real life, the load on your websites and application can change in the cloud, you can create and get rid of servers very quickly the goal of an Auto Scaling Group is to scale out to match an increased load scale in to match an decreased load ensure we have a minimum and maximum number of machines running automatically register new instances to a load balancer ASG attributes A launch configuration AMI + instance type EC2 user data EBS volumes Security groups SSH key pair min size / max size / initial capacity network + subnets information load balancer information scaling policies Auto Scaling Alarms it is possible to scale an ASG based on CloudWatch alarms an alarm monitors a metric (such as average CPU) metrics are computed for the overall ASG instances Auto Scaling New Rules it is now possible to define better auto scaling rules that are directly managed by EC2 target average CPU usage number of requests on the ELB per instance average network in average network out these rules are easier to setup and can make more sense Auto Scaling Custom Metric send custom metric from application on EC2 to CloudWatch Create CloudWatch alarm to react to low / high values use the CloudWatch alarm as the scaling policy for ASG Good to know scaling policies can be on CPU, network… and can even be on custom metrics or based on a schedule ASGs use launch configurations or launch templates to update an ASG, you must provide a new launch configuration / launch template IAM roles attached to an ASG will get assigned to EC2 instances ASG are free, you pay for the underlying resources being launched having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create a new one as a replacement. ASG can terminate instances marked as unhealthy by an ELB (and then replace them) Auto Scaling Groups - Dynamic Scaling Policies target tracking scaling most simple and easy to setup example: I want to average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggered (example: CPU &gt; 70%), then add 2 units when a CloudWatch alarm is triggered (example: CPU &lt; 30%), then remove 1 unit Scheduled Actions anticipate a scaling based on known usage patterns example: increase the min capacity to 10 at 5pm on Fridays predictive scaling continuously forecast load and schedule scaling ahead Good metrics to scale on CPU Utilization average CPU utilization across your instances Request Count Per Target to make sure the number of requests per EC2 instances is stable Average network in / out if your application is network bound (heavy downloads / uploads) Any custom metric that you push using CloudWatch Scaling Cooldowns After a scaling activity happens, you are in the cooldown period (default 300 seconds) during the cooldown period the ASG will not launch or terminate additional instances (to allow for metrics to stablize) advice: use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period ASG default termination policy Find the AZ which has the most number of instances if there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration ASG tries to balance the number of instances across AZ by default ASG lifecycle hooks by default as soon as an instance is launched in an ASG its inservice you have the ability to perform extra steps before the instance goes in service (pending state) you have the ability to perform extra actions before the instance is terminated (terminating state), like extract logs, tools etc… ASG launch template vs Launch configuration both ID of the AMI, the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances Launch Configuration must be re-created every time launch template can have multiple versions create parameters subsets (partial configuration for re-use and inheritance) provision using both on demand and stop instances can use T2 unlimited burst feature recommended by AWS going forward RDS RDS stands for relational database service its a managed DB service for DB use SQL as a query language it allows you to create databases in the cloud that are managed by AWS Postgres MySQL MariaDB Oracle Microsoft SQL Server Aurora (AWS Proprietary database) Advantage over using RDS vs deploying DB on EC2 RDS is a managed service automated provisioning, OS patching continuous backups and restore to specific timestamp (point in time restore) monitoring dashboards read replicas for improved read performance multi AZ setup for DR (Disaster Recovery) maintenance windows for upgrades scaling capability (vertical or horizontal) storage backed by EBS but you can’t SSH into your RDS instance (its managed by AWS) RDS backups backups are automatically enabled in RDS automatically backups daily full backup of the database (during the maintenance windows) transaction logs are backed up by RDS every 5 minutes ability to restore to any point in time (from oldest backup to 5 minutes ago) 7 days retention (can be increased to 35 days) DB snapshots manually triggered by the user retention of backup for as long as you want RDS storage auto scaling helps you increase storage on your RDS DB instance dynamically when RDS detects you are running out of free database storage, it scales automatically avoid manually scaling your database storage you have to set maximum storeage threshold (maximum limit for DB storage) automatically modify storage if free storage is less than 10% of allocated storage low storage lasts at least 5 minuts 6 hours have passed since last modification useful for applications with unpredicatable workloads supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL server, Oracle) Read Replicas for read scalability up to 5 read replicas within AZ cross AZ cross region replication is ASYNC, so reads are eventually consisent, possible to read old data replicas can be promoted to their own DB applications must update the connection string to leverage read replicas Read replicas - use case you have a production database that is taking on normal load you want to run a reporting application to run some analytics you create a read replica to run the new workload there the production application is unaffected read replicas are used for SELECT only kind of operations (not DELETE, INSERT, UPDATE) Network cost in AWS there is a network cost when data goes from one AZ to another for RDS read replicas within the same region, you don’t pay that fee for read replicas across regions, you need to pay Multi AZ disaster recovery SYNC replication one DNS name - automatic app failaover to standby increase availability failover in case of loss of AZ, loss of network, instance or storage failure no manual intervention in apps not used for scaling (not handle traffic, only take over when master RDS fail) NOTE: the read replicas can be setup as Multi AZ for disaster recovery RDS - from single AZ to multi AZ zero downtime operation (no need to stop the DB) just click on modify for the database the following happens internally a snapshot is taken a new DB is restored from the snapshot in a new AZ synchronization is established between the two databases RDS security - encryption at rest possibility to encrypt the master and read replicas with AWS KMS - AES-256 encryption encryption has to be defined at launch time if the master is not encrypted, the read replica cannot be encrypted Transparent Data Encryption (TDE) available for Oracle and SQL server in flight encryption SSL certificate to encrypt data to RDS in flight provide SSL options with trust certificate when connecting to database Encryption operations Encrypting RDS backups snapshots of un-encrypted RDS databases are un-encrypted snapshots of encrypted RDS database are encrypted can copy a snapshot into an encrypted one to encrypt an un-encrypted RDS database create a snapshot of the un-encrypted database copy the snapshot and enable encryption for the snapshot restore the database from the encrypted snapshot migrate applications to the new database and delete the old database RDS security - network and IAM network security RDS database are usually deployed within a private subnet, not in a public one RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS access management IAM policies help control who can manage AWS RDS (through the RDS API) traditional username and password can be used to login into the database IAM based authentication can be used to login into RDS for MySQL and PostgreSQL RDS - IAM authentication IAM database authentication works with MySQL and PostgreSQL you don’t need a password, just an authentication token obtained through IAM and RDS API calls auth token has a lifetime of 15 minutes benefits network in / out must be encrypted using SSL IAM to centainly manage users instead of DB can leverage IAM roles and EC2 instance profiles for easy integration Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database) Aurora is AWS cloud optimized and claims 5x performance improvement over MySQL on RDS, over 3x performance of Postgres on RDS Aurora storage automatically grows from 10 GB to 64 TB Aurora can have 15 replicas while MySQL has up to 5, and the replication process is faster failover in Aurora is instantaneous Aurora costs more then RDS (20% more), but is more efficient Aurora High Availability and Read Scaling 6 copies of your data across 3 AZ 4 copies out of 6 needed for writes 3 copies out of 6 needed for reads self healing with peer to peer replication storage is striped across 100s of volumes one Aurora instance takes writes (master) automated failover for master in less than 30 seconds master + up to 15 Aurora read replicas serve reads support for cross region replication Aurora - Custom Endpoints define a subset of Aurora instances as a custom endpoint example: run analytical queries on specific replicas the reader endpoint is generally not used after defining custom endpoints Aurora serverless automated database instantiation and auto scaling based on actual usage good for infrequent intermittent or unpredictable workloads no capacity planning needed pay per second, can be more cost effective Aurora Multi-Master in case you want immediate failover for write node (HA) every node does Read and write - vs - promoting a read replica as the new master (faster failover) Amazon ElastiCache the same way RDS is to get managed relational databases elastiCache is to get managed Redis or Memcached caches are in memory databases with really high performance, low latency helps reduce load off databases for read intensive workloads helps make your application stateless AWS takes care of OS maintenance / patching, optimization, setup, configuration, monitoring, failure recovery and backups using ElastiCache involves heavy application code changes DB Cache applications queries ElastiCache, if not available, get from RDS and store in ElastiCache htlps relieve load in RDS Cache must have an invalidation strategy to make sure only the most current data is used in there (LRU, LFU) User session store user logs into any of the application the application wrties the session data into ElastiCache the user hits another instance of our application the instance retrieve the session data and the user is already logged in Redis Multi AZ witi auto faliover read replicas to scale reads and have High Availability data durability using AOF persistence backup and restore features Memcached multi-node for partitioning of data (sharding) no high availability (replication) non persistent no backup and restore multi-threaded architecture Patterns for ElastiCache Lazy loading all the read data is cached, data can become stale in cache write through adds or update data in the cache when written to a DB (no stale data) session store: store temporary session data in a cache (using TTL features) Reids use case Gaming leaderboard Redis Sorted sets guarantee both uniqueness and element ordering each time a new element added, its ranked in real time, then added in correct order Route 53 route 53 is a managed DNS (domain name system) DNS is a collection of rules and records which helps clients understand how to reach a server through its domain name in AWS, the most common records are A: hostname =&gt; IPv4 AAAA: hostname =&gt; IPv6 CNAME: hostname =&gt; hostname Alias: hostname =&gt; AWS resource Overview route 53 can use public domain names you own private domain names that can be resolved by your instances in your VPCs Route 53 has advanced features such as load balancing (through DNS, also called client load balancing) health checks routing policy: simple, failover, geolocation, latency, weighted… you pay $0.5 per month per hosted zone user will first send DNS request (http://myapp.mydomain.com) to route 53 asking for IP address route 53 will response will the ip address of that DNS and a TTL user browser will then send the HTTP request to the correct IP to reach the server next time when the user send DNS request, if the last request is not expire (check the TTL), browser will directly go to the last saved IP address, save traffic for route 53 CNAME vs Alias AWS resource (load balancer, cloudfront…) expose an AWS hostname and you want myapp.mydomain.com CNAME points a hostname to any other hostname only for non root domain (something.mydomain.com) Alias points a hostname to an AWS resource works for root domain and non root domain (mydomain.com) free of charge native health check Simple Routing policy use when you need to redirect to a single resource you can’t attach health checks to simple routing policy if multiple values (IP addresses) are returned, a random one is chosen by the client (client side load balancing) Weighted routing policy control the percentage of the requests that go to specific endpoint helpful to test percentage of traffic on new app version for example helpful to split traffic between two regions can be associated with health checks but on the client side the browser is not aware that it has multiple weighted endpoints in the backend Latency routing policy redirect to the server that has the least latency close to user super helpful when latency of users is a priority latency is evaluated in terms of user to designated AWS region Germany may be redirected to the US (if that’s the lowest latency) Route 53 health checks have X health checks failed =&gt; unhealthy (default 3) have X health checks passed =&gt; healthy (default 3) default health checks interval : 30 seconds (can set to 10 seconds with higher cost) about 15 health checkers will the the endpoint health in the background (from different regions) one request every 2 seconds on average (30 / 15) can have HTTP, TCP, and HTTPS health checks (no SSL verification) possible to integrate health check with CloudWatch health checks can be linked to route 53 DNS queries GeoLocation routing policy different from latency based this is routing based on user location here we specify traffic from the UK should go to this specific IP should create a default policy (in case there is no match on location) Geoproximity routing policy route traffic to your resources based on the geographic location of users and resources ability to shift more traffic to resources based on the defined bias to change the size of the geographic region, specify bias values to expand (1 to 99) - more traffic to the resources to shrink (-1 to -99) - less traffic to the resources resources can be AWS resources (specify AWS region) non-AWS resources (specify latitude and longitude) you must use route 53 traffic flow (advanced) to use this feature Multi value routing policy use when routing traffic to multiple resources want to associate a route 53 health checks with records up to 8 healthy records are returned for each multi value query multi value is not a substitute for having an ELB client browser will randomly choose a healthy record from returned records (client side fault tolerance) Route 53 as a Registrar a domain name registrar is an organization that manages the reservation of internet domain names GoDaddy Google Domains Domain registrar != DNS if you buy your domain on 3rd party website, you can still use route 53 create a hosted zone in route 53 update NS records on 3rd party website to use route 53 name servers (all 4 of them) Classic Solutions Architecture Stateful App with shopping cart ELB sticky sessions web clients for storing cookies and making our web app stateless ElastiCache for storing sessions (alternative: DynamoDB) for caching data from RDS Multi AZ RDS for storing user data read replicas for scaling reads multi AZ for disaster recovery tight security with security groups referencing each other Instantiating Applications Quickly EC2 instances use a golden AMI: install your applications, OS dependencies, beforehand and launch your EC2 instance from the golden AMI bootstrap using user data: for dynamic configuration, use User Data scripts Hybrid: mix golden AMI and User Data (Elastic Beanstalk) RDS databases restore from a snapshot: the database will have schemas and data ready EBS volume: restore from a snapshot, the disk will already be formatted and have data Beanstalk Developer problems on AWS managing infrastructure deploying code configuring all the databases, load balancers, etc… scaling concerns most web apps have the same architecture (ALB + ASG) all the developers want is for their code to run possibly, consistently across different applications and environments Elastic Beanstalk - overview Elastic Beanstalk is a developer centric view of deploying an application on AWS it uses all the component’s we have seen before: EC2, ASG, ELB, RDS… managed service automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration… just the application code is the responsiblity of the developer we still have full control over the configuration Beanstalk is free but you pay for the underlying instances Elastic Beanstalk - components application collectioin of Elastic Beanstalk components (environments, versions, configurations…) application version an iteration of your application code environment collection of AWS resources running an application version (only one application version at a time) Tiers web server environment tier worker environment tier you can create multiple environments (dev, test, prod…) S3 Buckets Amazon S3 allows people to store objects in buckets buckets must have a globally unique name buckets are defined at the region level naming convention No uppercase no underscore 3-63 characters long not an ip must start with lowercase letter or number Objects objects (files) have a key the key is the FULL path s3:&quot;//my-bucket/my_folder/another_folder/my_file.txt key is: my_folder/another_folder/my_file.txt prefix is: my_folder/another_folder/ object name is: my_file.txt there is no conecpt of directories within buckets just keys with very long names that contain slashes object values are the content of the body max object size is 5TB if uploading more than 5GB, must use multi-part upload metadata (list of text key / value pairs, system or user metadata) Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle Version ID (if versioning is enabled) Versioning you can version your files in Amazon S3 it is enabled at the bucket level same key overwrite will increment the version it is best pratice to version your buckets protect against unintended deletes (ability to restore a version) easy roll back to previous version notes any file that is not versioned prior to enabling versioning will have version null suspending versioning does not delete the previous versions Encryption for objects SSE-S3: encrypts S3 object using keys handled and managed by AWS SSE-KMS: leverage AWS KMS service to manage encryption keys SSE-C: when you want to manage your own encryption keys client side encryption it is important to understand which ones are adapted to which situation for the exam S3 Default Encryption one way to force encryption is to use a bucket policy and refuse any API call to PUT an S3 object without encryption headers another way is to use the default encryption option in S3 note: bucket policies are evaluated before default encryption e.g. if you have a bucket policy to reject all un-encrypted files from being upload to S3, then you can’t upload un-encrypted file even if you have the default encryption enabled. SSE-C Amazon S3 does not store the encryption key you provide HTTPS must be used (because you need to send the key in the request) Encryption key must provided in HTTP headers, for every request Client side encryption client library such as the Amazon S3 Encryption client clients must encrypt the data themselves before sending to S3 clients must decrypt the data themselves when retrieving the data from S3 customer fully manages the keys and encryption cycle Encryption in transit (SSL/TLS) Amazon S3 exposes HTTP endpoint, non encrypted HTTPS endpoint, encryption in flight You are free to use the endpoint you want, but HTTPS is recommended most clients would use the HTTPS endpoint by default HTTPS is mandatory for SSE-C (because you need to send the key in the request) Security User based IAM policies: which API calls should be allowed for a specific user from IAM console Resource based bucket policies: bucket wide rules from the S3 console - allows cross account Object ACL: finer grain Bucket ACL: less common Note: an IAM principal can access an S3 object if the user IAM permissions allow it OR the resource policy ALLOW it AND there is no explicit DENY JSON based policies Resources: buckets and objects Actions: Set of API to Allow or Deny Effect: Allow / Deny Principal: the account or user to apply the policy to use S3 bucket for policy to Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (cross account) CORS An origin is a scheme, host, and port CORS means Cross-Origin Resource Sharing Web browser based machanism to allow requests to other origins while visiting the main origin the requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (Access-Control-Allow-Origin) if a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers you can allow for a specific origin or * for all origins S3 Access logs for audit purpose, you may want to log all access to S3 buckets any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket that data can be analyzed using data analysis tools later (Amazon Athena) S3 Replication (Cross-Region or Same Region) Must enable versioning in source and destination cross region replication same region replication buckets can be in different accounts copying is asynchronous must give proper IAM permissions to S3 After activating, only new objects are replicated (existing objects will not be replicated) For DELETE operations can replicate delete markers from source to target (optional setting) deletions with a version ID are not replicated (to avoid malicious deletes) there is not chaining of replication if bucket one has replicatio into bucket two, which has replicatioin into bucket three then objects created in bucket one are not replicated to bucket three S3 Pre-signed URL can generate pre-signed URLs using SDK or CLI valid for a default of 3600 seconds users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT S3 Storage Class Standard - General Purpose High durability of objects across multiple AZ 99.99% availability over a given year sustain 2 concurrent facility failure use case: big data analytics, mobile and gaming applications, content distribution… Standard - IA suitable for data that is less frequently accessed, but requires rapid access when needed 99.9% availability low cost compared to GP use cases: as a data store for disater recovery, backups One Zone - IA Same as IA but data is stored in a single AZ 99.5% availability low latency and high throughput performance supports SSL for data at transit and encryption at rest low cost compared to IA use cases: storing secondary backup copies of on-premise data, or storing data you can re-create Intelligent Tiering same low latency and high throughput performance of S3 standard small monthly monitoring and auto-tiering fee automatically moves objects between two access tiers based on changing access patterns resilient against events that impact an entire AZ Amazon Glacier low cost object storage meant for archiving / backup data is retained for the longer term (10+ years) alternative to on-premise magnetic tape storage cost per storage per month + retrieval cost each item in glacier is called archive archives are stored in Vaults Amazon Glacier and Glacier Deep Archive Amazon Glacier - 3 retrieval options expedited (1 to 5 minues) standard (3 to 5 hours) bulk (5 to 12 hours) minimum storage duration of 90 days Amazon Glacier Deep Archive Standard (12 hours) Bulk (48 hours) Minimum storage duration of 180 days S3 Lifecycle Rules Transition actions it defines when objects are transitioned to another storage class e.g. move objects to Standard IA class 60 days after creation e.g. move to Glacier for archiving after 6 months Expiration actions configure to expire (delete) after some time e.g. access log files can be set to delete after a 365 days e.g. can be used to delete old versions of files (if versioning is enabled) e.g. can be used to delete incomplete multi-part uploads S3 Analytics - Storage Class Analysis You can setup S3 analytics to help determine when to transit objects from Standard to Standard IA does not work for One Zone - IA or Glacier S3 Select and Glacier Select retrieve less data using SQL by performing server side filtering can filter by rows and columns less network transfer, less CPU cost client-side S3 Event notifications can create as many events as desired SNS SQS Lambda function S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer if two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent if you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. Your S3 bucket needs to have permission to send message to SQS queue S3 Requester Pays in general, bucket owners pay for all S3 storage and data transfer costs associated with their buckets with Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket helpful when you want to share large datasets with other accounts the requester must be authenticated in AWS (cannot be anonymous) Glacier Vault Lock Adopt a WORM (Write Once Read Many) model lock the policy for future edits (can no longer be changed) helpful for compliance and data retention S3 Object Lock (versioning must be enabled) adopt a WORM (Write Once Read Many) model block an object version deletion for a specified amount of time object retention retention period: specifies a fixed period legal hold: same protection, no expiry date Modes Governance mode: users can’t overwrite or delete an object version or alter its lock settings unless they have special permissions Compliance mode: a protected object version can’t be overwritten or deleted by any user, including the root user. When an object is locked in compliance mode, its retention mode can’t be changed and its retention period can’t be shortened. AWS Athena serverless service to perform analytics directly against S3 files uses SQL language to query the files has a JDBC / ODBC driver (for BI tools) charged per query and amount of data scanned supports CSV, JSON, ORC, Avro, and Parquet (built on Presto) use cases: BI / Analytics / reporting / Logs / CloudTrail trails etc… AWS CloudFront CDN improves read performance, content is cached at the edge loctions 216 point of presence globally (edge locations) DDoS protection, integration with Shield, AWS web application firewall can expose external HTTPS and can talk to internal HTTPS backends Origins S3 bucket for distributing files and caching them at the edge enahnced security with CloudFront OAI (Origin Access Identity), this can block access directly to S3 CloudFront can be used as an ingress (to upload files to S3) Custom Origin (HTTP) Application Load Balancer EC2 instance S3 Website (must first enable the bucket as a static S3 website) Any HTTP backend you want CloudFront vs S3 Cross Region Replication CloudFront Global Edge Network files are cached for a TTL great for static content that must be available everywhere, maybe outdated for a while S3 Cross Region Replication must be setup for each region you want replication to happen files are updated in near real time read only great for dynamic content that needs to be available at low latency in few regions CloudFront Signed URL / Signed Cookies you want to distribute paid share content to premium users over the world we can use CloudFront Signed URL / Cookie, we attach a policy with includes URL expiration includes IP ranges to access the data from Trusted Signers (which AWS accounts can create signed URLs) Signed URL: access to individual files (one signed URL per file) Signed Cookies: access to multiple files (one signed Cookie for many files) Process user authenticate and authorized to the application the application send request to CloudFront to generate Signed URL / Cookie the application send the signed URL / Cookie back to user user use the signed URL / Cookie to access the file in CloudFront CloudFront fetch the file from S3 to the user CloudFront Signed URL vs S3 Pre-signed URL CloudFront Signed URL Allow access to a path, no matter the origin account wide key pair, only the root can manage it can filter by IP, path, date, expiration can leverage caching features S3 Pre-signed URL issue a request as the person who pre-signed the URL uses the IAM key of the signing IAM principal limited lifetime CloudFront - Price Class You can reduce the number of edge locations for cost reduction 3 price classes price class all: all regions price class 200: most regions, but excludes the most expensive regions price class 100: only the least expensive regions CloudFront - Multiple Origins to route to different kind of origins based on the content type e.g. one origin is from ALB and another origin is from S3 bucket Based on the path pattern /images/* /api/* /* CloudFront - Origin Groups to increase high availability and do failover Origin Groups: one primary and one secondary origin if the primary origin fails, the second one is used (CloudFront will send the same request to the secondary origin) CloudFront - Field Level Encryption protect user sensitive information through application stack adds an additional layer of security along with HTTPS sensitive information encrypted at the edge close to the user uses asymmetric encryption (public private key pair) usage client send sensitive information to the edge location edge location use public key to encrypt the information edge location send encrypted information to CloudFront CloudFront send information all the way to (CloudFront =&gt; ALB =&gt; Web Servers) Web server Web server uses the private key to decrypt the information AWS Global Accelerator Global users for our application you have deployed an application and have global users who want to access it directly they go over the public internet, which can add a lot of latency due to many hops we wish to go as fast as possible through AWS network to minimize latency Unicast IP vs Anycast IP Unicast IP one server holds one IP address Anycast IP all servers hold the same IP address and the client is routed to the nearest one AWS Global Accelerator leverage the AWS internal network to route to your application 2 Anycast IP are created for your application the Anycast IP send traffic directly to Edge locations the edge locations send the traffic to your application (through AWS private network) Global Accelerator vs CloudFront they both use the AWS global network and its edge locations around the world both services integrate with AWS shield for DDoS protection CloutFront improves performance for both cacheable content (images / videos) Dynamic content (such as API acceleration and dynamic site delivery) content is served at the edge location Global Accelerator improves performance for a wide range of applications over TCP or UDP proxying packets at the edge to applications running in one or more AWS regions good fit for non-HTTP use cases: such as gaming (UDP), IoT (MQTT) or Voice Over IP good for HTTP use cases that require static IP (if use Route 53 Geo location, client browser will cache the IP address and redirect user to the old IP for a TTL) good for HTTP use cases that require deterministic, fast regional failover AWS Snow Family Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS Snowball Edge (for data transfers) physical data transport solution alternative to moving data over the network pay per data transfer job provide block storage and Amazon S3 compatible object storage Snowball Edge Storage Optimzied Snowball Edge Compute Optimized Snowcore small, portable computing, anywhere, rugged and secure, withstands harsh environements light, 2.1 kg device used for edge computing, storage, and data transfer 8 TB usable storage must provide your own battery and cables can be sent back to AWS offline, or connect it to internet and use AWS datasync to send data Snowmobile transfer exabytes of data (1 EB = 1000 PB = 1,000,000 TB) each snowmobile has 100 PB of capacity high security better than snowball if you transfer more then 10 PB Edge computing process data while its being created on an edge location A truck on the road, a ship on the sea, a mining station underground (no internet access) these locations may have limited / no internet access limited / no easy access to computing power we setup a snowball / snowcone device to do edge computing eventually we can ship back the device to AWS AWS OpsHub Historically, to use Snow Family devices, you need a CLI today, you can use AWS OpsHub (a software you install on your computer / laptop) to manage your snow family devices unlocking and configuring single or clustered devices transferring files launching and managing instances running on Snow family devices monitor device metrics (storage capacity, active instances) launch compatible AWS services on your devices Snowball into Glacier Snowball cannot import to Glacier directly you must use Amazon S3 first, in combination with an S3 lifecycle policy AWS Storage Gateway Bridge between on-premises data and cloud data in S3 use cases: disaster recovery, backup and restore, tiered storage File Gateway configured S3 buckets are accessible using the NFS and SMB protocol supports S3 standard, S3 IA, S3 One Zone IA bucket access using IAM roles for each File Gateway most recently used ata is cached in the file Gateway can be mounted on many servers integrated with AD (Active Directory) for user authentication On-premises server communicate with File Gateway (optionally with Authentication) File Gateway communicate with S3 buckets Volume Gateway block storage using iSCSI protocol backed by S3 backed by EBS snapshot which can help restore on-premises volumes cached volumes: low latency access to most recent data stored volumes: entire dataset is on premises, scheduled backups to S3 On-premises server communicate with Volume Gateway using iSCSI protocol Volume Gateway communicate with S3 bucket to create EBS snapshots Volume Gateway is often used as data backup Tape Gateway some companies have backup processes using physical tapes with tape gateway, companies use the same processes but, in the cloud Virtual Tape Library (VTL) backed by Amazon S3 and Glacier backup data using existing tape-based processes Storage Gateway - Hardware appliance if you don’t have on-premises virtual server, you can buy from Amazon AWS FSx AWS FSx for Windows EFS is a shared POSIX system for Linux system FSx is a fully managed Windows file system share drive supports SMB protocol and Windows NTFS Microsoft Active Directory integration, ACLs, user quotas can be accessed from your on-premises infrastructure can be configured to be Multi-AZ data is backed up daily to S3 Amazon FSx for Lustre Lustre is a type of parallel distributed file system, for large scale computing the name Lustre is derived from Linux and Cluster Machine Learning, High Performance Computing Video processing, Financial Modeling and Electronic Design Automation Seamless integration with S3 can be used from on-premises servers FSx File System Deployment Options Scratch file system temporary storage data is not replicated high burst usage: short-term processing, optimize costs Persistent File System long-term storage data is replicated within same AZ replace failed files within minutes usage: long-term processing, sensitive data AWS Transfer Family a fully managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol supported protocols AWS Transfer for FTP (File Transfer Protocol) AWS Transfer for FTPS (File Transfer Protocol over SSL) AWS Transfer for SFTP (Secure File Transfer Protocol) Managed infrastructure, scalable, reliable, highly available pay per provisioned endpoint per hour + data transfers in GB store and manage users’ credentials within the services integrate with existing authentication systems (Microsoft Active Directory, LDAP, Okta…) AWS Storage Comparison S3: Object Storage S3 is going to be an object storage, it’s going to be serverless, you don’t have to prove incapacity ahead of time. It has some deep integration with so many database services. Glacier: Object Archival Glacier is going to be for object archival. So this is when we want to store objects for a long period of time. Retrieve it very very rarely, and when we retrieve these objects, they’re going to be taking a lot of time to get back to us because they are archived. EFS: Network File System for Linux instances, POSIX file system EFS is Elastic File System, and this is a network file system for Linux instances. It is a POSIX file system so that means for Linux again. And it is accessible from all your EC2 instances at once. So it is something that is going to be shared and across AZ. FSx for Windows: Network File System for Windows servers FSx for Windows is the same thing as EFS, but for Windows. So it’s a network file system for your Windows servers. FSx for Lustre: High performance computing Linux file system FSx for Lustre is Linux and cluster, so it’s for High Performance Computing Linux file system. This is where you’re going to do your HPC running. You only have insanely high IOPS, insanely big capacity. And it has integration with S3 in the back end. EBS volume: network storage for one EC2 instance at a time EBS volumes is your network storage for one EC2 instance at a time only. And it is bound to a specific availability zone that you create it in. And in case you wanted to change the AZ, you will need to create a snapshot, move that snapshot over, and create a volume from it. Instance Storage: physical storage for your EC2 instance (high IOPS) Instance Storage is going to be physical storage for your EC2 instance. And so, because it’s attached from the hardware, then it’s going to have a much higher IOPS than EBS. EBS volumes, as we remember, it is up to 16,000 IOPS or 64,000 IOPS for io1. But for Instance Storage, because it is physically attached to your EC2 instance, you can get, for some, millions of IOPS. Um, it’s going to be very high. But the risk is that if your EC2 instance goes down, then you will lose that storage permanently. Storage Gateway: file gateway, volume gateway, tape gateway Storage Gateway is going to be transporting files from on premise to AWS. So we have File Gateway, Volume Gateway for cache and stored, and Tape Gateway. Each with their use cases. Snowball / Snowmobile: to move large amount of data to the cloud, physically And then finally, Snowball/Snowmobile to move large amount of data to the cloud physically into S3. database: for specific workloads, usually with indexing and querying Amazon SQS (Simple Queuing Service) Fully managed service, used to decouple applications attributes unlimited throughput, unlimited number of messages in queue default retention of messages: 4 days, to maximum 14 days low latency limitation of 256KB per message sent can have duplicate messages (at least once delivery, occasionally) can have out of order message (best effort ordering) Producing Messages Produced to SQS using the SDK (SendMessage API) the message is persisted in SQS until a concumer deletes it Consuming Messages consumers (running on EC2 instances, servers, or AWS lambda) Poll SQS for messages (receive up to 10 message at a time) process the messages (example: insert the message into an RDS database) delete the messages using the DeleteMessage API Multiple EC2 instances consumers consumers receive and process messages in parallel at least once delivery (another consumer will receive the message if the first consumer didn’t process it fast enough) best-effort message ordering consumers delete messages after processing them we can scale consumers horizontally to improve throughput of processing SQS with Auto Scaling Group CloudWatch is monitoring the SQS length if SQS length is too long, CloudWatch will trigger an alarm Auto Scaling group will increase the number of EC2 instances if the alarm is triggered SQS Security encryption in flight encryption using HTTPS API at rest encryption using KMS keys client side encryption if the client wants to perform encryption / decryption itself Access control: IAM policies to regulate access to the SQS API SQS access policies (similar to S3 bucket policies) SQS Queue Access Policy Cross Account Access if other accounts want to poll message from the SQS queue, we could add policy to the SQS specify which account and allow it to call receiveMessage API publish S3 event notifications to SQS queue we upload an object to a S3 bucket S3 bucket triggered an event message to be sent to SQS queue we need to add a policy to SQS queue allowing the bucket to call SendMessage API to the queue SQS - Message Visibility Timeout after a message is polled by a consumer, it becomes invisible to other consumers by default, the message visibility timeout is 30 seconds that means the message has 30 seconds to be processed and deleted from the queue after the message visibility timeout is over, the message is visible in SQS for other consumers to receive if a message is not processed within the visiblity timeout, it will be processed again by other consumers a consumer could call the ChangeMessageVisibility API to get more time if visibility timeout is high (hours), and consumer crashes, re-processing will take time if visibility timeout is too low (seconds), we may get duplicates SQS - Dead Letter Queue if a consumer fails to process a message within the visibility timeout, the message goes back to the queue we can set a threshold of how many times a message can go back to the queue after the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) useful for debugging make sure to process the messages in the DLQ before they expire good to set a retention of 14 days in the DLQ SQS - Request - Response Systems producer send request with the reply-to queue ID to the Request queue responders receive the request from the Request queue and process it after processing, responders send the response to the corrent Response queue using the queue ID in the request to implement this pattern: use the SQS Temporary Queue Client it leverages virtual queues instead of creating / deleting SQS queues (cost effective) SQS - Delay Queue delay a message (consumers don’t see it immediately) up to 15 minutes default is 0 seconds (message is avaialble right away) can set a default at queue level can override the default on send using the DelaySeconds parameter SQS - FIFO queue Frist In First Out limited throughput exactly once send capability (by removing duplicates), the message that failed to be processed will be insert at the end of the queue messages are processed in order by the consumer Amazon SNS the event producer only sends message to one SNS topic as many event receivers (subscriptions) as we want to listen to the SNS topic notifications each subscriber to the topic will get all the messages (note: new feature to filter messages) subscribers can be SQS HTTP / HTTPS lambda Emails SMS messages Mobile notifications SNS integrates with a lot of AWS services many AWS services can send data directly to SNS for notifications CloudWatch (for alarms) Auto Scaling Groups notifications Amazon S3 (on bucket events) CloudFormation (upon state changes =&gt; failed to build etc…) etc… How to publish topic publish (using SDK) create a topic create a subscription publish to the topic direct publish (for mobile apps SDK) create a platform application create a platform endpoint publish to the platform endpoint works with Google GCM, Apple APNS, Amazon ADM… Security Encryption in flight encryption using HTTPS / API at rest encryption using KMS keys Client side encryption if the client wants to perform encryption / decryption itself Access Control: IAM policies to regulate access to the SNS API SNS access policies (similar to S3 bucket policies) useful for cross account access to SNS topics useful for allowing other services (S3…) to write to an SNS topic SNS + SQS: Fan Out Push once in SNS, receive in all SQS queues that are subscribers fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work ability to add more SQS subscribers over time make sure your SQS queue access policy allows for SNS to write e.g. S3 Events to multiple queues or lambda functions if you want to send the same S3 event to many SQS queues, or optionally lambda functions, use fan out pattern SNS FIFO topic similar features as SQS FIFO ordering by message group ID (all messages in the same group are ordered) deduplication using a deduplication ID or Content Based Deduplication can only have SQS FIFO queue as subscribers limited throughput (same throughput as SQS FIFO) Message Filtering JSON policy used to filter messages sent to SNS topic’s subscriptions if a subscription doesn’t have a filter policy, it receives every message AWS Kinesis make it easy to collect, process and analyze streaming data in real time ingest real time data such as application logs, Metrics, website clickstreams, IoT telemetry data… Kinesis Data Streams billing is per shard provisioned, can have as many shards as you want retention between 1 day (default) to 365 days ability to reprocess (replay) data once data is inserted in Kinesis, it can’t be deleted (immutability) data that shares the same partition goes to the same shard (ordering) producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent consumers: write your own: Kinesis Client Library (KCL), AWS SDK managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics Kinesis Firehose fully managed service, no administration, automatic scaling, serverless AWS redshift / Amazon S3 / ElasticSearch pay for data going through firehose near real time 60 seconds latency minimum for non full batches or minimum 32 MB of data at a time supports many data formats, conversions, transformations, compression supports custom data transformations, using AWS Lambda can send failed or all data to a backup S3 bucket Kinesis Data Streams Kinesis Data Firehos Streaming service for ingest at scale load streaming data into S3 / redshift / ES / Thrid party / custom HTTP write custom code (producer, consumer) fully managed real time (~200ms) near real time (buffer time min 60 seconds) manage scaling (shard splitting / merging) automatic scaling data storage for 1 to 365 days no data storage support replay capability doesn’t support replay capability Kinesis Data Analytics (SQL application) perform real time analytics on Kinesis Streams using SQL fully managed, no servers to provision automatic scaling real time analytics pay for actual consumtion rate can create streams out of the real time queries use cases time series analytics real time dashboards real time metrics Kinesis vs SQS ordering let’s assume we have 100 trucks, 5 Kinesis shards, 1 SQS FIFO Kinesis data streams on average you will have 20 trucks per shard trucks will have their data ordered within each shard the maximum amount of consumer in parallel we can have is 5 can receive up to 5MB/s of data SQS FIFO you only have one SQS FIFO queue you will have 100 group ID you can have up to 100 consumers (due to the 100 group ID) you have up to 300 messages per second (or 3000 if using batching) better to use if you have a dynamic number of consumers SQS SNS Kinesis consumer pull data push data to many subscribers standard: pull data (2MB per shard), enhanced-fan out: push data (2MB per shard per consumer) data is deleted after being consumed data is not persisted (lost if not delivered) possibility to replay data can have as many workers as we want up to 12,500,000 subscribers - no need to provision throughput no need to provision throughput must provision throughput ordering guearantees only on FIFO queues FIFO capability for SQS FIFO ordering at shard level individual message delay capability integrates with SQS for fan out architecture pattern data expires after X days - - meant for real time big data analytics and ETL Amazon MQ SQS, SNS are cloud native serviecs, and they are using proprietary protocols from AWS traditional applications running from on-premises may use open protocols such as MQTT, AMQP, STOMP, OpenWire, WSS when migrating to the cloud, instead of re-engineering the application to use SQS, SNS, we can use Amazon MQ Container Docker Docker is a software development platform to deploy apps apps are packaged in containers that can be run on any OS apps run the same, regardless of where they are run any machine no compatibility issues predictable behavior less work easier to maintain and deploy works with any language, any OS, any techonology Where are Docker images stored Docker images are stored in Docker repositories public: Docker hub: https://hub.docker.com/ private: Amazon ECR (Elastic Container Registry) Public: Amazon ECR public Docker Containers Management to manage containers, we need a container managemenet platform ECS (Amazon’s own container platform) Fargate: Amazon’s own serverless container platform EKS: Amazon’s managed Kubernetes (open source) ECS (Elastic Container Service) Launch Docker containers on AWS you must provision and maintain the infrastructrue (the EC2 instance) AWS takes care of starting and stopping the containers has integrations with the Application Load Balancer ECS agent will be installed on the EC2 instances for ECS to know who to manage Fargate launch Docker containers on AWS you do not provision the infrastructure (no EC2 instances to manage) serverless offering AWS just runs containers for you based on the CPU / RAM you need for each container in Fargate it needs an ENI for the public to access it IAM roles for ECS tasks EC2 instance profile used by the ECS agent makes API calls to ECS service send container logs to CloudWatch logs pull docker image from ECR reference sensitive data in Secret Manager or SSM Parameter store ECS task role allow each task to have a specific role use different roles for the different ECS services you run task role is defined in the task definition ECS data volumes - EFS file systems works for both EC2 tasks and Fargate tasks ability to mount EFS volumes onto tasks tasks launched in any AZ will be able to share the same data in the EFS volume Fargate + EFS = serverless + data storage without managing servers use case: persistent multi-AZ shared storage for your containers Load Balancing for EC2 Launch type we get a dynamic port mapping the ALB supports finding the right port on your EC2 instances you must allow on the EC2 instnace’s security group any port from the ALB security group Load Balancing for Fargate each task has a unique IP (because of ENI) you must allow on the ENI’s security group the task port from the ALB security group Event Bridge user upload object to S3 S3 triggers event to Amazon Event Bridge Event Bridge triggers the container task to run the task will have the role to access S3 and DynamoDB task will get the object from S3 and save it to DynamoDB ECS Rolling updates when updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order, by specifying the minimum and maximum healthy percent ECR (Elastic Container Registry) store, manage and deploy containers on AWS, pay for what you use fully integrated with ECS and IAM for security, backed by Amazon S3 supports image vulnerability scanning, version, tag, image lifecycle but if we wanted to automate the whole process, we could be using a CICD, so Continuous Integration Continuous Deployment platform and for example, CodeBuild could help us with this to automate building a Docker image and then pushing it onto Amazon ECR to finally trigger an ECS update. EKS (Elastic Kubernetes Service) it is a way to launch managed Kubernetes clusters on AWS Kubernetes is an open source system for automatic deployment, scaling and management of containerized application it is an alternative to ECS, similar goal but different API EKS supports EC2 if you want to deploy worker nodes Fargate to deploy serverless containers use case: if your company is already using Kunernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes Kubernetes is cloud agnostic (can be used in any cloud, Azure, GCP…) Serverless serverless is a new paradigm in which the developers don’t have to manage servers anymore Serverless was pioneered by AWS lambda but now also includes anything that’s managed: database, messaging, storage serverless does not mean there are no servers, it means that you just don’t need to manage / provision / see them Lambda easy pricing pay per request and compute time integrated with the whole AWS suite of services integrated with many programming languages Node.js Python Java C# (.NET Core) Golang C# / Powershell Ruby Lambda container image the container image must implement the lambda runtime API ECS / Fargate is perferred for running arbitrary Docker images easy monitoring through AWS CloudWatch easy to get more resources per functions Lambda Limits Execution Memory allocation: 128MB - 10GB (64 MB increments) Maximum execution time: 15 minutes Environment variables (4 KB) disk capacity in the function container (in /tmp): 512 MB concurrency executions: 1000 (can be increased) Deployment lambda function deployment size (compressed zip): 50 MB size of uncompressed deployment (code + dependencies): 250 MB can use the /tmp directory to load other files at startup size of environment variables: 4KB Lambda@Edge you have deployed a CDN using CloudFront what if you wanted to run a global AWS lambda alongside? or how to implement request filtering before reaching your application? for this, you can use Lambda@Edge build more responsive applications you don’t manage servers, lambda is deployed globally customize the CDN content pay only for what you use you can use lambda to change CloudFront requests and responses after cloudfront receives a request from a viewer (viewer request) before cloudfront forwards the request to the origin (origin request) after cloudfront receives the response from the origin (origin response) before cloudfront forwards the response to the viewer (viewer response) you can also generate responses to viewers without ever sending the request to the origin DynamoDB fully managed, hgihly available with replication across 3 AZ NoSQL database, not a relational database scales to massive workloads, distributed database millions of requests per seconds, trillions of row, 100s of TB of storage fast and consistent in performance (low latency on retrieval) integrated with IAM for security, authorization and administration enables event driven programming with DynamoDB streams low cost and auto scaling capabilities DynamoDB - basics DynamoDB is made of tables each table has a primary key (must be decided at creation time) each table can have an infinite number of items (rows) each item has attributes (can be added over time - can be null) maximum size of a item is 400KB data types supported are Scalar Type: string, number, binary, booelan, null Document type: list, map set types: string set, number set, binary set Provisioned Throughput table must have provisioned read and write capacity units read capacity units (RCU), throughput for reads 1 RCU = 1 strongly consistent read of 4 KB per second 1 RCU = 2 eventually consistent read of 4 KB per second write capacity units (WCU), throughput for writes 1 WCU = 1 write of 1 KB per second option to setup auto scaling of throughput to meet demand throughput can be exceeded temporarily using burst credit if burst credit are empty, you will get a ProvisionedThroughputExeception it is then advised to do an exponential back off retry DynamoDB - DAX DynamoDB accelerator seamless cache for DynamoDB, no application re-write writes go through DAX to DynamoDB micro second latency for cached reads and queries solves the Hot Key problem (too many reads) 5 minutes TTL for cache by default up to 10 nodes in the cluster multi AZ (3 nodes minumum recommended for production) secure (encryption at rest with KMS, VPC, IAM, CloudTrail…) DynamoDB Streams changes in DynamoDB (create, update, delete) can end up in a DynamoDB stream this stream can be read by AWS lambda and we can then do react to changes in real time (welcome email to new users) analytics create derivative tables/ views insert into elasticSearch cloud implement cross region replication using Streams Stream has 24 hours of data retention Transactions (new from Nov 2018) all or nothing type of operations coordinated insert, update and delete across multiple tables include up to 10 unique items or up to 4MB of data On Demand (new from Nov 2018) no capacity planning needed (WCU / RCU) - scales automatically 2.5x more expensive than provisioned capacity helpful when spikes are un-predicatable or the application is very low throughput DynamoDB - Security and other features Security VPC endpoints available to access DynamoDB without internet access fully controlled by IAM encryption at rest using KMS encryption in transit using SSL / TLS backup and restore feature available point in time restore like RDS no performance impact Global Tables (cross region replication) multi region, fully replicated, high performance active active replication (data will be replicated to other tables no matter which table gets the data first) must enable DynamoDB Streams useful for low latency, Disater Recovery purposes Amazon DMS (Data Migration Service) can be used to migrate to DynamoDB (from Mongo, Oracle, MySQL, S3, etc…) you can launch a local DynamoDB on your computer for development purposes API Gateway AWS lambda + API gateway support for the webSocket Protocol handle API versioning handle different environments (dev, test, prod) handle security (authentication, authorization) create API keys, handle request throttling Swagger / Open API import to quickly define APIs Transform and validate requests and responses generate SDK and API specifications cache API responses API Gateway - Endpoint Types Edge Optimized (default): for global clients requests are routed through the CloudFront edge locations (improves latency) the API gateway still lives in only one region Regional for clients within the same region could manually combine with CloudFront (more control over the caching strategies and the distribution) private can only by accessed from your VPC using an interface VPC endpoint (ENI) use a resource policy to define access Security IAM Permissions create an IAM policy authorization and attach to User / Role API gateway verfies IAM permissions passed by the calling application good to provide access within your own infrastructure leverage Sig v4 capability where IAM credential are in headers Lambda authorizer users AWS lambda to validate the token in header being passed option to cache result of authentication helps to use OAuth / SAML / third party type of authentication lamdba (authorizer) must return an IAM policy for the user Cognito User Pools cognito fully manages user lifecycle API gateway verifies identity automatically from AWS cognito no custom implementation required Cognito only helps with authentication, not authorization AWS Cognito we want to give our users an identity so that they can interact with our application AWS Cognito User Pools create a serverless database of user for your mobile apps simple login: username or email / password combination possibility to verify emails / phone numbers and add MFA can enable federated identities (Facebook, Google, SAML…) sends back a JSON web tokens (JWT) can be integrated with API gateway for authentication AWS Cognito - Federated Identity Pools goal provide direct access to AWS resources from the client side How login to federated identity provider - or remain anonymous get temporary AWS credentials back from the federated identity pool these credentials come with a pre-defined IAM policy stating their permissions example provide (temporary) access to write to S3 bucket using Facebook login AWS Cognito Sync deprecated - use AWS AppSync now store preferneces, configuration, state of app cross device synchronization offline capability (synchronization when back online) requires federated identity pool in Cognito store data in datasets AWS SAM - Serverless Application Model framework for developing and deploying serverless applications all the configurations is YAML code lambda functions DynamoDB tables API Gateway Cognito User Pool SAM can help you to run Lambda, API Gateway, DynamoDB locally SAM can use CodeDeploy to deploy lambda functions Databases Comparison RDS managed postgreSQL / MySQL / Oracle / SQL server must provision an EC2 instance and EBS volume type and size support for read replicas and multi AZ security through IAM, security groups, KMS, SSL in transit backup / snapshot / point in time restore managed and scheduled maintenance monitoring through CloudWatch use case: store relational datasets (RDBMS / OLTP (online transactional processing)), perform SQL queries, transactional inserts / update / delete is available Aurora compatible API for PostgreSQL / MySQL Data is held in 6 replicas, 3 AZ auto healing capability multi AZ, auto scaling read replicas read replicas can be global Aurora database can be global for DR or latency purpose define EC2 instance type for Aurora instances same security / monitoring / maintenance features as RDS Aurora Serverless - for unpredicatble / intermittent workloads Aurora multi-master - for continuous write failover no need to provision use case: same as RDS, but with less maintenance / more flexibility / more performance ElasticCache managed Redis / Memcached (similar offering as RDS, but for caches) in memory data store, sub-millisecond latency must provision an EC2 instance type support for Clustering (Redis), and Multi AZ, read replicas (sharding) security through IAM, security groups, KMS, Redis Auth backup / snapshot / point in time restore managed and scheduled maintenance monitoring through CloudWatch use case: key value store, frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL DynamoDB AWS proprietary technology, managed NoSQL database serverless, provisioned capacity, auto scaling, on demand capacity can replace ElastiCache as a key value store highly available, multi AZ by default, read and writes are decoupled, DAX for read cache reads can be eventually consistent (occasional old data) or strongly consistent (always latest data) security, authentication and authorization is done through IAM DynamoDB Streams to integrate with AWS lambda backup and restore feature, global table feature monitoring through CloudWatch can only query on primay key, sort key, or indexes use case: serverless applications development, distributed serverless cache, doesn’t have SQL query language available, has transactions capability from Nov 2018 S3 great for big objects, not so great for small objects (because of latency) serverless, scales infinitely, max object size is 5TB strong consistency Tiers: S3 standard, S3 IA, S3 One Zone IA, Glacier, for backups features: versioning, encryption, cross region replication etc… security: IAM, bucket policy, ACL encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit use case: static files, key value store for big files, website hosting Athena fully serverless query engine with SQL capabilities used to query data in S3 pay per query output results back to S3 secured through IAM use case: one time SQL queries, serverless queries on S3, log analytics RedShift Redshift is based on PostgreSQL, but it is not used for OLTP its OLAP - online analytical processing (analytics and data warehousing) Columnar storage of data (instead of row based) massively parallel query execution pay as you go based on the instances provisioned has a SQL interface for performing the queries BI tools such as AWS quicksight or Tableau integrate with it data is loaded from S3 / DynamoDB, DMS, other DBs from 1 node to 128 nodes, up to 160 GB of space per node leader node: for query planning, results aggregation compute node: for performing the queries, send results to leader Redshift Spectrum: perform queries directly against S3 (no need to load) backup and restore, security VPC / IAM / KMS, monitoring Redshift enhanced VPC routing: COPY / UNLOAD goes through VPC Snapshots / DR Redshift has no multi AZ mode snapshots are point in time backups of a cluster, stored internally in S3 snapshots are incremental (only what has changed is saved) you can restore a snapshot into a new cluster automated: every 8 hours, every 5 GB, or on schedule, set retention manual: snapshot is retained until you delete it you can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS region Redshift Specturm query data that is already in S3 without loading it must have a Redshift cluster available to start the query the query is then submitted to thousands of Redshift Spectrum nodes data doesn’t need to be loaded into Redshift first AWS Glue managed extract, transform, and load (ETL) service useful to prepare and transform data for analytics fully serverless service Glue Data Catalog Glue data catalog: catalog of datasets (metadata) S3 =&gt; AWS Glue Data Crawler =&gt; AWS Glue Data Catalog =&gt; Amazon Athena Neptune fully managed graph database when do we use graphs? high relationship data social networking: users friends with users, replied to comment on post of user and likes other comments knowledge graphs (Wikipedia) highly available across 3 AZ, with up to 15 read replicas point in time recovery, continuous backup to Amazon S3 support for KMS encryption at rest + HTTPS ElasticSearch example: in dynamoDB, you can only find by primary key or indexes with ElasticSearch you can search any field, even partially matches it is common to use ElasticSearch as a complement to another database ElasticSearch also has some usage for Big Data applications you can provision a cluster of instances built in integrations: Amazon Kinesis data Firehose, SSL and VPC comes with Kibana (visulization) and Logstash (log ingestion) - ELK stack AWS CloudWatch CloudWatch Metrics CloudWatch provides metrics for every servcies in AWS Metric is a variable to monitor (CPU Utilization, Network In…) metrics belong to namespaces dimension is an attribute of a metric (instance id, environment, etc…) up to 10 dimensions per metric metrics have timestamps can create CloudWatch dashboards of metrics EC2 Detailed monitoring EC2 instance metrics have metrics every 5 minutes with detailed monitoring (for a cost), you get data every 1 minute use detailed monitoring if you want to scale faster for your ASG the AWS free tier allows us to have 10 detailed monitoring metrics Note: EC2 memory usage is by default not pushed (must be pushed from inside the instance as a custom metric) CloudWatch Custom Metrics possiblity to define and send your own custom metrics to CloudWatch example: memory usage, disk space, number of logged in users… use API call PutMetricData ability to use dimensions (attributes) to segment metrics instance.id environment.name metric resolution (StorageResolution API parameter - two possible values) Standard: 1 minute high resolution: 1 / 5 / 10 / 30 seconds - higher cost important: accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly) CloudWatch Dashboards great way to setup custom dashboards for quick access to key metrics and alarms dashboards are global dashboards can include graphs from different AWS accounts and regions you can change the time zone and time range of the dashboards you can setup automatic refresh (10s, 1m, 2m, 5m, 15m) dashboards can be shared with people who don’t have an AWS account (public, email address…) pricing 3 dashboards (up to 50 metrics) for free $3 dollar / dashboard / month after CloudWatch Logs applications can send logs to CloudWatch using the SDK CloudWatch can collect log from elastic beanstalk: collection of log from application ECS: collection from containers AWS lambda: collection from function logs VPC flow logs: VPC specific logs API Gateway CloudTrail based on filter CloudWatch log agents: for example on EC2 machines Route53: log DNS queries CloudWatch Logs can go to batch exporter to S3 for archival Stream to ElasticSearch cluster for further analytics Logs storage architecture log groups: arbitrary name, usually representing an application log stream: instances within application / log files / containers can define log expiration policies (never expire, 30 days, etc…) using the AWS CLI we can tail CloudWatch logs to send logs to CloudWatch, make sure IAM permissions are correct security: encryption of logs using KMS at the group level CloudWatch Logs for EC2 by default, no logs from your EC2 machine will go to CloudWatch you need to run a CloudWatch agent on EC2 to push the log files you want make sure IAM permissions are correct the CloudWatch log agent can be setup on-premises too CloudWatch Logs Agent vs Unified Agent CloudWatch Logs Agent old version of the agent can only send to CloudWatch logs CloudWatch unified agent collect additional system-level metrics such as RAM, processors,etc… collect logs to send to CloudWatch logs centralized configuration using SSM Parameter Store CloudWatch Alarms Alarms are used to trigger notifications for any metric various options (sampling, percentage, max, min, etc…) alarm status OK INSUFFICIENT_DATA ALARM period length of time in seconds to evaluate the metric high resolution custom metrics: 10 / 30 or multiples of 60 seconds Alarm Targets Stop, terminate, reboot or recover an EC2 instance trigger auto scaling action send notification to SNS (from which you can do pretty much anything) EC2 instance recovery status check instance status = check the EC2 VM system status = check the underlying hardware recovery same private, public, elastic IP, metadata, placement group CloudWatch Alarm: good to know alarms can be created based on CloudWatch Logs Metrics Filters to test alarms and notifications, set the alarm state to alarm using CLI CloudWatch Events event pattern: intercept events from AWS services (sources) example soruces: EC2 instance start, codebuild failure, S3 trsuted advisor can intercept any API call with CloudTrail integration schedule or Cron (example: create an event every 4 hours) A JSON payload is created from the event and passed to a target compute: lambda, batch, ECS task integration: SQS, SNS, Kinesis data streams, Kinesis data firehose Orchestration: step functions, codepipeline, codebuild maintenance: SSM, EC2 actions Amazon EventBridge eventbridge is the next evolution of CloudWatch Events default event bus: generated by AWS services (CloudWatch events) partner event bus: receive events from SaaS service or applications custom event buses: for your own applications event buses can be accessed by other AWS accounts Rules: how to process the events (similar to CloudWatch Events) Amazon EventBridge builds upon and extends CloudWatch events it uses the same service API and endpoint, and the same underlying service infrastructure eventbridge allows extension to add event buses for your custom applications and your thrid party SaaS apps event bridge has the schema registry capability eventbridge has a different name to mark the new capabilities over time, the CloudWatch events name will be replaced with eventbridge AWS CloudTrail provides governance, compliancen and audit for your AWS account CloudTrail is enabled by default get an history of events / API calls made within your AWS account by console SDK CLI AWS Service can put logs from CloudTrail into CloudWatch logs or S3 a trail can be applied to all regions (default) or a single region if a resource is deleted in AWS, investigate CloudTrail first CloudTrail Insights enable CloudTrail insights to detect unusual activity in your account inaccurate resource provisioning hitting service limits bursts of AWS IAM actions gaps in periodic maintenance activity CloudTrail insights analyzes normal management events to create a baseline and then continuously analyzes write events to detect unusual patterns anomalies appear in the CloudTrail console event is sent to Amazon S3 an eventbridge event is generated (for automation needs) CloudTrail Events retention events are stored for 90 days in CloudTrail to keep events beyond this period, log them to S3 and use Athena AWS Config helps with auditing and recording compliance of your AWS resources helps record configurations and changes over time questions that can be solved by AWS Config is there unrestricted SSH access to my security groups do my buckets have any public access how has my ALB configuration changed over time you can receive alerts (SNS notifications) for any changes AWS Config is a per-region service can be aggregated across regions and accounts Config Rules - remediations automate remediation of non-compliant resources using SSM automation documents use AWS-managed automation documents or create custom automation documents tip: you can create custom automation documents that invokes lambda function you can set remediation retries if the resource is still non-compliant after auto-remediation Config Rules - notifications use eventbridge to trigger notifications when AWS resources are non-compliant ability to send configuration changes and compliance state notifications to SNS (all events - use SNS filtering or filter at client-side) CloudWatch vs CloudTrail vs Config CloudWatch performance monitoring (metrics, CPU, network, etc…) and dashboards event and alerting log aggregation and analysis CloudTrail record API calls made within your account by everyone can define trails for specific resources global service Config record configuration changes evaluate resources against comliance rules get timeline of changes and compliance Example for an Elastic Load Balancer CloudWatch monitoring incoming connections metric visualize error codes as a percentage over time make a dashboard to get an idea of your load balancer performance CloudTrail track who made any changes to the load balancer with API calls Config trakc security group rules for the load balancer track configuration changes for the load balancer ensure an SSL certificate is always assigned to the load balancer (compliance) AWS STS (Security Token Service) allows to grant limited and temporary access to AWS resources token is valid for up to one hour (must be refreshed) AssumeRole within your own account: for enhanced security cross account access: assume role in target account for perform actions there AssumeRoleWithSAML return credentials for users logged with SAML AssumeRoleWithWebIdentity return credentials for users logged with an IDP (facebook, google…) AWS recommends against using this, and using Cognito instead GetSessionToken for MFA, from a user or AWS account root user define an IAM role within your account or cross-account define which principals can access this IAM role use AWS STS to retrieve credentials and impersonate the IAM role you have access to (AssumeRole API) temporary credentials can be valid between 15 minutes to 1 hour Identity Federation in AWS federation lets users outside of AWS to assume temporary role for accessing AWS resources these users assume identity provided access role federations can have many flavors SAML Custom Identity Broker Amazon Cognito Single Sign On Non-SAML with AWS Microsoft AD using federation, you don’t need to create IAM users (user management is outside of AWS) SAML 2.0 Federation needs to setup a trust between AWS IAM and SAML (both ways) SAML enables web based, cross domain SSO uses the STS API: AssumeRoleWithSAML note federation through SAML is the old way of doing things Amazon SSO federation is the new managed and simpler way AWS Directory Services AWS Managed Microsoft AD create your own AD in AWS, manage users locally, supports MFA establish trust connections with your on-premises AD AD Connector Directory Gateway (proxy) to redirect to on-premises AD users are managed on the on-premises AD (not on AWS) Simple AD AD compatible managed directory on AWS cannot be joined with on-premises AD (users managed on AWS only) AWS Organizations global service allows to manage multiple AWS accounts the main account is the master account - you can’t change it other accounts are member accounts member accounts can only be part of one organization consolidated billing across all accounts - single payment method pricing benefits from aggregated usage API is available to automate AWS account creation Multi account strategies create accounts per department, per cost center, per dev/test/prod, based on regulatory restrictions (using SCP), for better resource isolation, to have separate per-account service limits, isolated account for logging multi account vs one account multi VPC use tagging standards for billing purposes enable CloudTrail on all accounts, send logs to central S3 account send CloudWatch logs to central logging account establish cross account roles for admin purpose Service Control Policies (SCP) whitelist or blacklist IAM actions applied at the OU or Account level does not apply to the master account SCP is applied to all the users and roles of the account, including Root the SCP does not affect service linked roles service linked roles enable other AWS services to integrate with AWS organizations and can’t be restricted by SCPs SCP must have an explicit Allow (does not allow anything by default) use cases restrict access to certain services (for example: can’t use EMR) enforce PCI compliance by explicitly disabling services AWS Organization - moving accounts to migrate accounts from one organization to another remove the member account from the old organization send an invite to the new organization accept the invite to the new organization from the member account if you want the master account of the old organization to also join the new organization remove the member accounts from the organization using the procedure above delete the old organization repeat the process above to invite the old master account to the new org IAM Advanced IAM for S3 ListBucket permission applies to arn:aws:s3:::test bucket level permission GetObject, PutObject, DeleteObject applies to arn:aws:s3:::test/* object level permission IAM Roles vs Resource Based Policies when you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role when using a resource based policy, the principal doesn’t have to give up his permissions IAM permission boundaries IAM permission boundaries are supported for users and roles (not groups) advanced feature to use a managed policy to set the maximum permissions an IAM entity can get if a user has been assigned a permission boundary so that it can only access S3, then no matter what permission policies it has, it can only access to S3, nothing else. AWS Resource Access Manager (RAM) share AWS resources that you own with other AWS accounts share with any account or within your organization avoid resource duplication VPC subnets allow to have all the resources launched in the same subnets must be from the same AWS organization cannot share security groups and default VPC participants can manage their own resources in there participants can’t view, modify, delete resources that belong to other participants or the owner AWS transit gateway route53 resolver rules license manager configurations AWS SSO centrally manage single sign on to access multiple accounts and third party business applications integrated with AWS organizations support SAML 2.0 markup integration with on-premises active directory centralized permissioin management centralized auditing with CloudTrail AWS Security Encryption in flight (SSL) data is encrypted before sending and decrypted after receiving SSL certificate help with encryption (HTTPS) encryption in flight ensures no MITM (man in the middle) attach can happen Server side encryption at rest data is encrypted after being received by the server data is decrypted before being sent it is stored in an encrypted form thanks to a key (usually a data key) the encryption / decryption keys must be managed somewhere and the server must have access to it Client side encryption data is encrypted by the client and never decrypted by the server data will be decrypted by a receiving client the server should not be able to decrypt the data AWS KMS (key management service) anytime you hear encryption for an AWS service, it is most likely KMS easy way to control access to your data, AWS manages keys for us fully integrated with IAM authorization seamlessly integrated into EBS S3 Redshift RDS SSM but you can also use the CLI / SDK KMS - Customer Master Key (CMK) Types Symmetric (AES-256) first offering of KMS, single encryption key that is used to encrypt and decrypt AWS services that are integrated with KMS use Symmetric CMKs you never get access to the key unencrypted (must call KMS API to use) Asymmetric (RSA and ECC key pairs) public (Encrypt) and private (decrypt) key used for encrypt / decrypt, or sign / verify operations the public key ios downloadable, but you can’t access the private key unencrypted use case: encryption outside of AWS by users who can’t call the KMS API Pricing able to fully manage the keys and policies create rotation policies disable enable able to audit key usage (using CloudTrail) 3 types of customer master keys (CMK) AWS managed service default CMK: free user keys created in KMS: $1 / month user keys imported (must be 256-bit symmetric key): $1 / month plus pay for API call to KMS KMS 101 anytime you need to share sensitive information, use KMS database passwords credentials to external service private key of SSL certificates the value in KMS is that the CMK used to encrypt data can never be retrieved by the user, and the CMK can be rotated for extra security never ever store your secrets in plaintext, especially in your code encrypted secrets can be stored in the code / environment variables KMS can only help in encrypting up to 4 KB of data per call if data &gt; 4KB, use envelope encryption KMS key policies control access to KMS keys, similar to S3 bucket policies different: you cannot control access without them default KMS key policy created if you don’t provide a specific KMS key policy complete access to the key to the root user = entire AWS account the root user can administer the key and all IAM accounts can use the key gives access to the IAM policies to the KMS key custom KMS key policy define users, roles that can access the KMS key define who can administer the key useful for cross-account access of your KMS key KMS Automatic Key Rotation for Costumer managed CMK (not AWS managed CMK) if enabled: automatic key rotation heppens every 1 year previous key is kept active so you can decrypt old data new key has the same CMK ID (only the backing key is changed) SSM Parameter Store secure storage for configuration and secrets optional seamless encryption using KMS serverless, scalable, durable, easy SDK version tracking of configurations / secrets configuration management using path and IAM notifications with CloudWatch events integration with CloudFormation AWS Secrets Manager newer service, meant for storing secrets capability to force rotation of secrets every X days automate generation of secrets on rotation (uses lambda) integration with Amazon RDS secrets are encrypted using KMS CloudHSM AWS provisions encryption hardware dedicated hardware (HSM = Hardware Security Module) you manage your own encryption keys entirely (not AWS) HSM device is tamper resistant supports both symmetric and asymmetric encryption (SSL/TLS keys) no free tier available must use the CloudHSM client software refshift support CloudHSM for database encryption and key management good option to use with SSE-C encryption AWS Shield AWS shield standard free service that is activated for every AWS customer provides protection from attacks such as SYN/UDP floods, reflection attacks and other layer 3 / layer 4 attacks AWS Shield Advanced optional DDoS mitigation service ($3000 per month per organization) protect against more sophisticated attack on EC2, ELB, CloudFront, Global Accelerator, Route53 AWS WAF (Web Application Firewall) protects your web applications from common web exploits (layer 7) layer 7 is HTTP (vs layer 4 is TCP) deploy on Application Load Balancer, API Gateway, CloudFront define web ACL rules can include: IP addresses, HTTP headers, HTTP body, URI strings protects from common attack - SQL injection and cross-site scripting (XSS) size constraints, geo-match rate based rules (to count occurrences of events) - for DDoS protection AWS GuardDuty intelligent threat discovery to protect AWS account uses machine learning algorithms, anomaly detection, third party data one click to enable (30 days trial), no need to install software input data includes CloudTrail log: unusual API calls, unauthorized deployments VPC flow logs: unusual internal traffic, unusual IP addresses DNS logs: compromised EC2 instances sending encoded data within DNS queries can setup CloudWatch event rules to be notified in case of findings CloudWatch events rules can target AWS lambda or SNS can protect against CryptoCurrency attacks AWS Inspector automated security assessments for EC2 instances analyze the running OS against known vulnerabilities analyze against unintended network accessibility AWS Inspector agent must be installed on OS in EC2 instances after the assessment, you get a report with a list of vulnerabilities possibilitiy to send notifications to SNS AWS Macie Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII) AWS VPC Understanding CIDR (Classless Inter-Domain Routing) - IPv4 CIDR are used for security groups rules, or AWS networking in general they help to define an IP address range a CIDR has two components the base IP the subnet mask the base IP represents an IP contained in the range the subnet masks defines how many bits can change in the IP the subnet masks basically allows part of the underlying IP to get additional next values from the base IP /32 allow for 1 IP = 2^0 /31 allow for 1 IP = 2^1 /30 allow for 1 IP = 2^2 /29 allow for 1 IP = 2^3 /28 allow for 1 IP = 2^4 /27 allow for 1 IP = 2^5 /26 allow for 1 IP = 2^6 /25 allow for 1 IP = 2^7 /24 allow for 1 IP = 2^8 /16 allow for 1 IP = 2^16 /0 allow for 1 IP = 2^32 quick memo /32 - no IP number can change /24 - last IP number can change /16 - last IP two numbers can change /8 - last IP 3 numbers can change /0 - all IP numbers can change Private vs public IP allowed ranges the internet assigned number authority established certain blocks of IPv4 addresses for the use of private (LAN) and public addresses private IP can only allow certain values 10.0.0.0 - 10.255.255.255 (in big networks) 172.16.0.0 - 172.31.255.255 (default AWS one) 192.168.0.0 - 192.168.255.255 (home networks) all the rest of the IP on the internet are public IP Default VPC walkthrough all new accounts have a default VPC new instances are launched into default VPC if no subnet is specified default VPC have internet connectivity and all instances have public IP we also get a public and private DNS name VPC in AWS - IPv4 you can have multiple VPCs in a region (max 5 per region) max CIDR per VPC is 5, for each CIDR min size is /28 = 16 IP addresses max size is /16 = 65536 IP addresses because VPC is private, only the private IP ranges are allowed your VPC CIDR should not overlap with your other networks Subnets - IPv4 AWS reserves 5 IPs addresses (first 4 and last 1 IP address) in each subnet these 5 IPs are not available for use and cannot be assigned to an instance if CIDR block is 10.0.0.0/24, reserved IP are 10.0.0.1: network address 10.0.0.2: reserved by AWS for the VPC router 10.0.0.3: reserved by AWS for mapping to Amazon provided DNS 10.0.0.4: reserved by AWS for future use 10.0.0.255: network broadcast address, AWS does not support broadcast in a VPC, therefore the address is reserved exam tip if you need 29 IP addresses for EC2 instances, you can’t choose a subnet of size /27 (32 IPs) you need at least 64 IP, subnet size /26 (64 - 5 &gt; 29, but 32 - 5 &lt;= 29) Internet Gateways internet gateways helps our VPC instances connect with the internet it scales horizontally and is HA and redundant must be created separately from VPC one VPC can only be attached to one IGW and vice versa internet gateway is also a NAT for the instances that have a public IPv4 internet gateways on their own do not allow internet access route tables must also be edited If you launch an EC2 instance, to give the instance public internet, you need to edit the route table. associate route table to the subnet add a rule in route table, so that when instance is trying to access a public IP, it will route the traffic to the internet gateway For instances in private subnet, if they were to access it through the internet gateway, they would also be accessible from the internet (not we want). So we need NAT NAT instances - network address translation allows instances in the private subnets to connect to the internet must be launched in a public subnet must disable EC2 flag: source / destination check must have elastic IP attached to it route table must be configured to route traffic from private subnets to NAT instance NAT instance will then route traffic to the internet gateway because of the route table rules. NAT Gateway AWS managed NAT, higher bandwidth, better availability, no admin needed pay by the hour for usage and bandwidth NAT is created in a specified AZ, uses an EIP cannot be used by an instance in the subnet (only from the other subnets) requires an IGW (private subnet -&gt; NAT -&gt; IGW) no security group to manage / required NAT Gateway with HA NAT gateway is resilient within a single AZ must create multiple NAT gateway in multiple AZ for fault-tolerance there is no cross AZ failover needed because if an AZ goes down it doesn’t need NAT DNS Resolution in VPC enableDnsSupport (DNS Resolution settings) default is True helps decide if DNS resolution is supported for the VPC if True, queries the AWS DNS server at 169.254.169.253 enableDnsHostname (DNS Hostname setting) False by default for newly created VPC True by default for default VPC won’t do anything unless enableDnsSupport is True if True, assign public hostname to EC2 instance if it has a public if you use custom DNS domain names in a private zone in Route 53, you must set both these attributes to true Network ACLs and Security Group Security group is the firewall of EC2 Instances. Network ACL is the firewall of the VPC Subnets. Security groups are tied to an instance whereas Network ACLs are tied to the subnet. Network ACLs are applicable at the subnet level, so any instance in the subnet with an associated NACL will follow rules of NACL. That’s not the case with security groups, security groups has to be assigned explicitly to the instance. This means any instances within the subnet group gets the rule applied. With Security group, you have to manually assign a security group to the instances. Security groups are stateful: This means any changes applied to an incoming rule will be automatically applied to the outgoing rule. e.g. If you allow an incoming port 80, the outgoing port 80 will be automatically opened. Network ACLs are stateless: This means any changes applied to an incoming rule will not be applied to the outgoing rule. e.g. If you allow an incoming port 80, you would also need to apply the rule for outgoing traffic. Rules: Allow or Deny Security group support allow rules only (by default all rules are denied). e.g. You cannot deny a certain IP address from establishing a connection. Network ACL support allow and deny rules. By deny rules, you could explicitly deny a certain IP address to establish a connection example: Block IP address 123.201.57.39 from establishing a connection to an EC2 Instance. Security Group Network ACL Operates at the instance level Operates at the subnet level supports allow rules only supports allow rules and deny rules is stateful: return traffic is automatically allowed, regardless of any rules is stateless: return traffic must be explicity allowed by rules we evaluate all rules before deciding whether to allow traffic we proces rules in number order when deciding whether to allow traffic applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on automatically applies to all instances in the subnets it’s associated with (therefore, you don’t have to rely on users to specify the security group) VPC Peering connect 2 VPCs, privately using AWS network make them behave as if they were in the same network must not have overlapping CIDR VPC peering connection is not transitive (must be established for each VPC that need to communicate with one another) e.g. if we connect VPC A to VPC B and also connect VPC B to VPC C, this doesn’t mean VPC A is connected to VPC C. you can do VPC peering with another AWS account you must update route tables in each VPC’s subnets to ensure instances can communicate VPC Peering - good to know VPC peering can work inter region, cross account you can reference a security group of a peered VPC (work cross account) VPC Endpoint endpoints allow you to connect to AWS services using a private network instead of the public www network they scale horizontally and are redundant they remove the need of IGW, NAT, etc… to access AWS services interface provisions an ENI (private IP address) as an entry point (must attach security group) - most AWS services gateway provisions a target and must be used in a route table - S3 and DynamoDB Flow Logs capture information about IP traffic going into your interfaces VPC flow logs (includes the other two) subnet flow logs elastic network interface flow logs helps to monitor and troubleshoot connectivity issues flow logs data can go to S3 / CloudWatch Logs captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift, WorkSpaces Bastion Hosts we can use a Bastion Host to SSH into our private instances the bastion is in the public subnet which is then connected to all other private subnets Bastion Host security group must be tightened exam tip: make sure the bastion host only has port 22 traffic from the IP you need. not from the security groups of your other instances Site to Site VPN Virtual Private Gateway VPN concentrator on the AWS side of the VPN connection VGW (VPC Gateway) is created and attached to the VPC from which you want to create the Site-to-Site VPN connection Customer Gateway software application or physical device on customer side of the VPN connection ip address use static, internet-routable IP address for your customer gateway device if behind a CGW (Cloud Gateway) behind a NAT, use the public IP address of the NAT Direct Connect (DX) provides a dedicated private connection from a remote network to your VPC dedicated connection must be setup between your data center and AWS direct connect locations you need to setup a Virtual Private Gateway on your VPC access public resources (S3), and private (EC2) on same connection use cases increase bandwidth throughput: working with large data sets - lower cost more consistent network experience - applications using real time data feeds hybrid environments (on premises + cloud) supports both IPv4 and IPv6 Direct connect gateway if you want to setup a direct connect to one or more VPC in many different regions (same account), you must use a direct connect gateway Connection types dedicated connections 1 Gbps and 10 Gbps capacity physical ethernet port dedicated to a customer request made to AWS first, then completed by AWS direct connetion partners hosted connections 50 Mbps, 500 Mbps to 10 Gbps connection requests are made via AWS direct connect partners capacity can be added or removed on demand 1,2,5,10 Gbps available at select AWS direct connect partners lead times are often longer than 1 month to establish a new connection Encryption data in transit is not encrypted but is private AWS direct connect + VPN provides an IPsec-encrypted private connection good for an extra level of security, but slightly more complex to put in place Resiliency High resiliency for critical workloads one connection at multiple locations maximum resiliency for critical workloads maximum resilience is achieved by separate connections terminating on separate devices in more than one location Egress (outgoing) only internet gateway Egress only internet gateway is for IPv6 only similar function as a NAT, but a NAT is for IPv4 good to know: IPv6 are all public addresses therefore all our instances with IPv6 are publicly accessible Egress only internet gateway gives our IPv6 instances access to the internet, but they won’t be directly reachable by the internet after creating an Egress only internet gateway, edit the route tables AWS Private Link - VPC Endpoint Service How to expose services in your VPC to other VPCs? Option1: make it public goes through the public www tough to manage access Option2: VPC peering must create many peering relations (peering relation is one-to-one) opens the whole network(maybe you just want one of your services to be exposed, not the whole VPC) AWS Private Link most secure and scalable way to expose a service to 1000s of VPCs does not require VPC peering, internet gateway, NAT, route tables requires a network load balancer (service VPC) and ENI (customer VPC) customer VPC =&gt; ENI =&gt; private link =&gt; NLB =&gt; service VPC if the NLB is in multiple AZ, and the ENI in multiple AZ, the solution is fault tolerant EC2 Classic and AWS ClassicLink (deprecated) EC2 classic: instances run in a single network shared with other customers Amazon VPC: your instances run logically isolated to your AWS account classLink: allows you to link EC2 instances to a VPC in your account must associate a security group enables communication using private IPv4 addresses removes the need to make use of public IPv4 addresses or Elastic IP addresses Likely to be distractors at the exam AWS VPN Cloudhub provide secure communication between sites, if you have multiple VPN connections low cost hub-and-spoke model for primary or secondary network connectivity between locations it is a VPN connection so it goes over the public internet Transit Gateway for having transitive peering between thousands of VPCs and on premises, hub-and-spoke(star) connection regional resource, can work cross-region share cross account using Resource Access Manager (RAM) you can peer transit gateway across regions route tables: limit which VPC can talk with other VPC works with direct connect gateway, VPN connections supports IP multicast (not supported by any other AWS service) share direct connect between multiple accounts VPCs =&gt; transite gateway -&gt; direct connect gateway =&gt; AWS direct connect endpoint =&gt; customer router Transit Gateway: site-to-site VPC ECMP ECMP = equal cost multi path routing routing strategy to allow to forward a packet over multiple best path use case: create multiple site-to-site VPN connections to increase the bandwidth of your connection to AWS VPC Summary CIDR: IP range VPC: Virtual Private Cloud =&gt; we define a list of IPv4 or IPv6 CIDR Subnets: Tied to an AZ, we define a CIDR for a subnet Internet gateway: at the VPC level, provide internet access Route table: must be edited to add routes from subnets to the IGW, VPC peering connections, VPC endpoints, etc… NAT Instances: gives internet access to the instances in private subsnets, old, must be setup in a public subnet, disable source / destination check flag NAT gateway: managed by AWS, provides scalable internet access to private instances, IPv4 only Private DNS + route 53: enable DNS resolution + DNS hostnames (VPC) NACL: stateless, subnet rules for inbound and outbound, don’t forget ephemeral ports Security groups: stateful, operate at the EC2 instance level VPC Peering: connect two VPC with non overlapping CIDR, non transitive VPC endpoints: provide private access to AWS services (S3, DynamoDB, CloudFormation, SSM) within VPC, no need to go through internet gateway Bastion Host: public instance to SSH into, that has SSH connectivity to instances in private subnets Site to Site VPN: setup a customer gateway on Data center, a Virtual Private Gateway on VPC, and site to site VPN over public internet Direct Connect: setup a virtual private gateway on VPC, and establish a direct private connection to an AWS direct connection location Direct Connect Gateway: setup a direct connect to many VPC in different regions internet gateway Egress: like a NAT gateway, but for IPv6 Private Link / VPC endpoint services connect services privately from your service VPC to customers VPC doesn’t need VPC peering, public internet, NAT gateway, route tables must be used with network load balancer and ENI ClassicLInk: connect EC2 classic instances privately to your VPC VPC Cloudhub: hun and spoke VPN model to connect your sites Transit gateway: transitive peering connections for VPC, VPN and DX Networking cost in AWS use private IP instead of public IP for good savings and better network performance use same AZ for maximum savings (at the cost of HA) IPv6 for VPC IPv4 cannot be disabled for your VPC and subnets you can enable IPv6 to operate in dual-stack mode your EC2 instance would get at least a private internal IPv4 and a public IPv6 they can communicate using either IPv4 or IPv6 IPv6 Troubleshooting if you cannot launch an instance in your subnet it is not because it cannot acquire an IPv6 (the space is very large) it is because there are no available IPv4 in your subnet solution: create a new IPv4 CIDR in your subnet Disaster Recovery Overview any event that has a negative impact on a company’s business continuity or finances is a disaster disaster recovery is about preparing for and recovering from a disaster what kind of disaster recovery on premises =&gt; on premises: tranditional DR, very expensive on premises =&gt; AWS Cloud: hybrid recovery AWS Cloud Region A =&gt; AWS Cloud Region B need to define 2 terms RPO: recovery point objective RTO: recovery time objective Backup and restore (High RPO) needs more time to recover Pilot Light a small version of the app is always running in the cloud useful for the critical core (pilot light) very similar to backup and restore faster than backup and restore as critical systems are already up Warm Standby full system is up and running but at minimum size upon disaster, we can scale up to production load Multi Site / Hot Site Approach very low RTO (minutes or seconds) - very expensive full production scale is running AWS and on premises Disaster Recovery Tips backup EBS snapshots, RDS automated backups / snapshots, etc… regular pushes to S3 / S3 IA / Glacier, Lifecycle policy, cross region replication from on premises: snowball or storage gateway HA use route 53 to migrate DNS over from region to region RDS multi AZ, elastiCache multi AZ, EFS, S3 site to site VPN as a recovery from direct connect replication RDS replication, AWS Aurora + global databases database replication from on premises to RDS storage gateway automation cloudFormation / elastic beanstalk to re-create a whole new environment recover / reboot EC2 instance with cloudwatch if alarms fails AWS lambda functions for customized automations chans Netflix has a simian-army randomly terminating EC2 DMS - Database Migration Service quickly and securely migrate databases to AWS, resilient, self healing the source database remains available during the migration supports homogeneous migrations: Oracle to Oracle Heterogeneous: Microsoft SQL server to Aurora continuous Data replication using CDC you must create an EC2 instance to perform the replication tasks AWS Schema Conversion Tool (SCT) convert your database’s schema from one engine to another example OLTP: sql server or Oracle =&gt; MySQL, PostgreSQL, Aurora example OLAP: Teradata or Oracle =&gt; Amazon Redshift you do not need to use SCT if you are migrating the same DB engine on premise postgreSQL =&gt; RDS postgreSQL the DB engine is still PostgreSQL (RDS is just a platform) On Premises Strategy with AWS ability to download Amazon Linux 2 AMI as a VM use VMWare, KVM, VirtualBox to run VM VM import / Export migrate existing applications into EC2 create a DR repository strategy for your on premises VMs can export abck the VMs from EC2 to on premises AWS application discovery services gather information about your on premises server to plan a migration server utilization and dependency mappings track with AWS migration hub AWS database migration server (DMS) replicate on premise =&gt; AWS AWS =&gt; AWS AWS =&gt; on premises AWS server migration service (SMS) incremental replication of on premises live servers to AWS AWS DataSync move large amount of data from on premise to AWS can synchronize to: Amazon S3 (any storage class, including Glacier), Amazon EFS, FSx for Windows move data from your NAS or file system via NFS or SMB replication tasks can be scheduled hourly, daily or weekly leverage the DataSync agent to connect to your systems can setup a bandwidth limit AWS Backup fully managed service centrally manage and automate backups across AWS services no need to create custom scripts and manual processes supported services FSx EFS DynamoDB EC2 EBS RDS Aurora AWS storage gateway (volume gateway) supports cross region backups supports cross account backups supports PITR (point in time recovery) for supported services on demand and scheduled backups tag based backup policies you create backup policies known as Backup Plans backup frequency backup window transition to cold storage retention period More Solution Architectures Compute and Networking EC2 enhanced networking (SR-IOV) higher bandwidth, higher PPS (packet per second), lower latency option1: Elastic Network Adapter (ENA) up to 100 Gbps option2: Intel, up to 10 Gbps, legacy Elastic Fabric Adapter (EFA) improved ENA for HPC, only works for Linux great for inter node communications, tightly coupled networks leverages message passing interface (MPI) standard bypasses the underlying linux OS to provide low latency, reliable transport CloudFormation a declarative way of outlining your AWS infrastructure, for any resources for example, within a CloudFormation template, you say I want a security group 2 EC2 instances using this security group 2 Elastic IPs for these EC2 machines 1 S3 bucket a load balancer in front of these machines then cloudformation creates those for you, in the right order, with the exact configuration that you specify templates have to be uploaded in S3 and then referenced in CloudFormation to update a template, we can’t edit previous ones, we have to re-upload a new version of the template to AWS stacks are identified by a name deleting a stack deletes every single artifact that was created by CloudFormation Deploying CloudFormation templates manual way editing templates in the CloudFormation designer using the console to input parameters, etc… automated way editing templates in a YAML file using the AWS CLI to deploy the templates recommended way when you fully want to automate your flow CloudFormation - stacksets create, update, or delete stacks across multiple accounts and regions with a single operation administrator account to create stacksets trusted accounts to create, update, delete stack instances from stacksets when you update a stackset, all associated stack instances are updated throughout all accounts and regions AWS Step Functions build serverless visual workflow to orchestrate your lambda functions represent flow as a JSON state machine features: sequence, parallel, conditions, timeouts, error handling can also integrate with EC2, ECS, on premise servers, API gateway possiblity to implement human approval feature AWS SWF - simple workflow service coordinate work amongst applications code runs on EC2 concept of activity step and decision step has built in human intervention step step function is recommended to be used for new applications, except if you need external signals to intervene in the processes if you need child processes that return values to parent processes Amazon EMR EMR stands for Elastic Map Reduce EMR helps creating Hadoop clusters (big data) to analyze and process vast amount of data the clusters can be made of hundreds of EC2 instances also supports Apache Spark, HBase, Presto, Flink EMR takes care of all the provisioning and configuration auto scaling and integrated with Spot instances use cases: data processing, machine learning, web indexing, big data AWS Opsworks Chef and Puppet help you perform server configuration automatically, or repetitive actions they work great with EC2 and on premises VM AWS Opsworks = managed Chef and Puppet it is an alternative to AWS SSM they help with managing configuration as code helps in having consistent deployments works with Linux and Windows AWS Elastic Transcoder convert media files (video + music) stored in S3 into various formats for tablets, PC, smartphone, TV etc… features: bit rate optimization, thumbnail, watermarks, captions, DRM, progressive download, encryption 4 components jobs: what does the work of the transcoder pipeline: queue that manages the transcoding job presets: template for converting media from one format to another notifications: SNS for exmaple pay for what you use, scales automatically, fully managed AWS WorkSpaces managed, secure cloud desktop great to eliminate management of on premise VDI (virtual desktop infrastructure) on demand, pay per by usage secure, encrypted, network isolation integrated with Microsoft active directory AWS AppSync store and sync data across mobile and web apps in real time makes use of GraphQL (mobile technology from Facebook) client code can be generated automatically integrations with DynamoDB real time subscriptions Cost Explorer visualize, understand and manage your AWS costs and usage over time create custom reports that analyze cost and usage data analyze your data at a high level, total costs and usage across all accounts choose an optimal savings plan forecast usage up to 12 months based on previous usage CheatSheet CodeCommit: service where you can store your code. Similar service is GitHub CodeBuild: build and testing service in your CICD pipelines CodeDeploy: deploy the packaged code onto EC2 and AWS Lambda CodePipeline: orchestrate the actions of your CICD pipelines (build stages, manual approvals, many deploys, etc) CloudFormation: Infrastructure as Code for AWS. Declarative way to manage, create and update resources. ECS (Elastic Container Service): Docker container management system on AWS. Helps with creating micro-services. ECR (Elastic Container Registry): Docker images repository on AWS. Docker Images can be pushed and pulled from there Step Functions: Orchestrate / Coordinate Lambda functions and ECS containers into a workflow SWF (Simple Workflow Service): Old way of orchestrating a big workflow. EMR (Elastic Map Reduce): Big Data / Hadoop / Spark clusters on AWS, deployed on EC2 for you Glue: ETL (Extract Transform Load) service on AWS OpsWorks: managed Chef &amp; Puppet on AWS ElasticTranscoder: managed media (video, music) converter service into various optimized formats Organizations: hierarchy and centralized management of multiple AWS accounts Workspaces: Virtual Desktop on Demand in the Cloud. Replaces traditional on-premise VDI infrastructure AppSync: GraphQL as a service on AWS SSO (Single Sign On): One login managed by AWS to log in to various business SAML 2.0-compatible applications (office 365 etc)","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"Denodo Data Virtualization","slug":"Denodo-Data-Virtualization","date":"2021-08-02T00:28:08.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2021/08/02/Denodo-Data-Virtualization/","link":"","permalink":"http://hellcy.github.io/2021/08/02/Denodo-Data-Virtualization/","excerpt":"","text":"Data Virtualization Data Virtualization is a logical layer which Delivers business data in real time to consuming applications or business users Integrates data from disparate sources, locations, and formats, without replicating the data Enables faster access to all data, less replication and cost, and more agility to change Connects: to disparate data sources Combines: related data into views Consume: in business applications Pillars of Data Virtualization Universal Data Access: hides the complexity of underlying data sources Unified Virtual Data Layer: Common virtual canonical business model Universal Data Publishing: multiple publishing interfaces and access patterns Unified Data Governance: Central point for data governance Agile High Performance: Multiple delivery process to meet different SLA(Service Level Agreement)'s Data Integration Strategies Data silos make it challenging for business users to access and analyze all of the available data within an organization. To bring the data together, companies typically use Extract, Transform, and Load processes (ETL) copy the data from different silos and move it to a central location (e.g. Data Warehouse) Enterprise Service Buses (ESB) establish a communication system for applications, enabling them to share information Data Virtualization (DV) reate real time, integrated views of data in data silos, and makes them available to applications, analysts and business users ETL In an ETL process the data is extracted from a source, transformed and loaded into another data system Pros and Cons (Pro) ETL processes are efficient and effective at moving data in bulk (Con) Moving data to another system means that a new repository must be maintained (Con) ETL processes are not collaborative, the end-users must wait until the data is ready ESB Pros Applications are decoupled They can be used for orchestrate business logic using message flows Cons ESBs are sutable only for operational use cases that involve small result sets Queries are static, can only be scheduled and restricted to one source at a time. Data Virtualization Data virtualization supports a wide variety of sources and targets, which makes it an ideal data integration strategy to complement ETL processes and ESBs for ETL Seamlessly connecting on-premises and cloud components real-time integration of disparate data sources for ESB The ESB can connect to the data virtualization layer to access external sources that cannot be easily added to the ESB Donodo Platform Architecture Denodo platform comes with the various in-built components as follows Virtual Dataport: it is the core component, the data virtualization server Data catalog: a self-service data discovery tool Scheduler: it is used for scheduling and executing batch jobs Solution Manager: it provides cantralized management of all the servers Virtual Dataport Virtual Dataport enables business applications to process a series of distributed and heterogeneous data sources, as if the data were contained in a large virtual database it acts as a mediator that provides a structured and unified view of the data contained in all the data sources included in the system Data Catalog help business users, data scientists to discover the data assets that are available through the Denodo platform it promotes self-service and discovery capabilities for business users, enabling them to explore both data and metadata in a single web fronend tool with this tool, the end user will be able to access a graphical representation of the business entities and associations, as well as the data linage and tree view information it includes reporting capabilities and export options (CSV, excel, Tableau etc…) Data catalog is designed to provide organizations with three core benefits Enterprise-wide directory of datasets available for consumption from a business friendly web interface fast business decisions by enabling a much more rapid and comprehensive understanding and access to all enterprise data and metadata Governed self service BI/Analytics and data consumption that is still owned and maintained by IT Significant IT time, resource and cost reduction Denodo Scheduler exporting data to a database or files such as Tableau Data Export, delimited files, CSVs, flat files, MS Excel Indexing data to enable google-like keyword searches on content Cache loading Periodical management tasks such as data statistic gathering, looking for changes in sources, etc… Denodo Scheduler enables a hybrid, DV to ETL pattern Jobs can be scheduled for a specific time and can have dependencies of other jobs Detailed execution reports that can be sent by email Support for data extraction from sources with limited query capabilities Persistent jobs and Transparent retries in the event of failure Possiblity of parallel execution of the different queries involved in the same job Denode Solution Manager easy promotion of new metadata to different environments in multi-location architectures with different clusters from a centralized web UI it provides automated lifecycle management capabilities, DevOps tasks are greatly simplified and gaining the agility it’s centralized web UI allows a simpler and more efficient management of deployments, streamlining continuous delivery strategies. Denodo platform tools and applications Denode platform control center Allow to start and stop all the Denodo platform servers and its tools it allows configuring some additional functions it can be launched either by selecting the shortcuts created during the installatioin Or by executing the denodo_platform.sh script from the bin folder of the Denodo platform installation Web design studio A web based tool, aimed for developers to develop the Virtual DataPort elements Web design studio can be started from the Denodo platform control center by executing the designstudio_startup.sh script from the bin folder it can be accessed using the URL: http://localhost:9090/denodo-design-studio currently, it does not support configuring the administration options Data Catalog A web based self service tool, aimed for business users to query and serach the organization data they can generate new knowledge and can take better decisions data catalog can be started, either from the Denodo platform control center by executing the datacatalog_startup.sh script from the bin folder of the Denodo platform installation the default URL is: http://localhost:9090/denodo-data-catalog this tool is not included in Denodo Standard 8.0 version Scheduler Aministration Tool A web tool, used to connect to the scheduler server for creating and managing the scheduler jobs scheduler administartion tool can be started, either using the Denodo platform control center executing the scheduler_webadmin_startup.sh script from the bin folder of the Denodo platform installation the default URL to launch the tool is: http://localhost:9090/webadmin/denodo-scheduler-admin Solution Manager Administration Tool A web tool, used to connect to the Solution Manager and License Manager Servers it provides an unified access to all the web applications of the Denodo platform Data Catalog Web Design Studio Diagnostics and Monitoring tool Scheduler web administration tool The Solution Manager Administration Tool can be started, either by Using the Denodo Solution Manager Control center Using the solutionmanagerwebtool_startup.sh script from the bin folder of the Solution Manager installation the default URL to launch the Solution Manager Administration tool http://localhost:9090/solution-manager-web-tool Diagnostics and Monitoring tool A web tool, aimed for both developers and administrators who wants to monitor the current state of a server, a cluster or an environment diagnose the past state of a server to identify the cause of a problem diagnostics and monitoring tool can be started, either using The Denodo platform control center by executing the diagnosticmonitoringtool_startup.sh script default URL: http://localhost:9090/diagnostic-monitoring-tool Southbound Connectors Specialized connectors to access specific data repositories or applications to retrieve their schema and data these connectors are configurable by using a virtual editor or SQL like commands that are part of the Denodo platform it helps in creating data sources to retrieve data from a repository of data in the Denodo platform E.g. a relational database, a REST or SOAP web service, a MS Excel file, etc… Data Model The Derived views are created over base views to combine the data from different sources or to apply any kind of transformations Denodo platform offers graphical drag and drop tool for modeling the derived views it used extended relational algebra for data combination and transformation. Northbound Connectors One of the essential capabilities of Data Virtualization is about the different access mechanisms provided to the consumers, these access mechanisms are called Northbound Connectors Denodo platform offers flexible delivery options to suit all type of users and application at almost no cost if facilitates the creation of a single point of access and interaction with the underlying data source and abstracted views in a standard way Denodo Platform Security Data in Transit When users access data from Denodo Platform the data is moving Data moves through the network From the data sources to Denodo From Denodo to the final users and consuming applications This is referred as Security in Transit To ensure that the data is well protected and secure, it is possible to encrypt the connection Data in transit refers to all communications between the Denodo Platform and Data sources, between data consumers and Denodo Platform it can be secured through TLS at the connection level Denodo supports TLS 1.2 or 1.3 TLS configuration is automatized via a Denodo TLS configurator script Any encrpytion algorithm supported by the default Java Cryptography Providers of the Denodo Platform JRE (Java 11) can be used Strongest ciphers can be enabled installing Java Cryptography Extension (JCE), unlimited strength jurisdiction policy files when full encryption at the transport level is not required Denodo’s build-in functions for encryption/decryption can be selectively applied to sensitive fields to prevent unauthorized access. Data in Rest Denodo Platform stores some opeartional data in a relational database and hard drive, these data are called as Data at Rest Metadata of the various Denodo Platform components and elements Data sources, views, stored procedures, web serviees sensitive information on the configuration of the server Data cached in a relational database data swapped to disk during query execution The Data at Rest can be secured by Denodo as follows Metadata: sensitive information are stored encrypted/hashed in the metadata repository Additiionally, you can enable Transparent Data Encryption to encrypt the database Cached Data: Denodo will transparently leverage any encryption mechanism available in the selected cache system For example, Orable Transparent Data Encryption Swapped Data: encrypt the swap folder by using OS file system encryption Authentication Protocols Used to publish data to consuming application standard username/password security mechanism Kerberos support HTTP-based authentication pass-through for the OData interface Web service security with HTTPS, HTTP basic/Digest, SAML 2.0, OAuth 2.0, HTTP SPNEGO and WS-Security protocols OAuth 2.0 in JDBC, ODBC connections is also available Denodo also offers different alternatives to integrate with identity, authentication and authorization services Denodo built-in security Integration with external entitlement services (LDAP/AD) Single sign on using OAuth, and SAML Integration with external custom entitlement service with specific security policies Performance Static Optimization The Denodo Optimizer analyzes the query and simplify it by rewriting it to improve the performance it focuses on maximize query delegation to the data sources it applies by default for all the queries it minimizes the network traffic it reduces the data volume that needs to be processed in the virtual layer it applies one or more simplifications to the same query Connecting and combining Data with Denodo Denodo platform allows to add remote databases as sources the recommended way to connect to databases is always using the JDBC data source JDBC is an acronym referred to Java Database Connectivity JDBC defines an API for the Java Programming language to access to any database using a suitable driver Denodo platform supports out-the-box a wide variety of databases relational databases oracle, MS server, PostgreSQL, MySQL, Amazon Aurora, Azure SQL, Apache Derby Data Warehouses Snowflake, Amazon Redshift, Azure SQL Data Warehouse, Google BigQuery, SAP HANA, Teradata, Yellowbrick Interactive Query Services Hive, Impala, Amazon Athena, Presto, Spark SQL By default, Denodo platform does not include some supported JDBC drivers, although it includes the adapter When a driver is not provided by Denodo, the .jar files of the driver must be uploaded to Virtual Dataport server using the Administration tool Import option in the File &gt; Extensioin Management &gt; Libraries When Denodo does not include an specific adapter for a data source if possible, select a similar adapter for example, to connect to MySQL 6.0, the adapter MySQL 5 can be selected if there are not similar adapters, the generic adapter can be used in both cases the driver must be provided and uploaded to the server Import a SQL Query Instead of using a table as the source for a base view, it is possible to generate a view from a SQL query the query will be sent to the database without modifications but Denodo platform may delegate to the source a bigger query over the given sentence, for example, when delegating conditions this new view will inherit its internal schema from the meta information associated to the query results This is useful in several scenarios using specific syntax to the accessed database that cannot be replicated in virtual dataport for example, using a function that is not available in virutal dataport situations when an exact query is needed for example, complex queries already created instead of duplicting them in Denodo using queries that have been optimized for the specific database being accessed. Import Stored Procedures Base views can be created graphically over stored procedure of the following databases IBM DB2, MS SQL Server and Oracle If the procedure has one or more cursors, one of the following options can be selected Stream output at the specified cursor the data that is returned by the cursor is flattened do not stream cursor parameters the data returned by the cursor will be in an array Querying views from Execution Panel Virtual Dataport is able to import data coming from different systems and with different structure and format to retrieve data form a source, some elements are created in Denodo A data source representing a repository of data base views repreesnting entities in the source these base views can be executed in virtual dataport Basic Derived Views Derived Views are new views created over other Denodo Views A derived view is another element in the virtual dataport catalog it may then appear in the FROM clause of any VQL query and may be used as a base for constructing new views or queries THe cache may be used on a derived view, and user privileges can be defined Functions Functions are used to generate new attributes in the schema of a view a function is defined as an identifier and a list of arguments that can be constants, fields or other functions e.g. len(‘this function gets the length of this text’) All virtual dataport functions can be browsed by using CTRL + SPACE in the Specify WHERE expression tab of the execution panel in the field expression dialog available while defining new fields for views Custom functions allow users to extend the set of functions available in Virtual dataport they are used in the same way as every other function Implemented as JAVA classes included in a JAR file that is added to the Virtual Dataport functions have to be stateless each function has to have a unique name and a different JAVA class different functions can be grouped in a single JAR file Union Views A union view allows the tuples from various input views to be output as a single view in standard relational algebra, union operation implies all the relations must have the same schema extended union all is also used whenever any of the input relations has an attribute that is not present in the others, it is added to the resulting view. When creating a view, more than just a UNION operation is performed some of the operations are performed through intermediate projection operations automatically added by virtual dataport e.g. changing the name of the view, deleting fields other intermediate opeartions can be added manually where conditions tab group by tab all the operations performed are shown in the tree view screen JSON/XML data source Virtual Dataport allows to import data from XML or JSON these formats are often used for transmitting structured data over a network connection these data sources allows Denodo platform to connect to RESTful web services connecting to a resource URI which returns a response with a payload formatted in XML or JSON (google APIs, Twitter etc…) For JSON data source a tuple root can be selected /JSONFile is the default option modify it when it is needed to access only a part of the document rather than the entirety of it. For XML date sources there are two options to create base views do not stream output, the base view will return a single compound value aggregating all the document information stream output at the specified level: it does not require the entire document to be realized in memory before processing it the contents will be split into different tuples Web service data source A web service is a software system designed to support interoperable machine-to-machine interaction over a network virtual dataport can use a SOAP web service as a data source and performs queries over it by invoking its operations in a web service source, the fields of the query are the input parameters of the web service operation Join Types A join view allows the relational algebra operation with the same name to be executed on a series of input views it combines records from two or more views by using values common to each INNER JOIN (default) LEFT OUTER JOIN RIGHT OUTER JOIN FULL OUTER JOIN CROSS JOIN each row from the first relation is combined with each row from the second one, this join returns the Cartesian product of the sets of rows from the joined tables Join Execution Methods Depending on the join type and the selected order, the user can define different join execution methods Merge Hash Nested Nested Parallel If the join execution method is not set, design studio will try to calculate the best query plan and all the queries against the join view will follow this plan Merge This join method requires the input data to be ordered by the join attributes, this can be achieved as follows when the data source allows opeartions, virtual dataport will automatically request the data ordered if it does not allow sorting operations, the ORDERBY clause can be used in the underlying views to sort the data in the required order the collation of both sources must be the same if the data source returns directly the data in the appropriate order the base view options allow administrators to specify it using the ‘Fields by which the data is sorted in the source’ field data sources are queried in parallel, ordered by the join conditiion and the non-matching data are discarded when valid, this strategy is usually the most efficient one and uses less memory than the others Hash Creates an in-memory hash table: for each different join attribute values on the right side, it will insert a key-value pair into the hash table the left side tuples that match these values will be in the final join view output This is the most efficient join method when the input views that cannot be ordered or are large or the query latency times for the data sources are high minimizes the sub-queries to the sources Nested Obtains data from one input view, then for each record obtained a subquery is executed in the other view using the join conditions if the second view comes from a relational data source, virtual dataport will optimize the process by running a single subquery that retrieves all the matching data from the second source if the amount of values obtained from the left side view of the join exceed a certain value (200 default), the server will group the queries to obtain the data from the rigih-side A nested join is a good option when the right side returns a lot of tuples and the left side only returns a few Nested Parallel Same as Nested but this method executes the sub queries in parallel an additional parameter allows to specify the maximum number of subqueries launched in parallel if the second data source is of JDBC/ODBC type, the use of NESTED PARALLEL is usually unnecessary and less efficient, NESTED option must be used Compound Types and Flatten Operation Denodo virtual dataport supports modeling data types with a complex structure using the types register and array an element of the type array can be considered a subview and an array type always has an associated register type that acts like the schema of the subview it is modeling Sometimes it is desirable to flatten a compound field that contains an array of registers typical in XML and web service data sources Flattening a register every register element will be a new field on the derived view output every array elemenet will be a new row it is possible to perform the inverse operation of faltten to create array or register elements from several fields this can be done using the NEST or REGISTER functions","categories":[],"tags":[{"name":"Data Virtualization","slug":"Data-Virtualization","permalink":"http://hellcy.github.io/tags/Data-Virtualization/"}]},{"title":"TIBCO Data Virtualization","slug":"TIBCO-Data-Virtualization","date":"2021-07-28T00:20:39.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/07/28/TIBCO-Data-Virtualization/","link":"","permalink":"http://hellcy.github.io/2021/07/28/TIBCO-Data-Virtualization/","excerpt":"","text":"Virtual Views Virtual Data Table Defined by SQL and TDV Metadata Contains SQL SELECT and any ANSI-Standard SQL Often federate data from multiple physical sources Encapsulate business meaning Hide complexity Enable re-use Views Created graphically on Model and Grid tabs Created Manually on SQL tab Graphical tabs will be removed Graphical tabs can be re-created in most cases Graphical tabs are not requried Move freely between graphical and manual modes Custom Functions User-defined SQL construct Accept parameters, return a single scalar output May be used in SELECT or WHERE clause Example: SELECT UPPER(column_name) AS column_name_upper TDV custom functions created by promoting SQL scripts or custom Java procedures Encapsulate business functionality specific to your enterprice Enable re-use across many projects and many data sources Any script or procedure that outputs a single scalar can be promoted to a Custom Function Layered Development Enhance the development process but does not reduce performance at run time Enables reuse of virtual resources within and across projects Layers are flexible Physical Layer Abstract away the physical characteristics of the data on individual data sources We generally build virtual views that are almost identical to the underlying physical data structures Business Layer Building coarse-grained data resources that have some important enterprise wide meaning and might be used by many different projects Application Layer Tailor the business layer resources to meet the needs of individual projects Publishing Virtual Databases Makes TDV resources accessible to authorized consumers Accessible as a Database via JDBC, ODBC, ADO.NET, OData Accessible as Web Services via REST, SOAP Many resource types may be published You can create as many Virtual Databases as you like Catelogs and Schemas are optional, However, Catelogs are required for ODBC clients Changes are automatically propagated to published resources Publishing Web Services we could publish procedures as Web Services In most cases, it is not good practice to publish entire database tables as Web Services, because there might be hundreds of millions of rows, and web services over HTTP are generally not aimed at such use cases Stored procedures however, can accept input parameters which can be used to filter results, therefore, a common practice is to wrap a view within a stored procedure using one or more input parameters that force the data consumer to filter data requests Changes are automatically propagated to published resources Relational Data Source Connections Specify connection details Host, port, database name, user ID, password Introspect the data source and gather matadata Catalogs, schemas, tables, columns, etc… Accept and respond to queries Handle insert, update, and delete transactions Execute stored procedures Expose database artifacts for use in virtual views Understand capabilities of physical databases A JDBC Driveer .jar is required TDV is permitted to bundle some drivers other drivers must be downloaded and installed This is a one-time process Copy the driver into the appropriate TDV driver TDV restart is required REST Clients Uniform interface to web resources Based on URIs Manipulate textual represntations of web resources Based on HTTP verbs (GET, POST, DELETE, UPDATE) Human-readable and self-describing Stateless Contract-free Automate Authentication Process Soap UI cURL Chrome Advanced REST Client SQL Script ANSI-SQL-compliant Scripting language Enables procedural logic, including Conditional execution Looping Pipelining Exception handling Etc… Procedural logic Parameter-driven processing Calls to system procedures and custom procedures Exception handling and logging Scripts can call other resources, including Other scripts Java procedures Packaged queries TDV procedure library XSLT and Streaming Transformations XSLT Transformations Extensible Stylesheet Language Transformations Industry-standard transformation language In DV, most commonly used for flattening XML Streaming Transformations Similar to XSLT Editor Useful for large data sets Relational data structures provide a common format for data federation Hierarchical XML structures must be flattened in order to participate in data federation projects Procedure Joins A special type of join between a View and a Stored Procedure Stored Procedure is executed once per View row - for unique values only Data from the View row provides input parameters to the Procedure Results from all executions are aggregated and then used for the join Augment View data Complex calculations Procedural logic External data in non-standard format Triggers Execute based on Time parameters System events User-defined events JMS messages Actions include Send email Execute a procedure Gather statistics Re-introspect Notify adminstrators of important TDV conditions Provide automated response to important TDV conditions Automate common maintenance tasks Enable developers to execute multiple asynchronous actions TDV Rights System-wide capabilities, including Tools, such as Studio Administration, such as config, status, and users Access to resources, such as Views and Procedures Rights may be assigned to Groups Users During development Prevent unauthorized users from accessing developer tools and resources Enable developers to access needed resources During ongoing operation Enable System Administrators to limit access to operational controls SOAP Data Sources Enable TDV to introspect SOAP based web services Responses can be transformed, federated and published Access to data on internet or intranet Commonly-used API for enterprise applications Cache Index Management Indexes can improve Cache read times for Views and Procedures Single-Table, Multi-Table Full and inCremental Refreshes Index Management helps Reduce cache refresh time reduce index create/update time Manually-Indexes Cache Single-Table Cache Multi-Table Cache Caching Multi Table Materialized Views or Procedures Wide range of relational database targets Simple or highly abstract Automatic refresh on configurable schedule Cache data held: In a relational database - contrasts with file cache On multiple tables - contrasts with single-table cache Useful when Response time trumps latency A physical data source has restricted access Efficient use of indexes is desirable (*) A specific benefit of multi table caching is that it provides physical separation of cache versions which means indexes on a cache are more efficient. Single table caches use a cache key column to maintain logical separation of current and previous cache versions, data rows representing the previous expired version are not deleted until all transactions using the old version have completed, this means the old and new cache data may exist simultaneously in a cache for a period of time. TDV will use the appropriate cache key to ensure that correct data is always returned. However, database indexes built on cache columns may be inefficient because they will contain extraneous rows. Multi-table cache avoids this issue and enables indexes to be as efficient as possible, but it takes more space. Caching Single Table Materialized Views or Procedures Wide range of relational database targets Simple or highly abstract Automatic refresh on configurable schedule Cache data held: In a relational database - contrasts with file cache On multiple tables - contrasts with single-table cache Useful when Response time trumps latency A physical data source has restricted access Caching Policies All or nothing Define cache refresh/expiration schedules for groups of resources Maintain consistency across multiple cached resources Enhance cache performance when dependencies are present Not supported for incremental caching Enhance data consistency across multiple caches Ensure accuracy and efficiency with dependent caches Cache policy can mix single-table and multi-table caching Caching Incremental Pull Based Initial - full load Subsequent refreshes - changes only useful when Cached data set is large full refresh is time-consuming Caching Stored Procedures Cache resutls for stored procedure ‘Variants’ Each variant cached upon first use Number of cached variants is configurable Default is 32, max is 99999999 when limit is reached, LRU variant is purged All input parameters must be scalars Performance improvements for Long-running procedures, external web services highly repetitive, high-concurrency procedures Unreliable data source connections CORS operations Cross-Origin Resource Sharing HTTP security standard for scripts and other operations Applies to script operations like XMLHttpRequest Servers may permit or deny CORS requests Does not apply to simple request like &lt;img&gt; src Requests(GET HEAD POST) execute in one step other requests must be pre-flighted using OPTIONS Customers serve HTML template pages from a web server in one domain and then populate these pages with REST resources from served from a TDV instance Custom Datasource Adapters New datasource adapters created by reconfiguring a template based on an existing adapter No coding required The template may be a product-specific datasource, or a generic JDBC datasource Three Common use cases Settings changes on a supported datasource custom functionality in a supported datasource connectivity to a datasource not supported by TDV out of the box Custom Java Procedure (CJP) An API that enables Java code to interact with TDV TDV provides a set of interfaces to be implemented The CJP is installed as a data source in TDV","categories":[],"tags":[{"name":"Data Virtualization","slug":"Data-Virtualization","permalink":"http://hellcy.github.io/tags/Data-Virtualization/"}]},{"title":"MySQL and Database Design","slug":"MySQL-and-Database-Design","date":"2021-07-23T05:10:31.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/07/23/MySQL-and-Database-Design/","link":"","permalink":"http://hellcy.github.io/2021/07/23/MySQL-and-Database-Design/","excerpt":"","text":"Data Definition Language 123456789SHOW DATABASES;CREATE DATABASE Test;USE Test;SHOW TABLES;DROP DATABASE Test; Data Types INT: Whole numbers FLOAT(M, D): Decimal numbers (approximate) M: length D: length after the decimal point allow rounding, e.g. for FLOAT(3,1), number 6.25 will be rounded up to 6.3 DECIMAL(M, D): Decimal numbers (percise) CHAR(N): Fixed length character VARCHAR(N): Varying length character ENUM(‘M’, ‘F’): Value form a defined list BOOLEAN: True or False values DATE: Date (YYYY-MM-DD) DATETIME: Date and time (YYYY-MM-DD HH-mm-SS) TIME: Time (HHH-mm-SS), can be larger than 24 hours YEAR: Year (YYYY) Primary and Foreign keys Primay Key A primay key is a column, or set of columns, which uniquely identifies a record within a table A primay key must be unique A primay key cannot be NULL A table can only have one primary key Foreign Key A foreign key is used to link two tables together A foreign key is a column whose values match the values of another tables primay key column The table with the primary key is called the reference, or parent table and the table with the foreign key is called the child table A table can have multiple foreign keys SQL Queries Create Table 12345678910111213141516171819202122CREATE TABLE products ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30), price DECIMAL(3, 2));CREATE TABLE customers ( id INT AUTO_INCREMENT PRIMARY KEY, first_name VARCHAR(30), last_name VARCHAR(30), gender ENUM(&#39;F&#39;, &#39;M&#39;), phone_number VARCHAR(11));CREATE TABLE orders ( id INT AUTO_INCREMENT PRIMARY KEY, product_id INT, customer_id INT, order_time DATETIME, FOREIGN KEY (product_id) REFERENCES products(id), FOREIGN KEY (customer_id) REFERENCES customers(id)); Alter Table 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 1. show details of a tableDESCRIBE &lt;tablename&gt;;# 2. add column to tableALTER TABLE productsADD COLUMN coffee_origin VARCHAR(30);# 3. delete column from tableALTER TABLE prodcutsDROP COLUMN coffee_origin;# 4. add primary key to tableALTER TABLE &lt;tablename&gt;ADD PRIMARY KEY (columnname);# 5. delete primary key from tableALTER TABLE &lt;tablename&gt;DROP PRIMARY KEY;# 6. make up a constraint name when you add foreign keyALTER TABLE &lt;tablename&gt;ADD CONSTRAINT &lt;constraintname&gt;FOREIGN KEY (&lt;columnname&gt;) REFERENCES &lt;tablename&gt;(&lt;columnname&gt;);# exampleALTER TABLE peopleADD CONSTRAINT FK_PeopleAddressFOREIGN KEY (address_id) REFERENCES addresses(id);# 7. delete foreign key from tableALTER TABLE &lt;tablename&gt;DROP FOREIGN KEY &lt;constraintname&gt;;# exampleALTER TABLE peopleDROP FOREIGN KEY FK_PeopleAddress;# 8. add constraint to tableALTER TABLE &lt;tablename&gt;ADD CONSTRAINT &lt;constraintname&gt; UNIQUE (&lt;columnname&gt;);# exampleALTER TABLE petsADD CONSTRAINT u_species UNIQUE (species);# 9. delete constraint from tableALTER TABLE &lt;tablename&gt;DROP INDEX &lt;constraintname&gt;;# exampleALTER TABLE petsDROP INDEX u_species;# 10. change column name, using back ticks for column names, you can change the data type at the same time too.ALTER TABLE &lt;tablename&gt;CHANGE &#96;old_column_name&#96; &#96;new_column_name&#96; &lt;data type&gt;# exampleALTER TABLE petsCHANGE &#96;species&#96; &#96;animal_type&#96; VARCHAR(20);# 11. change a column data typeALTER TABLE &lt;tablename&gt;MODIFY &lt;columnname&gt; &lt;date type&gt;# exampleALTER TABLE addressesMODIFY city VARCHAR(30); Delete Table 1DROP TABLE products; Delete All Data from Table 1TRUNCATE TABLE products; Data Manipulation Language Insert 123456INSERT INTO &lt;tablename&gt; (&lt;column1&gt;, &lt;column2&gt;, &lt;column3&gt;)VALUES (&#39;value1&#39;, &#39;value2&#39;, &#39;value3&#39;);# exampleINSERT INTO products (name, price, coffee_origin)VALUES (&#39;Espresso&#39;, 2.50, &#39;Brazil&#39;); Update 1234567891011UPDATE &lt;tablename&gt;SET &lt;columnname&gt; &#x3D; &#39;value&#39;WHERE &lt;columnname&gt; &#x3D; &#39;value&#39;;# exampleUPDATE productsSET coffee_origin &#x3D; &#39;Sri Lanka&#39;WHERE id &#x3D; 7;# set safe update to false so we can use other columns (not just primary key) in WHERE clause to update valuesSET SQL_SAFE_UPDATES&#x3D;0; Delete 12345DELETE FROM peopleWHERE name &#x3D; &#39;John&#39;;# delete all rows in a table, just don&#39;t add the WHERE clauseDELETE FROM people; Selecting from a table 123456789101112131415161718192021222324252627282930SELECT &lt;columnname&gt; FROM &lt;tablename&gt;WHERE &lt;columnname&gt; &#x3D; &lt;value&gt; AND &lt;columnname&gt; &#x3D; &lt;value&gt;OR &lt;columnname&gt; &#x3D; &lt;value&gt;;SELECT * FROM customersWHERE last_name IN (&#39;Taylor&#39;, &#39;Bluth&#39;, &#39;Armstrong&#39;);SELECT * FROM ordersWHERE order_time BETWEEN &#39;2017-01-01&#39; AND &#39;2017-01-07&#39;;# select all customers whos last name starts with &#39;W&#39;SELECT * FROM customersWHERE last_name LIKE &#39;W%&#39;;SELECT * FROM productsORDER BY price ASC;SELECT * FROM productsORDER BY price DESC;SELECT DISTINCT * FROM products;# list 5 rows start at 6th rowSELECT * FROM customersLIMIT 5 OFFSET 5;# name aliasSELECT name as coffee, price, coffee_origin as countryFROM products; Select From Multiple Tables What is a join Joins allow you to retrieve data from multiple tables in a single select statement To join two tables there needs to be a related column between them There are many different kinds of join INNER JOIN Will retrieve data only when there is matching values in both values LEFT JOIN Will retrieve all data from the left table and matching rows from the right table RIGHT JOIN Will retrieve all data from the right table and matching rows from the left table There is no FULL JOIN in MySQL 123456SELECT products.name, orders.order_time FROM ordersINNER JOIN products ON orders.product_id &#x3D; products.id;# with table aliasSELECT p.name, o.order_time FROM orders oJOIN products p ON o.product_id &#x3D; p.id; 12345SELECT o.id, o.order_time, c.phone_number, c.last_nameFROM orders oLEFT JOIN customers c ON o.customer_id &#x3D; c.idORDER BY o.order_timeLIMIT 10; 12345# Join multiple tablesSELECT p.name, p.price, c.first_name, c.last_name, o.order_timeFROM products pJOIN orders o ON p.id &#x3D; o.product_idJOIN customers c ON c.id &#x3D; o.customer_id; Database Design Normalization Normalization is the process of efficiently organizing data in a database To eliminate redundant data To only store related data in a table Reduce storage space Reduce insert, update and deletion anomalies Improve query performance First Normal Form Tables are in first normal form if No repeated rows of data Columns only contain a single value Table has a primary key Second Normal Form Tables are in second normal form if They conform to first normal form Every column that is not a primary key of the table is dependent on the whole of the primary key Third Normal Form Tables are in third normal form if They conform to second normal form Every column that is not a primary key is only dependent on the whole of the primary key (they can’t dependent on other columns than primary key) Relationships Tables are related through primary and foreign keys One to One Where a key to one table appears no more than once as the key in another table and vice versa One to Many Where a primary key of one table can be in multiple rows of a foreign key column of another table Many to Many Where two tables can have many instances of each other Constraints NOT NULL: a column can’t contain any null values UNIQUE: a column can’t contain any duplicate values of data PRIMARY KEY: a column that uniquely identifies each row of data FOREIGN KEY: a column which is related to a primary key in another table CHECK: controls the values that can be inserted into a column DEFAULT: if no values is inserted into a column, you can set a default value Aggregate Functions Perform a calculations on data within a column and returns one result row Can use GROUP BY clauses to group the results by one or more columns Can use a HAVING clause in a similar way to a WHERE clause in a SELECT statement to filter the resutls set. Subqueries Can be used in a SELECT, INSERT, UPDATE or DELETE query The nested query can be in the WHERE clause or the FROM Two types of subquery Non-correlated Correlated Non-correlated subquery The inner query can run independently of the outer query Inner query runs first and produces a result set, which is then used by the outer query 123456SELECT id, start_time FROM screeningsWHERE film_id IN( SELECT id FROM films WHERE length_min &gt; 120); Correlated subquery The inner query can’t run independently of the outer query The inner query runs for every row in the outer query 123456SELECT screening_id, customer_id, ( SELECT COUNT(seat_id) FROM reserved_seat WHERE booking_id &#x3D; b.id)FROM bookings b; SQL Functions Functions are stored programs which can be passed parameters and return a value we have already seen some MySQL functions - aggregate functions","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://hellcy.github.io/tags/SQL/"}]},{"title":"Data Modeling Fundamentals","slug":"Data-Modeling-Fundamentals","date":"2021-07-20T10:40:54.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2021/07/20/Data-Modeling-Fundamentals/","link":"","permalink":"http://hellcy.github.io/2021/07/20/Data-Modeling-Fundamentals/","excerpt":"","text":"What is Data Model? Unlike a real database or a real big data environment in the real world, the data model doesn’t have data in it. It is a representation of what that data actually is in the real world. It provides us with a great deal of insight into a lot of the characteristics and rules that apply to our data. Major data subjects Attributes of data subjects Relationships among data subjects Business rules for our data Value of a data model Abstraction from database implementation specifics Helps even with relational databases Even more valuable with ‘non-intuitive’ data implementations Basic Data Modeling Concepts and Terminology Data Subjects Commonly called ‘entities’ Some methodologies use ‘objects’ or ‘classes’ Somewhat analogous to a database table Think it as something that exists Attributes Analogous to a database column Think ‘field’ Attributes typically associated with entities (subjects) Attribute types often shared across multiple entities Relationship among Data Subjects Business Rules for data Cardinality Mandatory or optional relationships Permissible attribute values (including NULLs) Data change dynamics (when a row is deleted, other rows in other tables will be deleted too) Compare Transactional Data Modeling to Analytical Data Modeling Transactional Analytical Conceptual Level mirror real world dimensional Logical level (relationsl) data normalization rules with deliverate denormalization fact and dimension tables in accordance with best practices Logical level (non-relational) NoSQL, OODBMS constructs cubes, columnar databases Physical level blocks/tracks, MPP distribution blocks/trakcs, MPP distribution, AWS buckets, HDFS NameNodes and DataNodes The Building Blocks of Data Modeling Entities Think it as\\ the real-world subject. collection of attributes (fields) Representing entities in a model Classic ER modeling: square, squared-off (not round) corners Crow’s foot notation Attributes Field, the details of an entity Represented by a circle Attributes have descriptions and rules Data types and sizes Whether NULL values allowed Permissible values Attribute domains Reuseable general classes of descriptions Applied to selective attributes that fit the domain Supported by some data modeling software tools Example: Valid Business Date is between 1/1/1980 and 21/31/2199, attribute not only have type DATE but also have domain ‘Valid Business Date’ Multi-valued attribute (MVA) More than one possible values for each instance Example: a student could have more than one email addresses represented by double circles Modeling software often doesn’t permit MVAs in crow’s foot notation MVAs is one of the two most obvious differentiators between conceptual and logical modeling the other is many-to-many relationships vs database intersection tables Hierarchies for the Entities A special type of relationship Two or more entities that have a lot in common but also at least a little bit different Parent and Child entities Concept of inheritance Hierarchies can be inclusive (the item can be in both child entities) or exclusive (the item can be in only one child entity) Constraints for your Attributes Data types and sizes Whether NULL values allowed Permissible values Range of values (can only be the value inside a range) List of values (enum, can only be one of the values in the list) Strong and Weak entities Think instead in terms of dependencies Independent entities Dependent entities Identification dependency Existence dependency Strong entity exists on its own terms exists independent of any other entity does not require any other entity instances to help identify its own instances Weak entity needs some help to identify specific instance of that entity can’t exist without an instance of another entity or both Relationships Classic ER notation: a diamond shape Crow’s foot notation: a stright line Multiple relationships between two entities Recursive relationship involving just one entitiy Ternary (three-entity) relationships Relationships that seem like entities Cardinalities The number of something Each relationship has two cardinalities Maximum cardinality Minimum cardinality Representing cardinality in various notations Classic ER Crow’s foot Maximum Cardinality The maximum number of instances of both sides of a relationship typical values: 1 or M Can also be a specific numeric value 1:1 relationship An instance from each side of a relationship is related to exactly 1 instance from the other side Business rules: a university has exactly 1 president a person can only be president of 1 university 1:M relationship An instance from one side of a relationship is related to 1 or more instances from the other side Business rules: A faculty member can advise many students A student is advised by only one faculty member M:M relationship Any instance from either side of the relationship can be associated with one or many instances from the other side Business rules: A student can enroll in 1 or many classes A class can enroll 1 or many students Specific number of max cardinality An instance from one side of a relationship can be related to at most some number from the other side can be one-directional or bi-directional One-directional 1 to (some number) Bi-directional (some number) to (some number) Example A student can take no more than 7 classes in one semester, a class can have many students. (7:M) A student can take no more than 7 classes and any class can enroll up to but no more than 300 students (7:300) Minimum Cardinality 3 possible value for minimum cardinality 0: optional/partial participation 1: mandatory/total participation n: some explicit number of minimum instances A full-time lecturer must teach at least 6 classes Difficult to represent explicit numbers with 0 | 1 notation for minimum cardinality Number pairs Left: min cardinality, right: max cardinality An active student must enroll in at least 1 but up to many courses A course can have many students but could possibly have zero students Normalization 1st Normal Form Every row (tuple) must be unique No repeating groups MVAs is a violation of 1st NF 2nd Normal Form Must be in 1st NF No Partial key dependencies must have single-column primary key 3rd Normal Form Must be in 2NF No non-key dependencies All attributes are dependent on the primary key The key, the whole key, nothing but the key. At the conceptual level, there is no real issues violating normalization. We address the issues at logicl level. Then, sometimes we deliverately violate the normalization at the physical level to improve performance based on real situations. Conceptual level to logical level Addressing normalization violations 1NF violations: move offending data to separate table 2NF violations: partial key dependencies, move offending attribute to correct entity 3NF violations: non-key dependencies, same as 2NF, move attribute the correct place Transform many-to-many relationships Add intersection entity to your model Also referred to as associative entity Purpose: decompose M:M relationship into multiple semantically equivalent relationships Software for Data Modeling Advantages of data modeling tools Enforcement of methodology, technique, and notation rules Automated or semi-automated forward and reverse engineering Options available Microsoft Visio CA ERwin ER/Studio Data Architect https://dbmstools.com/categories/data-modeling-tools Microsoft Visio From guided drawing templates to semantically aware models Business process models Data models Multi-functioned drawing tool Close alignment with Microsoft SQL server Widely used CA ERwin One of the oldest data modeling tools still being used Rich capabilities for forward and reverse engineering ER/Studio Data Architect Another old timer that still is a market leader Rich feature set including dimensional modeling (for analytical)","categories":[],"tags":[{"name":"Data","slug":"Data","permalink":"http://hellcy.github.io/tags/Data/"}]},{"title":"Data Warehouse","slug":"Data-Warehouse","date":"2021-07-20T04:50:16.000Z","updated":"2022-02-10T14:39:24.300Z","comments":true,"path":"2021/07/20/Data-Warehouse/","link":"","permalink":"http://hellcy.github.io/2021/07/20/Data-Warehouse/","excerpt":"","text":"What is Business Intelligence? BI is the act of transforming raw/operational data into useful information for business analysis How Does it Work? BI based on Data Warehouse technology extracts information from a company’s operational systems The data is transformed (cleaned and integrated), and loaded into Data Warehouse Since this data is credible, it is used for business insights. Why Data Warehouse? Data collected from various sources and stored in various databases (Oracle, SQL server, MySQL…) cannot be directly visualized The data first needs to be integrated and then processed before visualization takes place. What is Data Warehouseing? A central location where consolidated data from multiple locations (databases) are stored DWH is maintained separately from an organization’s operational database. (DWH is another copy) End uses access it whenever any information is needed. Data Warehouse is not loaded every time new data is added to databases. What are The Advantages of a Data Warehouse? Strategic questions can be answered by studying trends (from past data). Data Warehousing is faster and more accurate Data Warehouse is not a product that a company can go and purchase, it needs to be designed and depends entirely on the company’s requirement. End User =&gt; Take the data from opeartional systems =&gt; Integrate the data from multiple sources =&gt; Standardize the data and remove inconsistencies =&gt; Store the data in format suitable for easy access =&gt; Return result to end user Properties of a Data Warehouse A Data Warehouse is a subject-oriented, integrated, time-variant and non-volatile collection of data in support of managemnet’s decision-making process Subject-oriendted: Data is categorized and stored by business subject rather than by application. (data in data warehouse are suitable for business requirements) Integrated: Data on a given subject is collected from disparate sources and stored in a single place Time-variant: Data is stored as a series of snapshots, each representing a period of time Non-volatile: Typically data in the data warehouse is not updated or deleted Key Terminologies Related to DWH Architechture OLTP (Online Transaction Processing) vs OLAP (Online Analytical Processing) Relational Database (OLTP) Analytical Data Warehouse (OLAP) Contains current data Contains historical data Useful in running the business Useful in analyzing the business Based on Entity Relationship Model Based on Star, Snowflake and Fact Constellation Schema Provides primitive and highly detailed data Provides summarized and consolidated data Used for writing data into the database Used for reading data from data warehouse Database size ranges from 100MB to 1GB Data warehouse size ranges from 100GB to 1TB Fast, provides high performance Highly flexible, but not fast Number of records accessed is in tens Number of records accessed is in millions Example: All bank transactions made by a customer Example: Bank transactions made by a customer at a particular time OLTP Examples: A supermarket server which records every single product purchased at that market A back server which records every time a transaction is made for a particular account A railway reservation server which records the transactions of a passenger OLAP Examples: Bank manager wants to know how many customers are utilizing the ATM of his branch. Based on this he may take a call whether to continue with the ATM or relocate it. An insurance company wants to know the number of policies each agent has sold. This will help in better performance management of agents. ETL ETL is the process of extracting the data from various sources, transforming this data to meet your requirement and then loading it into a target data warehouse. (Tools: Talend, Informatica…) Data Mart Data mart is a smaller version of the data warehouse which deals with a single subject. Data mart are focused on one area, hence, they draw data from a limited number of sources Time taken to build data marts is very less compared to the time taken to build a data warehouse Data Warehouse Data Marts Enterprise wide data Department wide data Multiple subject areas Single subject data Multiple data sources Limited data sources Occupies large memory Occupies limited memory Longer time to implement Shorter time to implement Types of Data Mart Dependent Data Mart The Data is first extracted from the OLTP systems and them populated in the central DWH From the DWH, the data travels to the Data Mart Independent Data Mart The data is directly received from the source system This is suitable for small organizations or small groups within an organization Hybrid Data Mart The data is fed both from OLTP systems as well as the Data Warehouse Metadata Metadata is defined as data about data Matadata in a DWH defines the source data. (Flat file, Relational Database and other objects) Matadata is used to define which table is source and target, and which concept is used to build business logic called transformation to the actual output. DWH Architecture","categories":[],"tags":[{"name":"Data","slug":"Data","permalink":"http://hellcy.github.io/tags/Data/"}]},{"title":"Introduction to Azure","slug":"Introduction-to-Azure","date":"2021-06-03T04:01:51.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/06/03/Introduction-to-Azure/","link":"","permalink":"http://hellcy.github.io/2021/06/03/Introduction-to-Azure/","excerpt":"","text":"Documentation Best documentation in the world Azure Interaction Experiences Portal CLI VS Code Visual Studio Enablement Getting certified","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"http://hellcy.github.io/tags/Azure/"}]},{"title":"Scrum Master Notes","slug":"Scrum-Master-Notes","date":"2021-06-02T23:08:48.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/06/03/Scrum-Master-Notes/","link":"","permalink":"http://hellcy.github.io/2021/06/03/Scrum-Master-Notes/","excerpt":"","text":"Sprint Overview Roles Product Owner - The voice of customer, responosible for the Product Backlog Scrum Master - Servant Leader, responsible for ensuring the team is fully functional and productive Scrum Team - self-organising and self-managing, responsible for the outcomes of the sprint Ceremonies (meetings) Sprint Planning Daily Scrum (stand up) - daily updates Sprint Retro - reflective look back Sprint Review - demo to product owner and customer and give feedback Artefacts Product Backlog - list of objectives need to be done to deliver the product. Product Owner is responsible for the Product Backlog, decides what should be in it Sprint Backlog - list of tasks for the current sprint, Scrum Team move tasks from Product Backlog to the Sprint Backlog and only focus on these tasks for the sprint Scrum Board - visible to client, tasks for the day Burndown Chart - visible to client, tasks for the day Sprint Delivery - Tasks finished at the end of the sprint, should be the same as Product Backlog if nothing had gone wrong Non Core Roles Stakeholders Scrum Guidance Body Vendors Ceremonies Sprint Planning Agreeing and committing to what tasks will be done in the next sprint, move tasks from Product Backlog to Sprint Backlog Daily Stand Up The team provides short and concise progress updates daily Sprint Review / Showcase Allows stakeholders to assess progress and re-prioritise Product Backlog if necessary Sprint Retro Where performance inefficiencies are discussed and opportunities for improvement are identified. What went well, what didn’t go well, what can we start / stop doing (for scrum team itself) Non Official Ceremonies Release Planning Identifying and communicating how often a solution will be released and what features will be included Elaboration Ask questions and understand the problem in Backlog Backlog Grooming Review and redefinition of existing tasks Backlog Prioritisation Prioritise the Backlog tasks Risk Review Sessions Scrum Artifacts Prodcut Backlog List of tasks within the project Sprint Backlog List of tasks to be done within the next sprint Scrum Board and Burndown Chart Tracking progress and commmunications in an open and visual way Scrum Principles Empriical Process Control making decisions based on observation and experimentation rather than detailed upfront planning Transparency, Allows all components to be observed by everyone Inspection, Monitoring in place to ensure deliverables conform to stated requirements Adaption, Lessons learn through transparency and inspection used to improve performance Self Organisation Deliver greater value Team buy-in and shared ownership Higher levels of motivation Innovative and creative environment Collaboration Awareness, be aware of each other’s work Articulation, paritition work into units Appropriation, adapting technology to one’s own situation Value Based Prioritisation Delivering value to the customer on an early and continuous basis Methods used: Paried Comoparison, KANO, MoSCoW (Must, Should, Could, Won’t), 100 point comparison Timeboxing The rhythm to which stakeholders work / contribute Iterative Development Deliver value throughout the project, incorporating change as part of the process, adapt to the changing requirements Scrum Aspects Business Justification based on the concept of value-driven delivery Quality Definition of done, a set of rules that are applicable to all user stories Ways of working, a set of guidelines that make up a social contract between the Scrum Team Change Adapt of change, it is inevitable Risk Need to be identified and mitigated early, they are inevitable Scrum Phases Initiate The Initiate phase includes the processes related to initiation of a project – these are all pre-sprint activities such as forming of the Scrum Core Team, identification of stakeholder, development of high level requirements (epics), creation of the Prioritised Product Backlog and release planning. Plan and Estimate This phase consists of processes related to planning and estimating tasks for the upcoming Sprint (Sprint Backlog). These activities happen during the Sprint Planning Meeting. Implement Implementation is related to the execution of the tasks and activities to create a project’s products. These activities include creating the various deliverables, conducting Daily Stand Up Meetings, and grooming (i.e., reviewing, fine-tuning, and regularly updating) the Product Backlog at regular intervals. Review and Retrospect This phase is concerned with reviewing the deliverables and the work that has been done during the sprint (Sprint Review Meeting) for acceptance and determining ways to improve the practices and methods used to do project work and for incorporation into future sprints (Retrospect Sprint Meeting). Release The release phase is focused on delivering/shipping the Accepted Deliverables to the customer and identifying, documenting, and internalising the lessons learned during the project. Writing a User Story User Stories will allow the Product Owner, Stakeholders and the Scrum Team to discuss, prioritise and deliver requirements and features As a Role, I want to Perform an action, so I can achieve an objective Epic, Feature, Story story delivers business value User Story Estimate story points Acceptance Criteria Conditions that developers and testers will use to complete the stories the story will need to satisfy acceptance criteria to be considered complete Scrum of Scrums Chief Product Owner Chief Scrum Master","categories":[],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://hellcy.github.io/tags/Scrum/"}]},{"title":"Git cheatsheet","slug":"git-cheatsheet","date":"2021-06-02T03:58:53.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2021/06/02/git-cheatsheet/","link":"","permalink":"http://hellcy.github.io/2021/06/02/git-cheatsheet/","excerpt":"","text":"Common git commands make directory a git repository 1git init Adds files in the to the staging area for Git. 1git add &lt;file or directory name&gt; Adding a commit with message 1git commit -m &quot;Commit message&quot; current state of the repository 1git status Create a new branch 1git branch &lt;branch name&gt; Checkout an existing branch 1git checkout &lt;branch name&gt; Merge changes into current branch 1git merge &lt;branch name&gt; Add remote repository 1234git remote &lt;command&gt; &lt;remote_name&gt; &lt;remote_URL&gt;# connect remote repo to the local repo, the remote repo has name origin, followed by its URLgit remote add origin git@account_name.git.beanstalkapp.com:&#x2F;acccount_name&#x2F;repository_name.git get the latest version of a repository 1git pull &lt;branch_name&gt; &lt;remote_URL&#x2F;remote_name&gt; Sends local commits to the remote repository 1234567891011# push local commits to a remote repogit push &lt;remote_URL&#x2F;remote_name&gt; &lt;branch&gt;# push local branch MASTER to remote repo ORIGIN, git push origin master# push all local branches to remote repogit push -all origin# -u flag will keep track all pushes so next time you can just use git pushgit push -u origin master show the chronological commit history for a repository 1git log make a copy to another remote repo 1git fork make a copy from a remote repo to the a local repo 1git clone Temporarily save local changes before pulling from remote repo (local changes are not ready for commit) 12# stash basically has the same logic as commit, git will just save the changes in different places so it will not appear in your commit historygit stash save Reapply local changes after pulling 1git stash pop change the starting poing of a branch 12# this is make the commit history looks cleaner. But will cause problems when working with others because when you do it, your branch&#39;s starting poing is different to others and becomes a different branchgit rebase git squash combine multiple commits into one commit, make the commit history looks cleaner","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://hellcy.github.io/tags/git/"}]},{"title":"TCP/IP","slug":"TCP-IP","date":"2021-05-26T06:17:17.000Z","updated":"2021-05-26T08:00:46.307Z","comments":true,"path":"2021/05/26/TCP-IP/","link":"","permalink":"http://hellcy.github.io/2021/05/26/TCP-IP/","excerpt":"","text":"","categories":[],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://hellcy.github.io/tags/TCP-IP/"}]},{"title":"C# LINQ","slug":"C-LINQ","date":"2021-04-20T13:07:27.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/20/C-LINQ/","link":"","permalink":"http://hellcy.github.io/2021/04/20/C-LINQ/","excerpt":"","text":"LINQ SQL-like syntax in C# and Visual Basic Query any type of collection (IEnumerable&lt;T&gt;) Query external data sources (xml, databases, JSON, CSV) SQL Query vs LINQ Query Syntax SQL LINQ SELECT * FROM Products FROM prod IN Products SELECT prod SELECT Name FROM Products FROM prod in Products SELECT prod.Name SELECT * FROM Products WHERE ListPrice &gt; 10 FROM prod in Products WHERE prod.ListPrice &gt; 10 SELECT prod Two LINQ Syntaxes Query Method FROM prod in Products SELECT prod Products.Select(prod =&gt; prod) FROM prod in Products SELECT prod.Name Products.Select(prod =&gt; prod.Name) FROM prod in Products WHERE prod.ListPrice &gt; 10 SELECT prod Products.Where(prod =&gt; prod.ListPrice &gt; 10).Select(prod =&gt; prod) LINQ Operations Select Projection (Change shape, select only certain properties from an object) Order (ascending/descending) Get an Element (find, first, last, single) Filter (where) Iteration/Partioning (foreach, skip, take) Quantify (any, all, contains) Set Comparison (equal, except, intersection) Set Operations (union, concat) Joining (inner joins, outer joins) Grouping (groupby, subquery, groupjoin) Distinct Sets (distinct) Aggregation (count, sum, min, max, average) Select and Order Operations Projection (only select specific columns from an object) 123456789101112131415161718192021if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products select new Product &#123; ProductID &#x3D; prod.ProductID, Name &#x3D; prod.Name, Size &#x3D; prod.Size, &#125;).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.Select(prod &#x3D;&gt; new Product &#123; ProductID &#x3D; prod.ProductID, Name &#x3D; prod.Name, Size &#x3D; prod.Size &#125;).ToList();&#125; Projection with Anonymous Class 12345678910111213141516171819202122232425262728293031323334353637if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax var products &#x3D; (from prod in Products select new &#123; Identifier &#x3D; prod.ProductID, ProductName &#x3D; prod.Name, ProductSize &#x3D; prod.Size &#125;); &#x2F;&#x2F; Loop through anonymous class foreach (var prod in products) &#123; sb.AppendLine($&quot;Product ID: &#123;prod.Identifier&#125;&quot;); sb.AppendLine($&quot; Product Name: &#123;prod.ProductName&#125;&quot;); sb.AppendLine($&quot; Product Size: &#123;prod.ProductSize&#125;&quot;); &#125;&#125;else&#123; &#x2F;&#x2F; Method Syntax var products &#x3D; Products.Select(prod &#x3D;&gt; new &#123; Identifier &#x3D; prod.ProductID, ProductName &#x3D; prod.Name, ProductSize &#x3D; prod.Size &#125;); &#x2F;&#x2F; Loop through anonymous class foreach (var prod in products) &#123; sb.AppendLine($&quot;Product ID: &#123;prod.Identifier&#125;&quot;); sb.AppendLine($&quot; Product Name: &#123;prod.ProductName&#125;&quot;); sb.AppendLine($&quot; Product Size: &#123;prod.ProductSize&#125;&quot;); &#125;&#125; Ordering Data When using Method Syntax to order data, the .Select() method is optional when you are simply selecting the complete object as the return value 123456789101112131415public void OrderBy()&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name select prod).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderBy(prod &#x3D;&gt; prod.Name).ToList(); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; Order by Descending 123456789101112131415public void OrderByDescending()&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name descending select prod).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderByDescending(prod &#x3D;&gt; prod.Name).ToList(); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; Order by Two Fields 123456789101112131415public void OrderByTwoFields()&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Color descending, prod.Name).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderByDescending(prod &#x3D;&gt; prod.Color).ThenBy(prod &#x3D;&gt; prod.Name).ToList(); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; Extract Multiple or Single Elements Where Expression 123456789101112131415161718public void WhereExpression()&#123; string search &#x3D; &quot;L&quot;; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products where prod.Name.StartsWith(search) select prod).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.Where(prod &#x3D;&gt; prod.Name.StartsWith(search)).ToList(); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; Where with multiple fields 123456789101112131415161718public void WhereTwoFields()&#123; string search &#x3D; &quot;L&quot;; decimal cost &#x3D; 100; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products where prod.Name.StartsWith(search) &amp;&amp; prod.StandardCost &gt; cost).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.Where(prod &#x3D;&gt; prod.Name.StartsWith(search) &amp;&amp; prod.StandardCost &gt; cost).ToList(); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; Using Custom Extension Method Extension Method Extension methods enable you to “add” methods to existing types without creating a new derived type, recompiling, or otherwise modifying the original type. Extension methods are static methods, but they’re called as if they were instance methods on the extended type. The most common extension methods are the LINQ standard query operators that add query functionality to the existing System.Collections.IEnumerable and System.Collections.Generic.IEnumerable&lt;T&gt; types. Extension methods are defined as static methods but are called by using instance method syntax. Their first parameter specifies which type the method operates on. The parameter is preceded by the this modifier. Extension method is just a static method under the hood. In the example, the result of (from prod in Products select prod) is an IEnumerable&lt;Product&gt; which is why ByColor() can be applied to this 12345678910111213141516public static IEnumerable&lt;Product&gt; ByColor( this IEnumerable&lt;Product&gt; query, string color)&#123; return query.Where(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; color);&#125;if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products select prod).ByColor(search).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.ByColor(search).ToList();&#125; Select a Single Item First, will throw an Exception if item not found. Last, same as First 12345678910111213141516171819try&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).First(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; search); &#125; else &#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.First(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; search); &#125; ResultText &#x3D; $&quot;Found: &#123;value&#125;&quot;;&#125;catch&#123; ResultText &#x3D; &quot;Not Found&quot;;&#125; FirstOrDefault, value will be null if item not found, will not throw an Exception LastOrDefault, same as FirstOrDefault 12345678910111213141516171819if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).First(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; search);&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.First(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; search);&#125;if (value &#x3D;&#x3D; null)&#123; ResultText &#x3D; &quot;Not Found&quot;;&#125;else&#123; ResultText &#x3D; $&quot;Found: &#123;value&#125;&quot;;&#125; Single, will throw an Exception if item not found or multiple items found. Single is supposed to be used to found a unique item, like primary key. The Exception thrown if multiple items are found is InvalidOperationException. 12345678910111213141516171819try&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).Single(prod &#x3D;&gt; prod.ProductID &#x3D;&#x3D; search); &#125; else &#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.Single(prod &#x3D;&gt; prod.ProductID &#x3D;&#x3D; search); &#125; ResultText &#x3D; $&quot;Found: &#123;value&#125;&quot;;&#125;catch&#123; ResultText &#x3D; &quot;Not Found, or multiple elements found&quot;;&#125; SingleOrDefault, value will be NULL if no item found, but will still throw an Exception if multiple items found. 1234567891011121314151617181920212223242526try&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).SingleOrDefault(prod &#x3D;&gt; prod.ProductID &#x3D;&#x3D; search); &#125; else &#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.SingleOrDefault(prod &#x3D;&gt; prod.ProductID &#x3D;&#x3D; search); &#125; if (value &#x3D;&#x3D; null) &#123; ResultText &#x3D; &quot;Not Found&quot;; &#125; else &#123; ResultText &#x3D; $&quot;Found: &#123;value&#125;&quot;; &#125;&#125;catch&#123; ResultText &#x3D; &quot;Multiple elements found&quot;;&#125; Extract Distinct Values, Assign Values and Partition Collections Set Operations Iterate over entire collection Set a property value in collection (similar to a SQL UPDATE) In this example, the object has a NameLength property and we need to assign the value prop.Name.Length to this prop.NameLength property. For the Query approach, we need to declare a tmp variable because it has to be a statement, not an assignment. But the Method approach doesn’t have this issue. 123456789101112131415161718public void ForEach()&#123; if (UseQuerySyntax) &#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products let tmp &#x3D; prod.NameLength &#x3D; prod.Name.Length select prod).ToList(); &#125; else &#123; &#x2F;&#x2F; Method Syntax Products.ForEach(prod &#x3D;&gt; prod.NameLength &#x3D; prod.Name.Length); &#125; ResultText &#x3D; $&quot;Total Products: &#123;Products.Count&#125;&quot;;&#125; In this example we have a Sales object, and we need to calculate how many item we have sold for a certain product. 12345private decimal SalesForProduct(Product prod)&#123; return Sales.Where(sale &#x3D;&gt; sale.ProductID &#x3D;&#x3D; prod.ProductID) .Sum(sale &#x3D;&gt; sale.LineTotal);&#125; We could then use this to set the TotalSales property for each Product 123456789101112if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products let tmp &#x3D; prod.TotalSales &#x3D; SalesForProduct(prod) select prod).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products.ForEach(prod &#x3D;&gt; prod.TotalSales &#x3D; SalesForProduct(prod));&#125; Take Specific Amount of Elements Take the first 5 elements from the list 123456789101112if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name select prod).Take(5).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderBy(prod &#x3D;&gt; prod.Name).Take(5).ToList();&#125; TakeWhile(): take elements while condition is true 123456789101112if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name select prod).TakeWhile(prod &#x3D;&gt; prod.Name.StartsWith(&quot;A&quot;)).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderBy(prod &#x3D;&gt; prod.Name).TakeWhile(prod &#x3D;&gt; prod.Name.StartsWith(&quot;A&quot;)).ToList();&#125; Skip specific amount of elements 123456789101112if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name select prod).Skip(20).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderBy(prod &#x3D;&gt; prod.Name).Skip(20).ToList();&#125; Skip elements while condition is true 123456789101112if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in Products orderby prod.Name select prod).SkipWhile(prod &#x3D;&gt; prod.Name.StartsWith(&quot;A&quot;)).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; Products.OrderBy(prod &#x3D;&gt; prod.Name).SkipWhile(prod &#x3D;&gt; prod.Name.StartsWith(&quot;A&quot;)).ToList();&#125; Select Distinct Values 12345678910if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax colors &#x3D; (from prod in Products select prod.Color).Distinct().ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax colors &#x3D; Products.Select(prod &#x3D;&gt; prod.Color).Distinct().ToList();&#125; Identify What Kind of Data is Contained in Collections All() will return a true or false value to see if all items meet the requirement. 12345678910if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).All(prod &#x3D;&gt; prod.Name.Contains(search));&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.All(prod &#x3D;&gt; prod.Name.Contains(search));&#125; Any() will return true if any of the item meet the requirement. And will return false will all items doesn’t meet the requirement. 12345678910if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).Any(prod &#x3D;&gt; prod.Name.Contains(search));&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.Any(prod &#x3D;&gt; prod.Name.Contains(search));&#125; Contains can be used in primitive types and objects 12345678910111213bool value &#x3D; true;List&lt;int&gt; numbers &#x3D; new List&lt;int&gt; &#123; 1, 2, 3, 4, 5 &#125;;if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from num in numbers select num).Contains(3);&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; numbers.Contains(3);&#125; When using Contains() on a collection of objects. We need to use EqualityComparer, because by default objects are compared by reference not value. 123456789public class ProductIdComparer : EqualityComparer&lt;Product&gt; &#123; public override bool Equals(Product x, Product y) &#123; return (x.ProductID &#x3D;&#x3D; y.ProductID); &#125;&#125;public override int GetHashCode(Product obj) &#123; return obj.ProductID.GetHashCode();&#125; Now when calling Contains() method, we pass in the Comparer object, so it will loop through all products and compare each one with our prodToFind Product. The Comparer will use prodToFind as the first parameter and each Product as the second parameter. 123456789101112131415int search &#x3D; 744;bool value &#x3D; true;ProductIdComparer pc &#x3D; new ProductIdComparer();Product prodToFind &#x3D; new Product &#123; ProductID &#x3D; search &#125;;if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from prod in Products select prod).Contains(prodToFind, pc);&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; Products.Contains(prodToFind, pc);&#125; Compare and Union Two Collections SequenceEqual() Compares two collections for equlity. For Simple data types (int, decimal, boolean…) it checks values For object data types checks reference If you want to compare values in objects, you need to create a comparer class to check the values inside each properties. 1234567891011121314151617&#x2F;&#x2F; Use SequenceEqual on primitivesbool value &#x3D; true;&#x2F;&#x2F; Create a list of numbersList&lt;int&gt; list1 &#x3D; new List&lt;int&gt; &#123; 1, 2, 3, 4, 5 &#125;;&#x2F;&#x2F; Create a list of numbersList&lt;int&gt; list2 &#x3D; new List&lt;int&gt; &#123; 1, 2, 3, 4, 5 &#125;;if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from num in list1 select num).SequenceEqual(list2);&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; list1.SequenceEqual(list2);&#125; If we want to compare each object in a collection by value, we need to create a new Comparer override the Compare method. 1234567891011121314151617181920bool value &#x3D; true;ProductComparer pc &#x3D; new ProductComparer();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list1 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list2 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Remove an element from &#39;list1&#39; to make the collections differentlist1.RemoveAt(0);if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax value &#x3D; (from num in list1 select num).SequenceEqual(list2, pc);&#125;else&#123; &#x2F;&#x2F; Method Syntax value &#x3D; list1.SequenceEqual(list2, pc);&#125; Except It finds all values in one list, but not the other, returns a collection of items. Similar to Contains and SequenceEqual, if we are comparing primitive types, we can just use it, but if we are comparing objects values, we need to create a Comparer class and override the Compare method. 12345678910111213141516List&lt;int&gt; exceptions &#x3D; new List&lt;int&gt;();&#x2F;&#x2F; Create a list of numbersList&lt;int&gt; list1 &#x3D; new List&lt;int&gt; &#123; 1, 2, 3, 4 &#125;;&#x2F;&#x2F; Create a list of numbersList&lt;int&gt; list2 &#x3D; new List&lt;int&gt; &#123; 3, 4, 5 &#125;;if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax exceptions &#x3D; (from num in list1 select num).Except(list2).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax exceptions &#x3D; list1.Except(list2).ToList();&#125; Using Except on a collection of objects 1234567891011121314151617181920ProductComparer pc &#x3D; new ProductComparer();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list1 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list2 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Remove all products with color &#x3D; &quot;Black&quot; from &#39;list2&#39;&#x2F;&#x2F; to give us a difference in the two listslist2.RemoveAll(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;Black&quot;);if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from prod in list1 select prod).Except(list2, pc).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; list1.Except(list2, pc).ToList();&#125; Intersect It finds all values in common between both lists Similar to Contains, SequenceEqual and Except, it compares values for primitive types and references for objects. We need to create comparer class to check values in properties. 123456789101112131415161718192021ProductComparer pc &#x3D; new ProductComparer();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list1 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list2 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Remove &#39;black&#39; products from &#39;list1&#39;list1.RemoveAll(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;Black&quot;);&#x2F;&#x2F; Remove &#39;red&#39; products from &#39;list2&#39;list2.RemoveAll(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;Red&quot;);if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from num in list1 select num).Intersect(list2, pc).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; list1.Intersect(list2, pc).ToList();&#125; Unions It adds the contents of two lists together. Union() checks for duplicates Concat() does not check for duplicates Use comparer class with objects Union() need Comparer to eliminate duplicates 12345678910111213141516ProductComparer pc &#x3D; new ProductComparer();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list1 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list2 &#x3D; ProductRepository.GetAll();if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from num in list1 select num).Union(list2, pc).OrderBy(prod &#x3D;&gt; prod.Name).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; list1.Union(list2, pc).OrderBy(prod &#x3D;&gt; prod.Name).ToList();&#125; Concat() Adds the contents of two collections with duplicates 123456789101112131415&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list1 &#x3D; ProductRepository.GetAll();&#x2F;&#x2F; Load all Product DataList&lt;Product&gt; list2 &#x3D; ProductRepository.GetAll();if (UseQuerySyntax)&#123; &#x2F;&#x2F; Query Syntax Products &#x3D; (from num in list1 select num).Concat(list2).OrderBy(prod &#x3D;&gt; prod.Name).ToList();&#125;else&#123; &#x2F;&#x2F; Method Syntax Products &#x3D; list1.Concat(list2).OrderBy(prod &#x3D;&gt; prod.Name).ToList();&#125; Joining Two Collections Together Inner Join 12345678910111213var query &#x3D; Products.Join(Sales, prod &#x3D;&gt; prod.ProductID, sale &#x3D;&gt; sale.ProductID, (prod, sale) &#x3D;&gt; new&#123; prod.ProductID, prod.Name, prod.Color, prod.StandardCost, prod.ListPrice, prod.Size, sale.SalesOrderID, sale.OrderQty, sale.UnitPrice, sale.LineTotal,&#125;); Inner Join with two fields 12345678910111213141516171819short qty &#x3D; 6;var query &#x3D; Products.Join( Sales, prod &#x3D;&gt; new &#123; prod.ProductID, Qty &#x3D; qty &#125;, sale &#x3D;&gt; new &#123; sale.ProductID, Qty &#x3D; sale.OrderQty &#125;, (prod, sale) &#x3D;&gt; new &#123; prod.ProductID, prod.Name, prod.Color, prod.StandardCost, prod.ListPrice, prod.Size, sale.SalesOrderID, sale.OrderQty, sale.UnitPrice, sale.LineTotal &#125;); Aggregating Data in Collections Count() 1234value &#x3D; Products.Count(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;Yellow&quot;);&#x2F;&#x2F; Another way using Wherevalue &#x3D; Products.Where(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;Yellow&quot;).Count(); Min() and Max() 1value &#x3D; Products.Min(prod &#x3D;&gt; prod.ListPrice); 1value &#x3D; Products.Max(prod &#x3D;&gt; prod.ListPrice); Average() and Sum() 1value &#x3D; Products.Average(prod &#x3D;&gt; prod.ListPrice); 1value &#x3D; Products.Sum(prod &#x3D;&gt; prod.ListPrice); Custom Calculation using Aggregate() The first parameter initialize an internal variable, which setup the start value. The second parameter is an anonymous function which you pass the initial value and loop through each item in the collection Aggregate Sum 1value &#x3D; Products.Aggregate(0m, (sum, prod) &#x3D;&gt; sum +&#x3D; prod.ListPrice); Aggregate Multiply 1value &#x3D; Products.Aggregate(0m, (sum, prod &#x3D;&gt; sum +&#x3D; prod.ListPrice * prod.Qty)); Aggregate with GroupBy and Having 1234567891011var stats &#x3D; Products.GroupBy(sale &#x3D;&gt; sale.Size) .Where(sizeGroup &#x3D;&gt; sizeGroup.Count() &gt; 0) .Select(sizeGroup &#x3D;&gt; new &#123; Size &#x3D; sizeGroup.Key, TotalProducts &#x3D; sizeGroup.Count(), Max &#x3D; sizeGroup.Max(s &#x3D;&gt; s.ListPrice), Min &#x3D; sizeGroup.Min(s &#x3D;&gt; s.ListPrice), Average &#x3D; sizeGroup.Average(s &#x3D;&gt; s.ListPrice) &#125;) .OrderBy(result &#x3D;&gt; result.Size) .Select(result &#x3D;&gt; result); Deferred Execution A LINQ query is a data structure ready to execute Query is not executed until a value is needed The execution happens with one of the folloing functions (foreach(), Count(), ToList(), OrderBy()…) Streaming Operators Results can be returned prior to the entire collection is read Examples: Distinct(), GroupBy(), Join(), Select(), Skip(), Take(), Union(), Where() Non-Streaming Operators All data in collection must be read before a result can be returned Examples: Except(), GroupBy(), GroupJoin(), Intersect(), Join(), OrderBy(), ThenBy() The yield keyword When write our own Filter function, we could use yield to make the function to be Streaming. So it returns data while looping through the collection. 1234567public static IEnumrable&lt;T&gt; Filter&lt;T&gt; (this IEnumrable&lt;T&gt; source, Func&lt;T, bool&gt; predicate) &#123; foreach(var item in source) &#123; if (predicate(item)) &#123; yield return item; &#125; &#125;&#125; In the below example, Where and Take are both Streaming Operators, so this query will loop through the collection until the requirement is met. That is when it found the first item that has Color red. It doesn’t need to go through the entire collection. 1Products &#x3D; Products.Where(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;red&quot;).Take(1).ToList(); However, in this example, because OrderBy() is an non-streaming operator, so it will loop through the entire list first, order them by prod.Name, then apply the Where condition. Non-streaming operator will go before the Streaming operator. 1Products &#x3D; Products.Where(prod &#x3D;&gt; prod.Color &#x3D;&#x3D; &quot;red&quot;).OrderBy(prod &#x3D;&gt; prod.Name).ToList();","categories":[],"tags":[{"name":"LINQ","slug":"LINQ","permalink":"http://hellcy.github.io/tags/LINQ/"}]},{"title":"Entity Framework Core 5","slug":"Entity-Framework-Core-5","date":"2021-04-20T12:54:33.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/20/Entity-Framework-Core-5/","link":"","permalink":"http://hellcy.github.io/2021/04/20/Entity-Framework-Core-5/","excerpt":"","text":"Microsoft’s cross-platform data access framework for .NET ORM (Object Relational Mapper) EF Core is an ORM, it is designed to reduce the friction between how data is structure in a relational database and how you define your classes. Without ORM, we need to write lots of code to transform database results to instances of the types in our software.","categories":[],"tags":[{"name":"Entity Framework","slug":"Entity-Framework","permalink":"http://hellcy.github.io/tags/Entity-Framework/"}]},{"title":"Building GraphQL APIs with ASP.NET Core","slug":"Building-GraphQL-APIs-with-ASP-NET-Core","date":"2021-04-20T12:21:54.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2021/04/20/Building-GraphQL-APIs-with-ASP-NET-Core/","link":"","permalink":"http://hellcy.github.io/2021/04/20/Building-GraphQL-APIs-with-ASP-NET-Core/","excerpt":"","text":"The consumer of a GraphQL API defines the data structure it want to receive in a query First, let us see how a REST API works. On the far right, there is data, for example in the form of a database, and there are entity classes. Each instance of an entity class represents one row of data in the table. An object-relational mapper like Entity Framework may take care of instantiating and populating these objects, but you don’t want to expose these entities directly. So they are converted to models or data transfer objects, objects that have a data structure that is easy to consume for clients, and maybe have some validation built in using attributes. Once you have the model, it’s the controller’s job to make it available to the outside world. There’s typically a controller for each type of model. For example, it could be a product controller, an order controller, etc. What controller is activated when a request comes in is determined by routing, which maps the URL to a certain controller, and all of these controllers react to HTTP methods. Each HTTP method triggers a different operation in the controller. A GET gets data, a POST introduces new data, etc. So there are typically quite a few controllers that have quite a few operations. With GraphQL, there are typically no models, there is something called Schema. This Schema declares what a consumer of the API can access. The Schema also knows how to get the data. The API support GET or POST request, and there is always a query in the request. Queries Determines what happens in the API Not tied to HTTP. HTTP is just a transport used to get the quert to the API Downside: HTTP Caching: when using HTTP, because now each request is not at a unique URL anymore, it is now difficult to do HTTP caching.","categories":[],"tags":[{"name":"GraphQL","slug":"GraphQL","permalink":"http://hellcy.github.io/tags/GraphQL/"}]},{"title":"Design Patterns - Repository","slug":"Design-Patterns-Repository","date":"2021-04-19T06:44:53.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/19/Design-Patterns-Repository/","link":"","permalink":"http://hellcy.github.io/2021/04/19/Design-Patterns-Repository/","excerpt":"","text":"A repository encapsulates the data access so the consumer on longer has to know about the underlying data structure Why this Design is Problematic The controller is tightly coupled with the data access layer it is difficult to write a test for the controller without side effects Hard to extend entities with domain specific behavior Benefits of the Repository Pattern The consumer(controller) is now separated (decoupled) from the data access Easy to write a test without side-effects In production, we use the Repository Pattern to communicate with the Data layer. In Test, we replace the Repository with a faked local Data store. This can be done using Strategy Pattern. Modify and extend entities before they are passed on to the consumer A sharable abstraction resulting in less duplication of code Improved maintainability","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Proxy","slug":"Design-Patterns-Proxy","date":"2021-04-19T04:19:43.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/19/Design-Patterns-Proxy/","link":"","permalink":"http://hellcy.github.io/2021/04/19/Design-Patterns-Proxy/","excerpt":"","text":"Problem Need to control access to a type for performance, security or other reasons. Client should not know if they were calling the real service or a proxy. Proxy also gives us time to do necessary things before sending request to real service and before sending response back to client.(like logging, caching, encrypt/decrypt…) Proxy has similar structure as the Decorator Pattern, but the intent is different, Decorator pattern is for adding extra funcationalities to the original class whereas Proxy is focusing on control the access to the object. This is another implementation of the Proxy Pattern, it doesn’t have the interface so we need to compose the RealService object in Proxy Class. One thing to notice is that the RealService properties and methods need to be marked as virtual for the Proxy Class to override them. Proxy Variants Virtual Proxy stand in for expensive to create objects Remote Proxy Hide the detail to work with remote data or services. Smart Proxy Performs additional actions when a resource is accessed Protective Proxy controls access to a sensitive resource by checking for whether or not the client is authorized to perform those operations. Virtual Proxy Stands in for an expensive-to-create object. Typically responsible for getting real object. UI placeholders. Lazy-loaded Entity Properties. 123456789101112131415public class ExpensiveToFullyLoad : BaseClassWithHistory&#123; public static ExpensiveToFullyLoad Create() &#123; return new VirtualExpensiveToFullyLoad(); &#125; public virtual IEnumerable&lt;ExpensiveEntity&gt; HomeEntities &#123; get; protected set; &#125; public virtual IEnumerable&lt;ExpensiveEntity&gt; AwayEntities &#123; get; protected set; &#125; protected ExpensiveToFullyLoad() &#123; History.Add(&quot;Constructor called.&quot;); &#125;&#125; When you have some expensive properties. You don’t want to create them when you don’t need them. So we could create a Proxy Class(VirtualExpensiveToFullyLoad), which will only create the property when its getting called. 12345678910111213141516171819202122232425262728public class VirtualExpensiveToFullyLoad : ExpensiveToFullyLoad&#123; public override IEnumerable&lt;ExpensiveEntity&gt; AwayEntities &#123; get &#123; if(base.AwayEntities &#x3D;&#x3D; null) &#123; base.AwayEntities &#x3D; ExpensiveDataSource.GetEntities(this); &#125; return base.AwayEntities; &#125; protected set &#x3D;&gt; base.AwayEntities &#x3D; value; &#125; public override IEnumerable&lt;ExpensiveEntity&gt; HomeEntities &#123; get &#123; if (base.HomeEntities &#x3D;&#x3D; null) &#123; base.HomeEntities &#x3D; ExpensiveDataSource.GetEntities(this); &#125; return base.HomeEntities; &#125; protected set &#x3D;&gt; base.HomeEntities &#x3D; value; &#125;&#125; When we test the class, we can see object history will only increase after we get the Entities from the class. 123456789101112[Fact]public void LogsCollectionLoadingToHistory()&#123; var obj &#x3D; ExpensiveToFullyLoad.Create(); var list &#x3D; obj.HomeEntities; Assert.Equal(2, obj.History.Count()); var anotherList &#x3D; obj.AwayEntities; Assert.Equal(3, obj.History.Count());&#125; We could also use the C# Lazy&lt;T&gt; type which will handle the lazy instantiation and thread-safe for use 123456789101112131415public class LazyExpensiveToFullyLoad : BaseClassWithHistory&#123; private Lazy&lt;IEnumerable&lt;ExpensiveEntity&gt;&gt; _homeEntities; public IEnumerable&lt;ExpensiveEntity&gt; HomeEntities &#123; get &#123; return _homeEntities.Value; &#125; &#125; private Lazy&lt;IEnumerable&lt;ExpensiveEntity&gt;&gt; _awayEntities; public IEnumerable&lt;ExpensiveEntity&gt; AwayEntities &#123; get &#123; return _awayEntities.Value; &#125; &#125; public LazyExpensiveToFullyLoad() &#123; History.Add(&quot;Constructor called.&quot;); _homeEntities &#x3D; new Lazy&lt;IEnumerable&lt;ExpensiveEntity&gt;&gt;(() &#x3D;&gt; ExpensiveDataSource.GetEntities(this)); _awayEntities &#x3D; new Lazy&lt;IEnumerable&lt;ExpensiveEntity&gt;&gt;(() &#x3D;&gt; ExpensiveDataSource.GetEntities(this)); &#125;&#125; Remote Proxy Client works with proxy as if remote resource were local. Hides network details from client. Centralizes knowledge of network details. Smart Proxy Performs additional logic around resource access. Example: Resource counting, Cache management, Locking shared resources Here we are trying to open the same file two times, normally this will throw an exception. 123456789101112var fs &#x3D; new FileSmartProxy();byte[] outputBytes1 &#x3D; Encoding.ASCII.GetBytes(&quot;1. ardalis.com\\n&quot;);byte[] outputBytes2 &#x3D; Encoding.ASCII.GetBytes(&quot;2. weeklydevtips.com\\n&quot;);using var file &#x3D; fs.OpenWrite(_testFile);using var file2 &#x3D; fs.OpenWrite(_testFile);file.Write(outputBytes1);file2.Write(outputBytes2);file.Close();file2.Close(); But we are using FileSmartProxy() Class, when we catch the exception, we will check if the file is already opened, and return the same reference to the file stream. 123456789101112131415161718192021222324252627public class FileSmartProxy : IFile&#123; Dictionary&lt;string, FileStream&gt; _openStreams &#x3D; new Dictionary&lt;string, FileStream&gt;(); public FileStream OpenWrite(string path) &#123; try &#123; var stream &#x3D; File.OpenWrite(path); _openStreams.Add(path, stream); return stream; &#125; catch (IOException) &#123; if(_openStreams.ContainsKey(path)) &#123; var stream &#x3D; _openStreams[path]; if(stream !&#x3D; null &amp;&amp; stream.CanWrite) &#123; return stream; &#125; &#125; throw; &#125; &#125;&#125; Protective Proxy Manages access to a resource based on authorization rules. Eliminates repetitive security checks from client code and othe resource itself. Acts as a gatekeeper around a resource Summary If we are not using the Proxy Pattern, we often end up mixing the concerns of access control, or lazy loading or other funcationality in the resource class itself. Every client the consume this class must perform this work. The concerns of access control are mixed with the concerns of client or the resource. Proxy Pattern helps us to separate this. Usually Proxy Pattern has built in class that support it.(Remote Proxy) Related Patterns Decorator: the structure is similar, but the intent of Decorator Pattern is to add funcationality. Whereas the intent of Proxy Pattern is to control access. Prototype: Prototype and Virtual Proxy Pattern both deal with objects that are expensive to create. But Virtual Proxy Pattern only provides a placeholder of the object and fetch it when required. The Prototype Pattern keeps a copy of the object on hand and can clone it when required. Adapter: similar structure, but the intent of the Adapter Pattern is to convert an incompatible interface into one that works for the client. Flyweight: designed to manage many reference to a shared instance.","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Adapter","slug":"Design-Patterns-Adapter","date":"2021-04-18T16:27:56.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/19/Design-Patterns-Adapter/","link":"","permalink":"http://hellcy.github.io/2021/04/19/Design-Patterns-Adapter/","excerpt":"","text":"Problem Incompatible interfaces between a client and a service provider. Adapters convert the interface of one class into an interface a client expects. Two Kinds of Adapters Object Adapters Hold an instance of the Adaptee Implement or inherit the adapter type Use composition and single inheritance C# doesn’t support multiple inheritance, and a design principle of C# is to prefer composition over inheritance. So C# is prefer object adapter. The Client is calling method on an adapter abstraction(IAdapter). A specific adapter is created for each specific adaptee. Class Adapters Inherit from the adaptee Implement the adapter interface The Client is calling a target class’s particular method, but it wants to use a different implementation(incompatibleMethod) now. The adapter class inherits from both classes and overrides the SomeMethod() call, so that instead of doing what it did in the target class, it now calls the IncompatibleMethod() and does any work necessary to make it compatible with the SomeMethod() interface. What C# can do is to implement the interface the Client is calling and rather than holding onto an instance of the concrete adaptee type, we can inherit from it. SomeMethod() calls IncompetibleMethod() and does any necessary work to modify it to work with the SomeMethod() interface. Example: We are going to read a list of People and there are two ways of doing it. First, we could read People from a file. Second, we could call a Web API. IAdapter interface will only have a method, GetCharacters(), get it returns a list of Peoson 1234public interface ICharacterSourceAdapter&#123; Task&lt;IEnumerable&lt;Person&gt;&gt; GetCharacters();&#125; Get list of Person from a web API is easy. 1234567891011public async Task&lt;List&lt;Person&gt;&gt; GetCharacters()&#123; using (var client &#x3D; new HttpClient()) &#123; string url &#x3D; &quot;https:&#x2F;&#x2F;swapi.co&#x2F;api&#x2F;people&quot;; string result &#x3D; await client.GetStringAsync(url); var people &#x3D; JsonConvert.DeserializeObject&lt;ApiResult&lt;Person&gt;&gt;(result).Results; return people; &#125;&#125; But get list of Person from a file need a parameter (filename) 123456public async Task&lt;List&lt;Person&gt;&gt; GetCharactersFromFile(string filename)&#123; var characters &#x3D; JsonConvert.DeserializeObject&lt;List&lt;Person&gt;&gt;(await File.ReadAllTextAsync(filename)); return characters;&#125; To make it work with the GetCharacters() method, we need to create an Adapter Class 12345678910111213141516public class CharacterFileSourceAdapter : ICharacterSourceAdapter&#123; private string _fileName; private readonly CharacterFileSource _characterFileSource; public CharacterFileSourceAdapter(string fileName, CharacterFileSource characterFileSource) &#123; _fileName &#x3D; fileName; _characterFileSource &#x3D; characterFileSource; &#125; public async Task&lt;IEnumerable&lt;Person&gt;&gt; GetCharacters() &#123; return await _characterFileSource.GetCharactersFromFile(_fileName); &#125;&#125; It implements the IAdapter interface, and inside GetCharacters() method, it calls the GetCharactersFromFile(_filename) method to make it compatible with our interface method. When we use it, it doesn’t need to know anything about the filename or which way we choose to get the list of Person. 1var people &#x3D; await _characterSourceAdapter.GetCharacters(); Related Patterns Decorator: has a similar structure, but the intent of a decorator is to add functionality. Bridge: has a similar structure, but it allows interfaces and their implementations to vary independently from one another. Proxy: similar structure, but its intent is to control access to a resource, not to convert an incompatible interface Repository: sometimes it acts an adapter, providing a common interface for persistence that can map various incompatible interfaces to a single common data access strategy Strategy: very frequently used with Adapter pattern as a way of injecting different implementations of behavior into a particular client class. Facade: the intent of facade is similar to the adapters in that it alters an interface to make it easier for a client to use. The difference is Facade often sits in front of multiple different types and its goal is to simplify a complex set of operation Summary An Adapter converts an incompatible interface into a compatible one In C#, the Adapter pattern uses composition and is known as an object adapter. It means that your adapter implementation will contain instances of the incompatible type and will delegate calls to this instalce’s incompatible methods or properties. Adapters can work with service providers but can also wrap result types.","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Decorator","slug":"Design-Patterns-Decorator","date":"2021-04-18T11:26:18.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/18/Design-Patterns-Decorator/","link":"","permalink":"http://hellcy.github.io/2021/04/18/Design-Patterns-Decorator/","excerpt":"","text":"Decorator Pattern A structural design pattern used for dynamically adding behavior to a class without making changes to that class. The Decorator Class will take an object implementing the same interface. This allows us to pass the object being decorated into the decorator object and allows the decorator object to act as a wrapper around this original object. The Decorator object will keep a reference of the object being decorated(the component object). Because the decorator object implement the same interface as the original component object, it now has a chance to intercept any method calls on the interface and inject some additional behavior into those calls. Decorator Class and be nested. This is the Example we are going to use. Using Decorator Objects 12345678&#x2F;&#x2F; Standard component instantiationIWeatherService weatherService &#x3D; new WeatherService();&#x2F;&#x2F; Instantiation with decorator objectsIWeatherService weatherService &#x3D; new CachingDecorator( new LogginDecorator( new WeatherService())); To achieve this, we need to make sure the original component class and all the decorator classes need to implement from the same interface. And all decortor classes need to take the object of type IWeatherService in their constructors. Logging Decorator Log how often a method was called, how long it took, parameters and responses. 123456public interface IWeatherService&#123; CurrentWeather GetCurrentWeather(String location); LocationForecast GetForecast(String location);&#125; This is the interface for our original WeatherService Class and our new Decorator Class 12345678910111213141516171819202122232425262728293031323334public class WeatherServiceLoggingDecorator : IWeatherService&#123; private IWeatherService _weatherService; private ILogger&lt;WeatherServiceLoggingDecorator&gt; _logger; public WeatherServiceLoggingDecorator(IWeatherService weatherService, ILogger&lt;WeatherServiceLoggingDecorator&gt; logger) &#123; _weatherService &#x3D; weatherService; _logger &#x3D; logger; &#125; public CurrentWeather GetCurrentWeather(string location) &#123; Stopwatch sw &#x3D; Stopwatch.StartNew(); CurrentWeather currentWeather &#x3D; _weatherService.GetCurrentWeather(location); sw.Stop(); long elapsedMillis &#x3D; sw.ElapsedMilliseconds; _logger.LogWarning(&quot;Retrieved weather data for &#123;location&#125; - Elapsed ms: &#123;&#125; &#123;@currentWeather&#125;&quot;, location, elapsedMillis, currentWeather); return currentWeather; &#125; public LocationForecast GetForecast(string location) &#123; Stopwatch sw &#x3D; Stopwatch.StartNew(); LocationForecast locationForecast &#x3D; _weatherService.GetForecast(location); sw.Stop(); long elapsedMillis &#x3D; sw.ElapsedMilliseconds; _logger.LogWarning(&quot;Retrieved weather data for &#123;location&#125; - Elapsed ms: &#123;&#125; &#123;@locationForecast&#125;&quot;, location, elapsedMillis, locationForecast); return locationForecast; &#125;&#125; This is our new Decorator Class, we implement from the IWeatherService Interface, it is taking the interface as a parameter in the constructor, and implemented two methods. In the GetCurrentWeather() method, it logs the time it takes to run the method, then calling the original _weatherService.GetCurrentWeather() method. Caching Decorator Cache weather conditions, forecasts for a city to reduce the number of external API calls. 12345678910111213141516171819202122232425262728293031323334353637383940414243public class WeatherServiceCachingDecorator : IWeatherService&#123; private IWeatherService _weatherService; private IMemoryCache _cache; public WeatherServiceCachingDecorator(IWeatherService weatherService, IMemoryCache cache) &#123; _weatherService &#x3D; weatherService; _cache &#x3D; cache; &#125; public CurrentWeather GetCurrentWeather(string location) &#123; &#x2F;&#x2F; if we can found value in the cache, return it &#x2F;&#x2F; otherwise get the current weather then add it to the cache for 30 mins string cacheKey &#x3D; $&quot;WeatherConditions::&#123;location&#125;&quot;; if (_cache.TryGetValue&lt;CurrentWeather&gt;(cacheKey, out var currentWeather)) &#123; return currentWeather; &#125; else &#123; var currentConditions &#x3D; _weatherService.GetCurrentWeather(location); _cache.Set&lt;CurrentWeather&gt;(cacheKey, currentConditions, TimeSpan.FromMinutes(30)); return currentConditions; &#125; &#125; public LocationForecast GetForecast(string location) &#123; string cacheKey &#x3D; $&quot;WeatherForecast::&#123;location&#125;&quot;; if (_cache.TryGetValue&lt;LocationForecast&gt;(cacheKey, out var forecast)) &#123; return forecast; &#125; else &#123; var locationForecast &#x3D; _weatherService.GetForecast(location); _cache.Set&lt;LocationForecast&gt;(cacheKey, locationForecast, TimeSpan.FromMinutes(30)); return locationForecast; &#125; &#125;&#125; And meanwhile in the HomeController, we need to build this onion like structure from inside to outside. 12345IWeatherService weatherService &#x3D; new WeatherService(apiKey);IWeatherService withLoggingDecorator &#x3D; new WeatherServiceLoggingDecorator(weatherService, _loggerFactory.CreateLogger&lt;WeatherServiceLoggingDecorator&gt;());IWeatherService withCachingDecorator &#x3D; new WeatherServiceCachingDecorator(withLoggingDecorator, memoryCache);_weatherService &#x3D; withCachingDecorator; The call stack will be: CachingDecorator =&gt; LoggingDecorator =&gt; WeatherService Decorator Summary Multiple decorators can be used in conjunction with one another Each decorator can focus on a single task, promoting separation of concerns Decorator classes allow functionality to be added dynamically Decorator Pattern Characteristics Implement the same base interface as the original object Take a instance of the original object as part of their constructor Add new behaviors to the original object they are wrapping Using Decorators with Dependency Injection Container .NET Core has built in IoC container which will help us to create WeatherService object when we need it and manage the lifetime of object. We could simplify the HomeController constructor to this 123456private readonly IWeatherService _weatherService;public HomeController(ILogger&lt;HomeController&gt; logger, IWeatherService weatherService)&#123; _weatherService &#x3D; weatherService;&#125; And in the startUp.cs, we configure the IoC container to this 12345678910111213141516171819202122public void ConfigureServices(IServiceCollection services)&#123; services.AddControllersWithViews(); services.AddMemoryCache(); String apiKey &#x3D; Configuration.GetValue&lt;String&gt;(&quot;OpenWeatherMapApiKey&quot;); services.AddScoped&lt;IWeatherService&gt;(serviceProvider &#x3D;&gt; &#123; String apiKey &#x3D; Configuration.GetValue&lt;String&gt;(&quot;OpenWeatherMapApiKey&quot;); var logger &#x3D; serviceProvider.GetService&lt;ILogger&lt;WeatherServiceLoggingDecorator&gt;&gt;(); var memoryCache &#x3D; serviceProvider.GetService&lt;IMemoryCache&gt;(); IWeatherService weatherService &#x3D; new WeatherService(apiKey); IWeatherService withLoggingDecorator &#x3D; new WeatherServiceLoggingDecorator(weatherService, logger); IWeatherService withCachingDecorator &#x3D; new WeatherServiceCachingDecorator(withLoggingDecorator, memoryCache); return withCachingDecorator; &#125;);&#125; Now whenever we need a IWeatherService object, it will be created and provided to us with this structure. (CachingDecorator =&gt; LoggingDecorator =&gt; WeatherService) When to use Decorator Pattern Cross cutting concerns Logging, Performance Tracking(Timer, StopWatch…), Caching, Authorization Manipulate data going to/from component object we need to encrypt and decrypt before being passed to a component Question: What if your component does not have an interface/extend from a base class? Extract an interface from the class What if you can’t modify the class? Adapter Pattern To put a class in front of your component and extract an interface from the Adapter Class Summary Design Patterns are about ideas Interfaces allow us to create loosely coupled designs the decorator pattern adds the ability to dynamically add behavior This is accomplished by wrapping around the original object and intercepting methods","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Factory and Abstract Factory","slug":"Design-Patterns-Factory-and-Abstract-Factory","date":"2021-04-17T13:40:29.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/17/Design-Patterns-Factory-and-Abstract-Factory/","link":"","permalink":"http://hellcy.github.io/2021/04/17/Design-Patterns-Factory-and-Abstract-Factory/","excerpt":"","text":"What is Factory Pattern A factory is an object for creating objects Factory Pattern Variations Simple Factory Factory Method Abstract Factory Factory Pattern Characteristics Client: Asks for a created product Shopping cart Creator: Facilitates a creation ShippingProviderFactory Product: The product of the creation ShippingProvider Instance The Client no longer needs to know how to create an object or exactly what flavor of that class it will use Simple Factory Example We have a ShoppingCart Class and inside this Class we create a shippingProvider object. It will create different shippingProvider based on order’s sender country 123456789101112if (order.Sender.Country &#x3D;&#x3D; &quot;Australia&quot;)&#123; &#x2F;&#x2F;Australia Post Shipping Provider&#125;else if (order.Sender.Country &#x3D;&#x3D; &quot;Sweden&quot;)&#123; &#x2F;&#x2F;Swedish Postal Service Shipping Provider&#125;else&#123; throw new NotSupportedException(&quot;No shipping provider found for origin country&quot;);&#125; But the shippingProvider object should not be created inside the ShoppingCart Class, ShoppingCart Class should just ask a ShippingProviderFactory Class for a shippingProvider object, and it will be provided one. So we should moved the code to a new ShippingProviderFactory Class and invoke this class’s Creation method. 1var shippingProvider &#x3D; ShippingProviderFactory.CreateShippingProvider(order.Sender.Country); One problem is not we are still hardcoding the Country inside our ShippingProviderFactory Class. We should add another layer of abstraction between the ShippingProviderFactory and the implementation of the ShippingProvider. Factory Method The Factory Method Pattern is introduced to allow for a flexible and extensible application 123456789101112131415public abstract class ShippingProviderFactory &#123; public abstract ShippingProvider CreateShippingProvider(string country); public ShippingProvider GetShippingProvider(string country) &#123; var provider &#x3D; CreateShippingProvider(country) &#x2F;&#x2F; we may want to do some common changes on the shippingProvider created &#x2F;&#x2F; before we return it back to the caller (ShoppingCart) if (country &#x3D;&#x3D; &quot;Sweden&quot; &amp;&amp; provider.InsuranceOptions.ProviderHasInsurance) &#123; provider.RequireSignature &#x3D; false; &#125; return provider; &#125;&#125; It contains two methods. The CreateShippingProvider() method will be implemented by its subclasses with different implementations. The GetShippingProvider() method will allow user to decide what’s passed into the creation. And it allows user to do additional common interactions with the result of the creation before it’s being passed back to the caller(ShoppingCart). Now we can create different implementations of the creation of a shippingProvider based on the input parameter(country). 123456789101112131415public class StandardShippingProviderFactory : ShippingProviderFactory&#123; public override ShippingProvider CreateShippingProvider(string country) &#123; return new StandardShippingProviderFactory(); &#125;&#125;public class GlobalExpressShippingProviderFactory : ShippingProviderFactory&#123; public override ShippingProvider CreateShippingProvider(string country) &#123; return new GlobalExpressShippingProvider(); &#125;&#125; In the caller Class (ShoppingCart) we can inject ShippingProviderFactory 123456&#x2F;&#x2F; inject ShippingProviderFactory into the ShoppingCart Constructorpublic ShoppingCart(Order order, ShippingProviderFactory shippingProviderFactory)&#123; this.order &#x3D; order; this.shippingProviderFactory &#x3D; shippingProviderFactory;&#125; Also compose the ShippingProviderFactory object on app start 1var cart &#x3D; new ShoppingCart(order, new StandardShippingProviderFactory()); Abstract Factory Pattern The abstract factory pattern provides a way to encapsulete a group of individual factories that have a common theme without specifying their concrete classes. It adds another layer of abstraction which allow users to choose which factory to use on app start. Different factories have the same methods but with different implementations 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public interface IPurchaseProviderFactory&#123; ShippingProvider CreateShippingProvider(Order order); IInvoice CreateInvoice(Order order); ISummary CreateSummary(Order order);&#125;public class AustraliaPurchaseProviderFactory : IPurchaseProviderFactory&#123; public IInvoice CreateInvoice(Order order) &#123; return new GSTInvoice(); &#125; public ShippingProvider CreateShippingProvider(Order order) &#123; var shippingProviderFactory &#x3D; new StandardShippingProviderFactory(); return shippingProviderFactory.GetShippingProvider(order.Sender.Country); &#125; public ISummary CreateSummary(Order order) &#123; return new CSVSummary(); &#125;&#125;public class SwedenPurchaseProviderFactory : IPurchaseProviderFactory&#123; public IInvoice CreateInvoice(Order order) &#123; if (order.Recipient.Country !&#x3D; order.Sender.Country) &#123; return new NoVATInvoice(); &#125; return new VATInvoice(); &#125; public ShippingProvider CreateShippingProvider(Order order) &#123; ShippingProviderFactory shippingProviderFactory; if (order.Sender.Country !&#x3D; order.Recipient.Country) &#123; shippingProviderFactory &#x3D; new GlobalExpressShippingProviderFactory(); &#125; else &#123; shippingProviderFactory &#x3D; new StandardShippingProviderFactory(); &#125; return shippingProviderFactory.GetShippingProvider(order.Sender.Country); &#125; public ISummary CreateSummary(Order order) &#123; return new EmailSummary(); &#125;&#125; Client (ShoppingCart) Class doesn’t need to know which factory to use, it just needs to know when to create a product using the factory. 1234567891011121314151617181920public ShoppingCart(Order order, IPurchaseProviderFactory purchaseProviderFactory) &#123; this.order &#x3D; order; this.purchaseProviderFactory &#x3D; purchaseProviderFactory;&#125;public string Finalize()&#123; var shippingProvider &#x3D; purchaseProviderFactory.CreateShippingProvider(order); var invoice &#x3D; purchaseProviderFactory.CreateInvoice(order); var summary &#x3D; purchaseProviderFactory.CreateSummary(order); summary.Send(); order.ShippingStatus &#x3D; ShippingStatus.ReadyForShippment; return shippingProvider.GenerateShippingLabelFor(order);&#125; The concrete factory object will be instantiated on app starts(or based on user input). 12345678910111213141516IPurchaseProviderFactory purchaseProviderFactory;if (order.Sender.Country &#x3D;&#x3D; &quot;Sweden&quot;)&#123; purchaseProviderFactory &#x3D; new SwedenPurchaseProviderFactory();&#125;else if (order.Sender.Country &#x3D;&#x3D; &quot;Australia&quot;)&#123; purchaseProviderFactory &#x3D; new AustraliaPurchaseProviderFactory();&#125;else&#123; throw new Exception(&quot;Country not supported.&quot;);&#125;var cart &#x3D; new ShoppingCart(order, purchaseProviderFactory); Factory Pattern in Testing Extract creation of mocked, facked or commonly oused intances in tests. We could use the Factory Pattern in our Unit Tests. it will be easier to test the parts that use them as you can inhect faked or mocked implementations 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public abstract class OrderFactory&#123; protected abstract Order CreateOrder(); public Order GetOrder() &#123; var order &#x3D; CreateOrder(); order.LineItems.Add( new Item(&quot;testA&quot;, &quot;testB&quot;, 100m), 1 ); order.LineItems.Add( new Item(&quot;TestC&quot;, &quot;TestD&quot;, decimal.MaxValue), 1 ); return order; &#125;&#125;public class StandardOrderFactory : OrderFactory&#123; protected override Order CreateOrder() &#123; var order &#x3D; new Order &#123; Recipient &#x3D; new Address &#123; To &#x3D; &quot;Yuan&quot;, Country &#x3D; &quot;Australia&quot; &#125;, Sender &#x3D; new Address &#123; To &#x3D; &quot;Someone else&quot;, Country &#x3D; &quot;Australia&quot; &#125; &#125;; return order; &#125;&#125;public class InternationalOrderFactory : OrderFactory&#123; protected override Order CreateOrder() &#123; var order &#x3D; new Order &#123; Recipient &#x3D; new Address &#123; To &#x3D; &quot;Yuan&quot;, Country &#x3D; &quot;Australia&quot; &#125;, Sender &#x3D; new Address &#123; To &#x3D; &quot;Someone else&quot;, Country &#x3D; &quot;Sweden&quot; &#125; &#125;; return order; &#125;&#125; Summary Separates the client(ShoppingCart) from the creation Introduce subclasses (StandardShippingProviderFactory, GlobalExpressShippingProviderFactory) and concrete implementations to add functionality. Factory Pattern is very common when writing tests","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Command","slug":"Design-Patterns-Command","date":"2021-04-16T08:15:56.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/16/Design-Patterns-Command/","link":"","permalink":"http://hellcy.github.io/2021/04/16/Design-Patterns-Command/","excerpt":"","text":"Command Pattern Characteristics Command Holds the instructions and references to things that it needs in order for it to be executed Receiver Command will execute Receiver Invoker Invoker will execute Command, and will also keep track of all executed commands Client Client decides which command to schedule for execution A command contains all the data to process the request now or at a later time. This means we could execute the command right away once the client schedule that command, or we could schedule all the commands to be executed later on in the lifetime of our application. Example: AddToCartCommand The product which should be added to the cart The shopping cart A way to check stock availability ICommand Interface Because we may need to implement different command, we should create a ICommand interface. It contains three methods. Execute() will execute the command. CanExecute() will check if a command can be execute of not. Undo() will undo all commands we executed before (using a Stack to maintain all executed commands) 123456public interface ICommand&#123; void Execute(); bool CanExecute(); void Undo();&#125; Next we need to implement CommandManager, which is the Invoker component. It contains a Stack Data Structure to maintain the Commands list. When the Client (UI Button) adds a command to the CommandManager, it will be added to the list. (We can also add extra feature like introduce a delay of executing commands or redo all commands later). 12345678910111213141516171819202122public class CommandManager&#123; private Stack&lt;ICommand&gt; commands &#x3D; new Stack&lt;ICommand&gt;(); public void Invoke(ICommand command) &#123; if (command.CanExecute()) &#123; commands.Push(command); command.Execute(); &#125; &#125; public void Undo() &#123; while (commands.Count &gt; 0) &#123; var command &#x3D; commands.Pop(); command.Undo(); &#125; &#125;&#125; Next we start to implement a command AddToCartCommand 1public class AddToCartCommand : ICommand It takes a shoppingCartRepository object, a productRepository object and a product 12345678public AddToCartCommand(IShoppingCartRepository shoppingCartRepository, IProductRepository productRepository, Product product)&#123; this.shoppingCartRepository &#x3D; shoppingCartRepository; this.productRepository &#x3D; productRepository; this.product &#x3D; product;&#125; Just a reminder, the Repository is a pattern for abstracting data access. We could have access the data store from a SQL DB, a web service or a CSV file, but our application doesn’t need to know that. In our case, the shoppingCartRepository and the productRepository are both just a local Dictionary data structure. 123456public bool CanExecute()&#123; if (product &#x3D;&#x3D; null) return false; return productRepository.GetStockFor(product.ArticleId) &gt; 0;&#125; CanExecute() will check if our productRepository actually has the required product. 12345678public void Execute()&#123; if (product &#x3D;&#x3D; null) return; productRepository.DecreaseStockBy(product.ArticleId, 1); shoppingCartRepository.Add(product);&#125; Execute() will decrease the product quantity by one and add it to shoppingCartRepository 12345678910public void Undo()&#123; if (product &#x3D;&#x3D; null) return; var lineItem &#x3D; shoppingCartRepository.Get(product.ArticleId); productRepository.IncreaseStockBy(product.ArticleId, lineItem.Quantity); shoppingCartRepository.RemoveAll(product.ArticleId);&#125; Undo() will put the product from shoppingCartRepository back to the productRepository 123456789101112131415161718192021var shoppingCartRepository &#x3D; new ShoppingCartRepository();var productsRepository &#x3D; new ProductsRepository();var product &#x3D; productsRepository.FindBy(&quot;SM7B&quot;);var addToCartCommand &#x3D; new AddToCartCommand(shoppingCartRepository, productsRepository, product);var increaseQuantityCommand &#x3D; new ChangeQuantityCommand( ChangeQuantityCommand.Operation.Increase, shoppingCartRepository, productsRepository, product);var manager &#x3D; new CommandManager();manager.Invoke(addToCartCommand);manager.Invoke(increaseQuantityCommand);manager.Invoke(increaseQuantityCommand);manager.Invoke(increaseQuantityCommand);manager.Invoke(increaseQuantityCommand); Finally we just need to compose all the necessary objects on app starts. And add the commands to CommandManager. Command Pattern in WPF 123456789101112131415161718192021222324252627282930public interface ICommand&#123; &#x2F;&#x2F; &#x2F;&#x2F; Summary: &#x2F;&#x2F; Occurs when changes occur that affect whether or not the command should execute. event EventHandler CanExecuteChanged; &#x2F;&#x2F; &#x2F;&#x2F; Summary: &#x2F;&#x2F; Defines the method that determines whether the command can execute in its current &#x2F;&#x2F; state. &#x2F;&#x2F; &#x2F;&#x2F; Parameters: &#x2F;&#x2F; parameter: &#x2F;&#x2F; Data used by the command. If the command does not require data to be passed, &#x2F;&#x2F; this object can be set to null. &#x2F;&#x2F; &#x2F;&#x2F; Returns: &#x2F;&#x2F; true if this command can be executed; otherwise, false. bool CanExecute(object parameter); &#x2F;&#x2F; &#x2F;&#x2F; Summary: &#x2F;&#x2F; Defines the method to be called when the command is invoked. &#x2F;&#x2F; &#x2F;&#x2F; Parameters: &#x2F;&#x2F; parameter: &#x2F;&#x2F; Data used by the command. If the command does not require data to be passed, &#x2F;&#x2F; this object can be set to null. void Execute(object parameter);&#125; WPF application has built in ICommand interface. If we want to use our Command implementation (RemoveAllFromCartCommand) with this ICommand interface. We could bind the method with a UI button, then create a RelayCommand Class, which will invoke RemoveAllFromCartCommand method. UI Button -&gt;(bind)-&gt; ICommand method -&gt;(invoke)-&gt; RelayCommand -&gt;(invoke)-&gt; RemoveAllFromCartCommand 1&lt;Button Margin&#x3D;&quot;0 5 5 0&quot; Command&#x3D;&quot;&#123;Binding RemoveAllFromCartCommand&#125;&quot;&gt;Clear&lt;&#x2F;Button&gt; 1public System.Windows.Input.ICommand RemoveAllFromCartCommand &#123; get; private set; &#125; 123456789RemoveAllFromCartCommand &#x3D; new RelayCommand( execute: () &#x3D;&gt; &#123; removeAllFromCartCommand.Execute(); Refresh(); &#125;, canExecute:() &#x3D;&gt; removeAllFromCartCommand.CanExecute()); 1234567891011121314151617181920212223242526272829303132public class RelayCommand : System.Windows.Input.ICommand&#123; private readonly Action execute; private readonly Func&lt;bool&gt; canExecute; public RelayCommand(Action execute, Func&lt;bool&gt; canExecute) &#123; this.execute &#x3D; execute; this.canExecute &#x3D; canExecute; &#125; public bool CanExecute(object parameter) &#123; return canExecute?.Invoke() ?? false; &#125; public void Execute(object parameter) &#123; execute?.Invoke(); &#125; public event EventHandler CanExecuteChanged &#123; add &#123; CommandManager.RequerySuggested +&#x3D; value; &#125; remove &#123; CommandManager.RequerySuggested -&#x3D; value; &#125; &#125; public void RaiseCanExecuteChanged() &#123; CommandManager.InvalidateRequerySuggested(); &#125;&#125; Summary Command Pattern converts the request from Client to an object(ICommand). And the children implementation of the ICommand (AddToCartCommand) will take the Receiver as one of its input parameters (ShoppingCartRepository, ProductRepository). And it will implement the Execute() method, decide what should the Receiver do in Execute() method. And the Receiver should have all the needed information about the request(Product)","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Strategy","slug":"Design-Patterns-Strategy","date":"2021-04-16T05:34:42.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/16/Design-Patterns-Strategy/","link":"","permalink":"http://hellcy.github.io/2021/04/16/Design-Patterns-Strategy/","excerpt":"","text":"Strategy pattern is also called Policy pattern Strategy Pattern Characteristics Context: has a reference to a strategy and invokes it Calls IStrategy.Method(object); IStrategy: Defines the interface for the given strategy Defines the contract Method(object) Strategy: A concrete implementation of the strategy Implementation of Method(object) Select an implementation at runtime based on user input without having to extend the class. Example: ISalesTaxStrategy is an interface. We have multiple different implementations of Strategies to calculate tax. They all implement the ISalesTaxStrategy interface. The code below doesn’t need to know what Strategy is chosen at this step. It only needs to invoke the GetTaxFor() Method. 123456public ISalesTaxStrategy SalesTaxStrategy &#123; get; set; &#125;public decimal GetTax()&#123; return SalesTaxStrategy &#x3D;&#x3D; null ? 0m : SalesTaxStrategy.GetTaxFor(this);&#125; What did we achieve? A more extensible, object oriented and dynamic implementation Easily add new strategies without affecting existing ones Cleaner approach with single responsiblity in mind Another thing we could do is to pass the interface to the GetTax() method. 123public decimal GetTax(ISalesTaxStrategy salesTaxStrategy) &#123; return salesTaxStrategy &#x3D;&#x3D; null ? 0m : salesTaxStrategy.GetTaxFor(this);&#125; And the concrete implementation of the strategy could be determined when we invoke the GetTax() Method 1order.GetTax(new SwedenSalesTaxStrategy() This is still meaning we have a hard dependency between the Order and the SalesTaxStrategy Strategy Pattern with Dependency Injection Pass the already created SalesTaxStrategy to the Order Contructor will help us remove the hard dependency between the Order and the Strategy. 12345678910private ISalesTaxStrategy _salesTaxStrategy;private IInvoiceStrategy _invoiceStrategy;private IShippingStrategy _shippingStrategy;public Order(ISalesTaxStrategy salesTaxStrategy, IInvoiceStrategy invoiceStrategy, IShippingStrategy shippingStrategy)&#123; _salesTaxStrategy &#x3D; salesTaxStrategy; _invoiceStrategy &#x3D; invoiceStrategy; _shippingStrategy &#x3D; shippingStrategy;&#125; Then Order(Context in Strategy Pattern) just need to invoke Strategy implementations without having to know which imeplementation it is invoking. 12345678910111213141516171819public decimal GetTax()&#123; return _salesTaxStrategy &#x3D;&#x3D; null ? 0m : _salesTaxStrategy.GetTaxFor(this);&#125;public void FinalizeOrder()&#123; if (SelectedPayments.Any(x &#x3D;&gt; x.PaymentProvider &#x3D;&#x3D; PaymentProvider.Invoice) &amp;&amp; AmountDue &gt; 0 &amp;&amp; ShippingStatus &#x3D;&#x3D; ShippingStatus.WaitingForPayment) &#123; _invoiceStrategy.Generate(this); ShippingStatus &#x3D; ShippingStatus.ReadyForShippment; &#125; else if (AmountDue &gt; 0) &#123; throw new Exception(&quot;Unable to finalize order&quot;); &#125; _shippingStrategy.Ship(this);&#125; On Application start we create different Strategies based on user input 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950switch (origin)&#123; case EnumTaxStrategy.Sweden: salesTaxStrategy &#x3D; new SwedenSalesTaxStrategy(); break; case EnumTaxStrategy.USA: salesTaxStrategy &#x3D; new USAStateSalesTaxStrategy(); break; default: salesTaxStrategy &#x3D; new SwedenSalesTaxStrategy(); break;&#125;switch (inputInvoiceStrategy)&#123; case EnumInvoiceStrategy.Email: invoiceStrategy &#x3D; new EmailInvoiceStrategy(); break; case EnumInvoiceStrategy.File: invoiceStrategy &#x3D; new FileInvoiceStrategy(); break; case EnumInvoiceStrategy.PrintOnDemand: invoiceStrategy &#x3D; new PrintOnDemandInvoiceStrategy(); break; default: invoiceStrategy &#x3D; new FileInvoiceStrategy(); break;&#125;switch (inputShippingStrategy)&#123; case EnumShippingStrategy.DHL: shippingStrategy &#x3D; new DHLShippingStrategy(); break; case EnumShippingStrategy.Fedex: shippingStrategy &#x3D; new FedexShippingStrategy(); break; case EnumShippingStrategy.SwedishPostalService: shippingStrategy &#x3D; new SwedishPostalServiceShippingStrategy(); break; case EnumShippingStrategy.UPS: shippingStrategy &#x3D; new UPSShippingStrategy(); break; case EnumShippingStrategy.USPS: shippingStrategy &#x3D; new UnitedStatesPostalServiceShippingStrategy(); break; default: shippingStrategy &#x3D; new DHLShippingStrategy(); break;&#125; Summary One of the most commonly used patterns Decouple the context and the concrete implementation Allows for a cleaner implementation in the context Easily extend with additional startegies without affecting current implementations Makes testing a lot easier as you can write mocked implementations to inject Identify existing implementations and where you have used the pattern before","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Design Patterns - Singleton","slug":"Design-Patterns-Singleton","date":"2021-04-15T12:03:02.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/15/Design-Patterns-Singleton/","link":"","permalink":"http://hellcy.github.io/2021/04/15/Design-Patterns-Singleton/","excerpt":"","text":"A singleton is a class designed to only ever have one instance. Singleton Features At any time, only 0 or 1 instance of the Singleton class exists in the application Singleton classes are created without parameters Assume lazy instantiation as the default A single, private, parameterless constructor Sealed class A private, static field holds the only reference to the instance A public static method provides access to the field Naive implementation of Singleton 1234567891011121314151617181920212223namespace Singleton&#123;#nullable enable public sealed class Singleton &#123; private static Singleton? _instance; public static Singleton Instance &#123; get &#123; &#x2F;&#x2F; lazy instantiate Logger.Log(&quot;Instance called&quot;); return _instance ??&#x3D; new Singleton(); &#125; &#125; private Singleton() &#123; &#x2F;&#x2F; cannot be created except within this class Logger.Log(&quot;Constructor invoked&quot;); &#125; &#125;&#125; The problem of this native implementation is Thread safety. In multi-thread environment, the If block can be reached by multiple threads concurrently, resulting in multiple instantiations of Singleton. Thread Safe Singleton One way to make sure the Singleton Instance will not be created in a multiple thread environment is to use a lock 1private static readonly object padlock &#x3D; new object(); This lock is a private static readonly object that will be shared by all references to the Singleton instance 1234567&#x2F;&#x2F; this lock is used on every reference to Singletonlock (padlock)&#123; Logger.Log(&quot;Instance called.&quot;); &#x2F;&#x2F; lazy instantiation return _instance ??&#x3D; new Singleton();&#125; A slight better way to add lock is to use the double check locking pattern, this gives us a better performance because we don’t need to check lock very often 1234567891011if (_instance &#x3D;&#x3D; null)&#123; &#x2F;&#x2F; this lock is used on every reference to Singleton lock (padlock) &#123; Logger.Log(&quot;Instance called.&quot;); &#x2F;&#x2F; lazy instantiation _instance &#x3D; new Singleton(); &#125;&#125;return _instance; Locking adds thread safety First version imposes lock on every access, not just first time Second version is better, but has some issues with the ECMA CLI spec that may be a concern Neither approach works as well as the next ones Static Constructors C# static constructors only run once per app domain static constructors are called when any static member of a type is referenced Make sure you use an explicit static constructor to avoid issue with C# compiler and beforefieldinit (beforefieldinit is a hint the compiler uses to let it know static initializers can be called sooner, and this is the default if the type does not have an explicit static constructor. Adding an explicit static constructor avoids having beforefieldinit applied, which helps make our singleton behavior lazier) 123456789101112131415161718192021222324252627282930public sealed class StaticConstructorSingleton : ISingleton&#123; &#x2F;&#x2F; reading this will initialize the instance public static readonly string GREETING &#x3D; &quot;Hi!&quot;; public static StaticConstructorSingleton Instance &#123; get &#123; Logger.Log(&quot;Instance called&quot;); return Nested._instance; &#125; &#125; private class Nested &#123; &#x2F;&#x2F;Tell C# compiler not to mark type as beforefieldinit static Nested() &#123; &#125; internal static readonly StaticConstructorSingleton _instance &#x3D; new StaticConstructorSingleton(); &#125; private StaticConstructorSingleton() &#123; &#x2F;&#x2F; cannot be created except within this class Logger.Log(&quot;Constructor invoked.&quot;); &#125;&#125; This approach is Thread-safe, no locks (good performance), but is complex and non-intuitive. Lazy One difference between this approach and the naive approach is that the private static readonly field is type of Lazy&lt;Singleton&gt; rather than just Singleton, this field is initilized at construction to create a new Lazy&lt;T&gt; instance, and a lambda function is passed into the Lazy&lt;T&gt; constructor with the logic needed to create the singleton instance. 1234567891011121314151617181920public sealed class LazyTSingleton&#123; &#x2F;&#x2F; reading this will initilize the instance private static readonly Lazy&lt;LazyTSingleton&gt; _lazy &#x3D; new Lazy&lt;LazyTSingleton&gt;(() &#x3D;&gt; new LazyTSingleton()); public static LazyTSingleton Instance &#123; get &#123; Logger.Log(&quot;Instance called.&quot;); return _lazy.Value; &#125; &#125; private LazyTSingleton() &#123; &#x2F;&#x2F; cannot be created except within this class Logger.Log(&quot;Constructor invoked.&quot;); &#125;&#125; This approach is very easy to understand and has the performance and thread safe feature. Singletons vs Static Classes Singletons Static Classes Can implement interfaces No interfaces Can be passed as an argument Cannot be passed as arguments Can be assigned to variables Cannot be assigned Support polymorphism Purely procedural Can have state Can only access global state Can be serialized No support for serialization Singleton Behavior Using Containers(IoC) .NET Core has built-in support for IoC Containers Classes request dependencies via constructor Classes should follow Explicit Dependencies Principle Container manages abstraction-implementation mapping Container manages instnace lifetime Manage Lifetime Using Container, not Class Design Easily manage and modify individual class lifetimes using an IoC container Can also be used by any service, console application, etc… 1234567public void ConfigureService(ServiceCollection services) &#123; services.AddTransient&lt;IOrderService, OrderService&gt;(); services.AddScoped&lt;IOrderRepository, OrderRepository&gt;(); services.AddSingleton&lt;IConnectionManager, ConnectionManager&gt;(); services.AddSingleton&lt;SomeInstance&gt;(new SomeInstance);&#125; Transient: A new instance of the type is provided any time a class requests that type as a dependency. Scope: Define a scope and any instance requested within that scope will be shared if it’s requested again within that socpe. The first request will get a new instance and all subsequent requests in that scope will get that same instance. Singleton: only one instance will be created and shared by all references. Just like the Singleton pattern. IoC containers are probably the best approach in systems that already use them. Otherwise, Laszy&lt;T&gt; provides an elegant, easily understood approach. Summary A Singleton class is designed to only ever have one instance created. The Singleton pattern makes the class itself responsible for enforcing Singleton behavior It’s easy to get the pattern wrong when implementing by hand Lazy&lt;T&gt; is one of the better ways to apply the pattern Singletons are different from Static Classes IoC/DI containers are usually a better place to manage instance lifetime in .NET applications.","categories":[],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"}]},{"title":"Creating Automated Browser Tests with Selenium in C#","slug":"Creating-Automated-Browser-Tests-with-Selenium-in-C","date":"2021-04-14T04:20:04.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/14/Creating-Automated-Browser-Tests-with-Selenium-in-C/","link":"","permalink":"http://hellcy.github.io/2021/04/14/Creating-Automated-Browser-Tests-with-Selenium-in-C/","excerpt":"","text":"What is Selenium? Selenium is a portable framework for testing web applications. The tests can then run against most modern web browsers. Selenium runs on Windows, Linux, and macOS. Some of the features we can do using Selenium WebDriver Navigate to a specific page/forware/back Click the button with an ID Type text into the &lt;input&gt; Get the text content of the SPAN that has a CSS class Choose a radio button Check a tick box Get the title of the current page Maximum the browser window Take a screenshot Selenium WebDriver Testing Architecture The Limitations of Automated Browser Tests Slower than other types of tests (unit tests) Not a replacement of all manuall testing Additional dependencies (Selenium, WebDriver…) Setting up the test project Install NuGet Packages Selenium.WebDriver Selenium.WebDriver.ChromeDriver Your first test case The Web App has to be running for Selenium to work 1234567891011121314151617181920212223242526272829private string baseUrl &#x3D; &quot;http:&#x2F;&#x2F;localhost:29128&#x2F;&quot;;[Test][Category(&quot;Login&quot;)]public void ShouldLogin()&#123; using (IWebDriver driver &#x3D; new ChromeDriver()) &#123; driver.Navigate().GoToUrl(baseUrl); var userNameBox &#x3D; driver.FindElement(By.Id(&quot;username&quot;)); userNameBox.SendKeys(&quot;admin&quot;); Thread.Sleep(1000); var passwordBox &#x3D; driver.FindElement(By.Id(&quot;password&quot;)); passwordBox.SendKeys(&quot;admin&quot;); Thread.Sleep(1000); var submitButton &#x3D; driver.FindElement(By.Id(&quot;submit&quot;)); submitButton.Click(); Thread.Sleep(1000); var currentPageTitle &#x3D; driver.Title; Assert.That(currentPageTitle, Is.EqualTo(&quot;identityOne - Home Page&quot;)); &#125;&#125; Get Page Title 1234567891011[Test][Category(&quot;Test&quot;)]public void GetPageTitle()&#123; using (IWebDriver driver &#x3D; new ChromeDriver()) &#123; driver.Navigate().GoToUrl(baseUrl); Assert.That(&quot;identityOne - Home Page&quot;, driver.Title); &#125;&#125; Read current URL 1234567891011[Test][Category(&quot;Test&quot;)]public void ReadCurrentUrl()&#123; using (IWebDriver driver &#x3D; new ChromeDriver()) &#123; driver.Navigate().GoToUrl(baseUrl); Assert.That(baseUrl, driver.Url); &#125;&#125; Reload current page / go backward/forward 12345678910111213[Test][Category(&quot;Test&quot;)]public void ReloadCurrentPage()&#123; using (IWebDriver driver &#x3D; new ChromeDriver()) &#123; driver.Navigate().GoToUrl(baseUrl); driver.Navigate().Refresh(); driver.Navigate().Back(); driver.Navigate().Forward(); &#125;&#125; Manipulating HTML Elements 1234567891011121314151617181920212223242526272829303132333435363738394041[Test][Category(&quot;Test&quot;)]public void ReloadCurrentPage()&#123; using (IWebDriver driver &#x3D; new ChromeDriver()) &#123; driver.Navigate().GoToUrl(baseUrl); IWebElement textElement &#x3D; driver.FindElement(By.Id(&quot;username&quot;)); &#x2F;&#x2F; find element by ID string usernameText &#x3D; textElement.Text; &#x2F;&#x2F; Get HTML element text IWebElement buttonElement &#x3D; driver.FindElement(By.Name(&quot;button&quot;)); &#x2F;&#x2F; find element by Name buttonElement.Click(); &#x2F;&#x2F; Click a button or link IWebElement linkElement &#x3D; driver.FindElement(By.LinkText(&quot;link&quot;)); &#x2F;&#x2F; find element by LinkText linkElement.Click(); &#x2F;&#x2F; Click a button or link IWebElement buttonElement &#x3D; driver.FindElement(By.CssSelector(&quot;body&quot;)); &#x2F;&#x2F; find element by CssSelector IWebElement buttonElement &#x3D; driver.FindElement(By.ClassName(&quot;TestClass&quot;)); &#x2F;&#x2F; find element by class name IWebElement textElement &#x3D; driver.FindElement(By.TagName(&quot;td&quot;)); &#x2F;&#x2F; find element by tag name IWebElement linkElement &#x3D; driver.FindElement(By.PartialLinkText(&quot;Partial Text&quot;)); &#x2F;&#x2F; find element by PartialLinkText IWebElement linkElement &#x3D; driver.FindElement(By.XPath(&quot;&#x2F;html&#x2F;body&#x2F;div[4]&#x2F;p&#x2F;a&quot;)); &#x2F;&#x2F; find element by XPath &#x2F;&#x2F; this relative XPath will find all &lt;a&gt; elements with its text contains &#39;some text&#39; IWebElement linkElement &#x3D; driver.FindElement(By.XPath(&quot;&#x2F;&#x2F;a[text()[contains(.,&#39;some text&#39;)]]&quot;)); &#x2F;&#x2F; find element by Relative XPath &#x2F;&#x2F; WebDriverWait is given a timeout value indicating how long to wait for the condition. WebDriverWait wait &#x3D; new WebDriverWait(driver, TimeSpan.FromSeconds(1)); &#x2F;&#x2F; Selenium will try to find the linkElement until the timeout value is reached IWebElement linkElement &#x3D; wait.Until(d &#x3D;&gt; d.FineElement(By.LinkText(&quot;some text&quot;))); &#x2F;&#x2F; Selecting multiple elements ReadOnlyCollection&lt;IWebElement&gt; tableCells &#x3D; driver.FindElements(By.TagName(&quot;td&quot;)); Assert.That(&quot;first cell&quot;, tableCells[0].Text); &#125;&#125;","categories":[],"tags":[{"name":"Selenium","slug":"Selenium","permalink":"http://hellcy.github.io/tags/Selenium/"},{"name":"Functional Tests","slug":"Functional-Tests","permalink":"http://hellcy.github.io/tags/Functional-Tests/"}]},{"title":"Dependency Injection in .NET","slug":"Dependency-Injection-in-NET","date":"2021-04-12T11:05:18.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/04/12/Dependency-Injection-in-NET/","link":"","permalink":"http://hellcy.github.io/2021/04/12/Dependency-Injection-in-NET/","excerpt":"","text":"What is dependency injection? Dependency injection is a programming technique that makes a class independent of its dependencies. It achieves that by decoupling the usage of an object from its creation. This helps you to follow SOLID’s dependency inversion and single responsiblity principles. Benefits of Loose Coupling Easy to extend Easy to test Easy to maintain Facilitates parallel development (rare conflict) Facilitates late binding (runtime data binding) Dependency Injection Patterns Constructor Injection Property Injection Method Injection Ambient Context Service Locator Application Overview The application contains four layers View (UI elements) such as the buttons and the list box Presentation (UI logic): functions that the buttons call and the property that the data bound to the list box in the UI. Data Access: Code that knows how to interact with the data store. It knows how to make a web service call, and then translate the results into objects that the Presentation layer can use. Data Store: where we get the actual data, in this case, the web service. Tight Coupled Code In the initial code, all four layers are tightly coupled. In the view layer, it creates PeopleViewModel() 123456public PeopleViewerWindow()&#123; InitializeComponent(); viewModel &#x3D; new PeopleViewModel(); this.DataContext &#x3D; viewModel;&#125; In the Presentation layer, it creates serviceReader() 1234public PeopleViewModel()&#123; DataReader &#x3D; new ServiceReader();&#125; In the Data Access layer, it hardcoded the web service url 12345public class ServiceReader&#123; WebClient client &#x3D; new WebClient(); string baseUri &#x3D; &quot;http:&#x2F;&#x2F;localhost:9874&#x2F;api&#x2F;people&quot;;&#125; Potential problems Hard to create unit test. If I want to test UI element (like a button), I have to run the web services because they are tightly coupled. (PeopleViewerWindow needs PeopleViewModel which needs ServiceReader which needs WebClient). Hard to extend If I want to add another Data Store like read data from a CSV file or SQL DB, and I also want to have the option to choose to use cached data store. Then in the PeopleViewModel I need to write something like this 12345678910111213141516public PeopleViewModel() &#123; switch(dataReaderType) &#123; case &#39;service&#39;: DataReader &#x3D; new ServiceReader(); break; case &#39;service_cached&#39;: DataReader &#x3D; new CachedServiceReader(); break; case &#39;text&#39;: DataReader &#x3D; new CSVReader(); break; case &#39;text_cached&#39;: DataReader &#x3D; new CachedCSVReader(); break; case &#39;sql&#39;: DataReader &#x3D; new SQLReader(); break; case &#39;sql_cached&#39;: DataReader &#x3D; new CachedSQLReader(); break; &#125;&#125; This breaks the Single Responsibility Principle, which is one of the SOLID principles. Because it is now doing too many things Presentation logic Picking the data source (hardcoded web service url) Managing object lifetime Deciding when to use a cache Repository Pattern Mediates between the domain and data mapping layers using a collection-like interface for accessing domain objects. It separates our application from the data storage technology. In other words, we can say that a Repository Design Patternacts as a middleman or middle layer between the rest of the application and the data access logic. That means a repository pattern isolates all the data access code from the rest of the application. The advantage of doing so is that, if you need to do any change then you need to do in one place. Another benefit is that testing your controllers becomes easy because the testing framework need not run against the actual database access code. The idea is that the repository knows how to communicate with the data store whether it is using HTTP, reading a file from the file system, or making a database call. It then takes the data that comes back and turns it into normal C# objects that the rest of the application can understand. This is exactly what the service reader does now. It makes a HTTP request to the web service, then parses the JSON result into Person objects that the application can use. CRUD Repository The Interface Segregation Principle says that interfaces should only contain what the client needs. Normally a Repository should contain all Create, Read, Update and Delete. But in this case we only need Read. Using Dependency Injection to Build Loosely-coupled Application Create a new interface 12345public interface IPersonReader&#123; IEnumerable&lt;Person&gt; GetPeople(); Person GetPerson(int id);&#125; In the Presentation layer, inject IPersonReader 1234public PeopleViewModel(IPersonReader dataReader)&#123; DataReader &#x3D; dataReader;&#125; Now we don’t create new ServiceReader in the Presentation layer, instead we make it someone else’s responsibility by adding IPersonReader to a contrcutor parameter. IPersonReader could be ServiceReader or SQLReader or CSVReader but PeopleViewModel doesn’t care. IPersonReader need to be created before creating PeopleViewModel. In the UI layer, inject PeopleViewModel 123456public PeopleViewerWindow(PeopleViewModel peopleViewModel)&#123; InitializeComponent(); viewModel &#x3D; peopleViewModel; this.DataContext &#x3D; viewModel;&#125; Similar things happened in UI layer, now we don’t create new PeopleViewModel, instead we inject it to the constrctor. PeopleViewModel need to be created before creating PeopleViewerWindow but it doesn’t care, as long as it is being passed to the constructor. Dependency Inversion Principle This is Dependency Inversion Principle in action, now the View Model and the Viewer Window are no longer responsible for creating or managing the lifetime of the dependencies. Instead, the dependency, the data reader, is given to the View Model and the Viewer Window to use. Object composition 123456private static void ComposeObjects()&#123; var serviceReader &#x3D; new ServiceReader(); var peopleViewModel &#x3D; new PeopleViewModel(serviceReader); Application.Current.MainWindow &#x3D; new PeopleViewerWindow(peopleViewModel);&#125; The ServiceReader and PeopleViewModel objects have been created when the app starts. This code will run OnAppStarts. Get Data from CSV file Because now the presentation layer and the data access layer are loosely coupled, if we want to change the data source, we could just create a CSVReader() instead of ServiceReader() 1234567private static void ComposeObjects()&#123; &#x2F;&#x2F;var serviceReader &#x3D; new ServiceReader(); var serviceReader &#x3D; new CSVReader(); var peopleViewModel &#x3D; new PeopleViewModel(serviceReader); Application.Current.MainWindow &#x3D; new PeopleViewerWindow(peopleViewModel);&#125; We just need to implement CSVReader class and create a new CSV object. We don’t need to change any existing code. Decorator pattern Wrap an existing interface to add functionality The idea is we wrap an existing data reader, add the caching functionality and then expose the same data reader interface to the outside world. Our service reader implements the IPersonReader interface. We take that service reader and wrap it in a caching reader. This adds the caching funcationality that we need. The caching reader is also an IPersonReader. So it looks just like any other data reader to the rest of the application. By using a Decorator, we can wrap any of our existing data readers. 1234567891011121314151617181920212223public class CachingReader : IPersonReader&#123; private IPersonReader _wrappedReader; private TimeSpan _cacheDuration &#x3D; new TimeSpan(0, 0, 30); private IEnumerable&lt;Person&gt; _cachedItems; private DateTime _dataDateTime; public CachingReader(IPersonReader wrappedReader) &#123; _wrappedReader &#x3D; wrappedReader; &#125; public IEnumerable&lt;Person&gt; GetPeople() &#123;...&#125; public Person GetPerson(int id) &#123;...&#125; private bool IsCacheValid &#123;...&#125; private void ValidateCache() &#123;...&#125; private void InvalidateCache() &#123;...&#125;&#125; In this implementation, CachingReader implements IPersonReader, so it also has GetPeople and GetPerson functions. But it has some other functions (extra caching funcationality). 1234567private static void ComposeObjects()&#123; var wrappedReader &#x3D; new ServiceReader(); var reader &#x3D; new CachingReader(wrappedReader); var peopleViewModel &#x3D; new PeopleViewModel(reader); Application.Current.MainWindow &#x3D; new PeopleViewerWindow(peopleViewModel);&#125; When we want to use the CachingReader, we could first create a ServiceReader, so it has GetPeople and GetPerson function, and we inject this ServiceReader to the CachingReader’s contructor, which adds the extra caching funcationality. This follows the Open/Closed Principle. Existing data readers can be extended without being modified. This also follows the Liskov substitution principle. This principle says that descendent classes (CachingReader) should behave the same way the base class (ServiceReader) behave. Meaning we could substitude a child class (CachingReader) for a base class (ServiceReader) in our application, and the application does not know the difference. We have extended the behavior in the child class, but the calling code does not know the difference. Unit Testing with Dependency Injection Before when we need to test the ViewModel, the data service needs to run. Because of the old code looks like this. 1234567public PeopleViewModel() &#123; DataReader &#x3D; new ServiceReader();&#125;public class ServiceReader() &#123; WebClient client &#x3D; new WebClient();&#125; Now we can just create a fake Reader and provide some fake Person data. The test of ViewModel is isolated from the Data Store. 123456789101112[Test]public void Test() &#123; &#x2F;&#x2F; Arrange IPersonReader reader &#x3D; GetFakeReader(); var viewModel &#x3D; new PeopleViewModel(reader); &#x2F;&#x2F; Act viewModel.RefreshPeople(); &#x2F;&#x2F; Assert ...&#125; Below is the real unit test code. By passing the FakeReader() to the PeopleViewModel(), we don’t need to create WebClient any more, which makes it easier to write unit test. 1234567891011121314[TestMethod]public void People_OnRefreshPeople_IsPopulated()&#123; &#x2F;&#x2F; Arrange var reader &#x3D; new FakeReader(); var viewModel &#x3D; new PeopleViewModel(reader); &#x2F;&#x2F; Act viewModel.RefreshPeople(); &#x2F;&#x2F; Assert Assert.IsNotNull(viewModel.People); Assert.AreEqual(2, viewModel.People.Count());&#125; Next we want to test CSVReader(), but we need to put a real CSV data file in the project directory to make it work because it is expecting a filePath. 12345public CSVReader()&#123; string filePath &#x3D; AppDomain.CurrentDomain.BaseDirectory + &quot;People.txt&quot;; FileLoader &#x3D; new CSVFileLoader(filePath);&#125; Luckly, the FileLoader is a public property which can be overrided. 1public ICSVFileLoader FileLoader &#123; get; set; &#125; So we could create a FakeFileReader that provides the fake data. We override the property with our own behavior so it doesn’t depend on the file system. This is called Property Injection 12345678910[TestMethod]public void GetPeople_WithGoodRecords_ReturnsAllRecords()&#123; var reader &#x3D; new CSVReader(); reader.FileLoader &#x3D; new FakeFileLoader(&quot;Good&quot;); var result &#x3D; reader.GetPeople(); Assert.AreEqual(2, result.Count());&#125; Property Injection Class property is initialized for standard behavior. By default, the standard behavior is used. Property can be set to provide alternate behavior. One question is, why not use Constructor Injection like what we did in the previous charpter? Instead of creating CSVFileLoader in the CSVReader constructor, we could inject FileLoader. This is because we only use FakeFileLoader when doing unit test. In production, it will use CSVFileLoader 100% of the time. So Constructor injection is only good for when we want to force a decision on a dependency. Property injection is good for when we have a default dependency (CSVFileLoader) that we want to use most of the time. Dependency Injection Containers Autofac Ninject Unity Castle Windsor Spring.NET","categories":[],"tags":[{"name":"Dependency Injection","slug":"Dependency-Injection","permalink":"http://hellcy.github.io/tags/Dependency-Injection/"}]},{"title":"SQL Fundamentals","slug":"SQL-Fundamentals","date":"2021-04-11T10:05:17.000Z","updated":"2021-05-24T07:27:02.660Z","comments":true,"path":"2021/04/11/SQL-Fundamentals/","link":"","permalink":"http://hellcy.github.io/2021/04/11/SQL-Fundamentals/","excerpt":"","text":"WHERE 1SELECT * FROM Table WHERE ColumnName &lt;&gt; &#39;Some value&#39;. This Query will not return Null values. To return Null values we need to check if Column is NULL 1SELECT * FROM Table WHERE ColumnName IS NULL LIKE If a column has type varchar(50), and in one row its value doesn’t have length 50. Spaces will be added to the end. So WHERE ColumnName LIKE '%SomeValue' will return nothing. You can write the query like this. WHERE ColumnName LIKE '%SomeValue%' Functions LEFT(): return the left most char from string RIGHT(): return the right most char from string LTRIM(): remove the spaces on the left of string RTRIM(): remove the spaces on the right of string GROUP BY and HAVING GROUP BY will group values into different groups. HAVING can filter out some values after GROUP BY. JOIN INNER JOIN The default JOIN, rows will be returned if they appear in both tables CROSS JOIN Will return the combination of rows from Table A and Table B. If Table A has 4 rows and Table B has 10 row, it will return 40 rows. OUTER JOIN LEFT OUTER JOIN (LEFT JOIN) The OUTER keyword is optional. LEFT OUTER JOIN is the same as LEFT JOIN. Values will be returned if it appear in the LEFT table. It doesn’t need to be in the RIGHT table. RIGHT OUTER JOIN (RIGHT JOIN) RIGHT OUTER JOIN is the same as RIGHT JOIN. Values will be returned if it appear in the RIGHT table. It doesn’t need to be in the LEFT table. FULL OUTER JOIN (FULL JOIN) Values will be returned if they appear in either LEFT table or RIGHT table. UNION UNION can be placed between SELECT queries. Rows will be appended for each SELECT. Each SELECT must have same number of Columns and DataType needs to match. UNION ALL If SELECT queries return same rows, they will all be returned. INSERT INSERT SELECT The values returned from SELECT will be inserted immediately. Works will multiple rows. 123INSERT INTO TableA(ColumnA, ColumnB)SELECT ColumnA, ColumnBFROM TableB UPDATE and DELETE If we want to delete some value in a row, we could SET it to NULL 1UPDATE Table SET ColumnName &#x3D; NULL WHERE RowId &#x3D; 1 CREATE TABLE 123456CREATE TABLE TableName (ProductId int NOT NULL,Quantity int NOT NULL DEFAULT 1,ProductName varchar(10) NULL); CREATE VIEW 1234CREATE VIEW ViewName AS SELECT *FROM TableWHERE ... VIEW is a temp Table, it can save complex SELECT queries and can be reused later. TRANSACTION If there is error in the middle of a TRANSACTION, it will not COMMIT 123BEGIN TRANSACTION-- Multiple UPDATE&#x2F;INSERT&#x2F;DELETE queriesCOMMIT TRANSACTION SAVEPOINT and ROLLBACK If error happens in the middle of a TRANSACTION and we don’t want to ROLLBACK to the start. We can create SAVEPOINT and let it ROLLBACK to that point. 1234567891011BEGIN TRANSACTIONINSERT INTO Table(ColumnA, ColumnB) VALUES (1, 2);SAVE TRANSACTION PointOne;INSERT INTO Table(ColumnA, ColumnB) VALUE (1, 2);If @@ERROR &lt;&gt; 0 ROLLBACK TRANSACTION PointOne;COMMIT TRANSACTION In the above code, if the second INSERT failed, @@ERROR will return a non-zero value, it will ROLLBACK to SAVEPOINT PointOne. Constraint Primary key 1234CREATE TABLE Table ( RowId int NOT NULL PRIMARY KEY); Foreign key 12345CREATE TABLE Table ( RowId int NOT NULL PRIMARY KEY, ForeignId int NOT NULL REFERENCES TableB(Id)); Foreign Key values must come from Primary key in the other table. Primary key records cannot be deleted unless all Foreign key records were deleted first. Some DBMS support CASCADE DELETE, which will delete the Primary key record and related Foreign key record in other tables. UNIQUE One table could have multiple UNIQUE Constraint. CHECK Further restrict values in this Column 12345CREATE TABLE Table ( quantity int NOT NULL CHECK (quantity &gt; 0), gender varchar(1) NOT NULL CHECK (gender LIKE &#39;[MF]&#39;)); INDEX If you create an index on a Column, DB will sort this Column and store it. Next time you SELECT by this Column, DB will search faster (binary search) because it is sorted. But add index to a Column will decrease the efficiency of doing UPDATE/INSERT/DELETE on those Columns. Because DB needs to update INDEX on those Columns. 12CREATE INDEX Table_Column_IndexON Table (Column); TRIGGER TRIGGER will be execute when certain changes happen to a table 1234567CREATE TRIGGER Table_triggerON TableFOR INSERT, UPDATEAS UPDATE TableSET ColumnName &#x3D; Upper(ColumnName)WHERE Table.Id &#x3D; inserted.Id","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://hellcy.github.io/tags/SQL/"}]},{"title":".NET with NUnit Test","slug":"NET-with-NUnit-Test","date":"2021-04-08T13:06:15.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/08/NET-with-NUnit-Test/","link":"","permalink":"http://hellcy.github.io/2021/04/08/NET-with-NUnit-Test/","excerpt":"","text":"What is NUnit? NUnit is a unit-testing framework for all .Net languages. Initially ported from JUnit, the current production release, version 3, has been completely rewritten with many new features and support for a wide range of .NET platforms. NuGet Packages NUnit NUnit3TestAdapter Microsoft.NET.Test.Sdk Your First NUnit Test Case Add [TestFixture] and [Test] to mark code as tests Test can be run in Test Explorer and in Command Line Why Write Automated Tests? Help to find defects and regressions. When we make a change to the project, we may find that unintentionally break one of the existing tests. Something that once working is no longer working. Automated Tests give us greater confidence that the software is working as it should. Understanding the NUnit Test Framework NUnit Library Attributes e.g. [Test] Assertions Test Runner Recognizes attributes Execute test methods Report test results Test explorer donet test NUnit attributes Overview [TestFixture]: Mark a class that contains tests [Test]: Mark a method as a test [Category]: Organize tests into categories [TestCase]: Data driven test cases [Values]: Data driven test parameters [Sequential]: How to combine test data [SetUp]: Run code before each test [OneTimeSetUp]: Run code before first test in class NUnite Assertions Overview 123&#x2F;&#x2F; Constraint Model of assertions (newer)Assert.That(sut.Years, Is.EqualTo(1));Assert.That(test result, constraint instance); This Classic Model is still supported but since no new features have been added to it for some time. the constraint-based model must be used in order to have full access to NUnit’s capabilities. 1234Classic Model of assertions (older)Assert.AreEqual(1, sut.Years);Assert.NotNull(sut.Years);Assert.xyz(...); The Logical Arrange, Act, Assert Test Phases Arrange: Set up test objects, initialize test data Act: call methods, set property, to cause some effect in the project Assert: compare returned value/end state with expected Qualities of Good Tests Fast Repeatable Isolated: One Test should not depend on others to run Trustworthy Valuable Asserting on Different Types of Results Asserts: Evaluate and verify the outcome of a test based on a returned result, final object state, or the occurence of events observed during execution. An assert should either pass or fail. How many asserts per test? A single test usually focuses on testing a single ‘behaviour’. Multiple asserts are usually ok if all the asserts are related to testing this single behaviour. Asserting on Equality 1234567&#x2F;&#x2F; compare valueAssert.That(a, Is.EqualTo(...));Assert.That(a, Is.Not.EqualTo(...));&#x2F;&#x2F; compare referenceAssert.That(a, Is.SameAs(...));Assert.That(a, Is.Not.SameAs(...)); Adding custom failure message 1Assert.That(a, Is.EqualTo(...), &quot;Custom Error Message&quot;); Asserting on Floating Numbers 12Assert.That(a, Is.EqualTo(0.33).Within(0.001));Assert.That(a, Is.EqualTo(0.33).Within(10).Percent); Asserting on Null Values 1234string name &#x3D; &quot;yuan&quot;;Assert.That(name, Is.Null); &#x2F;&#x2F; failAssert.That(name, Is.Not.Null); &#x2F;&#x2F; pass Asserting on String Values 12345678910111213141516171819string name &#x3D; &quot;yuan&quot;;Assert.That(name, Is.Empty); &#x2F;&#x2F; failAssert.That(name, Is.Not.Empty); &#x2F;&#x2F; passAssert.That(name, Is.EqualTo(&quot;yuan&quot;)); &#x2F;&#x2F; passAssert.That(name, Is.EqualsTo(&quot;YUAN&quot;)); &#x2F;&#x2F; fail, case-sensitiveAssert.That(name, Is.EqualTo(&quot;YUAN&quot;).IgnoreCase); &#x2F;&#x2F; passAssert.That(name, Does.StartWith(&quot;yu&quot;)); &#x2F;&#x2F; passAssert.That(name, Does.EndWith(&quot;an&quot;)); &#x2F;&#x2F; passAssert.That(name, Does.Contain(&quot;ua)); &#x2F;&#x2F; passAssert.That(name, Does.Not.Contain(&quot;kk&quot;)); &#x2F;&#x2F; passAssert.That(name, Does.StartWith(&quot;yu&quot;) .And .EndWith(&quot;an&quot;)); &#x2F;&#x2F; passAssert.That(name, Does.StartWith(&quot;kk&quot;) .Or .EndWith(&quot;an&quot;)); &#x2F;&#x2F; pass Asserting on Boolean Values 12345678910bool isTrue &#x3D; true;Assert.That(isTrue); &#x2F;&#x2F; passAssert.That(isTrue, Is.True); &#x2F;&#x2F; passbool isFalse &#x3D; false;Assert.That(isFalse &#x3D;&#x3D; false); &#x2F;&#x2F; passAssert.That(isFalse, Is.False); &#x2F;&#x2F; passAssert.That(isFalse, Is.Not.True); &#x2F;&#x2F; pass Asserting within Ranges 1234567891011121314int i &#x3D; 42;Assert.That(i, Is.GreaterThan(42)); &#x2F;&#x2F; failAssert.That(i, Is.GreaterThanOrEqualTo(42)); &#x2F;&#x2F; passAssert.That(i, Is.LessThan(42)); &#x2F;&#x2F; failAssert.That(i, Is.GreaterThanOrEqualTo(42)); &#x2F;&#x2F; passAssert.That(i, Is.InRange(40, 50)); &#x2F;&#x2F; passDateTiem d1 &#x3D; new DateTime(2021, 2, 20);DateTiem d2 &#x3D; new DateTime(2021, 2, 25);Assert.That(d1, Is.EqualTo(d2)); &#x2F;&#x2F; failAssert.That(d1, Is.EqualTo(d2).Within(4).Days); &#x2F;&#x2F; failAssert.That(d1, Is.EqualTo(d2).Within(5).Days); &#x2F;&#x2F; pass Asserting on Objects 123456789101112131415161718192021222324252627class Product &#123; int ProductId &#123;get; set;&#125; string ProductName &#123;get; set;&#125; Product(int ProductId, string ProductName) &#123; this.ProductId &#x3D; ProductId; this.ProductName &#x3D; ProductName; &#125;&#125;var products &#x3D; new List&lt;Product&gt; &#123; new Product(1, &quot;a&quot;), new Product(2, &quot;b&quot;),&#125;;Assert.That(products, Has.Exactly(2).Items); &#x2F;&#x2F; passAssert.That(products, Is,Unique); &#x2F;&#x2F; passAssert.That(products, Has.Exactly(1) .Property(&quot;ProductName&quot;).EqualTo(&quot;a&quot;) .And .Property(&quot;ProductId).EqualTo(1));Assert.That(products, Has.Exactly(1) .Matches&lt;Product&gt;( item &#x3D;&gt; item.ProductName &#x3D;&#x3D; &quot;a&quot; &amp;&amp; item.ProductId &#x3D;&#x3D; 1 )); Controlling Test Execution Use [Ignore] to skip tests. [Ignore] could also be put before class to skip the entire test class 12345[Test][Ignore(&quot;Custom reason why we need to skip this test&quot;)]public void TestWillNotRun() &#123;&#125; Use [Category] to add test cases to categories, we can only run tests for certain category. In Test Explorer, we can group tests by Traits, which is just another name for Category One Test Case can belongs to multiple [Category]. [Category] can be applied to Class 12345[Test][Category(&quot;Category 1&quot;)]public void TestInCategoryOne() &#123;&#125; [SetUp] code will be executed before each test. So it is a good place to define variables and objects. 1234567891011121314public class TestClass &#123; private List&lt;Product&gt; products; private string test; [SetUp] public void Setup() &#123; products &#x3D; new List&lt;Products&gt; &#123; new Product(1, &quot;a&quot;), new Product(2, &quot;b&quot;), &#125;; test &#x3D; &quot;test&quot;; &#125;&#125; [TearDown] code will be executed after each test, it is the place to dispose all unnecessary objects 12345678public class TestClass &#123; [TearDown] public void Setup() &#123; if (products !&#x3D; null) &#123; ((IDisposable)products).Dispose(); &#125; &#125;&#125; [OneTimeSetUp] code will be executed once before the first test case. Define objects that will not be modified by test cases here. [OneTimeTearDown] code will be executed once after the last test case. Dispose any objects here. Data Driven Tests and Reducing Code Duplication [TestCase]: If we want to run the same test but with different data, we could pass different variables into the test function. 12345678910111213[Test][TestCase(200_000, 6.5, 30, 1264.14)][TestCase(200_000, 10, 30, 1755.14)][TestCase(500_000, 10, 30, 4387.86)]public void CalculateCorrectMonthlyRepayment(decimal principal, decimal interestRate, int termInYears, decimal expectedMonthlyPayment)&#123; var sut &#x3D; new LoanRepaymentCalculator(); var monthlyPayment &#x3D; sut.CalculateMonthlyRepayment( new LoanAmount(&quot;USD&quot;, principal), interestRate, new LoanTerm(termInYears)); Assert.That(monthlyPayment, Is.EqualTo(expectedMonthlyPayment));&#125; 12345678910[Test][TestCase(200_000, 6.5, 30, ExpectedResult &#x3D; 1264.14)][TestCase(200_000, 10, 30, ExpectedResult &#x3D; 1755.14)][TestCase(500_000, 10, 30, ExpectedResult &#x3D; 4387.86)]public decimal CalculateCorrectMonthlyRepayment_SimplifiedTestCase(decimal principal, decimal interestRate, int termInYears)&#123; var sut &#x3D; new LoanRepaymentCalculator(); return sut.CalculateMonthlyRepayment(new LoanAmount(&quot;USD&quot;, principal), interestRate, new LoanTerm(termInYears));&#125; Create Test Case from Centralized Data Class [TestCaseSource(typeof(Class_Name), &quot;Function_Name&quot;)] 1234567891011[Test][TestCaseSource(typeof(MonthlyRepaymentTestData), &quot;TestCases&quot;)]public void CalculateCorrectMonthlyRepayment_Centralized(decimal principal, decimal interestRate, int termInYears, decimal expectedMonthlyPayment)&#123; var sut &#x3D; new LoanRepaymentCalculator(); var monthlyPayment &#x3D; sut.CalculateMonthlyRepayment( new LoanAmount(&quot;USD&quot;, principal), interestRate, new LoanTerm(termInYears)); Assert.That(monthlyPayment, Is.EqualTo(expectedMonthlyPayment));&#125; Create Test Case with Data from File 1234567891011[Test][TestCaseSource(typeof(MonthlyRepaymentCsvData), &quot;GetTestCases&quot;, new object[] &#123; &quot;Data.csv&quot; &#125;)]public void CalculateCorrectMonthlyRepayment_Csv(decimal principal, decimal interestRate, int termInYears, decimal expectedMonthlyPayment)&#123; var sut &#x3D; new LoanRepaymentCalculator(); var monthlyPayment &#x3D; sut.CalculateMonthlyRepayment( new LoanAmount(&quot;USD&quot;, principal), interestRate, new LoanTerm(termInYears)); Assert.That(monthlyPayment, Is.EqualTo(expectedMonthlyPayment));&#125; Create Test Cases with Values, Sequential and Range Without [Sequential], it will create 3 * 3 * 3 = 27 test cases With [Sequential], it will only create 3 test cases 1234567891011[Test][Sequential]public void CalculateCorrectMonthlyRepayment_Combinatorial( [Values(100_000, 200_000, 500_000)] decimal principal, [Values(6.5, 10, 20)] decimal interestRate, [Values(10, 20, 30)] int termInYears)&#123; var sut &#x3D; new LoanRepaymentCalculator(); var monthlyPayment &#x3D; sut.CalculateMonthlyRepayment(new LoanAmount(&quot;USD&quot;, principal), interestRate, new LoanTerm(termInYears));&#125; Create Custom Category Attribute 123[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple &#x3D; false)]class ProductComparisonAttribute : CategoryAttribute&#123;&#125; Then we can use Custom Attribute like this 12345[Test][ProductComparison]public void CustomAttributeTest() &#123; &#125;","categories":[],"tags":[{"name":"Unit Tests","slug":"Unit-Tests","permalink":"http://hellcy.github.io/tags/Unit-Tests/"},{"name":".NET","slug":"NET","permalink":"http://hellcy.github.io/tags/NET/"}]},{"title":"JavaScript Fundamentals","slug":"JavaScript-Fundamentals","date":"2021-04-02T23:31:11.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2021/04/03/JavaScript-Fundamentals/","link":"","permalink":"http://hellcy.github.io/2021/04/03/JavaScript-Fundamentals/","excerpt":"","text":"Data Types undefined Boolean Number String Symbol null object undefined and null in JavaScript undefined mean a variable has been declared but not yet been assigned a value. The data type of undefined variable is also undefined. (undefined is a data type). 123var x;alert(x); &#x2F;&#x2F; undefinedalert(typeof x); &#x2F;&#x2F; undefined null is an assignment value, it can be assigned to a variable as a representation of no value. The data type of null variable is an object. 123var x &#x3D; null;alert(x); &#x2F;&#x2F; nullalert(typeof x); &#x2F;&#x2F; object undefined and null are equal in value but different in type. 12345typeof undefined; &#x2F;&#x2F; undefinedtypeof null; &#x2F;&#x2F; objectnull &#x3D;&#x3D;&#x3D; undefined; &#x2F;&#x2F; falsenull &#x3D;&#x3D; undefined; &#x2F;&#x2F; true Declare variable var: normal way to declare a variable. let: the variable you declared will only be used within the scope of where you declare it. const: the variable value will never change Scope Local scope Global scope JavaScript has function scope, each function creates a new scope. Scope determines the accessibility of these variables. Variables defined inside a function are not accessible from outside the function Array push: Add anitem to the end of the array pop: Remove an item from the end of the array shift: Remove an item from the beginning of an array unshift: Add an item to the beginning of an array indexOf: find the index of an item in the array splice: Remove items from an index position. 12345678910111213141516let vegetables &#x3D; [&#39;Cabbage&#39;, &#39;Turnip&#39;, &#39;Radish&#39;, &#39;Carrot&#39;]console.log(vegetables)&#x2F;&#x2F; [&quot;Cabbage&quot;, &quot;Turnip&quot;, &quot;Radish&quot;, &quot;Carrot&quot;]let pos &#x3D; 1let n &#x3D; 2let removedItems &#x3D; vegetables.splice(pos, n)&#x2F;&#x2F; this is how to remove items, n defines the number of items to be removed,&#x2F;&#x2F; starting at the index position specified by pos and progressing toward the end of array.console.log(vegetables)&#x2F;&#x2F; [&quot;Cabbage&quot;, &quot;Carrot&quot;] (the original array is changed)console.log(removedItems)&#x2F;&#x2F; [&quot;Turnip&quot;, &quot;Radish&quot;] slice: copy an array Object.freeze const: creates an immutable binding, you cannot re-assign a new value to the binding. But if you delcare a const array or object. You can set new value to the element in the array or object object.freeze: makes an object immutable, so you cannot change its properties. Object Dynamic Properties The property ‘test’ is a dynamic property, it’s value will be evaluated during runtime. So obj will have a property name ‘test’ with value 52. 12345678910const test &#x3D; &#39;answer&#39;;const obj &#x3D; &#123; p1:10, p2:20, [test]: 52&#125;;console.log(obj.test);console.log(obj.answer); Anonymous function functions without name. You can also pass that function to a variable 123var magic &#x3D; function() &#123; return new Date();&#125; Arrow function You can convert anonymous function to arrow function 12345var magic &#x3D; () &#x3D;&gt; &#123; return new Date();&#125;var magic &#x3D; () &#x3D;&gt; new Date(); Arrow function with parameters 1var myConcat &#x3D; (var1, var2) &#x3D;&gt; arr1.concat(arr2); Example: Filter the array with only positive numbers and return the square of all remain elements in a new array Array.filter: The filter() method creates a new array with all elements that pass the test implemented by the provided function. Array.map: The map() method creates a new array populated with the results of calling a provided function on every element in the calling array. 123456789101112131415const array &#x3D; [1,2,-3,4,-5,6,7];const squareList &#x3D; arr &#x3D;&gt; &#123; const squaredList &#x3D; arr.filter(function callback(num) &#123; return Number.isInteger(num) &amp;&amp; num &gt; 0; &#125;).map(function square(num) &#123; return num * num; &#125;); return squaredList;&#125;const squaredIntegers &#x3D; squareList(array);console.log(squaredIntegers); Using Arrow functions 123456789const array &#x3D; [1,2,-3,4,-5,6,7];const squareList &#x3D; arr &#x3D;&gt; &#123; const squaredList &#x3D; arr.filter(num &#x3D;&gt; Number.isInteger(num) &amp;&amp; num &gt; 0).map(num &#x3D;&gt; num * num); return squaredList;&#125;const squaredIntegers &#x3D; squareList(array);console.log(squaredIntegers); Example: Write a function that takes multiple parameters and add them up 123456function sum(x, y, z) &#123; args &#x3D; [x, y, z]; return args.reduce((accumulator, currentValue) &#x3D;&gt; accumulator + currentValue, 0);&#125;console.log(sum(1,2,3)); Regular functions give access to their calling environment while arrow functions give access to their defining environment The value of the ‘this’ keyword inside a regular function depends on HOW the function was CALLED (the OBJECT that made the call) In arrow functions, this keyword doesn’t mean the caller of the arrow function. The value of the ‘this’ keyword inside an arrow function depends on WHERE the function was DEFINED (the scope that defined the function). This makes it great for delayed execution cases like events and listeners. 1234567891011121314const test &#x3D; (function test () &#123; const testerObj &#x3D; &#123; func1: function() &#123; console.log(&#39;func1&#39;, this); &#125;, func2: () &#x3D;&gt; &#123; console.log(&#39;func2&#39;, this); &#125;, &#125;; testerObj.func1(); testerObj.func2(); &#125;)() Using Rest operator to represent multiple parameters Rest Operator: The rest parameter syntax allows a function to accept an indefinite number of arguments as an array, Reduce: The reduce() method executes a reducer function (that you provide) on each element of the array, resulting in single output value. The reducer function takes four arguments: Accumulator Current Value Current Index Source Array Your reducer function’s returned value is assigned to the accumulator, whose value is remembered across each iteration throughout the array, and ultimately becomes the final, single resulting value. 12345function sum(...args) &#123; return args.reduce((accumulator, currentValue) &#x3D;&gt; accumulator + currentValue, 0);&#125;console.log(sum(1,2,3,4)); Spread syntax Spread syntax (…) allows an iterable such as an array expression or string to be expanded in places where zero or more arguments (for function calls) or elements (for array literals) are expected, or an object expression to be expanded in places where zero or more key-value pairs (for object literals) are expected. 123456789101112const arr1 &#x3D; [&#39;JAN&#39;, &#39;FEB&#39;, &#39;MAR&#39;, &#39;APR&#39;, &#39;MAY&#39;];let arr2;(function () &#123; &#x2F;&#x2F; spread arr1 into individual elements and create a new array by surrond it with [] arr2 &#x3D; [...arr1]; arr1[0] &#x3D; &#39;potato&#39;;&#125;)();&#x2F;&#x2F; arr2[0] will still be &#39;JAN&#39;console.log(arr2); Destructuring assignment The destructuring assignment syntax is a JavaScript expression that makes it possible to unpack values from arrays, or properties from objects, into distinct variables. 12345var obj &#x3D; &#123;x:3.6, y:7.4, z:6.5&#125;;const &#123;x : a, y: b, z: c&#125; &#x3D; obj;console.log(&#96;$&#123;a&#125; $&#123;b&#125; $&#123;c&#125;&#96;); Destructuring Assignment: Nested Objects 123456789101112const LOCAL_FORECAST &#x3D; &#123; today: &#123;min : 72, max: 83&#125;, tomorrow : &#123;min : 73.3, max: 84.6&#125;&#125;;function getMaxOfTmw(forecast) &#123; const &#123;tomorrow : &#123;max : maxOfTomorrow &#125;&#125; &#x3D; forecast; return maxOfTomorrow;&#125;console.log(getMaxOfTmw(LOCAL_FORECAST)); Destructuring Assignment: Arrays 12const[x, y, , z] &#x3D; [1,2,3,4,5,6];console.log(x, y, z); Destructuring Assignment: Pass an object 1234567891011121314const stats&#x3D; &#123; max: 56.7, standard_deviation: 4.34, median: 34.54, mode: 23.5, min: -0.4, average: 45.6&#125;function half(&#123;max, min&#125;) &#123; return (max + min) &#x2F; 2.0;&#125;console.log(half(stats)); Object Literal Declarations Using Simple Fields 123const createPerson &#x3D; (name, age, gender) &#x3D;&gt; ( &#123;name, age, gender&#125; )console.log(createPerson(&#39;Yuan Cheng&#39;, 27, &#39;male&#39;)); Functions in Objects 123456789const bicycle &#x3D; &#123; gear : 2, setGear: function(newGear) &#123; this.gear &#x3D; newGear; &#125;&#125;;bicycle.setGear(3);console.log(bicycle.gear); we could remove the ‘function’ keyword 123456789const bicycle &#x3D; &#123; gear : 2, setGear(newGear) &#123; this.gear &#x3D; newGear; &#125;&#125;;bicycle.setGear(3);console.log(bicycle.gear); class syntax 123456var SpaceShuttle &#x3D; function(targetPlanet) &#123; this.targetPlanet &#x3D; targetPlanet;&#125;var zeus &#x3D; new SpaceShuttle(&#39;Jupiter&#39;);console.log(zeus.targetPlanet); Using Class and Constructor 12345678class SpaceShuttle &#123; constructor(targetPlanet) &#123; this.targetPlanet &#x3D; targetPlanet; &#125;&#125;var zeus &#x3D; new SpaceShuttle(&#39;Jupiter&#39;);console.log(zeus.targetPlanet); getters and setters 123456789101112131415class Book &#123; constructor(author) &#123; this._author &#x3D; author; &#125; &#x2F;&#x2F; getter get writer() &#123; return this._author; &#125; &#x2F;&#x2F; setter set writer(updatedAuthor) &#123; this._author &#x3D; updatedAuthor; &#125;&#125; import vs require import functions from other js files 1import &#123;capitalzeString&#125; from &quot;string_function&quot; export The export statement is used when creating JavaScript modules to export live bindings to functions, objects, or primitive values from the module so they can be used by other programs with the import statement. Bindings that are exported can still be modified locally; when imported, although they can only be read by the importing module the value updates whenever it is updated by the exporting module. below code is saved in a js file: string_function 1export const capitalizeString &#x3D; str &#x3D;&gt; str.toUpperCase(); * to import import * will import every functions you export in the other file to a object 1import * as capitalizeStrings from &quot;capitalize_strings&quot;; Named export vs export default Named export: With named exports, one can have multiple named exports per file. Then import the specific exports they want surrounded in braces. The name of imported module has to be the same as the name of the exported module. Export default: One can have only one default export per file. The naming of import is completely independent in default export and we can use any name we like. No curly braces needed when import Promises A promise is an object that might deliver data at a later point in the program. Fetch API will return a promise, to consume that promise, we do a .then call on the result of fetch and supply a callback function. The Fetch API will have a raw response ‘resp’, you need to call the .json method on that response object. The json method is also a asynchronous function. It also returns a promise. So we do another .then call on the result of the json function 123456789const fetchData &#x3D; () &#x3D;&gt; &#123; fetch(&#39;https:&#x2F;&#x2F;api.github.com&#39;).then(resp &#x3D;&gt; &#123; resp.json().then(data &#x3D;&gt; &#123; console.log(data); &#125;); &#125;);&#125;;fetchData(); The above code works, but it is difficult to read. We could use async/await. 123456789const fetchData &#x3D; async () &#x3D;&gt; &#123; const resp &#x3D; await fetch(&#39;https:&#x2F;&#x2F;api.github.com&#39;); const data &#x3D; await resp.json(); console.log(data);&#125;;fetchData(); the async function is another way for us to consume promises without having us to use .then calls","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://hellcy.github.io/tags/JavaScript/"}]},{"title":"AWS SAA Architecting For Performance Efficiency","slug":"AWS-SAA-Architecting-for-Performance-Efficiency","date":"2021-03-16T04:36:37.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2021/03/16/AWS-SAA-Architecting-for-Performance-Efficiency/","link":"","permalink":"http://hellcy.github.io/2021/03/16/AWS-SAA-Architecting-for-Performance-Efficiency/","excerpt":"","text":"Part One: Understanding the Design Principles Part Two: Considering Compute Performance Options Part Three: Reviewing Storage Performance Options Part Four: Examing Database Performance Options Part Five: Evaluating Network Performance Options Part Six: Preparing to Improve Your Architecture Part Seven: Monitoring Your Architecture Part Eight: Understanding the Trade-offs Part One: Understanding the Design Principles There are three main differences compare traditional on premises application and cloud application. 1. Cost. 2. Security and 3. Performance. We are going to focus on number three. Performance. Cloud services are changing fast. Go global: AWS have many regions, deploy application to the region that close to the user to reduce the latancy. Go global: local region will comply to the laws and regulations Go global Think serverless Use new technologies Experiment often Right tool for the task Part Two: Considering Compute Performance Options What does compute performance includes? Processing -&gt; CPU Capacity -&gt; Storage Scaling Responsive Economical Understand your workload Undetstand AWS compute Need to gather and analyze data, and testing AWS compute options EC2 Elastic Cloud Compute The default option, virtualized servers. IssA (infrustructure as a service) Choose resources. You own the OS.(You are responsible for patching the OS and config all aspects of it.) EC2 General resources vCPUs Memory Storage Network EC2 extra features Burstable: if your EC2 is not using its full compute power, you gain credits which you can use in the future when you need it to burst the compute power of your instance for a short period of time. GPU FPGA (Filled Programmable Gate Arrays): Allows you to create customized hardware accelerators Instance Types General Purpose: Standard, balanced Compute optimized: high compute power Memory optimized: for memory intensive workloads Accelerated computing: GPU or FPGA Storage optimized: high storage Bare metal EC2 Auto Scaling Metrics based: scale up or down based on the metric you choose Schedule based: scale up or down for a booked time Health based: replace unhealthy instances ECS Elastic Container Service Similar workloads as EC2 Migrate apps to the cloud, long running apps, batch processing, Microservices Better utilize resources. Can run multiple containers on a single instance. ELB, balance traffic to each container, Autoscaling. AWS Fargate: manages the instances on which your containers run. you don’t need to manage the server instances. AWS Lambda FaaS (Function as a service). Serverless computing. Backend processing, Event processing, Stream processing, Data processing AWS resource triggers: other resources can trigger Lambda functions You can choose memory needed for a lambda function Advantages: Simply execute code. We don’t need to worry about the servers that run our Lambda codes. Automatic scaling for Lambda function. Fault tolerant: if a function fails, AWS will trigger the function again Pay for usage Applying our knowledge 1. A company called Globomantics wants to move their application to cloud. They have customers globally. The first application they want to move to cloud is an app that collections data from clinical trails. Doctors enter information each time they do checkings. Considerations for Choice First app of many Time should be fast Predicatable usage They want to use ECS. Lift and shift: Easy to containerize the app. Able to scale. Able to choose instance sizes. Allows them to leverage for future applications. Different containers in a single instance. Save costs 2. They also want to build a new web application for the cloud. Allow people to register medical devices. Share medical devices globally. Considerations for Choice Manage costs Global reach Minimal maintenance They want to go with Lambda. Services behind a static site to save costs. Only pay for runtime. Lambda scales besed on demand. Can be deployed to multiple regions. No servers to maintain. Part Three: Reviewing Storage Performance Options S3 data is encrypted. Access Managemenet(IAM), Lifecycle management. Query in place.(Don’t need to move the data to query it using SQL like command) Shared Access Low latency High thoughput: move data in or out S3 quickly High Availability: available for multiple availability zones High durability: data is duplicated across multiple availability zones Standard Intelligent Tier Standard Infrequent Access(IA)(high latency) One-Zone Infrequent Access(low availability, low durability) Glacier Immutable, data do not change once they are in Glacier. Durable Query stored data without retrieval. Archival storage Encrypted Access Control Audit logging Latency options. Expedited. latency in minutes. Standard. Default, number of hours to get data back. Bulk. cheaper and takes longer. Economical, put data in Glacier is cheap and high durable. Deep Archive. The cheapest and longest. normally 6-12 hours. Do not access data frequently, 2-3 times a year. EBS (Elastic Block Storage) Attached to EC2 instances. Multi-Attach, storage volumn can be attached to up to 16 instances. Instances must be in same availability zone. Data is Replicated to multiple availbility zones. high availability and durability Access control. Provisioned IOPS SSD Standard Purpose IOPS SSD Cold HDD Thoughput optimized HDD Snapshots: a snapshots of a storage in that time and can be shared to other zones. Elastic volumes. pay for what actually stored. EFS General Purpose MAX I/O Same example as before. Migrating to AWS storage. It is one of the web applications. It requires global access. Data will be collected from clinical trials. And some data will be entered on daliy basis. They need shared access of data. Data must be durable. Data will be stored in a long term basis. S3 and Glacier: high durable. Access from ECS containers. Multiple access. Long term storage. New application. Provide medical devices to people who need it. Global user base. Lower cost. Local access. Non-critical images(could be lost, no big problem). S3: single region. Reduced Redundancy Storage(RRD). Part Four: Examing Database Performance Options Install on EC2 we could choose to install a Database on a EC2 instance. But that means we need to do all the backup, restore ourselves. We are not using the serverless managed services provided by AWS. But in some cases, we have to choose this way. Situations like: 1. Control Environment. we want to control everything. 2. Certified. Maybe the services in AWS are not certified by the customer. 3. Specific tools. Our application needs some tools that have to work with standalone database. RDS DynamoDB Redshift RDS Default choice. Complex queries. Consistent transactions. Multi-AZ Read replicas Encryption Backups and snapshots Instance type Storage type Network setup Backup DynamoDB Flexible structure Flexible structure Less complex queries: You are able to query on particular keys, the partition key and any secondary keys that you define. Can’t join tables. Low latency Transactions Global tables: store data in multiple regions Encryption Evolving schema: supports changes and growth in your application(add/remove columns…) Integration with Lambda Partition key: store data on different nodes of the database Secondary indexes Provisioned capacity: number of reads and writes. Dynamodb will auto scale on-demand capacity: pay for what you use. No auto scaling. Redshift Large scale analytics Setup in minutes Warehouse and data lake Encryption Scale to petabytes Query S3 Economical Node type Dense compute: fast CPUs, large RAM and SSD for fast performance Dense storage Same example as before. They want to have minimum effort to do the migration They want to leverage managed services Improve availability They have decided to use RDS. Using SQL server. Structured data. No servers to manage. High availability New application. DynamoDB. Flexible data structure(NoSQL). Trigger action(Lambda integration). Flexible cost structure(On-demand pricing). Global tables. Part Five: Evaluating Network Performance Options Region and AZ Regions are geographical area. One region may have multiple AZs that are also isolated to each other. While the AZs are isolated geographically, they are connected by AWS that allows data to be transferred between each zones. Local Zones Some users still think the regions provided by AWS have high latency. They can choose to use local zones. They are built in large cities and connected to near by regions with low latency, high throughput connectivity. Local zones don’t have all the services provided by AWS as normal AZs. Why do we choose one region over another? Laws and Regulations: e.g. Some governments required that any data of their citizens remain in their countries User location: put application closer to your end users. Data location Cost CloudFront Global network: CloudFront is outside of AWS regions, that deliver our applications to end users Content delivery: Similar to CDN Static content: static content is cached to the place closer to end users to reduce latency Dynamic content: AWS also supported Dynamic content (intelligent caching) Intelligent: you can setup geo-restrictions to not allow edge content to deliver content to certain geo-locations Programmable: Lambda at edge: create serverless functions at edge locations. Put compute power closer to your end users. Route53 DNS solution for AWS: translate a user-friendly URL to the IP address Private DNS: Route53 supports private DNS, you can setup friendly names for your internal services Traffic flow If you deploy your solution to multiple regions, you can config so that your users only send requests to their regions. Latency routing: Traffic flow will determine the region that will be serving the content with the least latency Geographic routing: route users to the cloest region. Health based routing: not route users to a region that with unhealthy status. Round robin routing: route user to the next region available. Route traffic evenly to all regions. Direct Connect Instead of going through public internet, AWS will create a dedicated line for users to connect from AWS to your data center. It is encrypted and you can config the speed. VPC endpoints Normally, if your VPC wants to connect to other AWS services, it can only go through public internet, but with VPC endpoints, it can connect to other AWS services directly through the private internet AWS network EC2 instance types some EC2 instance types have better internet performance than others. Pay attention to it before you launch the EC2 instance. Choose the type that suitable for your applications. Apply our knowledge They want their application to be deployed in a single region but to multiple AZs. They use AWS ECS(elastic container service) to manage their application. And use AWS Fargate to manage their containers. They also choose to use Multi-AZ RDS for their DB. They also want their Data to be stored in S3. So they want a VPC endpoint for S3 to reduce latency. So when they want to access data in S3. they don’t need to go through public internet. They also have a new application that they want to have a friendly domain name. Reduced latency and managed cost. So they have decided to use Route53. because the application is hosted on S3 buckets. They need to register domain names for each S3 bucket. And configure Route 53 to route traffic to right S3 bucket using their domain names. They also want to have a global portal that has links to each deployed regions. Part Six: Preparing to Improve Your Architecture CI/CD pipeline (Continuous Integration/ Continuous Deployment) We need to have repeatable builds, repeatable infrastructrue and Controlled tests CloudFormation Infrastructure template(JSON or YAML) Automate creation Ensure consistency CloudFormation templates Format version Description Parameters Resources Output Part Seven: Monitoring Your Architecture Monitor Resources Application Operations Respond Ignore Manually Automate Modify CloudWatch Metrics Application Infrastructure AWS or on-premises Actions: Autoscaling. Actions: Messages Actions: Lambda functions can be triggered from CloudWatch Analytics: CloudWatch can store months of histoical data for you to analyse Create a log to delete S3 object Create CloudTrail Trail Create Lambda function Create CloudWatch Rule Create a CloudTrail Trail Create a Lambda function Create CloudWatch Rule Part Eight: Understanding the Trade-offs Time Cost Memory Efficiency Complexity Possible Trade-offs Queuing Partitioning Caching Compression Queuing AWS SQS (simple queuing service) Decouple (producer, consumer) Scale independently (add producer or delete consumer) Acceptable delay Time vs Efficiency Data Paritioning For example: RDBMS doesn’t have partition. so we need to consider what data goes into which instance of database, that increase complexity. Whereas many NoSQL DB already has partition. In DynamoDB, we have partition key for each table. The data that has the same partition key will go into the same node. So when choose a partition key, choose a key that roughly evenly distributed across the data. Complexity / consistency vs Time RDBMS vs NoSQL Distribution Maintenance Caching Cache: heavliy used data will be stored in memory. Read Replics: if you have a read replica of your DB, users can go into read replica if they only read. Reduce the traffic of your primary server. CDN: take data and store data closer to users place. Memory/ consistency vs Time Compression Code assets: reduce the size of source code to reduce the time to load the application Files: same. redurce the size of file will reduce the transfer time Time vs memory","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"React Overview","slug":"React-Overview","date":"2021-03-16T00:22:53.000Z","updated":"2021-05-24T07:27:02.660Z","comments":true,"path":"2021/03/16/React-Overview/","link":"","permalink":"http://hellcy.github.io/2021/03/16/React-Overview/","excerpt":"","text":"Setting up a Development Environment Create folder and package.json file 123mkdir diy-reactcd diy-reactnpm init --y Install Main Dependencies Install Express 1npm i express Install React and React-dom 1npm i react react-dom Install webpack 1npm i webpack webpack-cli Webpack is a module bunlder. A react application usually contains multiple modules and depends on many external modules too. When we ship the application to the browser we need to bundle all necessary files into a single bundle and ship it to the browser. Install Babel 1npm i babel-loader @babel&#x2F;core @babel&#x2F;node @babel&#x2F;preset-env @babel&#x2F;preset-react Babel is the package that compiles JSX into regular React API calls. Install Development Dependencies Install nodemon 1npm i -D nodemon nodemon is a package that lets us automatically restart node when we change things in node. Install ESLint 1npm i -D eslint babel-eslint eslint-plugin-react eslint-plugin-react-hooks ESLint will immediately analyze your code and tell you the problems, you can have consistent styling to your code by using ESLint. Configure ESLint Go to your project directory, create a new file called ‘.eslintrc.js’ and paste the below code. 12345678910111213141516171819202122232425262728293031323334module.exports &#x3D; &#123; parser: &#39;babel-eslint&#39;, env: &#123; browser: true, commonjs: true, es6: true, node: true, jest: true, &#125;, parserOptions: &#123; ecmaVersion: 2020, ecmaFeatures: &#123; impliedStrict: true, jsx: true, &#125;, sourceType: &#39;module&#39;, &#125;, plugins: [&#39;react&#39;, &#39;react-hooks&#39;], extends: [ &#39;eslint:recommended&#39;, &#39;plugin:react&#x2F;recommended&#39;, &#39;plugin:react-hooks&#x2F;recommended&#39;, ], settings: &#123; react: &#123; version: &#39;detect&#39;, &#125;, &#125;, rules: &#123; &#x2F;&#x2F; You can do your customizations here... &#x2F;&#x2F; For example, if you don&#39;t want to use the prop-types package, &#x2F;&#x2F; you can turn off that recommended rule with: &#39;react&#x2F;prop-types&#39;: [&#39;off&#39;] &#125;, &#125;; Configure Jest jest is the package to test React applications 1npm i -D jest babel-jest react-test-renderer Basic React application structure 123456789diy-react&#x2F; dist&#x2F; main.js src&#x2F; index.js components&#x2F; App.js server&#x2F; server.js dist: for distribution, webpack will put production-ready files to here src: for React code files components: for React components server: for server files Configure Webpack and Babel Under project root directory, create a new file called ‘babel.config.js’ and paste the below code 123module.exports &#x3D; &#123; presets: [&#39;@babel&#x2F;preset-env&#39;, &#39;@babel&#x2F;preset-react&#39;],&#125;; Under project root directory, create a new file called ‘webpack.config.js’ and paste the below code This will tell webpack to invoke babel for all files that end with .js. This is to convert JSX code to regular React API calls. 12345678910111213module.exports &#x3D; &#123; module: &#123; rules: [ &#123; test: &#x2F;\\.js$&#x2F;, exclude: &#x2F;node_modules&#x2F;, use: &#123; loader: &#39;babel-loader&#39;, &#125;, &#125;, ], &#125;,&#125;; Create npm scripts for development 1&quot;dev:server&quot;: &quot;nodemon --exec .&#x2F;node_modules&#x2F;.bin&#x2F;babel-node src&#x2F;server&#x2F;server.js --ignore dist&#x2F;&quot;, This script will run nodemon command, which is run babel-node on a server.js file 1&quot;dev:bundler&quot;: &quot;webpack -w --mode&#x3D;development&quot; This script will run the webpack command, in Watch mode(readable code, not minified) and in development node Reference Setting up React Development Environment For Angular, Vue and Ember they put fake JS in HTML. Whereas React put fake HTML in JS (JSX), React is Javascript-centric, makes JS more powerful to handle HTML React is lightweighted, you can slowly migrate your app from other technologies to React React is lean and you only import the package you want to use. React is one way data binding, it required more code, you need to expilictly declare a change handler. But this gives you more control and east to debug The Basics React’s Basic Concepts Components Like functions Input: props, state | Output: UI Reusable and composable Can be used as normal HTML tags &lt;Component /&gt; Can manage a private state Reactive updates When the state of a React component, the input, changes, the user interface it represents, the output, changes as well Virtual views in memory We don’t write HTML when building applications using React. We generate HTML using Javascript Build smaller components such as buttons, forms, then build more complex components using the smaller components. Each component consist of HTML and JS. Tradeoffs Framework vs Library Frameworks offer more opinion and standardization, but React’s library approach allows you to select only the tools that you need and pick the best tools for your use case. Data Binding Other frameworks strive to be concise, using techniques like two‑way binding and abstractions over JavaScript operations. But React is explicit, so code is more readable and scalable at the admitted expense of doing a little more typing on the keyboard. JS Centric React chooses to be JavaScript‑centric instead of template‑centric. React’s JavaScript‑centric approach is easier to understand and debug and requires learning less unique syntax, but at the cost of requiring modern JavaScript knowledge. Separate vs Single File Many frameworks utilize a separate template file. In contrast, each React component is a single autonomous file that you can work with and test in isolation. Standard vs Non-standard The web component standard has been around for years, yet it continues to lack broad adoption. Non‑standard approaches, like React and Angular, remain more popular because they offer the same power, more rapid innovation, and a superior developer experience. Community vs Corporate And React is corporate‑backed, which means its design is influenced by Facebook’s needs. But Facebook continues to accept input from the community and has evolved React into a highly flexible and well‑supported system. Decisions to make Develop environment - create-react-app Classes or Functions - Functions Types - PropTypes, TypeScript, Flow TypeScript is a superset of JavaScript that adds strong typing support and compiles down to plain JavaScript Flow: adding static type checking to JavaScrWipt States - Plain React, Flux, Redux, MobX Component State: Plain React Centralized State: Flux, Redux Observable State: MobX Styling - Plain CSS/Sass/Less, CSS in JS W","categories":[],"tags":[{"name":"React","slug":"React","permalink":"http://hellcy.github.io/tags/React/"}]},{"title":"ASP.NET Core with SignalR","slug":"ASP-NET-Core-with-SignalR","date":"2021-03-11T10:03:57.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2021/03/11/ASP-NET-Core-with-SignalR/","link":"","permalink":"http://hellcy.github.io/2021/03/11/ASP-NET-Core-with-SignalR/","excerpt":"","text":"Understanding the Real-time Web Polling Clients periodically ask the server if there’s an update. For each poll, a HTTP request is made, and the server either responds with a new status or 204 No Content Long Polling Clients send HTTP request to the server. But the server will not complete the request but leave it until there’s an update. When there’s no update within a certain timeframe, the request will time out. When that happens, the client will just start the process again by issuing a new request. Long Polling is more efficient than polling, but it is still using HTTP requests to ask for updates. Server Sent Events (SSE) a HTML5 feature, the server creats an HTTP connection to the client(browser) with Server Sent Events. The browser will listen for messages that will come in as a stream. The connection will remain open until it is actively closed. The browser will use an object called EventSource that has an event onmessage to process incoming messages. Using simple HTTP Auto reconnects No support for older browsers Easily polyfilled Maximum HTTP connections issue (6 connections at most) Only support text messages One-way connection Web Sockets A standardized way to use one TCP socket through which messages can be sent from server to client and vice versa and without the latency of HTTP. A TCP socket typically remains open for as long as the stream of the messages are not done. SignalR will use WebSockets most of the time because its the most efficient transport. Full duplex messaging (client to server and vice versa) No 6 connections limit For most browsers, the connection limit for web sockets is about 50 connections Multi data type support (text, binary) TCP socket upgrade (Regular HTTP request uses a TCP socket as well) The WebSockets standards uses a handshake mechanism to upgrade an existing socket used for HTTP traffic to a WebSocket. After that, messages can travel through the socket until the socket is actively closed. When closing, a reason for closing is communicated. Every WebScoket starts its life as a simple HTTP socket, A GET HTTP call is made to the server, requesting an upgrade of the socket. If the server agrees, the socket becomes a WebSocket from that point onwards. SignalR SignalR is an open source framework that wraps the complexity of real-time web transports. You don’t need to worry about the lower level transports like long polling, Server Sent Event or WebSockets. Transports WebSockets, Server Sent Events, Long Polling Requires client and server that supports transport Fallback mechanism (if browser doesn’t support WebSocket, then use SSE instead, etc…) Remote Procedure Call (RPC) Server can call a function in the Client and vice versa Hub A hub is a server-side class that sends messages to and receives messages from clients by utilizing RPC. A hub protocol is a format used to serialize parameters to and deserialize parameters from Differences with Classic SignalR Simplified connection model Single hub per connection Async Binary and custom protocols No jQuery dependency for JavaScript client Sticky session required Scaling Out Running on muiltiple servers Load Balancer picks server Problem with non-WebSockets transport, it could send the first request to the first server, then send the second request to the second server, who doesn’t know anything about the context of the message. We could solve this problem by using sticky sessions. As part of the response of the first request, the load balancer sets a cookie in the browser, indicating the server that was used. On subsequent request, the load balancer then reads the cookie and assigns the request to the same server. (IIS using Application Request Routing Affinity (ARR Affinity)) Another problem, let’s say a user is working on a web document using Office 365, and she invites others to join her, the other might end up at another server. When user 1 on the server changes the document, a message has to be sent to the others, but server 1 doesn’t know about users that are connected to hubs in other servers. To solve this, the servers need a way to share data. This can be done using a Database, but a faster alternative would be to use a Redis cache.","categories":[],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"http://hellcy.github.io/tags/ASP-NET-Core/"}]},{"title":"ASP.NET Core Fundamentals","slug":"ASP-NET-Core-Fundamentals","date":"2021-03-07T10:41:24.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2021/03/07/ASP-NET-Core-Fundamentals/","link":"","permalink":"http://hellcy.github.io/2021/03/07/ASP-NET-Core-Fundamentals/","excerpt":"","text":"","categories":[],"tags":[{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"http://hellcy.github.io/tags/ASP-NET-Core/"}]},{"title":"C# Fundamentals","slug":"C-Fundamentals","date":"2021-03-04T12:26:26.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2021/03/04/C-Fundamentals/","link":"","permalink":"http://hellcy.github.io/2021/03/04/C-Fundamentals/","excerpt":"","text":"dotnet CLI Create new dotnet project 12dotnet new consoledotnet new xunit Add unget package 1dotnet add package xunit --version 2.4.1 Add reference to another project in .csproj file 1dotnet add reference projectName.csproj Run unit test project 1dotnet test Run main project 1dotnet run Create solution file to include all projects 1dotnet new sln Add project into the solution file 1dotnet sln add PathToProjectFile.csproj Build all project in solution 1dotnet build C# passes variables by value unless you use keyword e.g. ref, out C# has garbage collector Auto property 12345678public string Name &#123; get &#123; return name; &#125; set &#123; name &#x3D; value &#125;&#125; This code can be simplified to 1public string Name &#123; get; set; &#125; difference between auto property and field 123public string Name &#123;get; set;&#125;public string Name; You can specify custom code in getter and setter functions. When serialize objects, fields may not be counted. readonly 1readonly public string category; readonly can only be assigned in constructor. const 1const public string CATEGORY const is more strict than readonly, you can’t assign it in constructor, it is not a variable. Once you initialized it, you can’t change its value. And when you access it. You can access it from the class name, not the object name. Because every object instance will have the same value for this const. So its better to access it from the class. Its more clear. 1234567&#x2F;&#x2F; Book is a class&#x2F;&#x2F; CATEGORY is a const in Book&#x2F;&#x2F; Category is a readonly field in BookConsole.WriteLine(Book.CATEGORY); &#x2F;&#x2F; constvar book1 &#x3D; new Book();Console.WriteLine(book1.Category); &#x2F;&#x2F; readonly field Delegate A delegate is a type that represents references to methods with a particular parameter list and return type. When you instantiate a delegate, you can associate its instance with any method with a compatible signature and return type. You can invoke (or call) the method through the delegate instance. 1234567891011121314151617public delegate string WriteLogDelegate (string logMessage);&#x2F;&#x2F; reference the main project to the test project in .csprojpublic class TypeTests&#123; [Fact] public void WriteLogDelegateCanPointToMethod() &#123; WriteLogDelegate log &#x3D; new WriteLogDelegate(ReturnMessage); var result &#x3D; log(&quot;Hello!&quot;); Assert.Equal(&quot;Hello!&quot;, result); &#125; private string ReturnMessage(string message) &#123; return message; &#125;&#125; Multi-Cast Delegate One delegate variable can points to multiple methods, and by invoking that delegate variable, all subscribed methods will be invoked too 1234567891011121314151617181920212223242526public delegate string WriteLogDelegate (string logMessage);&#x2F;&#x2F; reference the main project to the test project in .csprojpublic class TypeTests&#123; int count &#x3D; 0; [Fact] public void WriteLogDelegateCanPointToMethod() &#123; WriteLogDelegate log &#x3D; new WriteLogDelegate(ReturnMessage); log +&#x3D; IncrementCounter; var result &#x3D; log(&quot;Hello!&quot;); Assert.Equal(2, count); &#125; private string ReturnMessage(string message) &#123; count++; return message; &#125; private string IncrementCounter(string message) &#123; count++; return message.Tolower(); &#125;&#125; Event Sometimes after we have done someting, we want to broadcast it to all people that are interested. The broadcast delegate usually takes two parameters, the object sender and EventArgs. object is the base type in C#, all types and classes can be fitted into object. 123456789101112131415161718192021public delegate void GradeAddedDelegate(object sender, EventArgs eventArgs);public class Book &#123; public event GradeAddedDelegate GradeAdded; public void AddGrade(double grade) &#123; if (grade &lt;&#x3D; 100 &amp;&amp; grade &gt;&#x3D; 0) &#123; grades.Add(grade); &#x2F;&#x2F; broadcast &#x2F;&#x2F; if GradeAdded is null, then no one is listening to this event &#x2F;&#x2F; so there is no need to invoke the delegate. if (GradeAdded !&#x3D; null) &#123; GradeAdded(this, new EventArgs()) &#125; &#125; else &#123; throw new ArgumentException($&quot;Invalid &#123;nameof(grade)&#125;&quot;); &#125; &#125;&#125; In program.cs where we are trying to use this Book object 1234567891011121314class Program&#123; static void Main(string[] args) &#123; var book &#x3D; new Book(&quot;Yuan&#39;s Grade Book&quot;); &#x2F;&#x2F; add OnGradeAdded to this delegate book.GradeAdded +&#x3D; OnGradeAdded; &#125; static void OnGradeAdded(object sender, EventArgs e) &#123; Console.WriteLine($&quot;A grade was added.&quot;); &#125;&#125; OOP Inheritance 1234567891011121314151617181920212223242526public class NamedObject &#123; &#x2F;&#x2F; Name field public string Name &#123;get; set;&#125; &#x2F;&#x2F; constructor public NamedObject(string name) &#123; Name &#x3D; name; &#125;&#125;&#x2F;&#x2F; inherit from NamedObject base classpublic class BookBase : NamedObject &#123; public BookBase(string name) : base(name) &#123; Name &#x3D; name; &#125;&#125;public Program clas &#123; static void Main(string[] args) &#123; &#x2F;&#x2F; when you create a new instance of Book &#x2F;&#x2F; you assign the new name Book 1 to Book class &#x2F;&#x2F; Book class will pass the same name to its base class Book book1 &#x3D; new Book(&quot;Book 1&quot;); &#125;&#125; When you want to use some attributes or functions from a base class, instead of rewrite the code, you can inherit the base class. The constructor of the child class will also need to pass in the variable to its base class Polymophism 12345public abstract class BookBase : NamedObject &#123; public BookBase (string name) : base(name) &#123;&#125; public abstract void AddGrade(double grade);&#125; The abstract modifier indicates that the thing being modified has a missing or incomplete implementation. And it will be implemented by its child class. Child class who inherit the base class will need to add an override keyword to implement the abstract function Base class can have different implementations of it. A Car base class may have Truck and Sport Car as its children class and they have some same base field and functions but also many different. Interface Where abstract may content actual implementation of some field and functions (some is missing). The Interface class doesn’t have any implementations. 123456789namespace GradeBook&#123; public interface IBook &#123; void AddGrade(double grade); string Name &#123;get;&#125; event GradeAddedDelegate GradeAdded; &#125;&#125; Virtual For base class, there may be some functions that already have an implementation. But Virtual keyword meaning its children class may choose to override it. Difference between Virtual and Abstract Virtual methods have an implementation and provide the derived classes with the option of overriding it. Abstract methods do not provide an implementation and force the derived classes to override the method. So, abstract methods have no actual code in them, and subclasses HAVE TO override the method. Virtual methods can have code, which is usually a default implementation of something, and any subclasses CAN override the method using the override modifier and provide a custom implementation. using When we are writing text to file, we need to open the file and close it after finish. But if the program crashes when we editing the file, we will leave the file open. One thing we can do is to use the try catch block to catch the exception and close the file finally. C# has a short keyword for this purpose, the using keyword. It will call Dispose() function in IDisposable -&gt; TextWriter -&gt; StreamWriter to free up the memory and close the file. 1234using (var writer &#x3D; File.AppendText($&quot;&#123;Name&#125;.txt&quot;))&#123; writer.WriteLine(grade);&#125;","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"http://hellcy.github.io/tags/C/"}]},{"title":"AWS SAA - Design Cost-Optimized Architectures","slug":"AWS-SAA-Design-Cost-Optimized-Architectures","date":"2021-02-27T23:10:25.000Z","updated":"2021-03-05T14:00:55.637Z","comments":true,"path":"2021/02/28/AWS-SAA-Design-Cost-Optimized-Architectures/","link":"","permalink":"http://hellcy.github.io/2021/02/28/AWS-SAA-Design-Cost-Optimized-Architectures/","excerpt":"","text":"Part One: Understanding Cost Effective Storage in AWS Part Two: Understanding Cost Effective Compute in AWS Part Three: Understanding Database Pricing and Cost Optimization Part Four: Understanding Cost Optimized Network Architectures Part Five: Making Cost-optimized Decisions Part One: Understanding Cost Effective Storage in AWS Globomantics Global health care organization Been using AWS for some time Most core service such as EC2, RDS, S3 etc. We have been asked to identify solutions that will help reduce costs maintain the same level of service and availability Module Overview S3 Using S3 storage classes to reduce costs S3 glacier When to use S3 glacier and S3 glacier heep archieve EBS storage EBS pricing points and storage options S3 Storage Classes Influences availability, durability and cost for objects stored in S3 Applied at an object level, each S3 bucket can host objects with different classes An objects storage class can be changed throughout its lifetime Using the wrong storage class will lead to unnecessary spending Standard: Charged based on object size Standard - IA: Charged based on object size and retrieval One Zone - IA: Stores objects in a single AZ S3 Glacier: Used as an additional S3 storage class Intelligent - Tiering: Transitions objects between classes based on their access frequencies Lifecycle rules Use lifecycle rules to transition objects between classes and expire objects Caching Downloading objects cost money, use caching to avoid unnecessary downloads and reduce S3 costs Globomantics Requirements Use an appropriate storage class for each object Avoid one zone -IA as it reduces availability (store data we can reproduce in OZ - IA) Use lifecycle rules Transition to standard - IA Transition to S3 Glacier Expire objects(delete) S3 Glacier and Deep Archive Globomantics Requirements Need to store some data long term for compliance Data must be stored for at least 10 years Meet the following requirements Stored as cheaply as possible Still be highly durable and available Must be secure Data won’t be needed again except for compliance requests S3 Glacier Long term archival storage Two classes S3 Glacier and Deep Archive Using S3 glacier we can retrieve archives in minutes Using S3 glacier deep archive we can retrieve data within 12 hours Data in S3 glacier are not available to you. You need to request a retrival. Comparing Storage Costs S3 Standard - 10TB - eu-west-1 $245.64 S3 Glacier - 10TB - eu-west-1 $46.08 S3 Glacier Deep Archive - 10TB - eu-west-1 $18.44 EBS Storage Block storage for EC2 virtual machines Persistent storage of up to 16TB per disk SSD backed and HDD backed volumes Provisioned storage priced at a GB per month rate You are charged for the entire volume as soon as you created it. You can create a smaller EBS than increase it when you need it in the future Options Cold HDD volumes, $0.025 per GB per month Throughput optimized HDD volumes, $0.045 per GB per month General purpose SSD volumes, $0.10 per GB per month Provisioned IOPS SSD volumes, $0.125 per GB per month and $0.065 per provisioned IOPS per month EBS Snapshots Snapshots consist of the used space in an EBS volume not the provisioned space Charged on a per GB per month basis Additional cost for EBS fast snapshot restore If you have a 1000 GB provisioned EBS and only used 100 GB space, when you create a snapshot of this EBS, you will only be charged of 100 GB. Summary Use S3 storage classes to reduce costs Use S3 Glacier and its role in reducing costs EBS storage pricing Part Two: Understanding Cost Effective Compute in AWS Module Overview Discuss EC2 payment types Discuss right sizing EC2 to optimize costs Introduce cost benefits of serverless compute Pricing points EC2 instance uptime EBS storage Data transfer out Instance types On Demand instances Charged by the hour or second (minimum 60 seconds) No upfront commitment, billed when instances are in a running state Great when you want uninterrupted compute Reserved Instances 1-year or 3-year commitment pay all, parital or no upront (the more you pay upfront, the bigger discount you will get) Convertible RIs available Capacity reservation with Zonal RIs Instance size flexibility Up to 72% saving Spot Instances You are biding on unused capacity in an AZ If your bid is higher than the spot price you pay the lower amount Spot, Spot fleets (multiple machines, only launch when all of them can be launched at the same time), and spot blocks (multiple machines, only launch them when they can be running for a certain peroid of time) are available When you loose the spot bid 2-minute warning (to transfer your data) instance terminate/hibernate/stop depend on your choice In addition Scheduled reserved instance: Useful if you are only running your instance periodically Savings plans: Alternative to reserved instances, useful if you have mixed EC2 instance, AWS fargate and AWS lambda Globomantics Requirements Use a minture of EC2 instances sizes and types Currently only on-demand instance type used EC2 instance characteristic Some instances run 24/7 and are expected to do so for at least 1 year (reserved instances with 1-year commitment) Some instances are brought online for 48 hours every week to run weeekly batch jobs (scheduled reserved instances) Other instances are brought online as needed to run short processes that must be completed within 2 hours (on-demand or spot block) Note using spot block you might need to wait for some time (when your bid is higher) before your instances can be launched Right Sizing EC2 to Optimize Costs Eight instance familes: Groups of instances such as general purpose, compute optimized and memory optimized Instance sizes: Each family has a range of instance sizes that offer different combinations of resources Burstable and Fixed performance instances Fixed performance (e.g. M5) offers fixed compute Burstable performance (e.g. T3) provide a baseline level of CPU (e.g. 20%) with the ability to burst above the baseline. Standard and unlimited: For burstable instances, if you are not using your CPU, you will get tokens(credit) which you can use later when you need extra CPU power. For Standard, your compute power will reduce to original when you use up your tokens. For unlimited, you will be charged the on-demand price for the compute power but can still use the extra CPU power for as long as you need. Tools for Right EC2 Sizing Amazon CloudWatch: Monitor CPU, network throughput, disk I/O AWS Cost Explorer: Monitor your spending and view resource optimization recommendations AWS Trusted Advisor: Best practice advice including advice on reduing costs AWS Serverless Platform Compute: AWS lambda, AWS fargate Storage: S3 Data Stores: DynamoDB, Aurora API Proxy: Amazon API Gateway Integration: Amazon SNS, Amazon SQS Benefits of Serverless Compute No server management: No need to provision administer or maintain EC2 instance Flexible scaling: Scale automatically without downtime by adjusting capacity High availability: Built for automated high availability and fault tolerance Globomantics Requirements Deployed a 2-tier customer facing web application to AWS Deployed using EC2 and RDS MySQL Interested to know how this application would be deployed using serverless services? Would there be cost benefits? Summary Discussed different ways to pay for EC2 Demonstrated EC2 savings plans Discussed right sizing of EC2 Discussed how serverless compute can help reduce costs Part Three: Understanding Database Pricing and Cost-optimization Module Overview Discuss RDS Pricing Discuss DynamoDB Pricing RDS Pricing Points Instance type and size Database storage Data transfer out between AZs and between regions Backup storage Amazon RDS Instance Types General purpose: Including M4 and M5, good balance between computer memory and network resources Memory optimized: Including R4 and R5, designed for memory-intensive database workloads Burstable performance: Offering a baseline level of CPU with the ability to burst above the baseline RDS Payment Options On-Demand: Pay as you go, no upfront payments Reserved instances: 1-year or 3-year commitment for up to 69% saving RDS Storage General purpose SSD: From 20GB to 64GB prices at a $ per GB per month Provisioned IOPS SSD: Priced on a $ per GB per month plus $ per IOPS per month Magnetic storage: Cheapest storage, not recommended for new deployments Amazon Aurora Faster than MySQL and PostgreSQL Offers additional features like Aurora serverless Cheaper than both MySQL and PostgreSQL DynamoDB Pricing Points On-demend: Charged foe the data reads and writes your application performs Provisioned capacity: You buy the read and write capacity units that you need for your application DynamoDB Capacity Units WCU(Write Capacity Units) Each WCU is equivalent to one 1KB write per second. e.g. If each of your record is 10KB, and you need to write 5 records per second, then you need 50 WCU RCU(Read Capacity Units) Eventual &lt; Strongly &lt; transactional consistency Each RCU is equivalent to one 4KB strongly consistent read per second. Each RCU is equivalent to two 4KB eventual consistent read per second Each RCU is equivalent to 0.5 transactional consistent read per second DynamoDB auto scaling: Dynamically adjusts provisioned throughput in response to traffic patterns Reserved capacity: Purchase RCUs and WCUs with a 1-year or 3-year commitment at a reduced rate Additional DynamoDB Costs Global secondary indexes need their own capacity units Global DynamoDB tables will need additional capacity units DynamoDB backups will increase costs Summary Discussed RDS pricing options Discussed DynamoDB pricing options Part Four: Understanding Cost-optimized Network Architectures Module Overview Discuss using ELB and Auto Scale to reduce costs Discuss VPC routing and hybrid connectivity cost decisions Discuss using offloading to reduce costs ELB and Auto Scaling Globomantics Requirements Deployed a 3-tier customer facing web application to AWS Deployed using EC2 and RDS MySQL Peak time for the application is Friday and Saturday where up to two times the amount of compute is needed Right size the EC2 instances and RDS instances Introduce EC2 auto scale for the web and app tier Introduce load balancing for the app tier Auto Scaling Saves Money With auto scaling we design for the normal Auto scaling leads to better cost management Integrate with load balancing to make use of launched instance Using min and max values allow us to better predict costs VPC Routing and Hybrid Connectivity Decisions Globalmantics Requirements Connect Globalmantics HQ and smaller regional offices to their AWS deployed VPC Connect resources in their AWS deployed VPC to S3 Connect Globalmantics application VPC to a VPC that contains monitoring servers Cost is a major factor, all designs should balance performance, funcationality and cost For connecting HQ to AWS VPC, we can use Direct connect or site-to-site VPN, Direct connect will give us better performance but more expensive. For connecting Branch offices to AWS VPC, site-to-site VPN should be good enough considering offices are small. For connecting Application VPC to Monitoring VPC, we could use VPC peering, it will only charge us for data transfer. The others options are: Transit Gateway and site-to-site VPNs (more expensive). For connecting Application VPC to S3, we could use VPC endpoints. Other option is to use public gateway but that is less secure(need to go through public internet). AWS Connectivity Keep as much traffic as possible on the AWS backbone Consider using Direct Connect hosted connections Use AWS services to reduce development and management costs Balance performance, functionality and cost Offloading with CloudFront How can deploying a additional technology like CloudFront reduce costs? S3 charges a retrieval fee per GB and fees based on the type of request CloudFront charges a retrieval fee and a fee for HTTP or HTTPS requests CloudFront fees are cheaper than S3 fees Serving content from CloudFront can be cheaper then serving content from S3 Summary Learned how ELB and auto scale can help reduce costs Discussed VPC routing and hybrid connectivity options Learned how offloading can help reduce costs Part Five: Making Cost-optimized Decisions Overview Discuss factors that can affect costs Work with AWS tools to monitor and estimate costs Some factors that can affect cost AWS region and zone: Resource are priced per-region and per-availability zone EC2 size and type: Instance type and size will have a big impact on the cost of your compute S3 storage class: Choose the correct class for the objects you are storing Tips to Help Save Money in AWS EC2 Payment: Use the correct payment model Databases: Use reservation for RDS and DynamoDB Tag Everything: Introduce and effective tagging policy Intriduce SCPs(Service Control Policy): Use SCPs to restrict available features Monitor everything: Use all the monitoring tools available to you AutoScale: Implement AutoScale to avoid planning for peak Offloading: Use offloading in your architectures (CloudFront, ElasticCache, RDS Read Replicas) Turn things off: Shutdown and delete resources that you are not using Course Summary Storage and Compute S3 Storage classes S3 lifecycle rules EBS storage options EC2 pricing EC2 right sizing Serverless compute Databases and networks RDS pricing and optimization DynamoDB pricing and optimization Cost optimized networks ELB and autoscale Hybrid connectivity VPC connectivity Offloading with CloudFront","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"Java Learning Path","slug":"Java-Learning-Path","date":"2021-02-21T01:50:46.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2021/02/21/Java-Learning-Path/","link":"","permalink":"http://hellcy.github.io/2021/02/21/Java-Learning-Path/","excerpt":"","text":"入坑Java开发的学习之路 基础知识 编程语言： Java Python C 基本算法 基本网络知识： TCP/IP HTTP HTTPS 基本设计模式 工具方面 操作系统： Linux (CentOS/Ubuntu…) 代码管理： SVN / Git 持续集成(CI/CD): Jenkins Java项目管理工具： Maven / Gradle 框架方面 应用层框架 ssh: spring + structs + hibernate ssm: spring + spring mvc + mybatis spring boot 中间件 MQ 消息队列 RPC 通信框架 gRPC thrift dubbo spring cloud Elasticsearch 数据库 搜索引擎 数据库 SQL: MySQL / Postgre SQL NoSQL: Redis Memcached mongoDB elasticsearch 架构方面 分布式/微服务架构 spring cloud dubbo RPC通信 虚拟化/容器化 Docker k8s kubernetes 关注源码/性能 JDK源码以及部分设计思想 Spring源码 JVM 细节与排错","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"}]},{"title":"AWS SAA - Design Secure Applications and Architectures","slug":"AWS-SAA-Design-Secure-Applications-and-Architectures","date":"2021-02-15T11:06:03.000Z","updated":"2021-03-05T13:13:34.682Z","comments":true,"path":"2021/02/15/AWS-SAA-Design-Secure-Applications-and-Architectures/","link":"","permalink":"http://hellcy.github.io/2021/02/15/AWS-SAA-Design-Secure-Applications-and-Architectures/","excerpt":"","text":"Part One: Protecting AWS Credentials Part Two: Capturing and Analyzing Logs Part Three: Protecting Network and Host-level Boundaries Part Four: Protecting Data at Rest Part Five: Protecting Data in Transit Part Six: Configuring Data Backup, Replication, and Recovery Part One: Protecting AWS Credentials Security is about protecting data. The CIA Triad Confidentiality Integrity Availability Confidentiality: Only authorized parties can access data. (ACLs and encryption) Integrity: Data has not been improperly modified. Includes knowing if data has been modified. Availability: Authorized parties have access to data when they need it. Includes protecting systems that store, process, and deliver data. Defense in depth: Protecting the confidentiality, integrity, and availability of data by securing everything that touches the data, including storage, compute and networking Levels of Architecture: AWS services, Operating systems, Applications AWS Credentials Root User: Full access to all AWS resources. Only one root user per account. IAM principal: Any entity(could be a user or an application) that can perform actions on AWS services and resources. Policies determine what permissions a principal has Locking down the Root user: Enable MFA. Don’t use the root user for administrative tasks. Use a non-root IAM user with administrative permissions IAM Principal: The foundation of IAM. An entity that can take an action on an AWS service. Often used as a synonym for identity. Principles include users and roles A non-root principal has no permissions by default. Policies determine what permission a principal has You must grant permissions to a principal by associating it with a policy. Policy and Permission: A policy consists of multiple permission statements. A permission statement consists of 4 elements. Effect (allow or deny) Service (etc: EC2) Action/Operation (RunInstances) Resource (image/ami-fjdfjfsdk) Request condition(MFA, IP range, time…) (198.51.100.0/24) This permission will allow a principal to run an EC2 instance with certain AMI, when it is in certain IP range. AWS managed policies: AWS has many managed policies created for us to use. (they are updated regularly to include new services) The deny effect always takes precedence over the allow effect (deny &gt; allow) We can create inline policy for a user to deny he’s access to terminate any EC2 instances. This is the JSON representation of the policy We could use policy simulator to check the effectiveness of the policy. We could also create inline policy for a group. Summary Implement MFA for the root user User an administrative user instead of root user AWS managed policies are updated as new services and actions are added A policy permission consists of an effect, service, action/operation and resource A user policy is an inline policy embedded in a user A group policy is embedded in a group Customer Managed policies work like AWS managed policies, but are created and managed by you Part Two: Capturing and Analyzing Logs Module Overview Capturing events with CloudTrail Viewing Logs with CloudWatch Logs Creating alerts with CloudWatch Alarms Searching logs with Athena Tracking changes with AWS config CloudTrail logs are stored in S3. Limit what you log to control costs CloudTrail event types Management: Configuration changes to AWS services. Reading resources. Logging into the management console. Assuming a role. Data: Access to S3 objects. Lambda function execution CloudTrail: Logs AWS actions. Stores logs in S3 CloudWatch Logs: Aggregates logs from CloudTrail and non-AWS sources. Provides interface to view and search logs Create IAM service role Contains inline service policy that grants CloudTrail permissions to send logs to CloudWatch Logs Contains trust policy that allows CloudTrail to assume the role Role is an IAM principal for CloudTrail to use to authenticate to CloudWatch Logs Demo Create a log group in CloudWatch Logs Create an IAM role for CloudTrail to assume. The Role will have two permission statements. It can create log streams and it can put the log into CloudWatch Log groups. The CloudTrail and the Role have a trusted relationship. If we have a look at the JSON policy. It will allow CloudTrail to assume the Role. The Role will give its permissions to CloudTrail. Create CloudWatch Alarm we need to select a metric, in this case, IncomingLogEvents. We define the alarm so that it triggers the alarm if CloudTrail send more than 1 log to CloudWatch Logs within 1 minute period of time. We also created new topic with an email address so I will get notified if this happens. After the alarm has been created, the status is OK because we didn’t receive any logs in the last 1 minutes. And we treat missing logs as Good. When there is logs coming in, the status will be changed to In alarm for that 1 minute period and will be changed back to OK the next minute. Notice the alarm logs are not in real time. There maybe a couple of minutes delay. Why Athena? Maybe you don’t want to use CloudWatch Logs. You can use SQL like queries to search thought logs and all S3 objects. If the files are in correct format (e.g. csv, JSON, CloudTrail logs stored in S3 are in JSON format) Athena usrs SQL, so to search files in s3, we need to provide the schema. AWS provides the schema for CloudTrail Logs Demo Note you need to create a save location for the Athena search result in S3 before run the query. Tracking Configuration Changes in AWS Config Tracks configuration changes over time. AWS config can tell you the stats of all AWS services of any point of time in the past. Records changes in S3. Notifies of changes Summary CouldTrail tracks events ClousdWatch Logs aggregates logs from different sources CloudWatch Alarms trigger based on specific log activity Athena performs SQL queries against objects in S3 AWS Config tracks configuration states over time Part Three: Protecting Network and Host-level Boundaries Tic-tac-toe web application Host-level boundary is the default security group between the public subnet and the EC2 instance(auto created). NACL(netwoek access control list): controls traffic in and out the subnet having both the security group and NACL gives you layers of security around your instance. The game data stored in DynamoDB, to communicate with it, we have two options We can go over the public internet via igw(internet gateway) We can also use a VPC endpoint, which is a non-internet private connection Module Overview Creating a public subnet Creating and using an IAM instance profile Using SSH key pairs Using VPC endpoints Network access control lists Demo Create VPC Create subnet Create igw Attach igw to VPC Add routes in VPC’s main route table Add VPC’s inbound rules Create IAM role to allow access to DynamoDB Role will contain a trust policy to allow EC2 instances to assume the role Launch instance and attach instance profile Create VPC Create public subnet, we use the CIDR block the same as our VPC because we are only going to create one subnet for this VPC Create internet gateway Attach internet gateway to VPC Edit the main route table, add another record. Route all traffic to igw. By doing this we make our subnet a public subnet Add two inbound rules in our VPC’s default security group. Use my home IP as source. So only at my home can I access the VPC services by HTTP or SSH. Create IAM role to access DynamoDB Create a role and attach the policy to it. Because we choose EC2 as the service that will be using this role, the trusted entity is EC2. It also creates an instance profile for us. Create an EC2 instance 1ssh ec2-user@13.55.117.52 -i .\\hellcyAWSkey.pem Access EC2 instance via SSH You need to go to your private key location (pem) and change the access level to 400(linux) or change the Owner to yourself and remove all other groups and users. Check here 1sudo yum -y install git python2-pip.noarch install the app 1git clone https:&#x2F;&#x2F;github.com&#x2F;benpiper&#x2F;dynamodb-tictactoe-example-app clone the repo from github 1sudo pip install flask boto install dependecies 1sudo python application.py cd to the application folder and run the app. You can play the game with other people who logs in 1curl 169.254.169.254&#x2F;latest&#x2F;meta-data&#x2F;iam&#x2F;security-credentials&#x2F;tic-tac-toe-app this is the secret token generated by AWS STS(Security Token Service) using instance profile(from the role) to allow our instance to access DynamoDB. Using VPC Endpoints Now we are going to change our traffic to go through a private link via VPC endpoint. Demo Block outbound internet access from the instance Configure VPC endpoint 1nslookup dynamodb.ap-southeast-2.amazonaws.com look up the ip address of DynamoDB at our current AWS region 1sudo netstat -tp | grep python check outbound connection from our EC2 instance to DynamoDB. Note our application needs to be running for this to work. Create VPC Endpoint New route rules will be added to the route table. This is to change the traffic sent to the DynamoDB from the private VPC endpint instead of the public igw Note it is required to create a policy for this VPC endpoint. The policy controls what the requests can do to the DynamoDB. Remember we already created a policy for a role and attached that role to our EC2 instance. They are two different policies. One for EC2, One for VPC endpoint. in the VPC’s security group. We need to remove the outbound rule for accessing public internet. Add a new rule to access DynamoDB endpoint. Note that we can access to the EC2 instance because we still have the inbound rules. Security Groups are stateful. They track the state of the connections to and from your instance. If you allow the traffic into your instance, then the security group will automatically allow the instance to reply to that traffic. Note there is a file bootstrap-responsive.css missing in the course app. I used FlieZilla uploaded this file to EC2. Below is the FlieZilla connection settings. Note port is 22. Network Access Control Lists Security Group controls in/out traffic for a EC2 instance. While NACL controls all traffic in the subnet. They are two layers of security. Differences between Security Group and NACL Security Group Instance Level Stateful Unnumbered rules (rules don’t have order) NACL Subnet level Stateless Numbered rules (rules have order) Stateful means security groups will automatically allow reply traffic. Stateless means if you only have inbound rule to access the instance. But do not have the outbound rule. Then the traffic can not leave the subnet. You have to setup both in/out rules to access an instance in the subnet. Numbered rules means each rule will have a number. Smaller number rules will be applied first(high priority). Summary A public subnet has a default route to an internet gateway Use an IAM instance profile to grant an instance access to an AWS service Decide whether to connect to AWS endpoints via the internet or a VPC endpoint Security groups and network access control lists act as firewalls but differ in significant ways Part Four: Protecting Data at Rest Data at Rest is the data stored in a place(hard drive) Data in Transit is the data being sent in public internet Access Permissions Bucket policies User policies Access Control lists Encryption Requires access to a key to encrypt and decrypt data if the key is gone. so is the data! Module Overview Create a customer master key(CMK) Encrypt an EBS volumn S3 access control lists, bucket policies, and user policies Securely grant anonymous access to S3 object Encrypt S3 object Demo Create a customer master key using KMS Assign a key alias(friendly name) Define key administrators (people who manages the key) Define key users (people who will be using the key) Demo Encrypt the data on an unencrypted EBS volume Stop the web1 instance (tic-tac-toe) Take a snapshot of the root volume Make an encrypted copy of the snapshot Create an AMI(instance image) using the encrypted snapshot Launch another instance using the new AMI Demo S3 Access Permissions Create an S3 bucket Configure bucket access control lists Create a bucket policy There are two types of policies identity based policy we set this up in IAM, we can grant policy to a IAM user resource based policy we set this up in resource page. We can add IAM users to the policy to let the resource know that these people can access it. Create a folder in the new S3 bucket and try to access this folder using another IAM user who only has read access to S3 buckets. Then try to use the new user to create another folder in S3 (will fail). Note folder in S3 is just another object(just look like a folder). So it requires the same permission as objects. using Bucket inline policy generator, we can add another IAM user to the access list in S3 policy. Need to specify the IAM user ARN and S3 bucket ARN. After doing this we can create another folder using another IAM user. Demo CloudFront Origin Access Identity Create an S3 bucket Create an origin access identity(OAI) Grant OAI access to the bucket Create a CloudFront distribution Note to grant OAI access to the S3 bucket, we need to create an inline policy for the S3 bucket. Tell the bucket to let our OAI user have the read access. Now only through the CloudFront can we access the bucket files. 123456789101112131415161718192021# Load named profileexport AWS_PROFILE&#x3D;ben# Create S3 bucketaws s3api create-bucket --bucket yuan.com-cloudfront# Create and uplaod index.html documentecho &quot;Hello, world!&quot; &gt; index.htmlaws s3 cp index.html s3:&#x2F;&#x2F;yuan.com-cloudfront&#x2F;# Create the origin access identity (OAI)aws cloudfront create-cloud-front-origin-access-identity --cloud-front-origin-access-identity-config CallerReference&#x3D;&quot;demo&quot;,Comment&#x3D;&quot;OAI for yuan.com-cloudfront&quot;# Apply a bucket policy granting read access to the OAIaws s3api put-bucket-policy --bucket yuan.com-cloudfront --policy file:&#x2F;&#x2F;bucketpolicy.json# Verify bucket policyaws s3api get-bucket-policy --bucket yuan.com-cloudfront# Create a CloudFront distributionaws cloudfront create-distribution --distribution-config file:&#x2F;&#x2F;dist-config.json Demo Granting Anonymous Access with Object ACLs and Bucket Policies Grant anonymous access to an individual S3 object Grant read permissions to everyone using the object’s ACL (Resources based policy) Use a bucket policy to grant everyone permission to perform the GetObject action against the object (identity based policy) Note even if we logged as the IAM who has full access to S3 object (owner). When we try to access a certain file in S3 bucket. We will still get Access Denied response. Why is that? Because when we click the link to access S3 object, our browser will make an anonymous request to the object, which doesn’t not include any authentication information with it. Note if you are trying to modify bucket policy to give public access to a file. You have to make the bucket public first. Encrypting S3 Objects with KMS-managed Keys Generate a new CMK Enable encryption on our S3 bucket (existing files in bucket will not be encrypted) Verify that unauthorized users can’t decrypt data Summary Use KMS to create customer master keys Use the key policy to grant principals permission to use the key To encrypt data on an existing EBS volume, snapshot the volume, and make an encrypted copy of a snapshot Enabling KMS encryption on an S3 bucket doesn’t encrypt existing objects. Don’t delete a key that’s being used to encrypt or decrypt data! (you can’t decrypt data after key’s deleted) To control access to S3, you can use access control lists, bucket policies, or user policies(identity based policies) Use object ACLs to grant anonymous access to individual objects Bucket policies contain the principal element while user policies don’t Part Five: Protecting Data in Transit Encrypting data between users in public internet and AWS cloud Transport Layer Security (TLS) People sometimes incorrectly call this SSL(secure sockets layer, which is the old technology that nobody uses anymore) HTTPS uses TLS (S stands for security which the underlying protocol is TLS) Configure application to use TLS Application-dependent configuration independent of AWS Application Load Balancer (We use this in this course) Configure AWS application load balancer to use TLS Force all clients through the load balancer We are going to create a load balancer and a TLS certificate using AWS ACM, then install the certificate on the load balancer. All users need to access the load balancer to access our instances (which hosted the tic-tac-toe application). And all traffic between users and load balancer are encrypted. Module Overview Prepare the infrastructure to support an applicatioin load balancer Create a secure Application load balancer Demo Preparing for the Load Balancer Create a new subnet in a different zone (load balancer requires instances to be at different AZs) Bring up an instance named web2 Launch the application Reconfigure security group (permit access to/from load balancer) Demo Creating a Secure Application Load Balancer Use the AWS Certificate Manager to create a TLS certificate Create an Application Load Balancer Create a DNS record for the application Browse to the application using HTTPS If you use Route53 to control the DNS records of your domain. You can let AWS create the CNAME record for you. To create a load balancer, you need to specify the following Load Balancer type: HTTPS, internet facing VPC AZs (tic-tac-toe) Security groups (tic-tac-toe) Routing (new target group HTTP:80 to both instances) Register both instance as load balancer targets DNS name is the load balancer name we can browse to Create an A record in Route53 convert load balancer DNS name to a friendly domain name. Now you can browse to the URL using HTTPS Summary Choose where to terminate the TLS connection Individual instances (you need to configure TLS connection of your application and install TLS certificate on each instance) Application Load Balancer (only need to install one TLS certificate on the load balancer) ALB requires two availability zones ACM requires you to verify control of the domain name in the certificate Part Six: Configuring Data Backup, Replication, and Recovery Module Overview Versioning Lifecycle rules Cross-region replication Demo Versioning Versioning prevents accidental deletion and overwriting of data Enable versioning Upload object Restoring versions After you enabling the versioning of a S3 bucket, when you deleted a file. AWS will not delete the file but instead hide it and give the file a deletion marker. In the version list, you can see all versions of the same file. And if you delete the version with deletion marker, you can bring the deleted files back. Lifecycle Management Different storage classes provide different levels of redundency Standard is the default storage class Automatically migrate older objects to a cheaper class Automatically delete old objects Demo Liftcycle Management Examine object storage classes Create a lifecycle rule This image explains the current lifecycle I set up for the bucket. On day 30, objects will be moved to One-Zone IA On day 31, objects will be added a delete marker and become previous versions On day 32, objects will be moved to Glacier On day 39, objects will be deleted permenately Note, small objects in Glacier will have higher costs (objects &lt; 128KB) Demo Cross-Region Replication Congifure cross-region replication Replication doesn’t include existing objects When enable replication rules choose destination bucket choose keys to decrypt data in source bucket choose keys to encrypt data in destination bucket Summary Versioning Every change results in a new object version deleting an object creates a marker delete the marker to restore the object Lifecycle Management Move objects to different classes delete objects Cross-region replication Synchronously copy new objects to a different bucket Replicating to a different region offers protection against local catastrophes Couse Summary Remember that the goal of security is protect the confidentiality, integrity, and availability of data. It’s that CIA triad. Protecting AWS Credentials: At the start of this course, you learned how to configure identity and access management. This is like the building security system. It controls who can enter the building, what rooms they can go into, and so on. Capturing and Analyzing Logs: After that, you learned how to capture and analyze logs using CloudTrail and CloudWatch. In a physical building, this would be the building’s cameras, security guards, and log books. You’re not just concerned with what should happen, but what did happen. You want to know everything that’s going on inside that building. Protecting Network and Host-level Boundaries: However, perhaps the nature of your particular building is such that it’s not practical to identify everyone that exits and enters. If you’ve got a business that has clients coming in and out all the time, making them sign in and sign out can be a burden. In that case, you need a different way of controlling access to the building. This is analogous to protecting network and host-level boundaries in your AWS environment. Think of security groups and network access control lists. You’ll let strangers in your building, but you’re going to be strict about where they can go and what they can do. Protecting Data at Rest: Next, you learned how to protect at rest using encryption and access controls. Think of a combination safe that contains a secret message. The safe is locked in a room that you have to use a badge to gain access to. First, you swipe your badge to get into the room, and then once in the room, you must possess the correct combination to open the safe. Protecting Data in Transit: Next, you’ll learn how to protect data in transit by way of, yes, encryption. Again, if you want to take a top secret document out of the building and deliver it to someone, you might stick it in a locked briefcase or perhaps hire an armored courier to transport it for you. As long as the document is outside of the building, it remains under lock and key until it gets to its destination. Configuring Data Backup, Replication and Recovery: Lastly, we looked at how to perform data backup, replication, and recovery. Basically, if all else fails and your data does get destroyed, at least it’s not gone forever. You can get it back.","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"AWS SAA - Architecting for Reliability on AWS","slug":"AWS-SAA-Architecting-for-Reliability-on-AWS","date":"2020-10-15T03:30:54.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2020/10/15/AWS-SAA-Architecting-for-Reliability-on-AWS/","link":"","permalink":"http://hellcy.github.io/2020/10/15/AWS-SAA-Architecting-for-Reliability-on-AWS/","excerpt":"","text":"AWS SAA C02 exam will include 4 topic Resilience Performance Security Cost-Optimization Chapter One: Resilience Section One: Availability Resiliency: The ability of an application to avoid and recover from failure. Availability: The percentage of time that an application is performing as expected. Poor performance implies low availablility. Uptime isn’t the same as availability. The service level agreement(SLA) for each service includes its annual availability. The availability of a single EC2 instance is 90% The availability of an ELB(Elastic Load Balancer) and EFS(Elastic File System) is 99.99% The availability of a RDS(Relational DataBase System) multi-AZ(multi-Availability Zone) is 99.95% The availability of a Lambda is 99.95% The availability of a S3 is 99.9% The availability of a DynamoDB with Global Tables (replicates our database across multiple regions) is 99.999% Traditional web application: can be convert to AWS web application without having to change the code. For example, a traditional video processing application can use Elastic File System(EFS) to store video data. But it cannot use S3 to store video data. Because that requires changes of code. EFS provides a network file system (NFS) volume, NFS is an established standard that most Linux distributions support. For DB: we can use (Relational Database Service)RDS. It offers managed database engines (MySQL, MariaDB, PostgreSQL, Microsoft SQL server, Oracle…). AWS manages database infrastructure and backups. Loose Coupling One component doesn’t depend on a specific component (e.g. URL points to ELB, not a specific EC2), one-to-many relationship is EFS a single point of failure? No, Elastic services are always composed of redundant components, they just hide it. Elastic services are always loosely coupled with other services like EC2. Loose Coupling helps Performance: If our application’s performance is low and we want to upgrade our EC2 instances, because they are loose coupled, we can upgrade EC2 instances one by one and our application will still be available, ELB will just route traffic to other instances. Performance and Availability are linked. Simple Queue Service The concept of Loose Coupling can be applied to the application level too. For example, we have a video processing application, we can create two components for this application, the web interface part and the video processing part. Users go to the webpage and submit a request of a video with differnt options, and video processing part gets the request and start processing. But because processing videos take much more time than sending requests. We need a Message Queue Service to save all the requests in order. Simple Queue Service(SQS) is one option with high availability. Elastic Container Service Container helps you to deploy web serices easier. Build an image of your container, deploy the image to an instance, and launch the containser in the image. E.g. Docker You can have multiple containers in one instance, so it is like you are running multiple web services for the price of one instance. Also, if one container is down, the other containers on that instance will still be running. (Processes running inside the container are isolated from the host.) Cloud Native Applications Depend on a cloud service that can’t be deployed on-premises Examples: SQS, S3, DynamoDB Lambda, S3 and DynamoDB are three main serverless components. Rather than running the video processing function on a service on a EC2 instance, we could write a Lambda function to do that. Lambda is a serverless service (they are running on a server of course, but the server is managed by AWS and we don’t need to worry about it, so we call it serverless.) Advantages of using Cloud Native Architecture: Scalability, Performance, Convenience Disadvantages: Could vendor lock-in (have to use AWS services), Slightly lower availability (Cloud services are hard dependencies). However, because of the scalability of Cloud Applications, if we deploy the application to two different regions, we could improve the availability. (introduce more redundence) Trusted Advisor Where you can found the number of limits for all services on AWS. Section One Summary Availability is not cheap, we need to found the balance between availability and cost. We can achieve high availability use Redundancy and Loose Coupling The Simple Queue Service can act as a go-between for loosely coupled services. 1. Sending service places message in a queue. 2. Receiving service polls the queue for new messages. E.g. Online voting service The Elastic Container Service deploys microservices using Docker containers, can improve availability by running multiple containers on a single instance. Section Two: Setting up AWS Environment AWS Budget In the Billing section, you can create an AWS Budget and setup an email alert, AWS will send you an email when the budget amount has been reached. There are more options. AWS IAM AWS has two account types, Root User account and IAM account. We can setup a password policy for AWS accounts. AWS provides MFA for Root User account in case someone else knows your AWS account credentials. Delete root user access keys: if someone knows your root user access key, he can use CLI to do anything. MFA will not be required to use CLI if he knows root user access key. So it is recommanded to create an IAM user and create access key for IAM users. You can create IAM accounts and Groups and assign Policies to Users or Groups, IAM users will use Root account ID or Alias and their account details to login. CloudTrail: where AWS logs all events such as: user login, user create new resources, user attach policy to its account etc… Configure AWS account using AWS CLI. This command can also be used to change default AWS credentials in .aws folder. 1aws configure TLS Certificate TLS(Transport Layer Security): Make sure messages being transferred between Load Balancer and Clients are secure. ACM(Amazon Certificate Manager): we ask ACM to issue us a TLS certificate Route 53: when you purchase a domain, you need to config the DNS records, you can do this in the service where you purchased the domain (e.g. GoDaddy) or in Route 53. Note: Once your TLS certificate has been issued, you still can’t visit your website via https, you have to link your TLS certificate to other AWS services like CloudFront or ELB Section Two Summary Set up budget alert Create IAM policy Set up MFA for the root user Create IAM user View CloudTrail event history Configure AWS CLI Create a TLS certificate using ACM Section Three: VPC networks AWS managers underlying VPC infrastructrue and is responsible for reliability of VPC network components. You don’t need to worry about VPC failures. There are many redundencies built in. VPC contains one or more subnets. A subnet exists in an availability zone. An instance exists in a subnet. Because one instance only exists in one subnet which exists in one availability zone, it lacks of redundency and availability is not high. If the zone fails, the instance will fail. Availability zones: they are basically the data centers in different locations. if you have your instances running in different availability zones, it is highly unlikely that all zones fail. Client(me) can access a VPC via three ways: 1. Internet Gateway, 2. VPN network, 3. Direct Connect link provided by AWS. Transit Gateway: high availability service that can connect two networks together (two VPCs) Elastic IP Address EIP allows an instance to retain the same public IP address. EIP is bound to an ENI(Elastic Network Interface), which is attached to an instance. You can move an EIP to differnt ENI To check EC2 instances EIPs. 1aws ec2 describe-addresses To allocate new EIP 1aws ec2 allocate-address To release the EIP 1aws ec2 release-address --allocation-id (your_allocation_id) Global Accelerator Provides two anycast IPv4 addresses. While ELP is bound to a AWS region, Global Accelerator IPs doesn’t, Users connects to a global accelerator static IP will be routed to a nearest POP(points-of-presence), which then will provide you with resources in any region. VPC Architecture Public Subnet: has full access to the internet, can also be reached from the internet. Private Subnet: is isolated from the internet, cannot reach internet nor be reached from the internet. NAT Gateway: Provides outbound internet access for instance in Private Subnet Create VPC with Public Subnet and Private Subnet. Public Subnet has a default Route Rule which route all requests(0.0.0.0/0) to a IGW(Internet Gateway), this allows instance in Public Subnet access public internet(inbound and outbound). Private Subnet has a default Route Rule which route all requests(0.0.0.0/0) to a NAT(NAT Gateway), this allows instance in Private Subnet outbound only access to internet. NAT Gateway: Instance in private subnet send outbound traffic to NAT Gateway, NAT Gateway then sends traffic to Internet Gateway. Create multiple Public and Private subnets for redundency. To find a subnet by its CIDR block, we will be using the subnet ID to launch the instance. 1aws ec2 describe-subnets --filters Name&#x3D;cidr-block,Values&#x3D;&quot;10.0.11.0&#x2F;24&quot; Launch an EC2 instance into a public subnet: Note: subnetId is the ID of your public subnet, which you can get by using the describe-subnets command above. ImageID is the ID of the EC2 instance, it specify some options about the instance you want to launch. key-name is the name of the SSH key pair you created, which you can use later to login to the instance. You can create or manage your SSH key pair under your EC2 panel, NETWORK &amp; SECURITY tab. 1aws ec2 run-instances --subnet-id subnet-0aa8c9baa867b88f0 --image-id ami-0e6449745600ac1da --instance-type t3.micro --key-name hellcyAWSkey Note: You could optionally associate a public IP address to the instance you are about to launch. By using the command 1--associate-public-ip-address It will make EC2 to associate a temporary public IP address to this instance, and will close the ip address when the instance is stopped. I will not do that because I want to keep the IP and so I will associate an EIP to this instance. To allocate a new EIP Note before we have associated the EIP to the NAT gateway. Now we are going to associate the new EIP to the EC2 instacne 1aws ec2 allocate-address To associate the new EIP to the new EC2 instance 1aws ec2 associate-address --instance-id Your_Instance_Id --allocation-id Your_EIP_Allocation_Id To terminate the EC2 instance 1aws ec2 terminate-instances --instance-ids Your_Instance_Id To release the EIP so AWS will not charge us 1aws ec2 release-address --allocation-id Your_EIP_Allocation_Id Launch an EC2 instance into private subnet 1aws ec2 run-instances --subnet-id [private_subnet_id] --image-id ami-0e6449745600ac1da --instance-type t3.micro --key-name hellcyAWSkey To delete NAT gateway 1aws ec2 delete-nat-gateway --nat-gateway-id Your_NAT_Gateway_Id Don’t forget to release the EIP associated with the NAT gateway.(They will charge for un-associated EIP) AWS Shield Standard Free service that detects against DDoS attacks, always ON Direct Connect Low-latency connection to an AWS region. Bypasses the internet, Two types: Dedicated, Hosted Dedicated: Physical connection that terminates at a Direct Connection location, fast, 1 or 10 Gbps Hosted: Last-mile connection provided by a Direct Connect partner(Local ISP). 50 Mbps to 10 Gbps VPN Connection Encrypted IPsec connection over the internet, Unpredicatable latency, Can be implemented in two ways: Virtual private gateway, Transit gateway. Virtual private gateway: Enables you to establish a VPN tunnel with only one VPC. Doesn’t scale well when you have multiple VPCs, then you need to create multiple Virtual private gateway for each VPC you want to connect. Transit Gateway: Connects VPCs and on-premises networks, 1. Terminates multiple VPN connections, 2. Supports Direct Connect. Connects multiple VPCs together. Transit Gateway Route Tables: Control how traffic is routed between subnets. Can block traffic. To create a Transit Gateway 1aws ec2 create-transit-gateway To create a VPC 1aws ec2 create-vpc --cidr-block 127.27.0.0&#x2F;16 To create a Subnet in the VPC 1aws ec2 create-subnet --vpc-id Your_VPC_Id --cidr-block 172.27.1.0&#x2F;24 --availability-zone ap-southeast-2a To attach Transit Gateway to the subnet of the VPC 1aws ec2 create-transit-gateway-vpc-attachment --transit-gateway-id Your_TGW_Id --vpc-id Your_VPC_Id --subnet-ids Your_Subnet_Ids Section Three Summary Allocating and assigning EIP addresses Creating VPCs Creating public and private subnets Launching instances into subnets Transit gateways Section Four: Automated Deployments with CloudFormation Overview of the architecture: The architecture has two tiers, the web tier and the application tier. The client connects to the internet facing application load balancer, it proxies the connection to one of the instances in the web tier, and all of the instances in the web tier are running a web server. The web tier instance then opens a back-end connection to the internal load balancer, which then proxies the connection to one of the instances in the application tier. The idea is that the web tier instance grabs some information from an instance in the app tier, and it displays that information to a webpage, which presents to the client. The instances in the web tier and the app tier are going to be part of two different Auto Scaling groups. Auto Scaling is going to launch these isntances and make sure we always have a minimum number of healthy instances. If an instance fail, Auto Scaling will terminate it and launch a new one. CloudFormation JSON or YAML document that describes AWS resources. Infrastructrue as code. Used to create a stack. Stack: Created by a template, is a COLLECTION OF RESOURCES that you create, update, and delete as a single unit. You can manually manage individual resources in a stack. Multiple templates: Different teams manage different resources. Resources have different lifecycles. Distributing resources across different stacks makes them easier to manage. Template for this course can be downloaded from here. Name: app-stack.json and network-stack.json. app-stack.json depends on network-stack.json, so it calls network-stack.json to create the nested stack first, then it will create the parent stack by using some of the outputs from network-stack.json. Stack output: key-value pairs that CloudFormation makes available to other stacks and via the aws cloudformation describe-stacks CLI command. Application Load Balancers Supports HTTP and HTTPS traffic You can use any TCP port, default is 80 and 443 ALB listener receives connection from a client and proxies it to an instance in the target group Uses round-robin load balancing by default Can monitor health of instances ALB Schemes internet-facing: reachable from the internet, public IP address, Public DNS name internel: Not reachable from the internet, private IP address, Private DNS name Health Checks Each instance must pass its health check before receiving traffic. ALB will send HTTP GET request and looks for a success code every 10 seconds. Auto Scaling Groups Launch a certain number of instances into the Auto Scaling group Add the instacnes to the ALB target group Terminate and recreate unhealthy instances Scale in or out based on average group CPU utilization Follow the steps to deploy stack to AWS Cloudformation First validate the templates, you will see the output parameters if template format is correct. 1aws cloudformation validate-template --template-body file:&#x2F;&#x2F;app-stack.json 1aws cloudformation validate-template --template-body file:&#x2F;&#x2F;network-stack.json deploy the stack to AWS cloudformation 1aws cloudformation deploy --template-file &quot;app-stack.json&quot; --stack-name &quot;app-stack&quot; Change the TLS certificate ARN Change the S3 URL to yours Change the SSH key pairs for logging into EC2 instance Change the EC2 instance image ID Note: the template will only work for us-east-1 region, tried using ap-southeast-2 but failed when waiting for the cfn-signal. Probably because the application doesn’t exist in ap-southeast-2 docker market. 1aws cloudformation describe-stacks --stack-name &quot;app-stack&quot; Using the command above we can find the URL of the Internet-facing application load balancer. We securely(HTTPS) connect to this load balancer. And it redirect us to one of the instance in web tier. Note we do added a TLS certificate in the template. But the TLS certficate is for my website theyuancheng.com, and the domain of the EC2 instance does not match. So we are connecting using HTTP protocol. Also pay attention to the EC2 hostname, Web tier server information, ip-10-0-1-42 belongs to Public Subnet A, and one of the instance is running inside this subnet. App server information, ip-10-0-102-180 belongs to Private Subnet B, one of the instance is running inside this subnet too. If you refresh the URL a couple of times, you can see that the hostname ip we are connecting to changes, which means the load balancer redirect us to a different instance. Now, let us try to terminate both of the web tier instances and see if Auto Scaling will recreate the instances for us. 1aws autoscaling describe-auto-scaling-instances This command will list all the instances, we can find the instance ids in the list. 1aws ec2 termin ate-instances --instance-ids We can see Auto Scaling automatically recreate another two instances, the intance ids are different. Use this command to delete the stack. 1aws cloudformation delete-stack --stack-name &quot;app-stack&quot; After deleting the stack, use this command to check if the stack has been deleted. 1aws cloudformation describe-stacks Section Four Summary Use Stack template to deploy Auto Scaling multi-tier web application with load balancer. Stack will automatically rollback everything if deploy failed. Elastic load balancing and Auto Scaling work together. ELB provides health checks, Auto Scaling adds instances to the ELB target group. Section Five: Multi-region Applications with Route 53 Deploying a multi-region application Active-active redundancy using weighted resource records Active-passive redundancy using failover resource records Route 53 health checks We are going to deploy two cloudFormation stacks into the same region to simulate the multi-region application deployment. But because we have two URLs for the two Internet facing ALB, we need Route 53 to send traffic to these two ALBs. This is called an active-active scanario, because they are both active. We are going to create two weighted resource record sets with equal weight, so Route 53 will distribute traffic evenly. See the image above to create a new record. Create another record and route traffic to the two load balancers evenly. Checklist for creating records Routing Policy: Weighted Record name: www Turn on Alias Record Type: A Route traffic to: Classic load balancer, region (us-east-1), webtier-app-stack-1 Weight: 50 Record ID: app-stack-1 The connection to the instance is now secure. (Notice the lock icon at top left corner, in front of the URL) How to check if Route53 distribute the traffic evenly to two load balancers? Go to DNS checker and type in www.theyuancheng.com. And you will see that half of the traffic ending in one IP. Active-passive Redundancy using Failover Resource Records Primary region services all requests Secondary region does not service any requests unless the primary fails Also called active-standby architecture Two ways to run this architecture Pilot Light: Secondary region runs minimal amount of resources to keep costs down. (in our case, maybe only one instance in the web tier and one instance in the app tier (normally should be 2 and 2)). When we need the secondary region, Auto Scaling can increase the instance number when needed. Warm Standby: Secondary region has roughly the same capacity as the primary region. Quicker to start, doesn’t need Auto Scanling. Similar to what we did before, this time we create two records, one pointing to app-stack-1 and the other is pointing to app-stack-2. As long as app-stack-1 is healthy, Route 53 will not send any traffic to app-stack-2. For the Routing policy we choose Failover and Failover Record Type is Primary for the first record.(Secondary for the second record). Now both records are created. We can go the DNS checker and check the differences. Now they all resolve to the same set of IP addresses. Now let us try to shut down app-stack-1. 1aws cloudformation delete-stack --stack-name &quot;app-stack-1&quot; Now the DNS checker returns a different set of ip addresses over time. (from 236, 12 to 252, 207) Route 53 Health Checks We are going to use the two instances in app-stack-2, use below command to get the IPs 1aws ec2 describe-instances --query &quot;Reservations[*].Instances[*].PublicIpAddress&quot; --output&#x3D;text Create Route 53 health checks. Type in the IPs of the instances. Once both health checks have been created, we can try to shut down one of them. Create another record with instance IP and health check. Route 53 will distribute traffic evenly to both instances, if one of the instances fail, the Route 53 health check will detect that and Route 53 will stop sending traffic to it. If you don’t need ELB, DNS based load balancing is a cost-effective option. Course Summary Architecture for availability Setting up AWS environment VPC(Subnets, NAT Gateways, Direct Connect, VPN, Transit Gateways) CloudFormation, Elastic Load Balancing, Auto Scaling Multi-region Applications(Route 53, Active-active weighted records, Active-passive failover records)","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"AWS SAM and CloudFront","slug":"AWS-SAM-and-CloudFront","date":"2020-10-07T03:13:00.000Z","updated":"2021-05-24T07:27:02.657Z","comments":true,"path":"2020/10/07/AWS-SAM-and-CloudFront/","link":"","permalink":"http://hellcy.github.io/2020/10/07/AWS-SAM-and-CloudFront/","excerpt":"","text":"Part One: AWS SAM Part Two: AWS SSO Part Three: AWS CloudFront Part One: AWS SAM Note: This is a brief introduction about how to setup a Serverless Application Model using VS Code and Nodejs. I will probably be adding more details later. Visual Studio Code VS Code is an perfect IDE for writing/testing/deploying SAM. Some key points need attention are listed. AWS credentials in Users/UserName/.aws folder. Create yaml file with resources: lambda function, policies, runtime language, environment variable etc… Writing lambda function in Nodejs, CROS policy… CRUD operation: marshall/unmarshall object etc… AWS CLI: package and deploy SAM to AWS AWS CLI command to package and deploy SAM to AWS. 1sam deploy --template-file output-yamlFileName.yaml --stack-name Your_Stack_Name --capabilities CAPABILITY_IAM 1sam package --template-file yamlFileName.yaml --s3-bucket S3_Bucket_Name --output-template-file output-yamlFileName.yaml A very useful course I took about AWS SAM: Deploying Serverless Applications in AWS Using the Serverless Application Model Part Two: AWS SSO Create Custom SAML Application We can use AWS SSO as an IDP for our web application. Create new Custom SAML 2.0 application. This is IDP metadata, we will download it after finishing configuring return attributes. This is SP metadata, which we will prepare and upload. This is where we config the return attribute in SAML response. Supported attribute list can be found [here](https://docs.aws.amazon.com/singlesignon/latest/userguide/attributemappingsconcept.html?icmpid=docs_sso_console) Create Users Users can be created by simply fill this form. You can also setup MFA device after creating the user. Part Three: AWS CloudFront Restricting Access to Files in Amazon S3 Buckets You can optionally secure the content in your Amazon S3 bucket so that users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents someone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. (Using Origin Access Identity) Block all public access to S3 bucket When creating new distribution in CloudFront, do the following steps Select Web distribution Select the S3 bucket you want to connect to Select Redirect HTTP to HTTPS if you don’t want people to access your content by HTTP requests Select Yes for ‘Restrict Viewer Access’ Select Self as trusted signer Create Trusted Signer Note When 'Restrict Viewer Access' is selected, you can specify which account is the 'Trusted Signer'. Which means they have the permission to create signed URL or signed cookie for people to access your private content. Self is the default 'Truested Signer', which is the account your are currently using. You can also add another accounts by entering their account ID. Create another behavior Login page should be accessiable by public, so as the js, css and image files. So we should create another behavior to let CloudFront know which file access is retricted and which are not. Path Pattern decides which files can be set to public(also select No for 'Retrict Viewer Access' this time) Rules about Path Pattern can be found [here](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesPathPattern) Create CloudFront Key Pairs We need CloudFront key pairs to create signed URLs and signed Cookies, they are a pair of public and private keys AWS uses to encrypt requests people send to CloudFront, so it knows whether they are authenticated users. This step can only be done using AWS root account, IAM account cannot create CloudFront key pairs. Save the private key pem file and Access key ID to a save place. We will use it later. Create Signed URL This is the most important part. I used two ways to create signed URL, nodejs and C#. I use nodejs to create a lambda function as an API, which will return the signed URL generated. I also add this feature to the SAML project, so when IDP returned SAML response, I can add signed details to the URL and redirect user to the home page. Note that yaml file supports multiple line string (a vertical line followed by a hyphen), so we can add private key string in the yaml file as a environment variable, so as the public key. Also note expiry time input are in [Unix Epoch Time format](https://www.epochconverter.com/) in miliseconds(13 digits). However, the expires argument in the signed URL are in seconds format (10 digits). Also note that C# doesn't recognize private key string in PEM format. We have to convert it to XML format before create signed URL. The signed URL created will append three arguments at the end of the original URL: Expires, Signature and Key-Pair-Id Note the signature contains information about the original URL so you cannot reuse it with different URLs. We have to generate new signed URLs when direct user to other pages.(this will also refresh the session time, which is we want.) Create Signed Cookies NodeJs: Signed Cookies can be created using below code. I used this post as a reference. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&quot;use strict&quot;const AWS &#x3D; require(&quot;aws-sdk&quot;)exports.handler &#x3D; async (event, context, callback) &#x3D;&gt; &#123; var cfsign &#x3D; require(&#39;aws-cloudfront-sign&#39;); &#x2F;&#x2F; 5 seconds var expireTime &#x3D; Math.floor((+new Date() + 1000 * 300) &#x2F; 1000); var signingParams &#x3D; &#123; keypairId: process.env.PUBLIC_KEY, privateKeyString: process.env.PRIVATE_KEY, &#x2F;&#x2F; Optional - this can be used as an alternative to privateKeyString &#x2F;&#x2F;privateKeyPath: &#39;&#x2F;path&#x2F;to&#x2F;private&#x2F;key&#39;, expireTime: expireTime &#125; const body &#x3D; JSON.parse(event.body); &#x2F;&#x2F; Method 2: Generating singed Cookies let policy &#x3D; &#123; &#39;Statement&#39;: [&#123; &#39;Resource&#39;: body.url + &#39;*&#39;, &#39;Condition&#39;: &#123; &#39;DateLessThan&#39;: &#123;&#39;AWS:EpochTime&#39;: expireTime&#125; &#125; &#125;] &#125;; let policyString &#x3D; JSON.stringify(policy); const signer &#x3D; new AWS.CloudFront.Signer(signingParams.keypairId, signingParams.privateKeyString); const options &#x3D; &#123; url: body.url, policy: policyString &#125;; const signedCookie &#x3D; signer.getSignedCookie(options); var response &#x3D; &#123; statusCode: 200, headers: &#123; &quot;Access-Control-Allow-Origin&quot; : &quot;*&quot; &#125;, body: JSON.stringify(signedCookie) &#125; callback(null, response);&#125;; By default, CloudFront caches a response from Amazon S3 for 24 hours, so if you just updated contents in S3 bucket, CloudFront may still serve you the outdated content. Use the below code in cmd to force CloudFront to update its files from S3. 1aws cloudfront create-invalidation --distribution-id &#39;YOUR DISTRIBUTION ID&#39; --paths &quot;&#x2F;*&quot; References Task List for Serving Private Content using S3 and CloudFront Restricting Access to Amazon S3 Content by Using an Origin Access Identity Specifying the AWS Accounts That Can Create Signed URLs and Signed Cookies (Trusted Signers) Reformatting the CloudFront Private Key (.NET and Java Only) Using Signed URLs Creating a Signed URL Using a Canned Policy Code Example: Creating Amazon CloudFront Signed URLs in Node.js Code Example: Create a URL Signature Using C# and the .NET Framework Code Example: Create Signed Cookies using NodeJs Test Signed Cookies using Postman Epoch &amp; Unix Timestamp Conversion Tools Why is CloudFront serving outdated content from Amazon S3? RSA public/private keys in YAML Convert PEM to XML in C# CloudFront Path Pattern Rules","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"Wishlist 2021","slug":"Wishlist-2021","date":"2020-10-06T03:11:24.000Z","updated":"2022-02-10T14:39:24.301Z","comments":true,"path":"2020/10/06/Wishlist-2021/","link":"","permalink":"http://hellcy.github.io/2020/10/06/Wishlist-2021/","excerpt":"","text":"Get AWS SAA C02 certificate - finished Finish 1500+ Leetcode Problems (at least 2 problems per day) Get 2000+ points in Leetcode contests Setup a simple LAMP website - finished Get a new job by the end of year 2021 - finished","categories":[],"tags":[{"name":"Wishlist","slug":"Wishlist","permalink":"http://hellcy.github.io/tags/Wishlist/"}]},{"title":"Dynamic Programming","slug":"Dynamic-Programming","date":"2020-08-27T04:08:01.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2020/08/27/Dynamic-Programming/","link":"","permalink":"http://hellcy.github.io/2020/08/27/Dynamic-Programming/","excerpt":"","text":"Lecture 1 Introduction 什么是动态规划？ 1.计数型 有多少种方式走到右下角 有多少种方法选出k个数使得和是Sum 2.求最大最小值 从左上角到右下角的路径的最大数字和 最长上升子序列长度 3.求存在性 取石子游戏，先手是否必胜 能不能选出k个数使得和是sum Coin Change(最大最小型) 一般想法：先用面值大的硬币，最后想办法用小硬币 动态规划四个步骤 确定状态 一般来说，解决动态规划需要开一个数组，可能是一维的或者二维的，要确定数组的每一个元素代表什么 转移方程有几个变量就需要创建几维数组 确定状态需要两个意识：最后一步是什么，子问题是什么 转移方程 每个子问题到下一个子问题的过程 初始条件和边界情况 什么时候停下来？初始条件是什么? 计算顺序 一般是从小到大 Coin Change 从第一步算到m步，每一步算n次，其中m为最后的硬币sum，n为总共有多少种不用的硬币，时间复杂度为O（m*n） 最后一步是由前面几步的最小值确定的，所以先确定前面的值，从小到大，就能判断出最后一步的值 Unique Path:计数型 Jump Game：存在型 Lecture 2 Coordinates and Bit Operation 序列型：前i个，最小，方式数，可行性。。 Paint House 分别记录每栋房子之前的房子的每种颜色的最小花费 划分型 Decode ways 解密数字串即划分成若干段数字，每段数字对应一个字母 知道前N-1和N-2个数字分别有多少种方式，再相加 坐标型 需要找到序列中某些子序列或者网格中的某条路径：计数，最大，最小，存在性 Minimum Path Sum 空间优化：计算第i行时，只需要i行和i-1行的f值（滚动数组） Bomb Enemy 给定输入为序列或者网格矩阵 问题一般为：以第i个元素结尾的某种性质，到格子（i， j）的路径的性质 Minimum path sum打印路径：新建一个二维数组，记录每次的方向，然后从最后（右下角）往前推，最后记住要反转数组才是从开始到结束的顺序 位操作型动态规划 Counting bits: removing the last bits Lecture 3 Sequence 给定一个序列，转移方程f（i）下标i表示前i个元素的某种性质， f(0)就是空序列的性质 Paint House II: Similar to Paint House I, Optimization: pick the smallest and the second smallest cost of all colors, reduce the min calculation to O(1) and totoal time complexity from O(N * K * K) to O(N * K * 2), where N is the number of houses and K is the number of colors House Robber House Robber II:想办法把圈断开，分两种情况处理，变成两种序列情况 Buy and Sell Stock III:分成五个状态，分别记录下每天每个状态的情况（类似于Paint house） Russian Doll Envelope Lecture 4 划分型，博弈型，背包型 划分型 Perfect Squares Palindrome Partitioning II:1. find all possible palindrome from the string s ans store them in a 2d array expand palindrome start and end index to both sides. 2. partition dp: dp[i] = min(dp[i], dp[j] &amp;&amp; isPalin(s[j][i - 1])) Copy Books(LintCode) 划分型dp关键字：连续！Substring, continuous subarray 博弈型(两方游戏) 从第一部开始分析!!!问题规模会越来越小。 Coins in a line: Alince and Bob takes one or two coins each turn Alice先手还剩N个Coin，与Bob先手还剩N-1个Coin是一样的，所以每个局面都可以看成是先手但是剩下不同数量的Coin 背包型 你有一个背包，背包有最大承重 商店里有若干物品，都是免费拿 目标： 1. 装下最多重量的物品， 2. 装下最大总价值的物品。 3. 有多少种方式正好带走满满一书包物品 给定背包最大承重M， 物品的重量都是整数 每个方案的总重量都是从0到M 对于每个重量方案，我们要知道能不能做到 Note：背包问题中，dp数组大小和总承重有关 现在需要知道前N个物品能否拼出重量W 如果前N-1个物品能拼出W，当然前N个物品也可以拼出W 如果前N-1个物品能拼出W-A(N-1),A(N-1)为第N个物品的重量，那么前N个物品也能拼出W BackPack I - V Coin Change 1, 2 当每个物品只能用一次时，需要多开一维的dp数组去计算前N个物品的方式 Lecture 5 背包型, 区间型 背包型 Backpack II：现在每个物品有价值，求能带走最大多少价值的物品 f[i][j]: 用前i个物品拼出重量j时的最大总价值，j = -1表示不能拼出 dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - nums[i - 1]] + Value[i - 1]) 区间型：当子问题还是连续的index时 Scramble String Burst Ballons 消去型的题目要从后往前想，最后一步只剩一个，然后往前想 先从小的len开始计算 Lecture 6 双序列型 有两个序列 每个序列本身是一维的 可以转化为二维的动态规划 查看最后一个字符是否匹配，缩减问题规模 易错点： 记得初始化，空串处理，结果是否 + 1 Longest common subsequence Interleaving String Edit Distance Distinct Distance Regular Expression Matching Wildcard Matching Ones and Zeros","categories":[],"tags":[{"name":"Dynamic Programming","slug":"Dynamic-Programming","permalink":"http://hellcy.github.io/tags/Dynamic-Programming/"}]},{"title":"Harvard CS75 Web Development","slug":"Harvard-CS75-Web-Development","date":"2020-06-30T06:49:58.000Z","updated":"2021-05-24T07:27:02.659Z","comments":true,"path":"2020/06/30/Harvard-CS75-Web-Development/","link":"","permalink":"http://hellcy.github.io/2020/06/30/Harvard-CS75-Web-Development/","excerpt":"","text":"Chapter 0: HTTP Chapter 1: PHP Chapter 2: PHP Continued Chapter 3: MVC XML Chapter 4: SQL Chapter 5: SQL Continued Chapter 6: JavaScript Chapter 7: AJAX Chapter 8: Security Chapter 9: Scalability Chapter 0: HTTP What happens after you pressing Enter? The URL address will be translate to an IP address. 我们现在所用的IP address是ipv4,它由四组0-255的数字组成，总共32bits，可以有40亿总可能，我们现在每个人都有很多设备，ipv4的组合已经快不够用了。 所以新的ipv6要替代原来的ip address，它由8组4位的16进制组成，总共有128bits 怎么样将hostname网址转换成IP address呢？我们需要用到DNS（Domain name server）,他储存有ip address到hostname的一个mapping table，如果你访问的网址不在这个DNS里，他就会不断的访问上级服务器，直到访问到root server，root server知道谁有可能知道这个ip address，在不断的访问到下级服务器/DNS，直到找到并把hostname转换成ip address 互联网就像一个邮件系统，我们现在知道邮件该送给谁，我们知道自己的ip address(return address). 我们就可以把信息送给我们想送的网站，比如google.com，他收到信后，会拆开看我们相访问哪个网址，比如index.html,然后把我们想要的内容装进信封，颠倒送件人和收件人，然后送回来，我们拆开信封看到html信息，浏览器就会显示这些信息 Private Ip address 对于一个家庭来说，所有的设备共享一个公共ip，但是每个设备都会有自己的private ip，192.168.x.y. 或者172.16.x.y. 当你需要很多设备时，比如在大型公司，你可以使用10.x.y.z TCP/IP IP用来确认谁要向谁传送信息，TCP就是传送协议 Port Number 信息的传送有很多种，不光是HTTP，还有邮件，短信等等，我们使用不同的port number来让服务器知道进来的request是哪种请求，比如HTTP: TCP 80，我们让服务器听取port 80的网页请求，还可以SMTP: TCP 25让服务器听取port 25 的邮件请求， HTTPS： TCP 443, 加密的网页请求。 现在的浏览器会自动在网址后面加上port number Getting your own domain name 我们的设备虽然可以联网，但是他只有一个公共ip，如果我们想host我们自己的网站，我们就需要有一个域名 GoDaddy, NameCheap Hosting your own website 当我们有了域名之后，我们还需要一个web server用来存放我们的网站相关文件，HTML， CSS， JAVASCRIPT等等，这需要我们再找一个提供DNS服务的公司（dreamhost.com），向他们租用一些online storeage，他们也会提供两个ip addresses（还有一个是备用的）。并且与我们的域名关联 DNS 例子：如果你的网站是一个邮件服务网站，比如Google email。我们的email虽然可以是yuan@unicard.com，但是其实我们使用的是Google的邮件服务，DNS识别出以unicard结尾的邮件地址是属于Google的一个服务，并把他发送到Google的服务器。 NS Name Server 一个NS record的作用是告诉大家哪个name server知道关于我们域名的一些信息 NS stands for ‘name server’ and this record indicates which DNS server is authoritative for that domain (which server contains the actual DNS records). A domain will often have multiple NS records which can indicate primary and backup name servers for that domain A A record的作用是把ip address和域名相连(Domain name to ip address) CNAME Canonical name 与A record不同的是CNAME是把域名和域名相连(Domain name to domain name)，因为比如我们相用Google的邮件服务，所以我们想把yuan@unicard.com，关联到yuan@google.com,然后Google用他们呢的DNS server去把他转换成ip address，我们就不用担心Google改变他的ip address了。 再比如说，如果Dell公司的客户服务是外包出去的，那么我们访问Dell的网站，在通过Dell官网进入到他们的客户服务网站，我们就会发现我们从Dell.com 到了customerService.com，这样可能不太好，Dell想让他的网站看起来是一个整体，所以就可以用CNAME让customerService.com的名字隐藏起来，只显示成support.dell.com MX Mail Exchange MX record把负责处理请求的server和他们的ip address相连，这样就可以知道进来的email请求该由哪个server负责 What happens when you reach the target website? Web host Web host companies host multiple websites with one ip address, they share one ip address. Server response with different content based on the headers in the request If the hosting server is using Php v4.3, you have to use the same. If the hosting server is down, all websites related to this server is also down. VPS(Virtual Private Server) VMware, Parallels, Virtual Box Install multiple instances of Windows or Linux or MacOS on the server, and you can control the software and tools on that VPS SSH/SFTP SSH: Connecting to a remote server and execute commands on it. SFTP: Transfer files to a remote server. 123456telnet www.google.com 80GET &#x2F; HTTP&#x2F;1.1(press enter twice) If your webiste is hosted by a server that are shared with other website, you have to send a get request with a host header specify your domain name. Otherwise the server don’t know what content to return. nslookup google.com will list ip address for that host sudo vi /etc/hosts this is a local ip address &lt;-&gt; domain name mapping table, browser will check this file first before sending any request Chapter 1: PHP Apache Configuration file for Apache web server: httpd.conf, apache.conf, apache2.conf listen to port 80 of any ip addresses of incoming requests ServerName and ServerAlias: the destination of the incoming requests, both will need an A record in DNS to work (or CNAME) CustomLog, ErrorLog: Specify where to save the log files DocumentRoot: Root path of your website port 443 uses SSL(Secure socket layer), needs a certificate to be installed on the server. SSL: there are two keys, public key and private key, when a user visits our website, the website sends the public key to the user’s machine, user uses it to encrypt the message. Then when our website receives the message from the user, we use the private to decrypt it to get the original message. But this is not enough, we need to ask some Certificate Authorization(CA) for a certificate. This is because user doesn’t trust our website. but because our website is trusted by a CA, so the user can trust our website. Certificate needs to be digitally signed. SSLCertificateKeyFile: the private key on the server SSLCertificateFile: the certificate 12345mod_rewriteRewriteEngine OnRewriteCond %&#123;HTTP_HOST&#125; !^www\\.cs75\\.net [NC]RewriteRule (.*) https:&#x2F;&#x2F;www.cs75.net&#x2F;$1 [R&#x3D;301, L] HTTP_HOST is an environment variable, it is the host name. In English, the first line is a condition, it is saying if the host name is not starting with www.cs75.net(Regex), do the following line, [NC], no case, case insensetive (.*) one or more any characters, it remembers what user was typing, redirect user to the correct URL, 301 move permanently, the browser saves the result and next time will redirect you automatically. whereas 302 is move temporarily. The reason to do this is to make sure the URL in user’s browser is the website URL. Because there are multiple ways to visit the website and not typing the website URl. We could use ip address, we could use ‘udo vi /etc/hosts’(see end of lecture 0). So this will make sure that the URL will always be www.cs75.net no matter what user originally typed. XAMPP Linux, Apache, MySQL, PHP, Perl We can set up our dynamic website without using remote virtual server, people from outside world cannot visit it. But it is good for development purposes GET/POST GET: will change the state of the url, add parameters behind the quesiont mark ‘?’, parameters are separated by &amp;, everything will be shown in URL, not good to send sensitive or huge information POST: can upload files(images), it is not in URL, can send sensitive information. Post request cannot be copied. One of the downsides of POST is that when user click reload or backbutton, browser will try to submit the form again, you may end up buying things twice. One of the solutions would be, whenever user submitted a form, immediately redirect user to another website page with only GET request, so they cannot revisit the POST request page anymore. PHP Very well documented. Interpreted(alternative to compiled) language, you don’t need to compile it first then to run it, you can just give your code to a interpreter and it will run. Downside is performance. Once a C++ language be compiled, the complied code can be run by CPU superfast. whereas PHP needs to be interpreted everytime. suPHP Web servers usually have root user(administrator) and other users. If you are user A, and you want the web server to be able to use your PHP code. You have to set them to be readable. Then another user B would be able to see your PHP code. Use suPHP could solve this issue. It makes sure that web server can only execute A’s code when A’s logged in. And B cannot see it when B’s logged in. And A can only delete A’s file, so no one else could modify or break A’s code. If A’s website’s users upload files or images, they are stored in the server where only A can see. Variables Data Types(loose in PHP), PHP functions will return different data types based on situations Superglobals $_COOKIE, key values from browser $_ENV, lower details of user’s machine $_FILES $_GET, hash table $_POST, array $_REQUEST, details from requests $_SERVER, user agent, browser and OS $_SESSION, states, save values Command line mkdir, make directory cd, change directory cat/more, show content of a file ls, list content of a directory . current folder … parent folder ls -al, list permission settings for all files in current directory chmod, change mode of a file or directory: e.g. chmod a+r filename: give read access to a file called filename permission settings looks like this: -rw-rw-r–, the first - means it is a file or it can be ‘d’ for directory, it can then be split into three groups, they are the owner, the group and the world. Use $_GET[‘username’] to get the variable values user send by GET request. Use htmlspecialchars to escape all html tags to sanity check user inputs Chapter 2: PHP Continued PHP is an interpreted language, it can be run at anywhere no matter what the PC is like. Compiled language depends on the PC, it may run at one PC and may not run at another. POST request 123&lt;pre&gt; &lt;?php print_r($_POST) ?&gt;&lt;&#x2F;pre&gt; use print_r to recursivly display all data inside the POST request 123456&lt;? if (empty($_POST[&#39;fname&#39;]) || empty($_POST[&#39;lname&#39;])) &#123; header(&quot;Location: http:&#x2F;&#x2F;localhost&#x2F;yuan&quot;); exit; &#125;?&gt; check if necessary form information is filled(not empty). If not, redirect user back to the previous form page. header(“Location: URL”) is to redirect user 123456&lt;? if (empty($_POST[&#39;fname&#39;]) || empty($_POST[&#39;lname&#39;])): ?&gt; You must provide your full name and gender to continue. Go &lt;a href&#x3D;&quot;index.html&quot;&gt;back&lt;&#x2F;a&gt;&lt;? else: ?&gt; You are registered! &lt;pre&gt;&lt;? print_r($_POST) ?&gt;&lt;&#x2F;pre&gt;&lt;? endif ?&gt; 123456&lt;? if (empty($_POST[&#39;fname&#39;]) || empty($_POST[&#39;lname&#39;])) &#123; ?&gt; You must provide your full name and gender to continue. Go &lt;a href&#x3D;&quot;index.html&quot;&gt;back&lt;&#x2F;a&gt;&lt;? &#125; else &#123; ?&gt; You are registered! &lt;pre&gt;&lt;? print_r($_POST) ?&gt;&lt;&#x2F;pre&gt;&lt;? &#125; ?&gt; if else conditions in PHP supports both colon and curly brackets 12345678910111213if (!empty($_POST[&#39;fname&#39;]) &amp;&amp; !empty($_POST[&#39;lname&#39;])) &#123; $to &#x3D; &#39;chengyuan82281681@hotmail.com&#39;; $subject &#x3D; &#39;Registration&#39;; $body &#x3D; &quot;This person just registered!\\n\\n&quot; . $_POST[&#39;fname&#39;] . &quot;\\n&quot; . $_POST[&#39;lname&#39;]; $headers &#x3D; &quot;From: chengyuan82281681@hotmail.com\\r\\n&quot;; mail($to, $subject, $body, $headers);&#125;else &#123; header(&quot;Location: http:&#x2F;&#x2F;localhost&#x2F;yuan&#x2F;index.html&quot;); exit;&#125; Dot symbol is a concatenation symbol, connect two strings into one This mail function comes with PHP doesn’t uaually work on local network, because ISP blocks outbound port 25 for SMTP. Same reason as they block port 80. 1234567891011&lt;? if (isset($_POST[&#39;action&#39;])) &#123; if (empty($_POST[&#39;fname&#39;]) || empty($_POST[&#39;lname&#39;])) &#123; $error &#x3D; true; &#125; &#125;?&gt;&lt;? if ($error): ?&gt; &lt;div style&#x3D;&quot;color: red&quot;&gt;You must fill out the form!&lt;&#x2F;div&gt;&lt;? endif ?&gt; To be able to check form errors using PHP code, index.html has to be changed to index.php in index.php, setup a variable named ‘action’(other names work too) and send it to the backend, the server code will know if user submit a form by checking by variable. if form has errors, send user back to itself with errors. 1234$DORMS &#x3D; array(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;);&lt;? foreach ($DORMS as $dorm): ?&gt; &lt;option value&#x3D;&quot;&lt;?&#x3D; $dorm ?&gt;&quot;&gt;&lt;?&#x3D; $dorm ?&gt;&lt;&#x2F;option&gt; &lt;? endforeach ?&gt; Define array in PHP Foreach loop in PHP 12345678&lt;? foreach ($DORMS as $dorm) &#123; if (isset($_POST[&quot;dorm&quot;]) &amp;&amp; $_POST[&quot;dorm&quot;] &#x3D;&#x3D; $dorm) echo &quot;&lt;option selected&#x3D;&#39;selected&#39; value&#x3D;&#39;$dorm&#39;&gt;$dorm&lt;&#x2F;option&gt;&quot;; else echo &quot;&lt;option value&#x3D;&#39;$dorm&#39;&gt;$dorm&lt;&#x2F;option&gt;&quot;; &#125;?&gt; When redirect user back because there are some errors in the form, the selected value will stay Xdebug: extension of PHP to debug. Let website remember your login with sessions 1234567891011121314151617181920212223242526272829&lt;? &#x2F;&#x2F; 1 session_start(); &#x2F;&#x2F; 2 define(&quot;USER&quot;, &quot;yuan&quot;); define(&quot;PASS&quot;, &quot;123&quot;); &#x2F;&#x2F; 3 if (isset($_POST[&quot;user&quot;]) &amp;&amp; isset($_POST[&quot;pass&quot;])) &#123; if ($_POST[&quot;user&quot;] &#x3D;&#x3D; USER &amp;&amp; $_POST[&quot;pass&quot;] &#x3D;&#x3D; PASS) &#123; $_SESSION[&quot;authenticated&quot;] &#x3D; true; &#x2F;&#x2F; 4 save user in cookie for a week setcookie(&quot;user&quot;, $_POST[&quot;user&quot;], time() + 7 * 24 * 60 * 60); &#x2F;&#x2F; redirect user to home page, using absolute path. redirect(&quot;home.php&quot;); &#125; &#125; &#x2F;&#x2F; 5 function redirect($file) &#123; $host &#x3D; $_SERVER[&quot;HTTP_HOST&quot;]; $path &#x3D; rtrim(dirname($_SERVER[&quot;PHP_SELF&quot;]), &quot;&#x2F;\\\\&quot;); &#x2F;&#x2F; current php directory, trim out unwanted leading slashes header(&quot;Location: http:&#x2F;&#x2F;$host$path&#x2F;$file&quot;); exit; &#125;?&gt; this will enable session, must be at the top of your code. Make sure no whitespaces or any code in front of this. Define constants check user form data are exist and valid Session will be saved on the web server on the disk with a unique session id. Cookie will saved on the client machine. Everytime user visit the website, it will show us the cookies, web server will quickly open up the file saved with the same session id, and grab all the values(key-value pairs) to our webpage. Webpage also can access to the user Cookies. define functions in PHP Note: make sure to give web server user the write permission to the session folder. Otherwise it cannot write session variables to the file. Also, make sure to use session_start(); in home.php as well. So it can have access to the session variables after redirecting. Any sensitive information should not be stored in the Cookies. Note: make sure to give web server user the write permission to the session folder. Otherwise it cannot write session variables to the file. Also, make sure to use session_start(); in home.php as well. So it can have access to the session variables after redirecting. Chapter 3: MVC XML MVC Change permission to write and read directory or files: Read: r = 4 Write: w = 2 Execute: x = 1 adds them up to get the mode code. e.g. sudo chmod 644 filename means: give the file the permission level: -rw-r–r-- sudo chmod 700 filename: give the file the permission level -rwx------ commonly used mode of files: 644: can be read and write by the file owner, also can be read by other group of users and the whole world. 600: can be read and write by the file owner only. html 644 gif 644 jpg 644 css 644 js 644 png 644 php 600 For directory: Read means user can list all content inside a directory(same as the ls command in terminal). Execute means user can go into this directory(same as the cd command). Common permission level for directory: dir 711: user can go into this directory, can read/write all its files. Other groups or outside world can only go into this directory. If they know the exact filename in this directory and has read permission to that particular file. They can read it. But they can’t list all filenames of that directory. dir 755: user can go into this directory, can read/write all its files, Other groups or outside world can go inside and list all files. Use different hostname with only one web server. Change the hosts table file to give your domain a different name locally instead of localhost. sudo vi /etc/hosts Use RewriteModule to manipulate the URL, route user to correct places with cleaner URLs XML XML is an extensible markup language where you can extend structure without breaking existing data and applications. Adding more children to an element is 100% fine. Config files can be XML, you can add more KEYs and variables to the config files later. Escape entities(pre-defined keywords) in XML: &amp;amp;amp; &amp;amp;lt; &amp;amp;gt; &amp;amp;apos; &amp;amp;quot; Declare your own entity: &amp;lt;!ENTITY nbsp &quot;&amp;amp;#160;&quot;&amp;gt; SimpleXML API: See XML in a Tree Structure DOM(Document Object Model) RSS In PHP, we can use SimpleXML API to read an XML file and load its contents. 123456789&gt;? $dom &#x3D; simplexml_load_file(&quot;lectures.xml&quot;); foreach ($dom-&gt;xpath(&quot;&#x2F;lectures&#x2F;lecture&quot;) as $lecture) &#123; print &quot;&quot;; print &quot;Lecture &quot; . $lecture[&quot;number&quot;] . &quot;: &quot;; print $lecture-&gt;title; print &quot;&quot;; &#125;?&lt; Recall dot symbol . in PHP means concatenation. Select XML node from PHP using xpath: If you only want to display certain lecture, for example, lecture 3, you could add additional condition in xpath $dom-&gt;xpath(&quot;/lectures/lecture[@number='3']&quot;) @ is short for attributes, we can expend it as ‘attributes::’ We can also start at any given node and go to its parent, chlid or siblings by define the axis in the path(below is just an example and will not work): $dom-&gt;xpath(&quot;/parent::lectures/child::lecture[attributes::number='3']&quot;) Chapter 4: SQL CSV(comma separated values) PSV(Pipe separated values) pipe is the vertical line in the keyboard | TSV(Tab separated values) CSV fgetcsv: load csv file content into an array of arrays fputcsv: write content to a csv file XML: SimpleXML MySQL: database SQLite: Allow you to use SQL without an actual Database, it is just a file stored in your disk. SQL CREATE ALTER DROP SELECT INSERT UPDATE DELETE Connection MySQL to PHP: MyPHPAdmin 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;? &#x2F;&#x2F; enable sessions session_start(); &#x2F;&#x2F; connect to databsae if (($connection &#x3D; mysqli_connect(&quot;localhost&quot;, &quot;yuan&quot;, &quot;123&quot;, &quot;yuan_lecture&quot;)) &#x3D;&#x3D;&#x3D; false) die(&quot;Could not connect to database&quot;); &#x2F;&#x2F; if username and password were submitted, check them if (isset($_POST[&quot;user&quot;]) &amp;&amp; isset($_POST[&quot;pass&quot;])) &#123; &#x2F;&#x2F; prepare SQL $sql &#x3D; sprintf(&quot;SELECT * FROM users WHERE username&#x3D;&#39;%s&#39;&quot;, mysqli_real_escape_string($connection, $_POST[&quot;user&quot;])); &#x2F;&#x2F; execute query $result &#x3D; mysqli_query($connection, $sql); if ($result &#x3D;&#x3D;&#x3D; false) die(&quot;Could not query database&quot;); &#x2F;&#x2F; check whether we found a row if (mysqli_num_rows($connection, $result) &#x3D;&#x3D; 1) &#123; &#x2F;&#x2F; fetch row $row &#x3D; mysqli_fetch_assoc($connection, $result); &#x2F;&#x2F; check password if ($row[&quot;password&quot;] &#x3D;&#x3D; $_POST[&quot;pass&quot;]) &#123; &#x2F;&#x2F; remember that user&#39;s logged in $_SESSION[&quot;authenticated&quot;] &#x3D; true; &#x2F;&#x2F; redirect user to home page, using absolute path. redirect(&quot;home.php&quot;); &#125; &#125; &#125; function redirect($file) &#123; $host &#x3D; $_SERVER[&quot;HTTP_HOST&quot;]; $path &#x3D; rtrim(dirname($_SERVER[&quot;PHP_SELF&quot;]), &quot;&#x2F;\\\\&quot;); &#x2F;&#x2F; current php directory, trim out unwanted leading slashes header(&quot;Location: http:&#x2F;&#x2F;$host$path&#x2F;$file&quot;); exit; &#125;?&lt; in mysql_connect(), the first variable is the DB server, second is username of the DB, thrid is the password of the DB mysql_real_escape_string is preventing SQL injection attack. $result is a temporary table get from DB mysql_num_rows return number of rows in the temp table mysql_fetch_assoc return an associated array with keys and values from the temp table One way hash password It is not good to store password in the DB in plain text. So we should use a hash function to hash the password into some random characters, whenever user logs in, we use the same algorithm to hash the password and check if it equals to the same random characters in the DB. SQL indexes, Constraints PRIMARY KEY INDEX: doing a binary search or some other tree structure to a column, so it will be much faster to run a query. Cost more disk space. The below image shows that this table has two indexes, and it is using BTREE structure to store all values. UNIQUE FULLTEXT BLOB (Binary Large Object) -&gt; etc. Images Although you can store images into DB, it cost space, it would be better to store it in a folder(file system), and store the link to this file into the DB. Foriegn Key: is a primary key in another table Chapter 5: SQL Continued CRUD (Create Read Update Delete) model VARCHAR will save more space than using CHAR, because it uses different length of chars for each value. CHAR will use same length of chars for all values, but it will run query faster, because it knows the length of values, it can move to next value by adding the length to the current position. (For VARCHAR it has to keep searching until reach to the end of the value.) AI (Auto Increment) Good for IDs, plus one each time. When you delete the ID, the ID number will NOT be reused. Float numbers: 32-bit or 64-bit numbers are finite. If we are using it to represents real numbers(infinite), SQL will round the number to its closest number that it can find. e.g. if you enter 1.9, it might give you 1.89999999 instead. So it is not good to use Float to represent Money etc… important values MySQL Functions PDO(Portable Data Object) Assign variables to values, so when you change database next time, you don’t have to change the code. You just need to change the variables of PDO arguments. JOIN Join tables together. Get data from multiple tables at once. In the above example, we could further create another table that has Product_Id and Product_Name, and use only Product_Id in the orders table. Only the person that have sold products are displayed. 123SELECT Employees.Name, Orders.ProductFROM EmployeesJOIN Orders ON Employees.Employee_ID &#x3D; Orders.Employee_ID This is another syntax to use to join tables. This is the same as the other SQL syntax shown above. Left Join, Right Join Their are differnt type of joins, they decides which table should carry more weight in this query. For example, in the Employees table, if someone has left the company and is no longer exist in the Employees table. But his sell history is still in the orders table. We want to keep the sell history. Then we could use a Right Join, it will display all sell history related. But will show NULL in the Employee_ID(because there is no corresponding Employee_ID in the Employees table). Race Conditions(Atomicity) In computer programming, an operation done by a computer is considered atomic if it is guaranteed to be isolated from other operations that may be happening at the same time. Put another way, atomic operations are indivisible. INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1; This will help you to merge SELECT and UPDATE into one query. Potentially solve the race conditions issue(NOT enough though). Transactions Transactions means do the following queries atomically, do not allow any other queries perform in the meantime. 1234START TRANSACTION;UPDATE ACCOUNT SET BALANCE &#x3D; BALANCE - 1000 WHERE NUMBER &#x3D; 2;UPDATE ACCOUNT SET BALANCE &#x3D; BALANCE + 1000 WHERE NUMBER &#x3D; 1;COMMIT; In the example above, we transfer 1000 balance from account 1 to account 2 at the same time. 123456START TRANSACTION;UPDATE ACCOUNT SET BALANCE &#x3D; BALANCE - 1000 WHERE NUMBER &#x3D; 2;UPDATE ACCOUNT SET BALANCE &#x3D; BALANCE + 1000 WHERE NUMBER &#x3D; 1;SELECT BALANCE FROM ACCOUNT WHERE NUMBER &#x3D; 2;# suppose account number 2 has a negative balance here!ROLLBACK; In the example above, if we find something wrong happened after we perform the transaction, ROLLBACK will undo all the changes in the transaction. Locks (MyISAM) 1234LOCK TABLES account WRITE;SELECT balance FROM account WHERE number &#x3D; 2;UPDATE account SET balance &#x3D; 1500 WHERE number &#x3D; 2;UNLOCK TABLES; ‘LOCK TABLES account WRITE;’’ will lock the table ‘account’, prevening people from WRITING to the table. One downside is that LOCK will lock the entire table, so no one can write to the table even if they are updating other accounts. Add Foreign key constraints so that the DB knows the primary key in one table is related to another foreign key in another table. ON DELETE RESTRICT and ON UPDATE RESTRICT means if we want to delete something that is actually a primary key in a table, DB will reject it because it is related to another tables. If we changed it to CASCADE, then if we delete a record that is a primary key, DB will delete all records that are related to this primary key in other tables too. Chapter 6: JavaScript Javacript, like PHP, is also an interpreted language. Node.js is a server side js language. Global objects Array Boolean Date Function Math Number Object RegExp String Array 123var array &#x3D; [];array[0] &#x3D; &#39;abc&#39;;array.push(&#39;abc&#39;); Array doesn’t have a fixed length, you can use index or ‘push’ method to add elements. Put cursor in form field that is empty. Document is a super global object. It is a DOM(document object model) 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;html&gt;&lt;head&gt;&lt;script&gt; function validate(f) &#123; if (f.email.value &#x3D;&#x3D; &quot;&quot;) &#123; alert(&quot;You must provide an email adddress.&quot;); return false; &#125; else if (f.password1.value &#x3D;&#x3D; &quot;&quot;) &#123; alert(&quot;You must provide a password.&quot;); return false; &#125; else if (f.password1.value !&#x3D; f.password2.value) &#123; alert(&quot;You must provide the same password twice.&quot;); return false; &#125; else if (!f.agreement.checked) &#123; alert(&quot;You must agree to our terms and conditions.&quot;); return false; &#125; return true; &#125;&lt;&#x2F;script&gt;&lt;title&gt;&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;form action&#x3D;&quot;process.php&quot; method&#x3D;&quot;get&quot; name&#x3D;&quot;registration&quot; onsubmit&#x3D;&quot;return validate(this);&quot;&gt; Email: &lt;input name&#x3D;&quot;email&quot; type&#x3D;&quot;text&quot;&gt; &lt;br&gt; Password: &lt;input name&#x3D;&quot;password1&quot; type&#x3D;&quot;password&quot;&gt; &lt;br&gt; Password (again): &lt;input name&#x3D;&quot;password2&quot; type&#x3D;&quot;password&quot;&gt; &lt;br&gt; I agree to the terms and conditions: &lt;input name&#x3D;&quot;agreement&quot; type&#x3D;&quot;checkbox&quot;&gt; &lt;br&gt;&lt;br&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;Submit&quot;&gt;&lt;&#x2F;form&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; check the form fields values at client side using Javascript. validate(this), is passing ‘document.forms.registration’ to the function. Both client side and server side form validation and necessary. User’s can change js using tools like inspector console and bypass the client side validation. Regular Expressions Object 1234var obj &#x3D; &#123;&#125;;obj.key &#x3D; value;obj[&#39;key&#39;] &#x3D; value;var obj &#x3D; &#123;key: value&#125;; Event handlers onblur onchange onclick onfocus onkeydown onkeyup onload onmousedown onmouseup onmouseout onmouseover onresize onselect onsubmit Call a function as a reference 123456789101112131415161718192021222324&lt;html&gt; &lt;head&gt; &lt;script&gt; function blinker() &#123; var blinks &#x3D; document.getElementsByTagName(&quot;blink&quot;); for (var i &#x3D; 0; i &lt; blinks.length; i++) &#123; if (blinks[i].style.visibility &#x3D;&#x3D; &quot;hidden&quot;) blinks[i].style.visibility &#x3D; &quot;visible&quot;; else blinks[i].style.visibility &#x3D; &quot;hidden&quot;; &#125; &#125; window.setInterval(blinker, 500); &lt;&#x2F;script&gt; &lt;title&gt;&lt;&#x2F;title&gt; &lt;&#x2F;head&gt; &lt;body&gt; &lt;center&gt; &lt;blink&gt;&lt;h1&gt;hello, world&lt;&#x2F;h1&gt;&lt;&#x2F;blink&gt; &lt;&#x2F;center&gt; &lt;&#x2F;body&gt;&lt;&#x2F;html&gt; Notice in the code above, when we call the blinker function in window.setInterval(blinker, 500), we are not adding () at the end of the function name. This is because we are passing a reference of that function(blinker) to it. If we use window.setInterval(blinker(), 500), we are passing the return result of blinker function to it, which will not work because blinker function doesn’t return anything. Anonymous function(lambda function) window.setInterval(function() { alert(“HI”); }, 5000); Javascript code can be minimized to save traffic. Variable names and spaces will be shrinked. Chapter 7: AJAX XML and json and data transfer machanism, json tends to be much more popular thesedays XMLHttpRequest abort() getAllResponseHeaders() getResponseHeader() open(method, url) open(method, url, async) open(method, url, async, user) open(method, url, async, user, password) send() send(data) setRequestHeader(header, value) XMLHttpRequest properties onreadystatechange readyState(0 unitialized, 1 open, 2 sent, 3 receiving, 4 loaded) responseBody responseText responseXML status(200 OK, 404 Not Found, 500 Internal Server Error) statusText 1234567891011121314151617181920212223242526272829303132333435363738function handler() &#123; &#x2F;&#x2F; only handle requests in &quot;loaded&quot; state if (xhr.readyState &#x3D;&#x3D; 4) &#123; if (xhr.status &#x3D;&#x3D; 200) &#123; &#x2F;&#x2F; get XML var xml &#x3D; xhr.responseXML; &#x2F;&#x2F; update price var prices &#x3D; xml.getElementsByTagName(&quot;price&quot;); if (prices.length &#x3D;&#x3D; 1) &#123; var price &#x3D; prices[0].firstChild.nodeValue; document.getElementById(&quot;price&quot;).innerHTML &#x3D; price; &#125; &#x2F;&#x2F; update low var lows &#x3D; xml.getElementsByTagName(&quot;low&quot;); if (lows.length &#x3D;&#x3D; 1) &#123; var low &#x3D; lows[0].firstChild.nodeValue; document.getElementById(&quot;low&quot;).innerHTML &#x3D; low; &#125; &#x2F;&#x2F; update high var highs &#x3D; xml.getElementsByTagName(&quot;high&quot;); if (highs.length &#x3D;&#x3D; 1) &#123; var high &#x3D; highs[0].firstChild.nodeValue; document.getElementById(&quot;high&quot;).innerHTML &#x3D; high; &#125; &#125; else alert(&quot;Error with Ajax call!&quot;); &#125; &#125; if PHP return data as XML type, ajax has a function called responseXML, but it is not very easy to get the data you want. Because XML data structure is quite complex. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;html&gt; &lt;head&gt; &lt;script&gt; &#x2F;&#x2F; an XMLHttpRequest var xhr &#x3D; null; &#x2F;* * void * quote() * * Gets a quote. *&#x2F; function quote() &#123; &#x2F;&#x2F; instantiate XMLHttpRequest object try &#123; xhr &#x3D; new XMLHttpRequest(); &#125; catch (e) &#123; xhr &#x3D; new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); &#125; &#x2F;&#x2F; handle old browsers if (xhr &#x3D;&#x3D; null) &#123; alert(&quot;Ajax not supported by your browser!&quot;); return; &#125; &#x2F;&#x2F; get symbol var symbol &#x3D; document.getElementById(&quot;symbol&quot;).value; &#x2F;&#x2F; construct URL var url &#x3D; &quot;quote7.php?symbol&#x3D;&quot; + symbol; &#x2F;&#x2F; get quote xhr.onreadystatechange &#x3D; function() &#123; &#x2F;&#x2F; only handle loaded requests if (xhr.readyState &#x3D;&#x3D; 4) &#123; if (xhr.status &#x3D;&#x3D; 200) &#123; &#x2F;&#x2F; evaluate JSON var quote &#x3D; eval(&quot;(&quot; + xhr.responseText + &quot;)&quot;); &#x2F;&#x2F; show JSON in textarea document.getElementById(&quot;code&quot;).value &#x3D; xhr.responseText; &#x2F;&#x2F; insert quote into DOM var div &#x3D; document.createElement(&quot;div&quot;); var text &#x3D; document.createTextNode(symbol + &quot;: &quot; + quote.price); div.appendChild(text); document.getElementById(&quot;quotes&quot;).appendChild(div); &#125; else alert(&quot;Error with Ajax call!&quot;); &#125; &#125; xhr.open(&quot;GET&quot;, url, true); xhr.send(null); &#125; &lt;&#x2F;script&gt; &lt;title&gt;&lt;&#x2F;title&gt; &lt;&#x2F;head&gt; &lt;body&gt; &lt;form onsubmit&#x3D;&quot;quote(); return false;&quot;&gt; Symbol: &lt;input id&#x3D;&quot;symbol&quot; type&#x3D;&quot;text&quot;&gt; &lt;br&gt;&lt;br&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;Get Quote&quot;&gt; &lt;&#x2F;form&gt; &lt;br&gt;&lt;br&gt; &lt;div id&#x3D;&quot;quotes&quot;&gt;&lt;&#x2F;div&gt; &lt;br&gt;&lt;br&gt; &lt;textarea cols&#x3D;&quot;80&quot; id&#x3D;&quot;code&quot; rows&#x3D;&quot;16&quot;&gt;&lt;&#x2F;textarea&gt; &lt;&#x2F;body&gt;&lt;&#x2F;html&gt; Encode ajax response to json and return to js as an object. json_encode is a PHP function that convert data into json string. ‘eval’ is a js function that convert json data into js object. 12345678910111213141516$(document).ready(function() &#123; $(&quot;#form&quot;).submit(function() &#123; $.ajax(&#123; url: &quot;quote7.php&quot;, data: &#123; symbol: $(&quot;#symbol&quot;).val() &#125;, success: function(data) &#123; $(&quot;#price&quot;).html(data.price); $(&quot;#high&quot;).html(data.high); $(&quot;#low&quot;).html(data.low); &#125; &#125;); return false; &#125;);&#125;); Another even cleaner example is to using jQuery. Dollar sign $ is short for ‘jQuery’. $(&quot;#form&quot;) is equals to jQuery(&quot;#form&quot;) jQuery is a library that provides more functionality to your code. $(document) is passing document to the jQuery library so you can use the extra features that jQuery provides. .ready is to make sure the whole html page is being loaded correctly. Because some of the code in ready function may require html elements. .ajax function takes one argument, which is in the parentheses, the argument is an object. JS only has two data structures, array and object, which represented by [] and {} respectively. the object in .ajax function has three keys, url, data and success. url takes a string, which is the url we will go to. data takes an object, which is the data we send. success takes an anonymous function, it is only being called when ajax request is success.(state is ready and response code is 200) the anonymous function after the .submit event handler return false because we don’t want to submit the form again, we already got the data from the ajax call. Content Types Specify the data type of the current file. HTML (text/html) (default) XML (text/XML) JSON (application/json) PHP + JSON json_encode($value): convert data to json on server side to be ready to send eval(string): convert data from json to object so we can access it on the client side Same Origin Policy We are not allowed to display data we get from another domain. We can make ajax call at JS and get response, but we cannot display it. You cannot embed it into your DOM. CORS can override this. Chapter 8: Security Obvious Threats Telnet, FTP, HTTP, MySQL suPHP, all users can only access to their own files. No one can delete or modify your files. Session Hijacking(scenarios) Physical Access Packet Sniffing: For website not using HTTPS Session Fixation: Guess the session ID XSS: Cross site scripting attack SSL Certificate Public key Cryptography There is a public key and a private key. Imagine we want to buy something from a website. Before we sending credit card information to the website. The website will send me the public key(can be see by everyone). And we are going to use the public key to encrypt the information. Only the private key can be used to decrypt the information. And only the website has the private key. Likewise, if the website wants to send me some information. I need to send it the public key too. And use my private key to decrypt it when I receive the information. SQL injection attacks mysql_real_escape_string(): put backslash to quote etc… Same-Origin Policy: You can only get data from the same origin as your HTML DOM. (ajax calls) CORS can override same origin policy, jsonp Attacks CSRF (Cross-site scripting forgery): If you login to some stock trade website recently and saved your login session. And you visited the bad website in the meantime and being tricked to click the link in their website. The link is actually a request and will send a request to the stock trade website to buy some stock. Note: User click a link could send both GET/POST request. Forms can be hidden and submitted by JS code. So just change the buying request to POST request doesn’t solve this issue. 1.The way to prevent this is whenever you want to send a request, the website will send a random session token back to you. And you have to add this token in the request to be able to make the request work. The CSRF link will never know what the token is. 2.CAPTCHA: enter some words that easy for human to see but not for machine. 3.Whenever you want to checkout, the website will make you log out immediately when you click the checkout button. XSS You can access to the Cookie from JS And there is a flawed website which writing values to its body, and you can be tricked to click the link and send your Cookie to other people. An attacker can use XSS to send a malicious script to an unsuspecting user. The end user’s browser has no way to know that the script should not be trusted, and will execute the script. Because it thinks the script came from a trusted source, the malicious script can access any cookies, session tokens, or other sensitive information retained by the browser and used with that site. Escape HTML special chars Chapter 9: Scalability Vertical Scaling 增加服务器的CPU，内存，硬盘等，但总会不够用 CPU: cores, L2 cache… Disk: PATA, SATA, SAS, RAID RAID(Redundent array of independent disks) RAID 0 :如果你有n块磁盘，原来只能同时写一块磁盘，写满了再下一块，做了RAID 0之后，n块可以同时写，速度提升很快，但由于没有备份，可靠性很差。n最少为2。 RAID 1: 正因为RAID 0太不可靠，所以衍生出了RAID1。如果你有n块磁盘，把其中n/2块磁盘作为镜像磁盘，在往其中一块磁盘写入数据时，也同时往另一块写数据。坏了其中一块时，镜像磁盘自动顶上，可靠性最佳，但空间利用率太低。n最少为2。 RAID 10: 是RAID 0 和RAID 1 的结合，同时写入n/2的硬盘并将剩下n/2作为备份，可靠性和速度都有，但是需要两倍的钱。 RAID 3：为了说明白RAID 5，先说RAID 3.RAID 3是若你有n块盘，其中1块盘作为校验盘，剩余n-1块盘相当于作RAID 0同时读写，当其中一块盘坏掉时，可以通过校验码还原出坏掉盘的原始数据。这个校验方式比较特别，奇偶检验，1 XOR 0 XOR 1=0，0 XOR 1 XOR 0=1，最后的数据时校验数据，当中间缺了一个数据时，可以通过其他盘的数据和校验数据推算出来。但是这有个问题，由于n-1块盘做了RAID 0，每一次读写都要牵动所有盘来为它服务，而且万一校验盘坏掉就完蛋了。最多允许坏一块盘。n最少为3. RAID 5：在RAID 3的基础上有所区别，同样是相当于是1块盘的大小作为校验盘，n-1块盘的大小作为数据盘，但校验码分布在各个磁盘中，不是单独的一块磁盘，也就是分布式校验盘，这样做好处多多。最多坏一块盘。n最少为3. RAID 6：在RAID 5的基础上，又增加了一种校验码，和解方程似的，一种校验码一个方程，最多有两个未知数，也就是最多坏两块盘。 RAM Horizontal Scaling 增加更多的服务器，而不是提升每个服务器的配置。当我们拥有多于一个服务器时，当用户向服务器发送请求时，我们要一个load balancer去将进来的request平均分配给所有的服务器。load balancer拥有一个public IP。而每一个服务器有一个private IP，他们不需要public IP Load Balancing 如何给服务器平均分配request？我们可以将所有可用的服务器IP列出来，第一个request给第一个服务器，第二个request给第二个服务器，以此类推直到回到第一个，然后循环，这种方法叫做round-robin，优点是他不需要主动询问服务器的当前状态如何 Caching 当用户通过load balancer登录到一号服务器时，他的登录信息如果保存在一号服务器，那么在他下一个request被分配到其他服务器时，他就需要再次登录，如果他在使用一个购物网站，他将一件商品加入到一号服务器的购物车中，然后又在二号服务器登录却找不到他的购物车，也不能结帐，这就会成为一个大问题 Shared Session State(Sticky session) 我们可以将session，也就是用户信息储存在另外一个服务器中 Shared Storage FC (Fiber Channel), iSCSI, MySQL, NFS Replicate your database, use more than one database to store sessions in case one goes down Cookie 当用户首次登陆时，load balancer可以想用户电脑中加入一个cookie，包含一些加密的服务器信息，所以当用户在短时间内再次访问时，load balancer就知道该将用户的请求发送到哪个服务器 Load Balancer Software: ELB(Amazon’s Elastic Load Balancer), HAProxy(High Availability Proxy), LVS(Linux Virtual Server) Hardware: Barracuda, Cisco, Citrix, F5 PHP Accelerators 当我们在调用python程序时，我们需要先将以py结尾的代码文件编译成可直接执行的文件，然后再运行可执行文件得到结果。这样做的目的是当我们想再次得到结果时，我们不需要再次编译，可以直接运行执行文件。 这样做可以提升效率，但是如果我们有任何的代码改动，我们就需要重新编译。 PHP Accelerators有一样的逻辑，用户在发送相同的请求时，网站会直接运行可执行文件以提升速度 Caching .html Caching就是一种将你经常访问的数据提前保存到你的电脑上以便下次快速显示的技术。对于PHP来说，HTML网页是自动生成的，意味着每次用户访问时PHP都会重新生成一个新的重复的HTML文件。 如果我们将所有的HTML网页都事先编译好储存在服务器上，用户访问时就可以快速拿到这些静态网页，因为不需要每次都进行编译。这就是一种Caching 这样做的坏处是，当你需要更改整个网站的风格时，你就需要更改所有的HTML文件。 MySQL Query Cache：MySQL会将一些query的结果caching，第一次运行时如果你的table很大，或者你要找的column没有index，他会运行一段时间，但是下次你就会更快的看到结果 memcached memory cache， 内存的读写要比硬盘快很多，所以我们如果有一百万个用户，服务器SQL拿到一个用户数据可能会需要很长时间，我们可以将这个用户数据保存在memory里面，下次就可以快速得到 下面是PHP将用户数据保存到memcache的代码 12345678910$memcache &#x3D; memchache_connect(HOST, PORT);$user &#x3D; memcache_get($memcache, $id);&#x2F;&#x2F; 如果内存里面没有这个用户的id，我们就从数据库中拿取，之后把他添加到内存中if (is_null($user))&#123; $bdh &#x3D; new PDO(DSN, USER, PASS; $result &#x3D; $dbh-&gt;query(&quot;SELECT * FROM users WHERE id &#x3D; $id&quot;); $user &#x3D; $result-&gt;fetch(PDO:FETCH_ASSOC); &#x2F;&#x2F; this is to get the associated array of data(username, email address,...) memcache_set($memcache, $user[&#39;id&#39;], $user)&#125; 如果我们一直将数据添加到内存中，内存总有一天会不够用，这时我们就需要删除一些数据来释放空间，我们可以删除最早的数据（LRU， Least Recent Used）或者最少用到的数据(LFU, Least Frequent Used) Data Replication: Master: slave 主从关系的服务器复制，所有的附属服务器要从主服务器中拿取数据，要将新数据写入主服务器，一切以主服务器为准，优点在于当主服务器down机时，我们可以自动化一个过程：因为所有服务器的数据都是一样的，我们可以将一个附属服务器晋升为新的主服务器。以保证服务不间断 适用于读多于写的网站，所有的读取都去附属服务器，所有的写入都去主服务器 缺点是当主服务器down机时，写入会短暂失效一段时间直到其中一个附属服务器成为新的主服务器 Data Replication: Master: Master - 当其中一个主服务器失效时，我们还有另外一个，从而保证不会有服务间断的时间.同样，读取请求发送到附属服务器，写入发送到主服务器 上面的图片还是有一个缺点，就是如果load balancer失效了，整个服务还是会断开。所以我们需要有两个相同作用的load balancer Load Balancer: active: active 每个load balancer都负责分配任务，并且他们会不断的每个一段时间向另一个load balancer发送一个heart beat，以证明自己的存在。如果任何一个load balancer没有收到另一个的心跳，他就将负责所有的流量 Load Balancer: active: passive 与之前相似，只不过一开始只有一个load balancer负责所有的流量，并且向passive的load balancer发送心跳，如果passive load balancer没有接收到心跳，他就把自己提升为active load balancer，并开始负责所有的任务。 Partitioning 将整个服务系统复制，供多个不同的客户使用，Facebook早期将不同学校的用户分到不同的服务器中，类似于harvard.facebook.com, MIT.facebook.com,以此来降低流量的压力。这样的话当你想联系不同大学的人时，就会有些困难。另外一个例子是我们可以将用户分配到不同的服务器中based on他们的名字，A-M到第一个，N-Z到第二个 High Availiability 不同的服务器之间互相听取对方的心跳，并随时准备take over当另外的服务器offline 网络层和web server层之间需要load balancer web server层和DB层之间也需要load balancer，load balancer会将第一个返回的DB的信息加到Cookie中返回给用户，这样用户在Cookie过期之前都会被route到同一个服务器, 这样就保证用户不会被分配到另一个服务器里面却没有他最新的数据，服务器之间也会相互同步。 每一层之间的load balancer也需要多个以保证一个offlice不会影响全局。可以使用active active或者active passive， DB也需要多个，可以是Master Master或者Master Slave. 最后就是这样的一个Data center也需要多个，就像AWS一样在US， Aisa， Europe都会有服务器。 Security 什么样的traffic可以进入data center？TCP 80和443 什么样的traffic可以从load balancer到web server？TCP 80. 我们可以在load balancer中加入证书并解密所有的traffic，然后之后的所有traffic都保持不加密的状态，因为我们已经进入到data center，不需要在担心安全问题，所以让load balancer去做揭秘这样的繁重工作，web server只负责应付无秘traffic 什么样的traffic从web server到DB？一般的SQL queries 也是 TCP 3306（port number 3306 is the default number SQL query uses） 注意web server之间并不能交流。 我们之所以设置这些规则，只让这些port的traffic进入，是因为加如其中一个web server被攻占了，那么它也只能向DB发送SQL请求，不能向其他web server发送443或者80请求，将破坏控制在最小。 Reference S75 (Summer 2012) Lecture 9 Scalability Harvard Web Development David Malan","categories":[],"tags":[{"name":"Web Development","slug":"Web-Development","permalink":"http://hellcy.github.io/tags/Web-Development/"}]},{"title":"Data Structures Advanced","slug":"Data-Structures-Advanced","date":"2020-05-10T06:33:35.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2020/05/10/Data-Structures-Advanced/","link":"","permalink":"http://hellcy.github.io/2020/05/10/Data-Structures-Advanced/","excerpt":"","text":"sliding window 123456789101112&#x2F;&#x2F;通过两层for循环改进算法for (i &#x3D; 0, i &lt; n; ++i) &#123; while (j &lt; n) &#123; if (满足条件) &#123; j++; 更新j状态 &#125; else if (不满足条件) &#123; break; &#125; &#125; &#x2F;&#x2F;更新i状态&#125; 在一维字符串或者数组中找到符合条件的字串 前向型指针题目 窗口类 remove nth node from end of list Minimum size subarray sum Longest substring without rapeating characters Minimum window substring Longest Substring with at most k distinct characters Longest Repeating Character Replacement Longest Turbulent subarray 快慢类 find the middle of the linked list linked list cycle 1 and 2 优化类型 优化思想通过两层for循环而来 外层指针依然是一次遍历 内层指针证明是否需要退回 two pointers题型分类 前向型 窗口型 快慢型 相向型 两个数组 第k大或者第k小问题 Union Find 并查集 一种用来解决集合查询合并的数据结构 支持O(1) find and O(1) union，O(n) space 检查两个元素是否属于同一集合 合并两个集合，将root node指向另一个root node 可以使用array或者哈希表实现Union find 查找 find O(n) time 123456public int find(int x) &#123; if (father[x] &#x3D;&#x3D; x) &#123; return x; &#125; return find(father[x]);&#125; 合并 union, O(n) time, 之合并两个root nodes，不管child nodes 12345678public void union(int a, int b) &#123; int root_a &#x3D; find(a); int root_b &#x3D; find(b); &#x2F;&#x2F; 如果两个root nodes已经相等，则两个集合已经合并，不需要再操作 if (root_a !&#x3D; root_b) &#123; father[root_a] &#x3D; root_b; &#x2F;&#x2F; 将root_a指向root_b &#125;&#125; 路径压缩：将find和union的时间复杂度变成O(1) 在查找元素的father的时候，第一次需要O(n)的时间，但是当找到最终的father时，将每一个路径中的father都更新成最终的father，以后在查找路径中的father就只需要O(1) 123456public int find(int x) &#123; if (father[x] &#x3D;&#x3D; x) &#123; return x; &#125; return father[x] &#x3D; find(father[x]); &#x2F;&#x2F; 将当前点的father更新为最终father&#125; Connecting graph 3 Connecting cities with minimum costs Number of Operations to Make Network Connected The Earliest Moment When Everyone Become Friends Lexicographically Smallest Equivalent String Number of Islands II Union find问题总结 原生操作 查询两个元素是否在用一个集合内。 合并两个元素所在的集合 派生操作 查询某个元素所在集合的元素个数。 查询当前集合的个数 Trie 常见考点 Trie直接实现 利用Trie前缀特性解题 矩阵类里面，字符串一个一个字符，深度优先遍历的问题 Trie和Hash拥有相同的时间复杂度，但是Trie的空间复杂度更小 Implement Trie Add and search word word search 2 -&gt; Trie + DFS Heap Trapping rain water 2 怎么通过Trapping rain water 1拓展到这道题的思路 怎么样想到利用heap？ 怎么想到由外向内遍历？ Find Median from Data Stream Sliding Window Median","categories":[],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://hellcy.github.io/tags/Data-Structures/"}]},{"title":"Think Python","slug":"Think-Python","date":"2020-04-03T03:18:28.000Z","updated":"2021-05-24T07:27:02.660Z","comments":true,"path":"2020/04/03/Think-Python/","link":"","permalink":"http://hellcy.github.io/2020/04/03/Think-Python/","excerpt":"","text":"Chapter 1: The First Program Chapter 2: Variables, expressions and statements Chapter 3: Functions Chapter 4: Conditionals and recursion Chapter 5: Fruitful functions Chapter 6: Iteration Chapter 7: Strings Chapter 8: Lists Chapter 9: Dictionaries Chapter 10: Tuples Chapter 1: The First Program Python interpreter Type python in the command line to start interpreter 1234Python 3.4.0 (default, Jun 19 2015, 14:20:21)[GCC 4.8.2] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; The first three lines contain information about the interpreter and the operating system it’s running on, so it might be different for you. But you should check that the version number, which is 3.4.0 in this example, begins with 3, which indicates that you are running Python 3. If it begins with 2, you are running (you guessed it) Python 2. The last line is a prompt that indicates that the interpreter is ready for you to enter pre. If you type a line of pre and hit Enter, the interpreter displays the result: 12&gt;&gt;&gt; 1 + 12 Basic Arthmetic 123456&gt;&gt;&gt; 40 + 242&gt;&gt;&gt; 43 - 142&gt;&gt;&gt; 6 * 742 The operator / performs division: 12&gt;&gt;&gt; 84 &#x2F; 242.0 You might wonder why the result is 42.0 instead of 42. I’ll explain in the next section. Finally, the operator ** performs exponentiation; that is, it raises a number to a power: 12&gt;&gt;&gt; 6**2 + 642 In some other languages, ^ is used for exponentiation, but in Python it is a bitwise operator called XOR. If you are not familiar with bitwise operators, the result will surprise you: 12&gt;&gt;&gt; 6 ^ 24 Values and Types If you are not sure what type a value has, the interpreter can tell you: 123456&gt;&gt;&gt; type(2)&lt; class &#39;int&#39;&gt;&gt;&gt;&gt; type(42.0)&lt; class &#39;float&#39;&gt;&gt;&gt;&gt; type(&#39;Hello, World!&#39;)&lt; class &#39;str&#39;&gt; Chapter 2: Variables, Expressions and Statements Statement An assignment statement creates a new variable and gives it a value: 123&gt;&gt;&gt; message &#x3D; &#39;And now for something completely different&#39;&gt;&gt;&gt; n &#x3D; 17&gt;&gt;&gt; pi &#x3D; 3.1415926535897932 Expressions An expression is a combination of values, variables, and operators. A value all by itself is considered an expression, and so is a variable, so the following are all legal expressions: 123456&gt;&gt;&gt; 4242&gt;&gt;&gt; n17&gt;&gt;&gt; n + 2542 When you type an expression at the prompt, the interpreter evaluates it, which means that it finds the value of the expression. In this example, n has the value 17 and n + 25 has the value 42. A statement is a unit of pre that has an effect, like creating a variable or displaying a value. Order of operations Parentheses have the highest precedence and can be used to force an expression to evaluate in the order you want. Since expressions in parentheses are evaluated first, 2 * (3-1) is 4, and (1+1)**(5-2) is 8. You can also use parentheses to make an expression easier to read, as in (minute * 100) / 60, even if it doesn’t change the result. Exponentiation has the next highest precedence, so 1 + 2**3 is 9, not 27, and 2 * 3**2 is 18, not 36. Multiplication and Division have higher precedence than Addition and Subtraction. So 2*3-1 is 5, not 4, and 6+4/2 is 8, not 5. Operators with the same precedence are evaluated from left to right (except exponentiation). So in the expression degrees / 2 * pi, the division happens first and the result is multiplied by pi. To divide by 2π, you can use parentheses or write degrees / 2 / pi. Comments comments, and they start with the # symbol. “”“fdsfsdfsd”&quot;&quot; for multiple lines of comments Chapter 3: Functions In the context of programming, a function is a named sequence of statements that performs a computation. When you define a function, you specify the name and the sequence of statements. Later, you can “call” the function by name. It is common to say that a function “takes” an argument and “returns” a result. The result is also called the return value. Python has a math module that provides most of the familiar mathematical functions. A module is a file that contains a collection of related functions. Before we can use the functions in a module, we have to import it with an import statement: 1&gt;&gt;&gt;&gt; import math Adding new functions So far, we have only been using the functions that come with Python, but it is also possible to add new functions. A function definition specifies the name of a new function and the sequence of statements that run when the function is called. 123def print_lyrics(): print(&quot;I&#39;m a lumberjack, and I&#39;m okay.&quot;) print(&quot;I sleep all night and I work all day.&quot;) The first line of the function definition is called the header; the rest is called the body. The header has to end with a colon and the body has to be indented. By convention, indentation is always four spaces. The body can contain any number of statements. A function can be inside another function Parameters and arguments Some of the functions we have seen require arguments. For example, when you call math.sin you pass a number as an argument. Some functions take more than one argument: math.pow takes two, the base and the exponent When you create a variable inside a function, it is local, which means that it only exists inside the function Encapsulation Wrapping a piece of pre up in a function is called encapsulation. One of the benefits of encapsulation is that it attaches a name to the pre, which serves as a kind of documentation. Another advantage is that if you re-use the pre, it is more concise to call a function twice than to copy and paste the body! Chapter 4: Conditionals and recursion Floor divisions and modulus The floor division operator, //, divides two numbers and rounds down to an integer e the modulus operator, %, which divides two numbers and returns the remainder Boolean expressions A boolean expression is an expression that is either true or false. The following examples use the operator ==, which compares two operands and produces True if they are equal and False otherwise: 1234&gt;&gt;&gt; 5 &#x3D;&#x3D; 5True&gt;&gt;&gt; 5 &#x3D;&#x3D; 6False Logical operators AND OR NOT Conditional execution 1234if x % 2 &#x3D;&#x3D; 0: print(&#39;x is even&#39;)else: print(&#39;x is odd&#39;) Chained conditions 123456if x &lt; y: print(&#39;x is less than y&#39;)elif x &gt; y: print(&#39;x is greater than y&#39;)else: print(&#39;x and y are equal&#39;) Recursion It is legal for one function to call another; it is also legal for a function to call itself. 123456def countdown(n): if n &lt;&#x3D; 0: print(&#39;Blastoff!&#39;) else: print(n) countdown(n-1) Keyboard input Python provides a built-in function called input that stops the program and waits for the user to type something. When the user presses Return or Enter, the program resumes and input returns what the user typed as a string 1234&gt;&gt;&gt; text &#x3D; input()What are you waiting for?&gt;&gt;&gt; text&#39;What are you waiting for?&#39; Chapter 5: Fruitful functions Functions that return a value 12345def absolute_value(x): if x &lt; 0: return -x else: return x Incremental development Start with a working program and make small incremental changes. At any point, if there is an error, you should have a good idea where it is. Chapter 6: Iteration The ability to run a block of statements repeatedly The while statement 12345def countdown(n): while n &gt; 0: print(n) n &#x3D; n - 1 print(&#39;Blastoff!&#39;) Determine whether the condition is true or false. If false, exit the while statement and continue execution at the next statement. If the condition is true, run the body and then go back to step 1. Break Sometimes you don’t know it’s time to end a loop until you get half way through the body. In that case you can use the break statement to jump out of the loop 123456while True: line &#x3D; input(&#39;&gt; &#39;) if line &#x3D;&#x3D; &#39;done&#39;: break print(line)print(&#39;Done!&#39;) Chapter 7: Strings Strings are not like integers, floats, and booleans. A string is a sequence of characters, which means it is an ordered collection of other values. In this chapter you’ll see how to access the characters that make up a string, and you’ll learn about some of the methods strings provide 123456&gt;&gt;&gt; fruit &#x3D; &#39;banana&#39;&gt;&gt;&gt; letter &#x3D; fruit[1]&gt;&gt;&gt; letter&#39;a&#39;&gt;&gt;&gt; len(fruit)6 Traversal with a for loop A lot of computations involve processing a string one character at a time. Often they start at the beginning, select each character in turn, do something to it, and continue until the end. This pattern of processing is called a traversal. One way to write a traversal is with a while loop: 12345index &#x3D; 0while index &lt; len(fruit): letter &#x3D; fruit[index] print(letter) index &#x3D; index + 1 another way to traverse is to use a for loop 12for letter in fruit: print(letter) Strings slices A segment of a string is called a slice. Selecting a slice is similar to selecting a character: 12345&gt;&gt;&gt; s &#x3D; &#39;Monty Python&#39;&gt;&gt;&gt; s[0:5]&#39;Monty&#39;&gt;&gt;&gt; s[6:12]&#39;Python&#39; The operator [n:m] returns the part of the string from the “n-eth” character to the “m-eth” character, including the first but excluding the last If you omit the first index (before the colon), the slice starts at the beginning of the string. If you omit the second index, the slice goes to the end of the string: 12345678&gt;&gt;&gt; fruit &#x3D; &#39;banana&#39;&gt;&gt;&gt; fruit[:3]&#39;ban&#39;&gt;&gt;&gt; fruit[3:]&#39;ana&#39;&gt;&gt;&gt; fruit &#x3D; &#39;banana&#39;&gt;&gt;&gt; fruit[3:3]&#39;&#39; Strings are immutable 123&gt;&gt;&gt; greeting &#x3D; &#39;Hello, world!&#39;&gt;&gt;&gt; greeting[0] &#x3D; &#39;J&#39;TypeError: &#39;str&#39; object does not support item assignment The in operator The word in is a boolean operator that takes two strings and returns True if the first appears as a substring in the second: 1234&gt;&gt;&gt; &#39;a&#39; in &#39;banana&#39;True&gt;&gt;&gt; &#39;seed&#39; in &#39;banana&#39;False String comparison Upper case letters come before lower case letters, we can use &gt; or &lt; or == to compare strings Chapter 8: Lists List is a sequence Like a string, a list is a sequence of values. In a string, the values are characters; in a list, they can be any type. The values in a list are called elements or sometimes items. There are several ways to create a new list; the simplest is to enclose the elements in square brackets ([ and ]): The elements of a list don’t have to be the same type. 12345&gt;&gt;&gt; cheeses &#x3D; [&#39;Cheddar&#39;, &#39;Edam&#39;, &#39;Gouda&#39;]&gt;&gt;&gt; numbers &#x3D; [42, 123]&gt;&gt;&gt; empty &#x3D; []&gt;&gt;&gt; print(cheeses, numbers, empty)[&#39;Cheddar&#39;, &#39;Edam&#39;, &#39;Gouda&#39;] [42, 123] [] Lists are mutable 1234&gt;&gt;&gt; numbers &#x3D; [42, 123]&gt;&gt;&gt; numbers[1] &#x3D; 5&gt;&gt;&gt; numbers[42, 5] Traversing a list 12for i in range(len(numbers)): numbers[i] &#x3D; numbers[i] * 2 List operations 123456789&gt;&gt;&gt; a &#x3D; [1, 2, 3]&gt;&gt;&gt; b &#x3D; [4, 5, 6]&gt;&gt;&gt; c &#x3D; a + b&gt;&gt;&gt; c[1, 2, 3, 4, 5, 6]&gt;&gt;&gt; [0] * 4[0, 0, 0, 0]&gt;&gt;&gt; [1, 2, 3] * 3[1, 2, 3, 1, 2, 3, 1, 2, 3] List slices 1234567&gt;&gt;&gt; t &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]&gt;&gt;&gt; t[1:3][&#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; t[:4][&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]&gt;&gt;&gt; t[3:][&#39;d&#39;, &#39;e&#39;, &#39;f&#39;] List methods append adds a new element to the end of a list: extend takes a list as an argument and appends all of the elements: sort arranges the elements of the list from low to high: 12345678910111213&gt;&gt;&gt; t &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; t.append(&#39;d&#39;)&gt;&gt;&gt; t[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]&gt;&gt;&gt; t1 &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; t2 &#x3D; [&#39;d&#39;, &#39;e&#39;]&gt;&gt;&gt; t1.extend(t2)&gt;&gt;&gt; t1[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]&gt;&gt;&gt; t &#x3D; [&#39;d&#39;, &#39;c&#39;, &#39;e&#39;, &#39;b&#39;, &#39;a&#39;]&gt;&gt;&gt; t.sort()&gt;&gt;&gt; t[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] Deleting elements If you know the index of the element you want, you can use pop: pop modifies the list and returns the element that was removed. If you don’t provide an index, it deletes and returns the last element. 123456&gt;&gt;&gt; t &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; x &#x3D; t.pop(1)&gt;&gt;&gt; t[&#39;a&#39;, &#39;c&#39;]&gt;&gt;&gt; x&#39;b&#39; If you don’t need the removed value, you can use the del operator: 1234&gt;&gt;&gt; t &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; del t[1]&gt;&gt;&gt; t[&#39;a&#39;, &#39;c&#39;] If you know the element you want to remove (but not the index), you can use remove: 1234&gt;&gt;&gt; t &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; t.remove(&#39;b&#39;)&gt;&gt;&gt; t[&#39;a&#39;, &#39;c&#39;] Objects and values In one case, a and b refer to two different objects that have the same value. In the second case, they refer to the same object. 12345678910&gt;&gt;&gt; a &#x3D; &quot;word1&quot;&gt;&gt;&gt; b &#x3D; &quot;word1&quot;&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a &#x3D; [1,2,3]&gt;&gt;&gt; b &#x3D; [1,2,3]&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a &#x3D;&#x3D; bTrue Aliasing If a refers to an object and you assign b = a, then both variables refer to the same object, even for a list Its also called shallow copy 1234&gt;&gt;&gt; a &#x3D; [1, 2, 3]&gt;&gt;&gt; b &#x3D; a&gt;&gt;&gt; b is aTrue For objects that may take lots of memory space, the program will prefer to only make a reference to its original object when you create another variable to points to it. List arguments When you pass a list to a function, the function gets a reference to the list. If the function modifies the list, the caller sees the change. 1234567def delete_head(t): del t[0]&gt;&gt;&gt; letters &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]&gt;&gt;&gt; delete_head(letters)&gt;&gt;&gt; letters[&#39;b&#39;, &#39;c&#39;] Chapter 9: Dictionaries A dictionary contains a collection of indices, which are called keys, and a collection of values. Each key is associated with a single value. The association of a key and a value is called a key-value pair or sometimes an item The function dict creates a new dictionary with no items. Because dict is the name of a built-in function, you should avoid using it as a variable name 123456&gt;&gt;&gt; eng2sp &#x3D; dict()&gt;&gt;&gt; eng2sp&#123;&#125;&gt;&gt;&gt; eng2sp[&#39;one&#39;] &#x3D; &#39;uno&#39; # add an item with key &#x3D; &#39;one&#39; and value &#x3D; &#39;uno&#39;&gt;&gt;&gt; eng2sp&#123;&#39;one&#39;: &#39;uno&#39;&#125; The order of the key-value pairs might not be the same. If you type the same example on your computer, you might get a different result. In general, the order of items in a dictionary is unpredictable. This is because when we insert items into the dictionary, it randomly put items into buckets, so it could be different everytime. (Hash) check whether a dictionary contains some key 1234&gt;&gt;&gt; &#39;one&#39; in eng2spTrue&gt;&gt;&gt; &#39;uno&#39; in eng2spFalse check whether a dictionary contains some value 123&gt;&gt;&gt; vals &#x3D; eng2sp.values()&gt;&gt;&gt; &#39;uno&#39; in valsTrue Python dictionaries use a data structure called a hashtable that has a remarkable property: the in operator takes about the same amount of time no matter how many items are in the dictionary. Instant lookup time! Dictionary as a collection of counters Suppose you are given a string and you want to count how many times each letter appears. There are several ways you could do it You could create 26 variables, one for each letter of the alphabet. Then you could traverse the string and, for each character, increment the corresponding counter, probably using a chained conditional. You could create a list with 26 elements. Then you could convert each character to a number (using the built-in function ord), use the number as an index into the list, and increment the appropriate counter. You could create a dictionary with characters as keys and counters as the corresponding values. The first time you see a character, you would add an item to the dictionary. After that you would increment the value of an existing item. An implementation is a way of performing a computation; some implementations are better than others. For example, an advantage of the dictionary implementation is that we don’t have to know ahead of time which letters appear in the string and we only have to make room for the letters that do appear. 12345678def histogram(s): d &#x3D; dict() for c in s: if c not in d: d[c] &#x3D; 1 else: d[c] +&#x3D; 1 return d Looping and dictionaries 123def print_hist(h): for c in h: print(c, h[c]) Given a dictionary d and a key k, it is easy to find the corresponding value v = d[k]. This operation is called a lookup. But what if you have v and you want to find k? You have two problems: first, there might be more than one key that maps to the value v. Depending on the application, you might be able to pick one, or you might have to make a list that contains all of them. Second, there is no simple syntax to do a reverse lookup; you have to search 12345def reverse_lookup(d, v): for k in d: if d[k] &#x3D;&#x3D; v: return k raise LookupError() Chapter 10: Tuples Tuples are immutable A tuple is a sequence of values. The values can be any type, and they are indexed by integers, so in that respect tuples are a lot like lists. The important difference is that tuples are immutable. 123&gt;&gt;&gt; t &#x3D; tuple()&gt;&gt;&gt; t() Most list operators also work on tuples. The bracket operator indexes an element: 1234567&gt;&gt;&gt; t &#x3D; (&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;)&gt;&gt;&gt; t[0]&#39;a&#39;&gt;&gt;&gt; t[1:3](&#39;b&#39;, &#39;c&#39;)&gt;&gt;&gt; t[0] &#x3D; &#39;A&#39;TypeError: object doesn&#39;t support item assignment Tuples and return values Strictly speaking, a function can only return one value, but if the value is a tuple, the effect is the same as returning multiple values. For example, if you want to divide two integers and compute the quotient and remainder, it is inefficient to compute x//y and then x%y. It is better to compute them both at the same time The built-in function divmod takes two arguments and returns a tuple of two values, the quotient and remainder. You can store the result as a tuple: 123&gt;&gt;&gt; t &#x3D; divmod(7, 3)&gt;&gt;&gt; t(2, 1)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://hellcy.github.io/tags/Python/"}]},{"title":"Data Structures","slug":"Data-Structures","date":"2020-03-02T03:04:10.000Z","updated":"2021-05-24T07:27:02.658Z","comments":true,"path":"2020/03/02/Data-Structures/","link":"","permalink":"http://hellcy.github.io/2020/03/02/Data-Structures/","excerpt":"","text":"Bianry Search Use O(1) to make a T(n) problem T(n/2). So the total time complexity would be O(logn). Ignore half of the problems in O(1) time. First Bad Version Search in a Big Sorted Array. Double the search number every time, then serach from n to 2n Find Minimum in Rotated Sorted Array. Divide the array into a normal sorted array and a smaller rotated sorted array. Search in a 2D matrix. Perform Binary Search 2 times. First find the row, then perform binary search on that row. Search for a Range. Find the target number then perform binary search on its left and right. Maximum Number in Mountain Sequence. Check mid number to see if it is incresing or decresing, we can return any mountain value. Bianry Tree and Divide &amp; Conquer There are 3 ways to traverse a binary tree. Pre order, In order and post order You need to know both resursive version and iterative version. DFS includes pre order, in order, post order, divide &amp; conquer Bianry tree has height from O(logn) to O(n), traverse a binary tree takes O(n) time. In order traverse a binary search tree is an increasing order traverse. Maximum depth of Binary tree. Resursive: Divide and conquer/ traverse Subtree with Maximum Average Lowest Common Ancestor: Divide &amp; Conquer, check leftsubtree and rightsubtree. See if they return A or B or null Validate Binary Search Tree. Bottom up, return max and min return to the parent node. Convert Binary Search Tree to Doubly Linked List. Inorder traverse it into a list and convert it. Flattern Binary Tree to Linked List. node.right = head of left subtree, tail of left subtree = head of right subtree. Insert and delete a node from a Binary Search Tree BFS make a list and put all start nodes into the list traverse the list to get new nodes for the next level use Queue for BFS Graph BFS Social Network 六度理论 你和世界上任何一个人之间最多间隔6个人 Linkedin BFS看第几层有相同好友 树和图的区别，树是单向的，图可以是双向的（或者单项） 在遍历图中，会有环，可以用HashSet检查是否曾经访问过某个节点 树的特性，树中如果有N个点，就会有N-1条边。树中的所有点连通。用Queue和Set访问并保存所有Graph中的点 构造Graph， 可以用 Map&lt;Integer, Set&lt;Integer&gt;&gt; Graph其实就是保存着（每个点和与他相邻的点的Set）的Map BFS on a 2D-array Number of islands Wall and Gates 什么时候使用BFS？ Tree的层次遍历 2D-array中求连通性，灌水 拓扑排序 图的最短路径 DFS Recursion Combination Permutation Graph Non-Recursion 什么时候使用DFS？ 找所有方案，排列，组合 找最优方案（最短，最长）（大部分是动态规划，也有可能是DFS） Questions Combination sum Palindrome Partitioning 所有的切割问题都是组合问题 切割abc字符串，有两个切割位 a1b2c， 可以切ab之间也可以切bc之间，把数字12放入代表切割位，所以切割的方式有 a b c [1,2] a bc [1] ab c [2] abc [] 以上可以看出四种切割方式可以用数字表示 所以n个字母的切割问题可以看作是n - 1个数字的组合问题 Backtracking Permutation的去重与Combination相似，可以先定义一个boolean数组存放visited信息 N皇后 每一行和每一列都是1…n的一种排列 如何判断皇后在同一斜线上： 横坐标与纵坐标之差相等，或者横坐标与纵坐标之和相等 Word Ladder: BFS, 把ListWord转换成graph的形式，对于每个word，先找到所有可能的变换，总共有单词长度*26种，然后这些变换中存在于Wordlist中的就是他的neighbor，就可以变成标准的BFS问题 Word Ladder 2：Backtracking + BFS， 先从End往Begin做BFS找到每个单词距离End的长度，保证Backtrack时不会遍历更远的单词。再从Begin往End做backtracking Linked List and Array Reverse linked list Reverse linked list in k group Use dummy node to save the list head 画图去理解linked list的变化 dummy node practices partition-list merge two sorted list reverse linked list 2 swap two nodes in linked list reorder list rotate list Copy list with random pointer -&gt; similar to clone graph Linked List Cycle 1 and 2 -&gt; Floyd’s Tortoise and Hare Sort List -&gt; how many sort algorithm has time complexity O(nlogn)? Quick Sort, with O(1) Space Complexity Merge Sort, with O(n) space Heap Sort Merge two sorted array Merge small array into big array intersection of two arrays Median of two sorted arrays -&gt; find the (m + n) / 2th number, use O(logn) Median Kth largest element Merge Sort practice Quick Sort practice Same integers will not remain their relative order in the old list after using quick sort. Merge sort will keep the relative order of same integers Merge sort will first split the array into two subarray until there is only one number left in the array, then sort, then merge the two sorted array back to one array. From small array to big array Qucik sort will first pick a pivot value, sort the entire array based on the pivot value. Then sort the two small array. From big array two small array Note in Quick Sort, when we find a value equals to the pivot value, we don’t swap it, leave it as where it is. We need to split these values as even as possible. Quick sort while loop condition: while (left &lt;= right). Don’t forget the equal sign Subarray PrefixSum, HashMap to store prefixsum, then linear scan, So O(n) time and space Maximum subarray Minimum subarray subarray sum equals k number of subarray equals k Two pointers O(n) time complexity 同向双指针 Move zeros Remove duplicate number in array Remove duplicate numbers in sorted array -&gt; tortise and hare cycle detection 相向双指针 valid palindrome rotate string -&gt; three step reverse, when reverse a string, one start from begin and another start from end recover rotated sorted array -&gt; similar to above, find the rotate point, three step reverse. Two Sum 1-7 Two Sum 1 -&gt; 最普通的Two Sum， 可以使用HashMap，或者先排序在使用相向双指针，这样可以不是用额外空间，但是时间复杂度是O(nlogn) Two Sum 2 - Data Structure Design -&gt; HashMap Two Sum 3 - Array is sorted -&gt; 相向双指针 Two sum 7 - unique pairs -&gt; 找到所有符合的数对,相向双指针，当找到一对时，left++,right–,并继续。如果遇到重复比如1,1,3,4,4,每次我们移动时，移动到与之前的数不一样的数为止。注意此方法要求数组是有序的。 3Sum -&gt; 使用Two sum的结果，对于array中的每一个不等的数，检查是否有一对数的和等于它的相反数。可以使用HashMap或者two pointers，记住two pointers必须先排序数组。Two pointers的速度远快于HashMap Valid Triangle Number -&gt; 检查有多少组三个数可以组成合法的三角形的三条边。注意我们只需要检查最小的两个数的和大于第三个数即可. 从最大的数开始loop，使用Two pointers，对于每一个符合的left and right， 所有大于left的数都是符合的，所以每当我们找到一组合法的left and right，count += （right - left）. 详情查看leetcode 611 对于求2个变量如何组合的问题，可以循环其中一个变量，然后研究另外一个变量如何变化 对于求3个变量如何组合的问题，可以循环其中一个变量，然后研究另外2个变量如何变化 对于求4个变量如何组合的问题，可以循环其中两个变量，然后研究另外2个变量如何变化 Two Sum less than k Two Sum greater than k -&gt; same as triangle count Two Sum closest to target -&gt; two pointers start from begin and end, keep updating diff 3 Sum closest to target -&gt; similar to above, loop one value and use two sum as a template 4 Sum Two Sum - difference equals to target -&gt; 同向双指针。两数之差等于target，先排序，两个pointers都在开始，如果他们的差大于target，小数往后移，如果他们的差小于target，大数往后移 O(n) time complexity Partition array move elements &lt; k to the left and elements &gt;= k to the right, this is a smaller step in quick sort Parition array while loop condition: while (left &lt; right). No equal sign Letters by case Partition array by parity Interleaving positive and negative numbers -&gt; first count positive and negative numbers, then decide starts from positive or negative, then use two pointers in the same direction. Sort Colors sort 3 colors in group, we can use partition array two times, first time split one color, next split the rest two colors. We could also count the number of each color and modify the array to match the occurance of colors three pointers i pointer only moves when find ones and zeros, if it finds two, it will swap it with right. Throw two to the right so left pointer will never find two. so when i pointer finds zero, after swap it with left pointer, throw zero to the left, left pointer will only throw one back, so they can both move to right by 1 Sort Colors 2 (rainbow sort) -&gt; 使用多次Partition， 每次将一般的颜色分到array的左右，比如只有四种颜色，一次partition将1，2分到左边，3，4分到右边。花费O（n）的时间将T(k)的问题变成T(k/2)的问题。总共的时间复杂度为O(nlogk) 这个解法有点像Quick Sort。注意Quick sort and Rainbow sort的while loop条件均为 while(left &lt; right)。可以不用要等号 其他比较高频的排序方法： Pancake Sort Sleep Sort Spaghetti Sort Bogo Sort 3 Step Reverse Three step reverse Recover rotated sorted array -&gt; First use 3 step reverse then two pointer reverse. 3 step reverse First find the rotate point Reverse the first part(before rotate point) of the String/array Reverse the second part(after rotate point) of the String/array Reverse the entire String/array Quick Select Quick Select 快速选择算法 思想类似于快速排序,利用O(n)的时间找到前right个大数，再看k与right的关系决定下一步recursion的范围 O(n) + O(n/2) + O(n/4) +… = O(n) time kth largest element Median kth smallest element Median of two sorted arrays Hash &amp; Heap 了解他们的原理和应用 TreeMap 队列Queue 支持操作O(1) push O(1) pop O(1) top,多做跟BFS有关的题目 栈Stack 支持操作O(1) push O(1) pop O(1) top,非递归实现DFS的主要数据结构 哈希表Hash 支持操作O(1) insert O(1) find O(1) delete, Hash table / Hash map / Hash set的区别是什么 Hash Set只存key不存value Hash map存key和value Hash table，多线程安全，当多个线程同时访问一个Hash table时，它可以保证数据安全，Hash Map无法做到 Hash function -&gt; 对于任意的Key，得到一个固定且无规律的介于0到capacity - 1的整数 数据结构分为连续性和离散型，array为连续性，List为离散型 一些著名的Hash算法：MD5， SHA-1， SHA-2，主要用于加密 用于算法中的Hash function很想进制转换 12345678int hashfun(String key) &#123; int sum &#x3D; 0; for (int i &#x3D; 0; i &lt; key.length(); ++i) &#123; sum &#x3D; sum * 31 + (int)(key.charAt(i)); sum &#x3D; sum % HASH_TABLE_SIZE; &#125; return sum;&#125; Magic number - 31 通过经验得出31的冲突更少（Collision） Magic number取质数更好，如果数太大影响效率，数太小冲突太多 Closed Hashing - line sweep 当哈希表发生冲突时，把新的数据插入到下一个空的位置 寻找，不断的找下一个位置直到找到value或者找到空位为止 删除，把值删除后用一个deleted标记出来，这个位置并不为空 缺点，当我们插入和删除的次数很多时，很多位置都会被标记为deleted，寻找的效率会变低 Open Hashing 每一个位置都是一个链表，当发生冲突时加到链表的开头，这样不用每次都遍历到链表末尾，寻找时搜寻整个链表 Rehashing 当hash table size不够用了怎么办？ 像ArrayList那样不断倍增 怎么定义满？当实际的存储个数达到总共空间的1/10(经验值)时，我们就需要rehash 回顾ArrayList的倍增：当ArrayList满时，把size扩大两倍，把前面的所有数复制到后一半新扩大的ArrayList中 Hash Table的倍增：如果移动之前的数，会影响到hash function，所以我们会先扩大hash table， 再把hash table中的所有值重新放到hash function算出它在新的hash table中的位置，重新插入 Rehashing很慢，所以在定义hash table的时候最好提前定义一个size，让他尽量不要rehash 在存储快的和存储慢的介质之间都会存在Cache的问题，不仅在CPU和内存之间有 使用链表储存数据的的插入顺序，使用哈希表判断将要插入的数据是否已经存在于链表中 LinkedHashMap = DoublyLinkedList + HashMap LRU Cache -&gt; 使用HashMap + DoublyLinkedList可以解决 Heap 支持操作O(logn) add O(logn) remove O(1) MIN or MAX,求最大值或最小值只可取其一 ugly number 2 top k largest number 2 -&gt; 使用PriorityQueue只保存k个数 merge k sorted list -&gt; Add all list heads to a heap, poll the smallest head, add to ans, then add next node to heap. 注意要会写heap的comparator Practice high five k closest points data stream median kth smallest number in sorted matrix Dynamic Programming 动态规划的分类 Triangle -&gt; 入门题 注意以下代码中n为三角形的高度 第一种方法 DFS Traverse, Top-down 123456789101112131415void traverse(int x, int y, int sum) &#123; if (x &#x3D;&#x3D; n) &#123; &#x2F;&#x2F; found a whole path from top to bottom if (sum &lt; best) &#123; best &#x3D; sum; &#125; return; &#125; traverse(x + 1, y, sum + A[x][y]); traverse(x + 1, y + 1, sum + A[x][y]);&#125;best &#x3D; MAXINT;traverse(0, 0, 0); 每一个节点都分出来两条路径，相当于一个binary tree， 它的节点个数是2^n, 所以这个方法的时间复杂度是O(2^n) 第二种方法 DFS Divide and Conquer, bottom-up 123456789101112&#x2F;&#x2F; return minimum path from (x, y) to bottomint divideConquer(int x, int y) &#123; if (x &#x3D;&#x3D; n) &#123; return 0; &#125; return A[x][y] + Math.min( divideConquer(x + 1, y), divideConquer(x + 1, y + 1) );&#125;divideConquer(0, 0); 与之前一样，每一个节点都分出来两条路径，相当于一个binary tree， 它的节点个数是2^n, 所以这个方法的时间复杂度是O(2^n) DFS实际上是在枚举，把所有的方案都列出来然后看哪个更好 我们做了很多重复计算，因为实际上我们只需要计算三角形所有节点（n2个）的最短路径，DFS却计算了（2n个）节点，很多节点我们计算了很多遍。所以我们需要把每个节点的结果保存下来，下次需要的时候就不需要重新算了 第三种方法 DFS Divide and conquer + Memorization 12345678910111213141516&#x2F;&#x2F; return minimum path from (x, y) to bottomint divideConquer(int x, int y) &#123; &#x2F;&#x2F; row index from 0 to n - 1 if (x &#x3D;&#x3D; n) return 0; &#x2F;&#x2F; if we already got the minimum path from (x, y) to bottom, just return it if (hash[x][y] !&#x3D; Integer.MAX_VALUE) return hash[x][y] &#x2F;&#x2F; set before return hash[x][y] &#x3D; A[x][y] + Math.min(divideConquer(x + 1, y), divideConquer(x + 1, y + 1)); return hash[x][y]&#125;initialize: hash[*][*] &#x3D; Integer.MAX_VALUE;answer: divideConquer(0, 0); 在每次计算节点最小值之前，先看看之前有没有算过这个节点的结果，如果有直接从数组中拿到结果并返回，没有算过我们再递归, O(n^2) time 记忆化搜索的本质：动态规划，省去了重复计算 多重循环 vs 记忆化搜索 第四种方法 多重循环，自底向上 1234567891011121314151617int[][] dp &#x3D; new int[row][row]; &#x2F;&#x2F; dp[i][j] 表示从i， j出发走到最后一层的最小路径长度&#x2F;&#x2F; 初始化，终点先有值,最后一层for (int i &#x3D; 0; i &lt; n; ++i) &#123; dp[n - 1][i] &#x3D; triangle[n - 1][i];&#125;&#x2F;&#x2F; 循环递推求解for (int i &#x3D; n - 2; i &gt;&#x3D; 0; --i) &#123; for (int j &#x3D; 0; j &lt;&#x3D; i; ++j) &#123; dp[i][j] &#x3D; Math.min(dp[i + 1][j], dp[i + 1][j +1]) + A[i][j] &#125;&#125;&#x2F;&#x2F; 求结果： 起点return dp[0][0];&#125; O(n^2) time,没有递归 第五种方法 多重循环，自顶向下 12345678910111213141516171819int[][] dp &#x3D; new int[row][row];&#x2F;&#x2F; 初始化，起点dp[0][0] &#x3D; triangle[0][0];&#x2F;&#x2F; 初始化三角形的左边和右边for (int i &#x3D; 1; i &lt; row; ++i) &#123; dp[i][0] &#x3D; dp[i - 1][0] + triangle[i][0]; &#x2F;&#x2F; 左边 dp[i][i] &#x3D; dp[i - 1][i - 1] + triangle[i][i]; &#x2F;&#x2F; 右边&#125;&#x2F;&#x2F; top downfor (int i &#x3D; 1; i &lt; row; ++i) &#123; for (int j &#x3D; 1; j &lt; i; ++j) &#123; dp[i][j] &#x3D; Math.min(dp[i - 1][j], dp[i - 1][j - 1]) + triangle[i][j]; &#125;&#125;Math.min(dp[row - 1][0], dp[row - 1][1], dp[row - 1][2]...); 这个方法中dp[x][y]表示从上到下到这个点的最短路径 O(n^2) time,没有递归,但是这个方法我们必须提前算出三角形的左边和右边，因为对于三角形的每一个结点dp[x][y]，我们在计算到他的最短路径的时候，我们需要用到它前一层的两个点dp[x-1][y]和dp[x-1][y-1].但是对于每一层的最左边和最右边的点，我们再上一层找不到再靠左边或者右边的点，所以为了避免越界，我们必须提前算出两条边的所有点。 什么情况下使用动态规划？ 满足下满三个条件之一： 求最大值最小值 判断是否可行 统计方案个数 上面这三个情况极有可能需要使用动态规划 什么情况下不使用动态规划？ 求出所有具体的方案，而不是方案的个数 -&gt; palindrome-paritioning 输入数据是一个集合而不是序列,意思是如果我们可以将数据调换位置，则很有可能不使用动态规划，动态规划需要有方向性 -&gt; longest-consecutive-sequence 暴力算法的复杂度已经是多项式级别。 动态规划擅长优化指数级别的复杂度（2^n, n!）到多项式级别的复杂度(n^2, n^3) 动态规划四要素 状态，储存小规模问题的结果 方程，状态之间怎么联系 初始化，最极限的小状态是什么，起点 答案，最终的状态是什么，终点 动态规划的类型，按照状态分类 坐标型 triangle 10% 接龙型 20% 划分型 匹配型 背包型 区间型 longest palindrome 树图型 tree matrix 博弈型 判断是否可行 坐标型动态规划 状态 f[x]:表示我从起点走到坐标x。。。 f[x][y]:表示我从起点走到坐标x, y… 方程：研究走到x， y这个点之前的一步 practice: minimum path sum 动态规划不应该存在循环依赖，从一个状态不会回到之前的一个状态 当我们初始化一个二维数组的动态规划时，需要初始化第0行和第0列 Unique path Unique path 2 Climbing Stairs Jump game Jump game 2 接龙型动态规划 告诉你一个规则然后求最长的状态可以是多少，比如数组中求最大递增子序列 Longest increasing subsequence Russian Doll Envelopes Largest Divisible Subset Frog jump 总结 动态规划的实质是记忆化搜索，避免重复计算中间结果 动态规划四要素：初始化，方程，起点，终点 什么时候使用动态规划:最优，可行，方案数（而非具体方案） 什么时候不使用：求具体方案，输入数据为集合而非序列（可调整顺序），暴力算法时间复杂度已经是O(n^2, n^3)","categories":[],"tags":[{"name":"Data Structures","slug":"Data-Structures","permalink":"http://hellcy.github.io/tags/Data-Structures/"}]},{"title":"Single Sign On Introduction","slug":"Single-Sign-On-Introduction","date":"2019-10-24T02:54:58.000Z","updated":"2021-05-24T07:27:02.660Z","comments":true,"path":"2019/10/24/Single-Sign-On-Introduction/","link":"","permalink":"http://hellcy.github.io/2019/10/24/Single-Sign-On-Introduction/","excerpt":"","text":"What is Single Sign On? Single-sign-on是一个很方便的东西，现在无论什么网站都需要登陆，而记住每个网站不同的用户名和密码又非常困难，那么作为一个好的产品，从用户的角度出发，就应该想到，如果我们只需要让用户登陆一次，就可以去到同一个平台下任何其他的产品，而不需要重复登陆，那该多好。 比如说Google Chrome，当你在浏览器中登陆了，那么你再去到Youtube或者Gmail等其他Google公司的产品，你将不再需要再次登陆。这是因为Google在你登陆的时候保存了 那么，这是怎么做到的呢，在single sign on with SAML的世界中，有两个非常重要的概念，一个是IDP，identity provider，另一个是SP，service provider。我们还是拿Google来举例子，当你需要登陆Youtube时，Youtube会将你跳转到一个Google登陆界面，登陆后他会保存你的登陆信息，然后再将你跳转回Youtube，你就发现你已经登陆上了，这时你需要查看Gmail，Gmail也会将你跳转到Google登陆界面，但是界面发现因为你已经登陆了Youtube，所以你所有的登陆信息已经被保存了，不需要你再次输入，于是自动把你跳转到Gmail，你就以为你什么都没做就自动登陆了Gmail。(虽然Google可能不是用SAML来登陆的) 什么是SAML? SAML可以认为是IDP和SP之间转递用户信息的一种格式规范，为了方便接收方能够理解传递过去的信息，必须规定一种规范使接收方很容易解码信息。 Broker 现在我们要做的是一个中间人的角色，用户所有的请求都会先到我们这里，我们会检查request header，如果我们发现这是一个GET request，我们就会直接把用户跳转到SSO的登陆界面。 如果我们发现这是一个POST request，并且成功解码了一起发过来的SAML，我们就要重新加密用户信息，并把用户跳转到相应的网页，由最终的Service provider来再次解码用户信息 如果SAML解码失败，我们也会将用户跳转到SSO登陆界面。让用户再次登陆 加密信息的方法有很多，我用的是windows自带的RijndaelManaged Class Reference Microsoft RijndaelManaged Class SAML How SSO works One Potential Security Issue 如果我们仔细想想这个流程，就会发现这中间存在一个安全隐患，如果用户在IDP输入完用户信息并验证通过后，IDP会把SAML response发回给SP，这时如果这个SAML被拦截并被篡改，SP并不知道这个SAML有没有被改动，还是会按照SAML上面的信息将用户登陆。 要解决这个问题，我们就需要检查SAML Response，这个检查分为两步，Signing check和Certificate check Signing Check 其实就是一种checksum，我们把整个SAML response转换成一种加密的text。这个text随着SAML一起返回给SP，当我们解密这个text后得到的SAML和返回的SAML对不上，我们就知道这个SAML已经被篡改过了。举个例子：我们收到了一个来自Sam的SAML response，当我们检查了signing之后，我们就能确定这个response自从从Sam手上发出后就没有被篡改过。但是另一个问题是，如果我们得信息被Tony拦截，Tony完全替换了整个SAML并发给我们，我们虽然知道SAML自从从Tony手中发出后没有被篡改，但是我们还是没有拿到正确的SAML。 Certificate check 这就需要Certificate check了，他其实就是提前保存在用户电脑中的受信任的证书，当我们发现SAML来自Tony而不是Sam，Tony并没有在受信名单中，那我们还是会拒绝这个SAML。只有当response来自我们所信任的人，并且这个response自从从他手上发出后就没有更改，我们才选择接受。 Reference SAML: Why is the certificate within the Signature? CheckSignature(X509Certificate2, Boolean) How to validate a SAML signature value How to: Verify the Digital Signatures of XML Documents Asp.Net Core SAML Response Signature Validation","categories":[],"tags":[{"name":"SSO","slug":"SSO","permalink":"http://hellcy.github.io/tags/SSO/"}]},{"title":"Learn Android Apprentice in 10 days","slug":"Learn-Android-Apprentice-in-10-days","date":"2019-09-02T10:01:47.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/09/02/Learn-Android-Apprentice-in-10-days/","link":"","permalink":"http://hellcy.github.io/2019/09/02/Learn-Android-Apprentice-in-10-days/","excerpt":"","text":"开始学习使用Android Studio 在iOS的学习告一段落了之后，现在开始学习Android应用的开发 他总共包括六个部分，前四个部分每一部分教你编写一个app，难度从低到高。后面两部分会告诉你怎么向下兼容和发布。跟之前的iOS教程很像。 TimeFighter Checklist Conclusion TimeFighter 这个app会从怎么set up Android Studio开始，我们直接跳过，到最开始写代码的部分 Constraint Layouts 和iOS App很像，手机app必须考虑到对象在手机屏幕上的位置问题，Android有提供很多种layoutdexuanze，其中Constraint是最常用的一种，他可以规定目标到屏幕的相对位置 Activities 在确定了诸如Textview，Button等对象的位置之后，我们需要在代码层面对其进行操作，我们可以在Aciticity中创建这些变量对象，然后通过在Layout中设置的ID找到他们，Activity其实就是iOS中的ViewController。 Strings 我们会将一个app中用到的所有文字集中在一个文件中，这个文件叫做Strings.xml，这样以后本地化加其他语言或者更改一个单词，这个单词虽然在app中可能出现了很多次，但是我们只需要在Strings里面改一次就可以了 Oriendtation changes 在手机屏幕方向改变时，系统会做三件事，1.save properties, 2. destroys current activity, 3. recreates the activity for the new orientation by calling onCreate and resets any properties specified by the developer 所以在改变方向时，我们需要及时保存需要用到的变量，在接下来现实的Activity中显示，保证过程不会丢失。 val and var Basically, val and var both are used to declare a variable. var is like a general variable and can be assigned multiple times and is known as the mutable variable in Kotlin. Whereas val is a constant variable and can not be assigned multiple times and can be Initialized only single time and is known as the immutable variable in Kotlin. App colors and styles Android project中有许多文件夹，其中res包含着app需要用到的所有资源resources，常用的Strings，Animations，Menus，Colors and Styles都在这里, 通过直接更改Colors里面颜色的hex值来改变app中元素的颜色，更方便的管理同一种类型的东西 Animations Animations也在res文件夹里，她负责调用以及调配动画，图片里的动画效果是使用内置的bounce_interpolator，在2秒钟内把目标元素增大2倍，以50%处为中心，并缩小至原来的大小。 Menu Menu同样他也在res文件夹中，他管理所有跟系统菜单相关的元素，比如我们想在屏幕上方的菜单栏中加入一个button，就需要在这里定义 我们加了一个Menu，当点击时会冒出一个AlertDialog 这就是第一个app所讲的全部内容了，基本就是把project中需要用到的功能讲了一遍，没有用到任何复杂的语法，更多的讲的是Android Studio这个IDE的使用。接下来进行第二个app Checklist RecyclerView The RecyclerView asks the Adapter for an item, or a ViewHolder at a given position. 2. The Adapter reaches into a pool of ViewHolders that have been created. 3. Either a a new ViewHolder is returned, or a new one is created. 4. The Adapter then binds this ViewHolder to a data item at the given position. 5. The ViewHolder is returned back to the RecyclerView for display In general, Adapters give your RecyclerView the data it wants to show. They have a clever way to calculate how many rows of data you want to show, which you’ll cover shortly. ViewHolders are the visual containers for your item. Think of them as cells in the table. This is where you tell your RecyclerView what each item should look like. These are basically little tiny layout items used to display the data at any given position in the list of data. As you scroll through a RecyclerView, instead of creating new ViewHolders, RecyclerView will recycle ViewHolders that have moved offscreen and populate them with new data, ready to be shown at the bottom of the list. This process repeats endlessly as you scroll through your RecyclerView. This recycling of ViewHolder to display list items helps to avoid janking in your app. RecycleView 的本质是循环使用table中的cell，当一个cell网上滑出屏幕时，我们可以让他重新出现在底部，但是显示不同的数据，这就要求我们在写代码时要把table，cell和数据分开来，每个部分各司其职 这个是cell部分，它由两个textview组成。对应上图中的viewHolder 这个是table部分，它包含多个cell，并可以循环利用cell。对应上图中的adapter。我们需要在adapter中implement recycleview必须的成分例如包含多少个viewholder，每个viewholder的数据应该从哪里取。。 除了recycleview本身，我们应该把data source设置成动态的，也就是说我们可以输入自己的数据，并加进recycleview中。 下面我们在代码层面解释一下如何构建一个recycleView 首先我们需要继承RecyclerView Adapter并implement这三个function，RecyclerView还有很多其他function可以override但是这三个是必须的 123override fun getItemCount(): Int &#123; return accounts.size&#125; getItemCount return一个Int，它表示这个表格中有几行，一般是return数据的size 12345678override fun onCreateViewHolder( parent: ViewGroup, viewType: Int): CustomViewHolder &#123; val view &#x3D; LayoutInflater.from(parent.context).inflate(R.layout.view_holder_custom, parent, false) return CustomViewHolder(view)&#125; onCreateViewHolder定义一个table cell的layout，我们返回一个layout文件，所以对于每个不同的recyclerView，我们还需要新建一个Layout file用于储存cell的layout 123456override fun onBindViewHolder(holder: CustomViewHolder, position: Int) &#123; holder.label.text &#x3D; &quot;Some text&quot; holder.itemView.setOnClickListener &#123; clickListener.listItemClicked(position) &#125;&#125; onBindViewHolder会把我们的数据放到我们新建的cell layout中，另外注意到我加了一个ClickListener，所以之后我们点击每行的时候就会运行listItemClicked function，然后在那里就可以添加另外的代码，可以跳到另一个View等。 123class CustomViewHolder(itemView: View): RecyclerView.ViewHolder(itemView) &#123; val sampleData &#x3D; itemView.findViewById(R.id.sampleData) as TextView&#125; 我们还需要一个class去连接table cell layout file 这样基本上一个RecyclerView就完成了，之后我们只需要将数据从Activity或者Fragment传入RecyclerView Adapter就可以了 SharedPreferences SharedPreferences lets you save small collections of key-value pairs that you can retrieve later. If you need a way to quickly save small bits of data in your app, SharedPreferences is one of the first solutions you should consider 这就需要用到sharedpreference，它类似于一个dictionary，里面由key-value pairs组成。可以储存size较小的数据。类似于表格信息 EditText 12val listTitleEditText &#x3D; EditText(this) listTitleEditText.inputType &#x3D; InputType.TYPE_CLASS_TEXT 创建一个input text field，并给它指明一个InputType， 这样Android就会显示合适的keyboard Intent 当我们需要让一个页面与另一个页面进行交流时，我们需要通过Intent将数据传送过去。 12345678private fun showListDetail(list: TaskList) &#123; val listDetailIntent &#x3D; Intent(this, ListDetailActivity::class.java) listDetailIntent.putExtra(INTENT_LIST_KEY, list) startActivity(listDetailIntent)&#125;&#x2F;&#x2F; get from other Activitylist &#x3D; intent.getParcelableExtra(MainActivity.INTENT_LIST_KEY) 在上面这个function中，this是我们现在所在的Activity，ListDetailActivity是我们将要过去的Activity，TaskList是我们要传送的数据。 我们需要规定一个Key，这样的新的页面中我们就知道用Key来获取相应的数据。 但是还有一个问题，就是自定义的object不能直接通过intent传送，我们需要把他变成Parcelable的object Parcelable 在定义object的class中，implement parcelable，Android Studio会自动把需要的function写好 Fragment Fragment是android语言中一个非常重要的部分，他必须附属于一个Activity，Fragment的本质是可以让相同的部分用同一个Fragment表示，并在多处使用，以节省代码长度，让App保持整洁一致。 12345678910111213141516171819202122232425262728293031323334353637383940class ListSelectionFragment : Fragment() &#123; &#x2F;&#x2F; 1 private var listener: OnListItemFragmentInteractionListener? &#x3D; null interface OnListItemFragmentInteractionListener &#123; fun onListItemClicked(list: TaskList) &#125; &#x2F;&#x2F; 2 companion object &#123; fun newInstance(): ListSelectionFragment &#123; val fragment &#x3D; ListSelectionFragment() return fragment &#125; &#125;&#x2F;&#x2F; 3override fun onAttach(context: Context) &#123; super.onAttach(context) if (context is OnListItemFragmentInteractionListener) &#123; listener &#x3D; context &#125; else &#123; throw RuntimeException(context.toString() + &quot; must implementOnListItemFragmentInteractionListener&quot;) &#125;&#125; &#x2F;&#x2F; 4 override fun onCreate(savedInstanceState: Bundle?) &#123; super.onCreate(savedInstanceState) &#125; &#x2F;&#x2F; 5 override fun onCreateView(inflater: LayoutInflater, container:ViewGroup?, savedInstanceState: Bundle?): View? &#123; return inflater.inflate(R.layout.fragment_list_selection, container,false) &#125; &#x2F;&#x2F; 6 override fun onDetach() &#123; super.onDetach() listener &#x3D; null &#125;&#125; Fragment由几个重要部分组成，首先，当Fragment第一次被附属于某个Activity时onAttach会被执行。然后onCreate， 在这两个地方你可以initialize一些变量等 123456789override fun onCreate(savedInstanceState: Bundle?) &#123; super.onCreate(savedInstanceState) transactions &#x3D; arguments?.getParcelableArrayList&lt;WestpacTransaction&gt;(&quot;Transactions&quot;)!! refundedAmount &#x3D; arguments?.getDouble(&quot;RefundedAmount&quot;)!! refundAmount &#x3D; arguments?.getDouble(&quot;RefundAmount&quot;)!! updatedBalance &#x3D; arguments?.getDouble(&quot;UpdatedBalance&quot;)!! primaryAccount &#x3D; paperCutAccountManager.readPrimaryAccount()&#125; 之后onCreateView，这个地方会把数据等object显示在View中，所以需要在这里把变量和UI object绑定。 1234567891011121314151617181920212223242526override fun onCreateView( inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?): View? &#123; val activity &#x3D; activity as AppCompatActivity? if (activity !&#x3D; null) &#123; activity.supportActionBar!!.show() activity.supportActionBar?.setDisplayHomeAsUpEnabled(false) activity.supportActionBar?.title &#x3D; &quot;Refund Complete&quot; activity.nav_view.isVisible &#x3D; false &#125; &#x2F;&#x2F; Inflate the layout for this fragment val view &#x3D; inflater.inflate(R.layout.fragment_refund_complete, container, false) view.AccountNameLabel.text &#x3D; primaryAccount.AccountName view.AccountBalanceLabel.text &#x3D; &quot;$&quot; + &quot;%.2f&quot;.format(updatedBalance) view.refundAmount.text &#x3D; &quot;$&quot; + &quot;%.2f&quot;.format(refundedAmount) view.DoneButton.setOnClickListener &#123; v -&gt; doneButtonPressed() &#125; view.refundCompleteTable.adapter &#x3D; RefundCompleteTableViewAdapter(transactions, this) view.refundCompleteTable.layoutManager &#x3D; LinearLayoutManager(activity) return view&#125; 之后是Companion object，当这个Fragment被创建时，如果你需要给它传值，它需要在这里被定义，有点像是Fragment的Constructor 1234567891011121314companion object &#123; val TAG &#x3D; RefundCompleteFragment::class.java.simpleName @JvmStatic fun newInstance(transactions: ArrayList&lt;WestpacTransaction&gt;, refundedAmount: Double, refundAmount: Double, updatedBalance: Double): RefundCompleteFragment &#123; val fragment &#x3D; RefundCompleteFragment() val args &#x3D; Bundle() args.putParcelableArrayList(&quot;Transactions&quot;, transactions) args.putDouble(&quot;RefundedAmount&quot;, refundedAmount) args.putDouble(&quot;RefundAmount&quot;, refundAmount) args.putDouble(&quot;UpdatedBalance&quot;, updatedBalance) fragment.arguments &#x3D; args return fragment &#125;&#125;","categories":[],"tags":[{"name":"Kotlin","slug":"Kotlin","permalink":"http://hellcy.github.io/tags/Kotlin/"},{"name":"Android Studio","slug":"Android-Studio","permalink":"http://hellcy.github.io/tags/Android-Studio/"}]},{"title":"Swift tips and tricks","slug":"Swift-tips-and-tricks","date":"2019-08-16T09:47:36.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/08/16/Swift-tips-and-tricks/","link":"","permalink":"http://hellcy.github.io/2019/08/16/Swift-tips-and-tricks/","excerpt":"","text":"Passing messages around view controllers 三种在view controllers中传递参数的方法，Delegation pattern, Notifications, Closures and action handlers。可以在这篇帖子中找到。 CS193P open course notes MVC Swift Programming Language More Swift Please find all relevant materials for Standford Programming course cs193p in here MVC Controller can talk to both Model and View. But Model and View should never speak to each other. For the View, it can communicate to the Controller, but in certain ways. Because View are all generic UI objects, to change some of its properties to fits the app’s objective, we need to send the UI objects, like buttons and labels back to the controller, let controller to implement its properties like background color, label content etc… so we use Action method to target the function that will be calling when certain UI objects are triggered. Another way to talk to controller is to use Delegate, for more complicated objects like scroll view or tables, we need to let controller know what we are doing at the moment, and controller is responsible for implementing the extra tasks while we are doing this things. Another important thing is that views do not own the data they display, they ask for the controller and controller grab the data from Data source and give it to the View, because if you have 50000 songs in a table, if table owns the data, it will be too big and costy to create such table object. So instead we use Data Source to provide data to the view. What about Model, can Model talk to the Controller? Yes, but not directly, because Model is UI independent, and Controller is UI dependent, so if a Model is updated and he wants everybody that is interested to be informed, it will broadcast this information to all the controllers, and those controllers that are listening will be notified and talk to Model and grab the changes. This way is called notifications One MVC model usually controls one screen on iPhone, and when multiple MVCs are talking to each other, they often treats other MVC as its View. So when one Screen wants to talk to another Screen, it uses delegate! Do not implement your app this way! struct and class They are similar, contains methods and variables, but struct has no inheritance, Another difference is structs are value types, and classes are reference types, so when we assign it to another variable, it gets copied. Arrays, ints, strings, dictionary are all structs, but swift doesn’t make a copy of all of them when we need it, it only copy them when a user modifies it. It’s called copy-on-write semantics. For classes, we do not make a copy of it when we need it, we make a reference to the class, so when we modify the properties of that class, the real class gets modified too. Try not to use initialiser in the view controller stride In swift, we don’t have for(; 😉 structrue, so we need stride method for a range with specific count value Swift Programming Language tuples They are nothing but a group of values, different types of values could be inside the same tuple, vars and methods are not allowed in tuples, its good for return multiple values from a method because a method can only return a single thing. stored properties(normal properties) and computed properties Computed properties are properties with get and set methods, you can have read only computed properties, which only has get method. get and set part will be executed when we get or set the variable. we use computed property because sometimes we can derive property from other place, like indexOfOneAndOnlyFaceUpCard GET can be derived by looking at all the cards and see if you can get only one card facing up and return that index. And SET can give the card that is facing up to the property. You can omit the GET word if it is a read only computed property Access control Protecting our internal implementations, by only give other people names of the methods that are allowed to be called. Internal: usable by any object in my app or framwork, its default private: callable only within this object private(set): means its only readable from outside the object, but not settable filePrivate: accessiable by any object in this source file public(for frameworks only): can be used by object outside this framework open(for frameworks only): public AND can sub-class(override) assertion: a method that in your program when you assert something is true, is not, the app will crash. It is a good way to protect your API. Extension Add vars and methods to other classes even if you don’t have the source. But there are restrictions: you can’t re-implement the methods that are already there. You can only add new ones. And, properties you add can have no stroage associated with them.(computed only) Enum Another variety of data structrue apart from struct and class. It can only have discrete states. Enum is a VALUE type, like structs, so it gets copied as it is passed around. Enum in Swift can have an associated data. Enum with associated value Checking enum’s state with Switch cases syntax using associated value in switch cases Enum can have methods, and you can test a enum’s state within that method with self keyword You can even change the enum’s state in its methods, by giving that method a mutating keyword to let Swift know. Optionals Optional is just an enum, it has two cases, one is nil, which measns it is not set yet, the other is some, which means it has some associated value with it. If you are trying to force unwrap an optional, what Swift really do it just throw an exception when that enum is in case nil, and do whatever you want to do with that enum in case ‘some’. Memory Management Automatic reference counting: Reference types are stored in the heap, everytime you create a pointer to a reference type in the heap, Swift will add One to a counter for that reference type, everytime when a pointer goes out of scope, Swift decrement the counter. And when the counter decrement to zero, Swift will instantly remove that reference type out of heap. Influence ARC by using ‘strong’, ‘weak’ and ‘unowned’ structs More Swift Protocol is the fourth data structure in Swift. It is basically a type, which contains a list of variables and a list of methods, without implementation. Any class or structs or enum that want to inherit protocol must implement all methods declared in that protocol. But, for objective C methods, implementations are optional. This is why when we override some will, did, set methods for some UI obejcts, we dont need to implement all methods from that protocol. Mutating functions: in protocol, some functions may be marked as mutating, when structs trying to inherit a protocol, because structs are value types, so structs get copied when we want to use it. However, it would be very inefficient if we make a copy every time we see it. So Swift uses copy-on-write system, we only make a copy of that struct when we are trying to modify it, in other words, mutating it. So we a struct is trying to change something in a function, and that function is inherted from a protocol, then this function has to be marked as mutating. Init: init functions are allowed to exist in protocol, however, in a class inherited that protocol, we need to add requried keyword before init function, this is because this class could have some other subclasses, and these classes also have to implement init function. Use protocol as a type Delegation: a very important use of protocol is delegation. Mutiple inheritance: if you want to use certain functions in some protocols, you could inherit that protocol and implement the functions you want to use.","categories":[],"tags":[{"name":"Swift","slug":"Swift","permalink":"http://hellcy.github.io/tags/Swift/"}]},{"title":"AWS Overview","slug":"AWS-Overview","date":"2019-08-05T09:03:22.000Z","updated":"2021-02-26T12:08:47.158Z","comments":true,"path":"2019/08/05/AWS-Overview/","link":"","permalink":"http://hellcy.github.io/2019/08/05/AWS-Overview/","excerpt":"","text":"什么是AWS？ AWS是Amazon的云服务，我现在用到的是其中的API Gateway和Lambda，API Gateway就是API网关，当网关收到request时，可以运行对应的Lambda function，生成response，然后返回。 Lambda function and API Gateway Introduction Create AWS Serverless Application 更改API域名 Custom Domain Names change lambda function outbound IP address to static Use IAM to restrict API Use S3 to host webpage AWS Simple Email Services Lambda function and API Gateway API Gateway 他总共分成6个部分，第一个部分是Test，也就是send test request的地方，sample request会先到达Method Request，在这里你可以往request里面加入Query string parameters, Http headers, Request body来让request符合Lambda的要求。 再下一个部分是Integration Request，在这里可以选择对应的Lambda function，让request知道该去往哪个Lambda，并且可以勾选Use Lambda Proxy integration, 如果勾选，它代表着AWS会帮你把request中的信息(包括query parameter, header, body等)保存到一个类似Dictionary的结构中，在lambda里面可以直接通过key name调用，不用手动提取信息。在下一部分就是运行相应的Lambda function了，这个在接下来的Lambda部分中讲。当Lambda运行完之后会生成一个response，如果勾选了之前的Proxy integration，这个地方就是灰色的，不能查看，因为它会帮你把response里的信息自动填充到header，body。。。并且使用自带的response template model，所以你就不用操心这个了。不然的话你可以手动设置response的格式。最后一个部分是Method response，当设置好reponse格式之后，你可以把response status code，连同optional的header和body返回给用户。还可以设置body的格式，比如JSON。 Lambda function 这里是你真正处理请求并返回数据的地方，AWS支持很多种语言的function。 我是用C#，这需要在Visual Studio中安装AWS package，在unget manager里面搜索AWS就可以找到AWS SDK了。安装之后就可以新建AWS solution并publish到AWS Lambda中。 这次我需要写的function就是调用Westpac API，所以其实是用户先调用AWS API，然后这个API再调用Westpac API。。。 如果request中含有parameter或者body信息，那么在function中的APIGatewayProxyRequest中就会含有这些参数信息，这就是之前勾选了proxy integration的好处 在上传时新建一个Lambda function，另外下面的Method name要对应刚刚C#中的method name，因为有时候我们会在一个solution中写多个method，连接多个Lambda function，所以要一一对应。 Test API Gateway and Lambda function 之后就是用AWS测试刚刚写好的API了，注意测试之前要先deploy API Gateway，确保与Lambda function对应，这个可以在lambda function中的API Gateway endpoint里看到，如果能看到endpoint url，就说明对应成功了。 测试的时候可以加query strings，就是request中的parameters，右边可以看到测试结果，因为这个是GET request，所以我们不能加Request Body。在POST request中是可以的。 这个是另外一个POST request with JSON body Create AWS Serverless Application 这更像是一个Lambda project，可以让API gateway一次连接多个Lambda functions，也可以在Visual Studio里调整API Gateway的一些设置 这是project包含的template文件，我们可以设置API的类型，Request的类型，路径，以及这个API对AWS的可操作权限等 发布后在API Gateway中就可以看到这个层级结构和template是完全一样的 之前讲到我们可以在API Gateway中测试API，其实在Visual Studio里，发布之前我们就可以debug他，就是debug所在的位置，只不过现在显示的是Mock test tool 他长这个样子 其中有两个需要注意的地方，第一个是我们需要调用的function，因为这是一个lambda projects，包含多个lambda functions，所以调用哪一个必须提前声明 还有一个就是input，因为我使用的proxy，所以在列表中选择proxy，会生成一个template，然后我们需要什么input就在相应的地方加就可以了 比如如果要在request body里加，就找到body所在的位置，query parameter和path parameter同理 最后说一下如何发布，跟之前的lambda function一样，只不过我们需要创建一个S3 bucket来存放所有的历史版本。 他会自动清理之前旧版的project，当状态显示的是update complete，我们就知道他已经可以使用了 Calling trusted frame from swift 另外一个任务是，因为我们不能经受任何信用卡信息，所以我们需要用到westpac trusted frame，通过调用这个Javascript library来让用户将信用卡信息直接发送到westpac的服务器，然后处理westpac的response就可以了。但是有一个问题是，我们希望用户能在我们的手机app上也可以调用westpac API，可以这个trusted frame是一个只支持web application的Javascript library，这就需要我们在Swift中加入WebKit，同时在用户在含有trusted frame的web page中submit form时，将返回的信息发送到Swift的某个Method中。简而言之，我们需要在javascript中调用Swift 更改API域名 Custom Domain Names AWS API的标准名字是这样的格式 1https:&#x2F;&#x2F;api-id.execute-api.region.amazonaws.com&#x2F;stage 这很复杂而且跟API的功能也没有任何联系 我们可以更改他的域名，让他的名字更有意义 在新建域名时，我们可以给域名增加certificate，这需要通过在AWS ACM(certificate manager)中新增certificate来实现 我们还需要在internet中新建一个我们想要的域名，然后用route 53来把这个域名和API关联起来，上图中的Target domain name就是自定义域名需要关联的域名。在route 53中找到hosted zones，然后新增一行record，把target domain name填入，再自定义一个域名就可以了 AWS API分为两种，一种是Edge Optimized，另一个是Regional，在API Gateway中每一个API的settings中可以看到，也可以更改其Endpoint type 最后测试一下看看域名是不是成功了，记住route 53需要一点时间才能把域名建好并且go live，所以等几分钟再试 本节参考了这些文章，可以点击以获取更多信息。 Set Up a Custom Domain Name for an API in API Gateway How to Create an Edge-Optimized Custom Domain Name change lambda function outbound IP address to static 我们还可以设置API outbound request的IP，有些时候，当我们需要调用外部API时，他们会有IP地址的限制，只有来自特定whitelist的IP地址发送的请求才会被接受。所以我们需要设置我们发送API时请求的IP地址。因为Lambda function是通过AWS发送的，在被发送时它的IP地址是随机的，虽然不是动态IP，但是每次发送的地址都不一样，我们没办法把所有的IP都加近whitelist。 针对这个问题，我们可以通过把lambda function加入VPC来解决，VPC可以看作是包含一组IP地址的内部云，我们给它加上一个出口，让所有在此VPC的请求都通过这个出口发送出去，这样所有的lambda都会只有一个我们指定的IP了。 我们具体来解释一下AWS VPC的构造 如果AWS EC2是一个云服务器，那么VPC就是这个云服务器的网络层，负责设备之间的信息传输。它由几个重要部分构成 Subnet: 包含一组指定范围的IP，IP格式参照CIDR Route table: 包含信息传输的规则，什么地方来的请求应该到什么地方去，类似一个交通枢纽，负责指挥数据的方向 Internet gateway: VPC的一个重要组成部分，负责VPC内部信息和外部网络的交流，就是这个VPC的出口 Elastic IP: 一个公开的IPv4地址，我们要把它赋给NAT，这样就可以让外部和NAT内部通过这个公开IP进行交流 NAT Gateway: Network Address Translator Gateway, 与Internet Gateway类似，不同的是它可以让private subnet中的数据通过它与外部网络交流 具体做法 新建一个VPC 新建一个Internet Gateway，与VPC相连 新建一个Public subnet，并在route table中新建一个规则，让所有通过这个public subnet的数据流向Internet Gateway 新建一个Elastic IP 新建一个NAT Gateway，把Elastic IP赋给它，并将它放入public subnet中 新建一个private subnet，并在route table中新建一个规则，让所有通过这个private subnet的数据流向NAT Gateway 将lambda function放入private subnet 这样一切就设置完成了，当lambda被执行时，数据会通过private subnet，根据route table的规则流向NAT gateway NAT Gateway将数据赋予Elastic IP，因为NAT在public subnet中，根据route table规则流向Internet Gateway Internet Gateway将数据发给外部网络，此时所有数据都会来自同一个Elastic IP 需要注意的点 在将lambda function放入subnet时，只给它private subnet，不要给public subnet，因为这样他就有可能不通过NAT gateway 给lambda function加VPC需要一个AWS permission 可以创建另一个lambda function，只接受特定IP来测试整个流程是否成功。图为限制IP的policy Reference AWS Lambda functions with a static IP Configuring a Lambda Function to Access Resources in a VPC AWS — Difference between Internet gateway and NAT gateway What Is Amazon VPC? What Is Amazon EC2? Elastic IP Addresses Use IAM to restrict API HTTP basic authentication是一种保护API免受外界攻击的方式，当我们尝试打开某些URL时，有时会出现这个界面让我们输入用户名和密码，这就是一种Basic authentication 这种机制依赖于HTTP Authentication Framework，他的步骤是： 某些人尝试访问一个受保护的URL 服务器返回401未授权的HTTP code，包括一个WWW-Authenticate header with value Basic 浏览器弹出要求用户输入用户名和密码的窗口 请求再次被发送，包括用户输入的授权信息在header中 那么这个简单的API authentication怎么实现呢？ 首先我们去到已经建好的API Gateway中，点击Gateway responses选项，选中Unauthorized，在这里我们可以设置返回什么样的信息给用户当他们尝试访问这个URL时。 当我们添加了WWW-Authenticate header，浏览器就会弹出窗口要求用户自证身份 之后我们需要写一个Lambda function去检查用户提交的信息是否是正确的，也就是检查用户名和密码。这个function也叫做custom authorizer 接下来我们需要让我们的API知道当他接收到Authentication信息时需要调用哪个lambda function来检查，所以去到API gateway的Authorizer选项，新建一个Authorizer，在lambda function中选中我们刚刚写好的lambda function 最后我们还需要让API gateway知道哪个URL endpoint是需要保护的，因为一个API gatewat往往包括很多个URL，我们需要指定一个或多个URL调用刚刚新建的Authorizer 最后别忘了Deploye API，不然所有的设置并不会生效 我们可以测试一下看看访问相同的URL，如果不添加authentication information的话，服务器就会返回401 Unauthorized 除了使用自定义的Authorizer，AWS也内置有IAM (Identity Access Management),在之前的Method Request如果我们选择使用AWS_IAM的话，也可以限制用户对于API的访问 但是我们必须在request中添加AWS signature，这需要使用到AWS给每个account的access key和secret key，其中secret key只有在一开始generate的时候才会看到，之后就看不到了，所以一定要保存在一个安全的地方，要不然的话只能重新generate一对新的key 另外我们可以在resource policy中限制可以访问API的ip address，所以即使用户输入了正确的Authentication information，我们也不能让没有权限的区域访问API 这种IP address limitation也可以在自定义的lambda function中使用 另外，如果要blacklist or whitelist a range of ip addresses，可以使用CIDR notation net mask，在ip address后面加一个斜杠和一个数字，数字表示从左往右有多少bit是在范围外的。 例如“110.142.216.1/24&quot; 就表示了ip range from 110.142.216.1 to 110.142.216.255 Reference HTTP Basic Auth with API Gateway and Serverless Control access to your APIs using Amazon API Gateway resource policies How do I use a resource policy to whitelist certain IP addresses to access my API Gateway API? API Gateway Resource Policy Examples Classless Inter-Domain Routing Use S3 to host webpage S3是一个储存文件的服务，一般来说我们需要把网页放到服务器里去host它，但是S3也提供了host webpage的功能，我们只需要新建一个bucket。 上传我们需要host的webpage 注意在设置里面关闭block public access，毕竟我们需要访问它来获得数据 最后我们需要打开hosting 这样的话我们就可以通过AWS S3来host webpage，不需要使用自己的服务器了 Reference AWS S3 Getting Started What is AWS S3? AWS Simple Email Services 使用AWS发送Email是一件非常简单的事情，通过以下步骤可以创建一个AWS User with email sending policyies并使用C#发送邮件 首先我们需要创建一个AWS User，拿到发送Email的Credentials 然后我们需要设置想用什么邮箱地址发送邮件，我们需要验证，并登陆对应邮箱点击验证链接 之后我们可以测试一下能不能发送Test Email，这里不需要用到Credentials 然后我们就可以看代码了 1234567891011121314151617181920212223242526272829303132333435363738394041public void SendEmail() &#123; String FROM &#x3D; &quot;sender email address&quot;; String FROMNAME &#x3D; &quot;Your Name&quot;; String TO &#x3D; &quot;receiver email address&quot;; String SMTP_USERNAME &#x3D; &quot;Credentials Username&quot;; String SMTP_PASSWORD &#x3D; &quot;Credentials Password&quot;; String HOST &#x3D; &quot;Your AWS host location&quot;; int PORT &#x3D; 587; String SUBJECT &#x3D; &quot;Email Subject&quot;; String BODY &#x3D; &quot;&amp;lth1&amp;gtEmail Title&amp;lt&#x2F;h1&amp;gt&quot; + &quot;&amp;ltp&amp;gtEmail Content&amp;lt&#x2F;p&amp;gt&quot;; MailMessage message &#x3D; new MailMessage(); message.IsBodyHtml &#x3D; true; message.From &#x3D; new MailAddress(FROM, FROMNAME); message.To.Add(new MailAddress(TO)); message.Subject &#x3D; SUBJECT; message.Body &#x3D; BODY; using (var client &#x3D; new System.Net.Mail.SmtpClient(HOST, PORT)) &#123; client.UseDefaultCredentials &#x3D; false; client.Credentials &#x3D; new NetworkCredential(SMTP_USERNAME, SMTP_PASSWORD); client.EnableSsl &#x3D; true; try &#123; Console.WriteLine(&quot;Attempting to send email...&quot;); client.Send(message); Console.WriteLine(&quot;Email sent!&quot;); &#125; catch (Exception ex) &#123; Console.WriteLine(&quot;The email was not sent.&quot;); Console.WriteLine(&quot;Error message: &quot; + ex.Message); &#125; &#125; &#125; 执行这个function几次看看能不能收到邮件，如何可以的话就成功了 AWS 还有统计邮件成功率的工具，也在SES tab里面 另外就是Configuration set是一个optional的设置，可以在AWS里面添加 除了用SMTP，我们还可以用AWS SDK发送邮件，这里就不介绍了，大致都是一样的，代码需要改一下 当你新建一个AWS用户时，你的邮件功能可以会受到限制，也就是说在Sandbox里面，收发数量，频率都会受限，可以给Amazon发送请求移除Sandbox，具体做法可以参考下面链接 Reference Sending a test email Send an email using SMTP Using AWS SES Configuration sets Moving out of sandbox","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"}]},{"title":"Payment Gateway API","slug":"Payment-Gateway-API","date":"2019-07-23T08:59:28.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/07/23/Payment-Gateway-API/","link":"","permalink":"http://hellcy.github.io/2019/07/23/Payment-Gateway-API/","excerpt":"","text":"什么是PCI compliance？ 现在很多公司都允许客户通过信用卡在网上直接付款购买公司的服务和产品，那么必不可少的就是公司需要对用户输入的信用卡信息和其他个人隐私保护，使得外人不能窃取这些信息，PCI DSS是一种数据安全协议，通过参与这个协议，客户就可以放心的将信用卡信息填在公司的网站上而不用担心泄露。因为如果客户的信息泄露了，公司将会受到严重的利息损失，其他客户也将不再愿意相信此公司的信誉。 WestPac QuickStream是一个第三方的payment gateway。我们可以使用这个API来管理用户的付款，他们提供一种trusted frame，使用它，所有的敏感信息将不会经过公司的服务器，而是直接从用户的电脑到Westpac的服务器，由他们来负责保护这些关键信息。 Trusted Frame将用户信息加密(tokenize)，并将加密后的token发送到公司服务器，这样即使公司也无法知道用户信息了。 详细的API使用可以在这里找到。","categories":[],"tags":[{"name":"API","slug":"API","permalink":"http://hellcy.github.io/tags/API/"}]},{"title":"Learn iOS apprentice in 10 days","slug":"Learn-iOS-apprentice-in-10-days","date":"2019-07-03T05:25:31.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/07/03/Learn-iOS-apprentice-in-10-days/","link":"","permalink":"http://hellcy.github.io/2019/07/03/Learn-iOS-apprentice-in-10-days/","excerpt":"","text":"什么是iOS apprentice？ 在完成了上一个任务之后，我总算迎来了又一个更加巨大的挑战，这次直接换了一个新的平台：iOS。我从来没有接触过iOS的编程，之前只是听说过Swift和Cocoa Touch，但是Xcode完全没有用过。于是经过一番上网查找，我发现了一个非常适合新手入门的教材： raywenderlich的iOS apprentice！ 他总共包括四个部分，每一个部分教你编写一个app，难度从低到高。 Bull’s Eye Checklist MyLocation StoreSearch Conclusion Bull’s Eye 从怎么样创建一个新的single view app说起，非常容易上手 Project Navigator Obejct Library Drag item to view Make connections from object item to View Controller Attributes Inspector Item Outlet Another view controller Segue Add Constraints 第一个app基本上解决了很多界面上的问题，storyboard和editor之前的交流也讲了很多，@IBAction， @IBOutlet，segue等等一切都有涉及。之后第二个app就会更深层次的接触到iOS特有的模式了，比如delegate，还有一切经典的design pattern，比如MVC Checklist 第二个app就没有那么简单了，他先讲了table view和navigation bar 跟普通的view不同的是它由一行行的cell组成，这些cell可以被重复使用，当用户往下滑动时，更多的内容会被显示但是并不是每一行data都被存放在一个新的table cell里。离开显示范围的cell会重新出现在底部，并显示出新的data。 Protocol 什么是protocol？他其实是一种已经被写好了的methods的集合，UITableView就是一个protocol，我们通过它来显示table view，但是如果我们想改变一些显示方式，让他更适应我们自己的app，那么就要override其中一些method，这很常见 一般让一个table view更好的显示，我们需要override三个methods 12345override func tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -&gt; Intoverride func tableView(_ tableView: UITableView, cellForRowAt indexPath: IndexPath) -&gt; UITableViewCelloverride func tableView(_ tableView: UITableView, didSelectRowAt indexPath: IndexPath) MVC model MVC model是一种常见的设计模式，将代码分割成三个部分，每一个部分只负责他自己的任务，这样让程序结构更清晰。Model主要负责储存数据，view负责显示数据给用户，而controller负责显示正确的数据，包括运算等等 Array 这个app将table cells中的数据储存在一个array中，新建了一个checklistItem model,这样的话每次如果有数据变动就只需要增加和删除array中的数据。 Initializer 在Swift语言中，所有的variable， obejct都必须被初始化，所以很多时候我们需要一个初始化器来负责检查漏掉的变量。注意当我们有两个view controller需要传递data时，method的执行顺序是：假设A呼叫B 123init() from Bprepare() from AviewDidLoad() from B Delegates Delegates是ios编程中非常重要的一个概念，当你需要把一些值从一个view传到另一个view时，你通常不希望这两个view相互知道太多除了需要值之外的其他东西，如果两个view之间的联系太紧密，那么他们和其他view再建立关联就会更麻烦。所以一种通常的办法是写一个protocol，包含所有delegate methods，再让另外一个view成为它的delegate。这种办法称为loose coupling。 A是B的delegate，然而B根本不知道A的存在，只知道他有一个delegate可以接受他想要传递的值。 建立delegate的步骤 在B中建立delegate protocol 在B中建立一个delegate optional variable 在B中，需要传递信息时呼叫delegate method 在A中conformB的delegate protocol（inheritance 告诉B，A是自己的delegate，一般在A的prepare method中做这一步 Optionals Optional variable在declare的时候用？来表示，表明这个变量有可能是nil，在使用的时候也需要加？在varibale name的后面，如果使用了exclamation mark叹号！，则表示在此时不管变量是什么，强行取得其中的值，此时程序员保证变量在此时不可能是nil，风险也由程序员承担。是用叹号叫做force unwrapping Weak Weak表示两个objects之间的关系，当A对B的关系是strong时表示A是B的owner。当两个obejcts之间都是strong的关系我们可能会产生ownership cycle的问题从而导致memroy leak，因为没有人有资格destroy它的owner。所以我们尽量要维护一种一强一弱的对应关系。 一般来说，A如果是B的owner，A给B传值时在A的prepare method中，此时B的instance已经被建立，A很容易把值传给B的properties，并指认成为B的delegate，B给A传值则需要在合适的时候呼叫delegate methods Save data app会在合适的时候将用户数据保存到手机本地，每一个app都有一个属于自己的folder，也叫做sandbox，每个sandbox之间不能访问，这就预防了一些恶意软件在用户不知情的情况下窃取保存在用户手机其他地方的数据。 数据一般会保存在这样的一个path中，我们可以看到application后面接着一串32的字母的应用ID .plist file 什么是.plist文件？它是一种ios用来储存数据的文件，它遵循XML格式，每一个app都有一个info.plist file，他就是用来保存这个app的配置信息 Special comments 在编写代码时我们可以使用 // MARK：- 的形式来告诉Xcode我们开始了一个section，这样在jump bar中我们就可以快速定位variable，methods的位置。 Type casts 我们使用 as! 来给一个variable赋予一种data type，告诉Swift以后把它当成某一种type来处理，因为有时候我们知道它的type，可是Swift却不知道。 Array of arrays 当app的结构变得复杂时，数据的结构也要相应的变化以适应这些变化，我们可以使用嵌套的array来储存更多的数据 AppDelegate.swift 这个文件用来负责当app刚启动时或快要结束时的一些情况，我们不需要每过一会就储存数据，只需要每当用户退出app或者切换app时，所以这些method需要在AppDelegate里面修改 Dictionary 和array类似，dictionary也是一种储存数据的类型，只不过它是将数据按照key-value pair来排列的，当需要拿到某一个数据时，我们需要给dictionary相应的key。 User Defaults User Defaults就是一个dictionary类型的数据储存文件，它包含户用的配置信息，我们使用它储存一些app刚开始时的默认配置。 Local Notifications 当获得用户许可后，app可以定时在app不活跃时像互用手机发送提醒 这些基本就是第二个app的全部内容了，关于代码的部分可以在我的github的iOSApprenticePratice里面找到，下面是整个app的final storyboard MyLocation App Overview 第三个app我们要做一个可以通过GPS显示当前位置信息，并可以储存，增加照片和类别，在以后可以查看的app。完成之后的效果如下 Tabbed Application 之前的checklist我们学会了如何制作一个有navigation bar的app，这回我们要在app中增加一个tab bar，就是下屏幕最低端有一个导航条，可以切换不同的页面。许多主流的app都有类似的功能。 CoreLocation and ask for permission 要想让app获得GPS信息，我们需要现象用户获得许可，如果用户拒绝让app使用GPS，我们则无法获得权限。我们需要在info.plist里面增加一个record，表示我们需要获得许可，之后我们还会请求照片查看和地图view的许可。 Reverse geocoding 当我们获得了GPS信息，也就是当前位置的经纬度之后，我们还需要将他们转换成地址信息，否则用户也无法直接通过经纬度对当前位置有什么具体的概念。我们要做的是使用swift内置的CLGeocoder.reverseGeocodeLocation来转换。然后我们就可以得到地址信息了。 Auto-resizing 自动调整尺寸大小是一个非常重要的功能，现在市面上的iPhone屏幕尺寸很多，我们如果想对每一个尺寸做优化，为非常耗时，所以使用auto-resizing，让swift知道element在屏幕上的相对位置，比如距离屏幕底端100px，宽度等于屏幕宽度。这样swift就可以画出他的坐标，对于不同大小的屏幕也不会超出屏幕之外。 Class Inheritance, overriding and casts 现在我们来说一下OOP中非常重要的一个概念，class。object就是class的一个具象，而class是这个具象的类。inheritance就是子类可以继承他的parent class的信息。而overriding是说子类可以改变parent class的一些信息，让他有自己的特点。比如汽车是一个parent class，而公交车就是汽车这个class的一个子类，汽车如果有一个property是说自己的轮子个数为4.那么公交车就可以overr这个property，将轮子个数改为8，或者12，或任意。。。casts则是说某些时候swift并不知道现在的变量到底是什么类，只知道他是属于哪个大类的，那么我们如果要使用只存在于子类的properties，我们就需要将这个变量cast成我们想要的类。一般用as! Tag Location Screen 接下来我们看看怎么给现在获得的位置信息添加一些其他的信息，比如description，photo，category。那么我们就需要一个table，这个table我们知道他会有多少行，所以不需要使用prototype cells，使用static cells就可以了。制作一个这样的table非常基本，description用text view，image picker我们之后会讲，category使用disclosure indicator，其他cell就用right detailed就可以。 The unwind segue 当我们点击category时会进入到另一个table with prototype cells。再选一个category后就会回到之前的这个tag location screen，像这种返回式的我们可以使用unwind segue来实现 HUD heads-up display是一种popover view，其实他就是一种view，只不过正常的view现实时其他的view会先被destory以节省memory，但是HUD往往会将背景设置成本透明并保留之前的view，让app有一种层叠的感觉。 Core Data Core Data相当于储存在本地的数据，swift使用SqlLite数据库储存本地数据，我们需要将所有的location信息储存在Core Data中，以便以后打开app时查看之前tag的地点 Notification center 跟iPhone提醒不一样的是，swift有一个Notification center可以让开发者方便的listen从app任何地方发出的notification。比如我想在用户任何时候对储存在Core Data里的Locations信息进行读取，增加，删除，修改的时候得到提醒，我就可以在appDelegate里设置一个notification method，任何地方发出的LocationsUpdatedNotification都会被我捕捉到，并在这个method里面进行相应的操作。 The Locations Tab: 这个view就是显示所有储存在Core Data Locations里面的信息。基本的table with prototype cells，难点在于将Core Data信息提取并转换成相应的type，还有就是在不关闭app的状态下，新增的location可以马上更新到Core Data并显示在此view中，不过有了notification center相信各位也知道怎么做了。 Custom table view cell: 将一个cell的layout写在一个新的文件中，使得代码更易读。跟写在LocationsTabViewController是一样的，只不过拉outlets的时候选新的文件就好了。 Map Kit View: 我们可以在info.plist里增加map view来获得显示apple map的许可。 Image Picker: 同样，请求获取照片查看器的许可 Ownership cycle in closure 先复习一下什么是closure，他其实就是一个method，没什么特殊的，只不过他需要依附于另一个method才能存在。在某些条件被满足时，closure block里面的代码会被执行，比如一段时间过后，api返回成功之后等等。在执行closure时，我们往往会用到被依附的method的property，这时我们需要使用self来显式调用。然而这表示我们对这个property有一种strong reference，并可能导致在closure被执行之前，即使被依附的method所在的class无法被destory，造成memory leak，这是我们就需要使用weak self来破坏ownership cycle。 剩下的就是一些外观，音效和动画的改变了，这里就不赘述了。 StoreSearch App Overview 这本书的最后一个app将用到向apple store发送request获取信息，使用version contorl tools，对iPad等大屏幕设备的适配，异步通信以及把app上传到app store，可以说是一个简单的软件开发流程了。最终完成的结果如下 Git version control 是一个代码管理软件，当我们完成一个阶段的代码后，希望把他保存成一个副本，以后可以随时返回到这个地方。Xcode可以接连到github等网站进行更好的代码管理，也可以使用terminal。我比较喜欢使用terminal，常用的几个指令是： 123git add *git commit -m &quot;commit message&quot;git push -u origin master 当我们想在已经工作的代码中新加一些功能却又害怕出错，然后又改不回来，我们可以使用git创建一个新的branch，然后在新的branch中肆意更改，不用害怕出问题，这就相当于一个副本，如果改不会去了可以直接删掉，git会保存最后工作的版本。若果一切顺利，也可以将新的branch与master合并 当我们在View Controller中创建outlet时，应该使用weak relationship，这是因为每一个view都基本是从另一个super view中继承而来的，所以如果使用strong relationship，将导致view无法被摧毁。 Create branch and merge NeXT Interface Builder: nib file or xib file, 是一种局部storyboard，比如如果我们想设计一种table cell，就可以创建一个新的nib file，然后将这种custom table cell发给任何一个storyboard中的table view， nib file给我们提供多的灵活性 Debug using Xcode 与其他IDE相似，可以设置breakpoint来暂停程序，查看此时各个变量的值。在debug console中输入p instanceName 可以print出想要查看的object的值 Calling the web service 这个app的第一个重头戏就是send API request, 这也是软件开发中最常见的也是必不可少的技能，API request最常见的有两种，GET 和 POST， GET一般用来从远端提取数据，POST一般用来添加或改变远端数据，而API就是本地程序和远端数据连接的接口。本书使用了最简单的没有任何限制的GET request。 encode the url text to escape special characters: 在向API发送请求时我们经常需要一同发送一些parameters，但是space和很多其他它特殊符号都是不能被正常处理的，所以我们需要encode url再发送 Parse JSON data: 返回的数据一般是以JSON的形式组成的，所以我们需要deserilize，Swift有提供官方的decoder可以直接使用 using network link conditioner: 从发送API请求到收到数据总会有那么几秒钟时间，这时如果你的程序是在主线程上，你将无法执行任何操作。直到获取数据。如果网络速度很慢，那么结果将非常糟糕，我们可以使用network link conditioner这一额外开发工具来模拟网速极慢的状态。 Asychronous networking 那么如果解决这几秒钟的类似于死机的状态呢？我们可以使用多线程，并将API请求的操作放到另外一个线程中，这样主线程将不受影响，注意，所有和界面变化相关的操作都应该放在主线程。所以即使数据没有收到，用户也可以进行其他操作，比如取消请求。。。 URLSession: 这是一个专门用来负责多线程的API。他可以负责下载请求，数据请求等很多工作。本书使用URLSession来处理异步通信问题。 Segmented Control: 这是一个很常用的UI模块, 每次切换时可以把segmentedIndex的值传给controller来负责update UI DownloadTask Show DetailView with Present Modally segue: 当点击每个table cell时，会出现这样的一个显示详细信息的窗口，并且也可以看到下面的table view，这个做法其实很简单，每当swift切换到一个新的view时，之前的view默认会被摧毁，但是我们可以改变delegate method，让之前的view保留，并把新的view背景设置成透明。 Dynamic type text: 一些app的字体可以根据用户的系统设置而改变，这就要求我们使用默认字体大小，类似于headline 对于一款手机app来说，好的排版是非常重要的，因为涉及到不同的屏幕尺寸，使用auto-resizing让排版适应所有类型的手机可以扩大目标受众，而不是只为一款手机开发 Landscape 对于像iPad和iPhone Plus的机型来说，很多时候我们会把手机横置，这个时候因为屏幕的宽度和高度变化，我们希望app可以呈现出另一种不同的layout以适应变化的屏幕。 对于这种情况，swift定义了size classes，我们可以通过查看size class区分手机什么时候是横置的。拿iPhone 6 Plus来举例。如下图，当手机竖置的时候，手机屏幕的高度（vertical）是regular模式，而宽度是compact模式，而当手机处于横置的状态时，手机的高度变成了compact模式，而宽度变成了regular模式。这样，我们在写代码的时候，就可以对每一种situation，制定不同的显示模式了。 Enums with associated values: 在StoreSearch这个app中，用户有没有perform search action一共有四种状态，1.还没有search。 2.正在searching。 3. search结束，并且没有找到任何结果。 4.search结束，并且找到了至少一种结果。 那么对于这样一种状态，与其定义四个变量然后在viewcontroller里面查看这四个变量的true/false值，不如直接定义一个enum然后只需查看这一个enum的值即可。 Internationalization 有时候，我们希望让我们的app被来自不同国家使用不同语言的人使用，那么我们就需要将我们的app翻译成不同的语言。我们可以在info tab里新加一种语言。 另外，针对不同的view，我们还需要对每个view新加对应的语言文件。 至于那些不在storyboard里显示的语言信息，我们需要使用NSLocalizedString把所有的语言文本标记出来，再添加到strings文件中去 split view contorlle for iPad: split view是大屏幕设备常用的显示模式，例如iPad和iPhone plus。它可以轻松的利用空间，显示出原本需要两个view才能显示出的内容。 config elements in storyboard based on size class: 但如果我们用之前的view size，在iPad等设备上就会显得很小，所以我们需要针对大屏幕设备进行优化。我们可以在attribute tab找到constant，点击加号来增加一个针对不同情况的尺寸，之前说到iPad设备在横置模式（landscape）下高度和宽度都是regular的，所以这里wR hR就是width Regular, height Regular。这种情况下我们将view size改为500. popovers view controller: 如图所示，这是一个在一个view之上的popover view,可以用来表示一个新的menu email compose sheet: 这是一个可以在app中打开的内建email系统，提前设置好标题和收件人，当用户执行这个method，直接输入email的内容，就可以发送到开发者的邮箱，前提是用户必须提前设置好email账户。 beta testing (test flight): 当你完成一个app之后，可以把它发布到app store，这需要几个步骤，首先你需要提交一个build，在进入iTunes connect查看提交的app，在这里你可以分配给同一个team的其他测试者，这一步称为internal testing，如果一切都没问题，你可以submit app to app store. 如果app通过apple的审查，就可以进行external testing，将app的测试码发给任何人，他们可以通过测试码下载并测试app。 Conclusion 本来想用10天的时间看完这本将近1100页的书，最后用了将近20天才完成。在做最后一个app的时候又有其他的工作加进来，周末又一直有活动，才又延长了这么长时间，不过完成了就是好的，对于一个对iOS没有任何经验的人来说，的确是一本不可多得的好书，如果对自己的编程能力有信心，之前又接触过Java或者C++,直接读这本是没有任何问题的，不过如果没有任何编程经验，从另外一本Swift Apprentice开始更好，从了解swift语言基本开始，使用Xcode playground。 这四个app基本解决了我制作其他app时的所有问题，常用的storyboard object都用了，constraints，delegate, extension, custom with nib, localizition, API calling, local library, local database, simple animation, multi-threading, git, debug到最后app publish都有涉及。 不过在跟着做完所有app之后，我觉得还是需要自己制作一个app，在脱离了guide之后，靠自己的能力解决所有过程中的问题，能使学到的知识更加牢固。","categories":[],"tags":[{"name":"iOS","slug":"iOS","permalink":"http://hellcy.github.io/tags/iOS/"},{"name":"Swift","slug":"Swift","permalink":"http://hellcy.github.io/tags/Swift/"}]},{"title":"Mifare card read and write","slug":"Mifare-card-read-and-write","date":"2019-06-24T05:17:06.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/06/24/Mifare-card-read-and-write/","link":"","permalink":"http://hellcy.github.io/2019/06/24/Mifare-card-read-and-write/","excerpt":"","text":"Mifare card families Mifare卡片有很多种类，每一种卡都有不用的读写方式 Mifare Classic Mifare Plus Mifare Ultralight Mifare DESFire Classic and Mitools Mifare Classic卡是一种卡片类型，它的安全等级不是特别的高，现在已经有可以破解的软件，Mitools就是其中一种。它的界面如下 Mifare卡片总共有16个sector，每一个sector中又有4个block，其中第一个sector是用来卡片生产的过程中记录卡片信息的，一般来说我们不去动他。剩下的15个sector可以给我们用来记录我们想要的信息，其中每个sector的前3个block可以自由使用，而第4个block是用来记录两个密码Key A和Key B。中间的7为则用来记录密码的形式，一般我们也不去动他。密码层在图中用粉色表示。 这是其中一张卡片读取后显示的信息 现在我们来看看怎么从sector7中提取出这张卡片的site code和card number 可以看到第一个block中含有我们需要的信息，而剩下的block2，block3都是0. 我们需要做的就是拿出block1中的信息，这个信息是Hexadecimal也就是16进制的，所以我们先把它变成2进制。1101 1000 1010 0011 1111 1000 0000 0000‬ 因为我们用的是 26 bit format 所以取前26位数 除去第一位even parity，取第二位往后8位：1011 0001，换成十进制就是177 再往后取16位：0100 0111 1111 0000. 十进制就是18416 这样我们就得到了site code: 177, card number: 18416 Ultralight card 与Classic相似的是，Ultralight卡也有很多可以读写的分区，只是不叫sector，而叫做page，从第四个到第15个page是提供给使用者读写的，其他则是与卡本身相关的信息。 每一个page可以保存四个byte的信息 需要注意的是，page2的第2，3个byte保存有lock bits，可以lock其他的page，而且一旦lock，就不能在改变回unlock的状态，比如，0是unlock，1是lock，如果把第二个byte中的第4个bit从0改变为1，那么page4就会被lock，我们不再拥有write page4的权利，但是可以read，page2中的lock bit也无法从1再变回成0. DESFire card DESFire卡的保护机制更加复杂，其中存储信息的空间叫做application，我们需要先选择正确的application和adpu command，然后伴随sw1, sw2两个parameter，最后还有加上一个expected response length 如果要write，就要再加上想要写入的data和数据长度。随着iOS13对于NFC的开放，使用iphone读写DESFire card有可能实现。 Reference Mifare Ultralight lock bits APDU response code list Use APDU commands to get some information for a card","categories":[],"tags":[{"name":"Mifare","slug":"Mifare","permalink":"http://hellcy.github.io/tags/Mifare/"}]},{"title":"Deploy api to IIS manager","slug":"Deploy-api-to-IIS-manager","date":"2019-06-20T05:06:58.000Z","updated":"2021-02-26T12:08:47.158Z","comments":true,"path":"2019/06/20/Deploy-api-to-IIS-manager/","link":"","permalink":"http://hellcy.github.io/2019/06/20/Deploy-api-to-IIS-manager/","excerpt":"","text":"Publish API using visual studio API 完成之后，我们还需要把它deploy到client server里。所以我们先用visual studio publish它 go to Build-&gt;Publish API, 然后点publish，记住publis的路径 这些文件就是一会要copy到client server的文件，也就是api 用remote desktop连接到client server 这个就是client server的IIS manager了，可以看到已经有很多API和app在列表中了 接下来新建一个app connection，路径上新建一个文件夹，把刚才的api文件放进去 更改一下web.config，连接到正确的client database，database name也要一致 测试一下，api有反应，那么就算deploy成功了","categories":[],"tags":[{"name":"API","slug":"API","permalink":"http://hellcy.github.io/tags/API/"},{"name":"IIS","slug":"IIS","permalink":"http://hellcy.github.io/tags/IIS/"}]},{"title":"How to use Gatling load testing tool","slug":"How-to-use-Gatling-load-testing-tool","date":"2019-06-18T03:05:31.000Z","updated":"2021-02-26T12:08:47.159Z","comments":true,"path":"2019/06/18/How-to-use-Gatling-load-testing-tool/","link":"","permalink":"http://hellcy.github.io/2019/06/18/How-to-use-Gatling-load-testing-tool/","excerpt":"","text":"What is load testing load and stress testing. Load testing verifies how the system function under a heavy number of concurrent clients sending requests over a certain period of time. However, the main goal of that type of tests is to simulate the standard traffic similar to that, which may arise on production. Stress testing takes load testing and pushes your app to the limits to see how it handles an extremely heavy load. 使用一些load testing tool往往比自己写一个console application来test要有效，不单单是因为它提供很多可视化的图标可以参考在正常情况下用户的访问情况，还因为它对用户访问处理的算法也更好，可以模拟同时访问的情况而不同我们自己写平行运算。 Gatling 这次我选的是Gatling. 一是因为它是免费的，二是因为它提供很多可视化图表 文件解压后可以看到这些文件夹，其中bin包含程序本身的运行文件。conf是程序的配置文件config，results会有每次测试的报告，一开始应该是空的。user_file是用户的测试配置文件，里面有两个文件夹，一个是resources，所有用到的关联文件类似csv file都应该放在里面。还有就是simulations，这个就是每次测试的配置文件，使用scala写的。但是也很容易懂。 scala文件就长这个样子，需要注意的就是baseUrl，feeder file students.csv，get后面的url剩余部分，还有就是setup里面的同时访问数量。现在我们没有用到任何csv file，但是如果有用到就把他放在resources文件夹里 然后进入bin并运行gatling.bat 选择一个simulation 我们可以看到100个request都完成了 图表也非常丰富 接下来我们看看如果模拟1000个不同的用户在接下来的20秒，因为不需要同时接受1000个用户，所以我们更改setup，使用rampUser 1setUp(scn.inject(rampUsers(1000) over (20 seconds)).protocols(httpProtocol)) 运行报错，但是用console看起来太不方便了。我们更改下config让他把error log导出到一个文件中 找到logback.xml,然后更改成这样，这样每次报错就会生成gatling.log 打开log发现他说：value over is not a member of io.gatling.core.Predef.RampBuilder。 说明没有识别关键词over，google一下这句话。发现是因为这个是旧版本的语法了。。。可是官方guide居然还没有更新。。。 好吧找到migration guide，发现over被替换成during了。。嗯。。真好 重新运行一下gatling.bat，现在行了，还挺像模像样的 现在大部分request都小于800ms 细看的话，同时在线人数基本在50以上，我们用不到那么多。。。并且将近一半的request其实只用了36ms。。 ok，还有一个问题就是这个report中response time的上下限太高了，我们平均都不会超过100所以要更改一下上下限 同样，还是去gatling.conf 改一下lowerBound, higherBound, 记住一定要把#去掉，不然这会被算作comment而没有实际作用 可以看到现在图表中的response time变成100ms何200ms了","categories":[],"tags":[{"name":"Load Testing","slug":"Load-Testing","permalink":"http://hellcy.github.io/tags/Load-Testing/"}]},{"title":"Create a Restful api using C#","slug":"Create-a-Restful-api-using-C-Sharp","date":"2019-06-13T02:26:52.000Z","updated":"2021-02-26T12:08:47.158Z","comments":true,"path":"2019/06/13/Create-a-Restful-api-using-C-Sharp/","link":"","permalink":"http://hellcy.github.io/2019/06/13/Create-a-Restful-api-using-C-Sharp/","excerpt":"","text":"今天拿到了一个任务 要写一个API，要求是每次学生登陆之后可以看到他的学生卡的相关信息，包括卡号，有效期，余额，还有可不可以自动充值等等。 那么我们就要先写一个stored procedure把相关信息提取出来,这些信息牵扯到4个table来自于两个database。 随便打一个StudentID试一下看看能不能成功运行 接着在Microsoft Doc tutorial可以帮助我们写一个简单的api模版。在用connectionString连接上database然后用postman测试一下 这样基本上api就算写的差不多了，接下来还要把格式改成要求的样子 所以把它改成JSON Object这样好改一点，刚才都是直接return DataTable。 结构基本就是这样，然后再测试一下看看能不能拿到相同的格式 要求中还说到要有基本的authentication。有可能就是在function前面加上authorize attribute，等下周问问leader。另外就是要log所有的api calls。包括api整个的运行时间（response time），StudentID，现在的系统时间。用stopwatch计算出每次call所需的时间，其他的都好弄。我先把output写在debug console里了，回头再改看看要不要直接output到一个file里面。 接下来就是要写一个console application来call我们的api了，毕竟不能一直用postman做测试啊。 用HttpClient去call api，已经成功了，但是有两个问题，一个是如果连续call10次的话每次用的时间都很长，用postman的时候很多时候只有20ms，但是现在有上千。。。第二个问题就是我还需要做一个simultaneously call。 一个周末过去了。。 然后就是每次同时call10次api，这个的话我上网查了查，同样只要改await就可以. 这个async，await的意思是：如果task前面有await，那么程序会暂停直到拿到task的返回值或task结束运行之后才会继续。而如果task前面没有await，意味着在它被创建的时候程序可以继续运行之后的代码，只要它们不依靠task的结果。所以我们可以创建10个没有await的task，最后在一起await然后output。 这样的话虽然每次api call的时间比一次一次call要长很多但是如果计算500次call的总时间的话平行运算还是有优势的 平行运算时每次api call的时间 500次call的总时间，总共算了10次，基本在8秒左右 单次运算时每次api call的时间 单次运算时每500次的总时间，总共算了10次，基本都超过了10秒 关于在api中加入basic authenticaiton：To access the web API method, we have to pass the user credentials in the request header. If we do not pass the user credentials in the request header, then the server returns 401 (unauthorized) status code indicating the server supports Basic Authentication. 新建一个class， inhert from IHttpModule，这样就可以让api support basic authentication了。但是我们还要让console在request中加入auth header。 试了一下可以运行，那么这个task到现在就基本完成了。看看leader有什么别的要求再说吧。","categories":[],"tags":[{"name":"C#","slug":"C","permalink":"http://hellcy.github.io/tags/C/"},{"name":"API","slug":"API","permalink":"http://hellcy.github.io/tags/API/"}]}],"categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://hellcy.github.io/tags/Java/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://hellcy.github.io/tags/Design-Patterns/"},{"name":"Elastic Search","slug":"Elastic-Search","permalink":"http://hellcy.github.io/tags/Elastic-Search/"},{"name":".NET","slug":"NET","permalink":"http://hellcy.github.io/tags/NET/"},{"name":"React","slug":"React","permalink":"http://hellcy.github.io/tags/React/"},{"name":"Spring","slug":"Spring","permalink":"http://hellcy.github.io/tags/Spring/"},{"name":"git","slug":"git","permalink":"http://hellcy.github.io/tags/git/"},{"name":"Wishlist","slug":"Wishlist","permalink":"http://hellcy.github.io/tags/Wishlist/"},{"name":"AWS","slug":"AWS","permalink":"http://hellcy.github.io/tags/AWS/"},{"name":"Data Virtualization","slug":"Data-Virtualization","permalink":"http://hellcy.github.io/tags/Data-Virtualization/"},{"name":"SQL","slug":"SQL","permalink":"http://hellcy.github.io/tags/SQL/"},{"name":"Data","slug":"Data","permalink":"http://hellcy.github.io/tags/Data/"},{"name":"Azure","slug":"Azure","permalink":"http://hellcy.github.io/tags/Azure/"},{"name":"Scrum","slug":"Scrum","permalink":"http://hellcy.github.io/tags/Scrum/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://hellcy.github.io/tags/TCP-IP/"},{"name":"LINQ","slug":"LINQ","permalink":"http://hellcy.github.io/tags/LINQ/"},{"name":"Entity Framework","slug":"Entity-Framework","permalink":"http://hellcy.github.io/tags/Entity-Framework/"},{"name":"GraphQL","slug":"GraphQL","permalink":"http://hellcy.github.io/tags/GraphQL/"},{"name":"Selenium","slug":"Selenium","permalink":"http://hellcy.github.io/tags/Selenium/"},{"name":"Functional Tests","slug":"Functional-Tests","permalink":"http://hellcy.github.io/tags/Functional-Tests/"},{"name":"Dependency Injection","slug":"Dependency-Injection","permalink":"http://hellcy.github.io/tags/Dependency-Injection/"},{"name":"Unit Tests","slug":"Unit-Tests","permalink":"http://hellcy.github.io/tags/Unit-Tests/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://hellcy.github.io/tags/JavaScript/"},{"name":"ASP.NET Core","slug":"ASP-NET-Core","permalink":"http://hellcy.github.io/tags/ASP-NET-Core/"},{"name":"C#","slug":"C","permalink":"http://hellcy.github.io/tags/C/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","permalink":"http://hellcy.github.io/tags/Dynamic-Programming/"},{"name":"Web Development","slug":"Web-Development","permalink":"http://hellcy.github.io/tags/Web-Development/"},{"name":"Data Structures","slug":"Data-Structures","permalink":"http://hellcy.github.io/tags/Data-Structures/"},{"name":"Python","slug":"Python","permalink":"http://hellcy.github.io/tags/Python/"},{"name":"SSO","slug":"SSO","permalink":"http://hellcy.github.io/tags/SSO/"},{"name":"Kotlin","slug":"Kotlin","permalink":"http://hellcy.github.io/tags/Kotlin/"},{"name":"Android Studio","slug":"Android-Studio","permalink":"http://hellcy.github.io/tags/Android-Studio/"},{"name":"Swift","slug":"Swift","permalink":"http://hellcy.github.io/tags/Swift/"},{"name":"API","slug":"API","permalink":"http://hellcy.github.io/tags/API/"},{"name":"iOS","slug":"iOS","permalink":"http://hellcy.github.io/tags/iOS/"},{"name":"Mifare","slug":"Mifare","permalink":"http://hellcy.github.io/tags/Mifare/"},{"name":"IIS","slug":"IIS","permalink":"http://hellcy.github.io/tags/IIS/"},{"name":"Load Testing","slug":"Load-Testing","permalink":"http://hellcy.github.io/tags/Load-Testing/"}]}