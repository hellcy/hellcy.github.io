<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://hellcy.github.io/atom.xml" rel="self"/>
  
  <link href="http://hellcy.github.io/"/>
  <updated>2022-02-20T07:28:58.580Z</updated>
  <id>http://hellcy.github.io/</id>
  
  <author>
    <name>Yuan Cheng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Java Reflection</title>
    <link href="http://hellcy.github.io/2022/02/20/Java-Reflection/"/>
    <id>http://hellcy.github.io/2022/02/20/Java-Reflection/</id>
    <published>2022-02-20T05:27:27.000Z</published>
    <updated>2022-02-20T07:28:58.580Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是java反射"><a class="markdownIt-Anchor" href="#什么是java反射"></a> 什么是Java反射</h1><ul><li>反射reflection是在运行时动态访问类与对象的技术</li><li>反射是JDK1.2版本后的高级特性，隶属于<code>java.lang.reflect</code></li><li>他将对象的创建时机从原本的编译时创建延迟到运行时创建</li><li>大多数Java框架都基于反射实现参数配置，动态注入等特性</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Scanner in &#x3D; new Scanner(System.in);</span><br><span class="line">System.out.println(&quot;please enter math operation&quot;);</span><br><span class="line">String op &#x3D; in.next();</span><br><span class="line">System.out.println(&quot;please enter first number&quot;);</span><br><span class="line">int a &#x3D; in.nextInt();</span><br><span class="line">System.out.println(&quot;please enter second number&quot;);</span><br><span class="line">int b &#x3D; in.nextInt();</span><br><span class="line">MathOperation operation &#x3D; null;</span><br><span class="line"></span><br><span class="line">operation &#x3D; (MathOperation) Class.forName(op).getDeclaredConstructor().newInstance();</span><br></pre></td></tr></table></figure><h1 id="反射的核心类"><a class="markdownIt-Anchor" href="#反射的核心类"></a> 反射的核心类</h1><ul><li>Class</li><li>Constructor</li><li>Method</li><li>Field</li></ul><h1 id="class类"><a class="markdownIt-Anchor" href="#class类"></a> Class类</h1><ul><li>Class是JVM中代表<code>类和接口</code>的类</li><li>Class对象具体包含了某个特定类的结构信息</li><li>通过Class类可以获取对应类的构造方法，方法，成员变量</li></ul><h2 id="class核心方法"><a class="markdownIt-Anchor" href="#class核心方法"></a> Class核心方法</h2><ul><li>Class.forName() - 静态方法，用于获取指定Class对象</li><li>classObj.newInstance() - 通过默认构造方法创建新的对象 （在Java9之后被deprecated）</li><li>classObj.getConstructor() - 获取指定的public修饰构造方法Constructor对象</li><li>classObj.getMethod() - 获取指定的public修饰方法Method对象</li><li>classObj.getField() - 获取指定的public修饰的成员变量Field对象</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 将Employee类加载到JVM，并返回对应Class对象</span><br><span class="line">Class employeeClass &#x3D; Class.forName(&quot;entity.Employee&quot;);</span><br><span class="line">System.out.println(&quot;Employee has been loaded to JVM&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; newInstance 调用默认构造方法创建新对象</span><br><span class="line">Employee emp &#x3D; (Employee) employeeClass.getDeclaredConstructor().newInstance();</span><br></pre></td></tr></table></figure><h2 id="创建class对象时可能抛出的异常"><a class="markdownIt-Anchor" href="#创建class对象时可能抛出的异常"></a> 创建Class对象时可能抛出的异常</h2><ul><li>InstantiationException<ul><li>实例化异常，对象无法被实例化</li><li>例如abstract抽象对象</li></ul></li><li>IllegalAccessException<ul><li>非法访问，在作用域外访问对象构造方法或成员变量</li><li>例如尝试访问私有构造方法</li></ul></li></ul><h1 id="constructor类"><a class="markdownIt-Anchor" href="#constructor类"></a> Constructor类</h1><ul><li>对Java类中的构造方法的抽象</li><li>Constructor对象包含了具体类的某个具体构造方法的声明</li><li>通过Constructor对象调用带参构造方法创建对象</li></ul><h2 id="constructor类的核心方法"><a class="markdownIt-Anchor" href="#constructor类的核心方法"></a> Constructor类的核心方法</h2><ul><li><p>classObj.getConstructor() - 获取指定public修饰的构造方法对象</p></li><li><p>constructorObj.newInstance() - 通过对应的构造方法创建对象</p></li><li><p>例子，在通过Class对象创建Constructor对象时，需要提供每一个参数的Class类</p></li><li><p>在Constructor对象创建Employee对象时，需要传入每一个参数</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Class employeeClass &#x3D; Class.forName(&quot;entity.Employee&quot;);</span><br><span class="line">Constructor constructor &#x3D; employeeClass.getConstructor(new Class[] &#123;</span><br><span class="line">        Integer.class,</span><br><span class="line">        String.class,</span><br><span class="line">        Float.class,</span><br><span class="line">        String.class</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Employee employee &#x3D; (Employee) constructor.newInstance(new Object[] &#123;</span><br><span class="line">        100, &quot;yuan cheng&quot;, 3000f, &quot;研发部&quot;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="创建constructor对象时可能抛出的异常"><a class="markdownIt-Anchor" href="#创建constructor对象时可能抛出的异常"></a> 创建Constructor对象时可能抛出的异常</h2><ul><li>InstantiationException<ul><li>实例化异常，对象无法被实例化</li><li>例如abstract抽象对象</li></ul></li><li>IllegalAccessException<ul><li>非法访问，在作用域外访问对象构造方法或成员变量</li><li>例如尝试访问私有构造方法</li></ul></li><li>InvocationTargetException<ul><li>当被调用的方法内部抛出了异常而没有被捕获时</li></ul></li><li>NoSuchMethodException<ul><li>没有找到与之对应的构造方法</li></ul></li></ul><h1 id="method类"><a class="markdownIt-Anchor" href="#method类"></a> Method类</h1><ul><li>Method对象指代某个类中的方法的描述</li><li>Method对象使用classObj.getMethod()方法获取</li><li>通过Method对象调用指定对象的对应方法</li></ul><h2 id="method类核心方法"><a class="markdownIt-Anchor" href="#method类核心方法"></a> Method类核心方法</h2><ul><li>classObj.getMethod() - 获取指定public修饰的方法的对象</li><li>methodObj.invoke() - 调用指定对象的对应方法</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Class employeeClass &#x3D; Class.forName(&quot;entity.Employee&quot;);</span><br><span class="line">Constructor constructor &#x3D; employeeClass.getConstructor(new Class[] &#123;</span><br><span class="line">        Integer.class, String.class, Float.class, String.class</span><br><span class="line">&#125;);</span><br><span class="line">Employee employee &#x3D; (Employee) constructor.newInstance(new Object[] &#123;</span><br><span class="line">        100, &quot;yuan&quot;, 3000f, &quot;研发部&quot;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Method method &#x3D; employeeClass.getMethod(&quot;updateSalary&quot;, Float.class);</span><br><span class="line">Employee newEmpoyee &#x3D; (Employee) method.invoke(employee, 1000f);</span><br><span class="line">System.out.println(newEmpoyee);</span><br></pre></td></tr></table></figure><h1 id="field类"><a class="markdownIt-Anchor" href="#field类"></a> Field类</h1><ul><li>对应某个具体类中成员变量的声明</li><li>Field对象使用classObj.getField()方法获取</li><li>通过Field对象可为某对象成员变量赋值/取值</li></ul><h2 id="field类核心方法"><a class="markdownIt-Anchor" href="#field类核心方法"></a> Field类核心方法</h2><ul><li>classObj.getField() - 获取指定public修饰的成员变量对象</li><li>fieldObj.set() - 为某对象指定成员变量赋值</li><li>fieldObj.get() - 获取某对象指定成员变量数值</li></ul><h2 id="field可能抛出的异常"><a class="markdownIt-Anchor" href="#field可能抛出的异常"></a> Field可能抛出的异常</h2><ul><li>NoSuchFieldException<ul><li>没有找到对应的Field</li><li>当尝试访问private的Field时也会抛出这个异常</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Class employeeClass &#x3D; Class.forName(&quot;entity.Employee&quot;);</span><br><span class="line">Constructor constructor &#x3D; employeeClass.getConstructor(new Class[] &#123;</span><br><span class="line">        Integer.class, String.class, Float.class, String.class</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Employee employee &#x3D; (Employee) constructor.newInstance(new Object[] &#123;</span><br><span class="line">        100, &quot;Yuan&quot;, 4000f, &quot;研发部&quot;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Field enameField &#x3D; employeeClass.getField(&quot;ename&quot;);</span><br><span class="line">String ename &#x3D; (String) enameField.get(employee);</span><br><span class="line">System.out.println(ename);</span><br><span class="line"></span><br><span class="line">enameField.set(employee, &quot;new yuan&quot;);</span><br><span class="line">System.out.println(employee);</span><br></pre></td></tr></table></figure><h1 id="getdeclared-系列方法"><a class="markdownIt-Anchor" href="#getdeclared-系列方法"></a> getDeclared。。。系列方法</h1><ul><li>之前的方法只能获取public对象</li><li>getDeclared。。系列方法可以获取非作用域内的构造方法，方法，成员变量 （private）</li></ul><h2 id="例子如果我们想获取当前对象的所有成员变量的值"><a class="markdownIt-Anchor" href="#例子如果我们想获取当前对象的所有成员变量的值"></a> 例子，如果我们想获取当前对象的所有成员变量的值</h2><ul><li>不管是private还是public，我们需要用到getDeclaredFields()</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Class employeeClass &#x3D; Class.forName(&quot;entity.Employee&quot;);</span><br><span class="line">Constructor constructor &#x3D; employeeClass.getConstructor(new Class[] &#123;</span><br><span class="line">        Integer.class, String.class, Float.class, String.class</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">Employee employee &#x3D; (Employee) constructor.newInstance(new Object[] &#123;</span><br><span class="line">        100, &quot;Yuan&quot;, 4000f, &quot;研发部&quot;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 获取当前类所有成员变量 private + public</span><br><span class="line">Field[] fields &#x3D; employeeClass.getDeclaredFields();</span><br><span class="line">for (Field field : fields) &#123;</span><br><span class="line">    if (field.getModifiers() &#x3D;&#x3D; Modifier.PUBLIC) &#123;</span><br><span class="line">        &#x2F;&#x2F; public fields</span><br><span class="line">        Object val &#x3D; field.get(employee);</span><br><span class="line">        System.out.println(field.getName() + &quot;: &quot; + val);</span><br><span class="line">    &#125; else if (field.getModifiers() &#x3D;&#x3D; Modifier.PRIVATE) &#123;</span><br><span class="line">        &#x2F;&#x2F; private fields</span><br><span class="line">        String methodName &#x3D; &quot;get&quot; + field.getName().substring(0, 1).toUpperCase()</span><br><span class="line">                + field.getName().substring(1);</span><br><span class="line">        Method getMethod &#x3D; employeeClass.getMethod(methodName);</span><br><span class="line">        Object ret &#x3D; getMethod.invoke(employee);</span><br><span class="line">        System.out.println(field.getName() + &quot;: &quot; + ret);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="反射在项目中的应用"><a class="markdownIt-Anchor" href="#反射在项目中的应用"></a> 反射在项目中的应用</h1><ul><li>切换网站的语言</li><li>根据不用的设备切换网站layout</li><li>不需要重新编译或者改变已有代码</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是java反射&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#什么是java反射&quot;&gt;&lt;/a&gt; 什么是Java反射&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;反射reflection是在运行时动态访问类与对象的技术&lt;/li&gt;
&lt;li&gt;反射是JDK1</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Factory Design Pattern</title>
    <link href="http://hellcy.github.io/2022/02/20/Factory-Design-Pattern/"/>
    <id>http://hellcy.github.io/2022/02/20/Factory-Design-Pattern/</id>
    <published>2022-02-20T04:52:51.000Z</published>
    <updated>2022-02-20T05:26:28.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="设计模式"><a class="markdownIt-Anchor" href="#设计模式"></a> 设计模式</h1><ul><li>前辈总结的设计经验</li><li>使代码更容易理解，更容易维护</li><li>让代码更加可靠</li></ul><h1 id="设计模式的分类"><a class="markdownIt-Anchor" href="#设计模式的分类"></a> 设计模式的分类</h1><ul><li>创建型模式<ul><li>帮助我们更加精巧的创建对象</li></ul></li><li>结构性模式<ul><li>重构，抽象，是代码更容易维护和扩展</li></ul></li><li>行为型模式<ul><li>针对现实中的具体场景进行优化</li></ul></li></ul><h1 id="工厂模式"><a class="markdownIt-Anchor" href="#工厂模式"></a> 工厂模式</h1><ul><li>用于隐藏创建对象的细节</li><li>核心： 工厂类，帮助我们创建具体对象</li><li>工厂模式可细分为<ul><li>简单工厂</li><li>工厂方法</li><li>抽象工厂</li></ul></li></ul><p><img src="/images/Factory-Design-Pattern/1.png" alt="" /></p><ul><li><p>所有的对象都实现同一个接口</p></li><li><p>工厂类负责判断具体创建那个对象，并返回相应的接口</p></li><li><p>用户只需要传入参数告诉工厂类，工厂类负责根据传入的参数判断具体创建那个对象</p></li><li><p>对象以接口的形式返回给用户，用户不需要知道具体创建了那个类的对象</p></li><li><p>用户类</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">I18N i18N &#x3D; I18NFactory.getI18NObject(&quot;china&quot;);</span><br><span class="line">Device device &#x3D; DeviceFactory.getDeviceObject(&quot;mobile&quot;);</span><br><span class="line">System.out.println(device.getHomePage());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>工厂类</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class DeviceFactory &#123;</span><br><span class="line">  &#x2F;&#x2F; 静态工厂，创建具体对象的方法是静态的</span><br><span class="line">  &#x2F;&#x2F; 在调用时不需要实例化工厂</span><br><span class="line">  public static Device getDeviceObject(String userAgent) &#123;</span><br><span class="line">    if (userAgent.equals(&quot;mobile&quot;)) &#123;</span><br><span class="line">      return new MobileDevice();</span><br><span class="line">    &#125; else if (userAgent.equals(&quot;desktop&quot;)) &#123;</span><br><span class="line">      return new DesktopDevice();</span><br><span class="line">    &#125; else return null;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>接口</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public interface Device &#123;</span><br><span class="line">  String getHomePage();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>具体实现</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class MobileDevice implements Device &#123;</span><br><span class="line">    public String getHomePage() &#123;</span><br><span class="line">        return &quot;mobile device home page&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class DesktopDevice implements Device &#123;</span><br><span class="line">    public String getHomePage() &#123;</span><br><span class="line">        return &quot;Desktop device home page&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;设计模式&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#设计模式&quot;&gt;&lt;/a&gt; 设计模式&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;前辈总结的设计经验&lt;/li&gt;
&lt;li&gt;使代码更容易理解，更容易维护&lt;/li&gt;
&lt;li&gt;让代码更加可靠&lt;/li&gt;
&lt;/ul</summary>
      
    
    
    
    
    <category term="Design Patterns" scheme="http://hellcy.github.io/tags/Design-Patterns/"/>
    
    <category term="Factory" scheme="http://hellcy.github.io/tags/Factory/"/>
    
  </entry>
  
  <entry>
    <title>Maven Basics</title>
    <link href="http://hellcy.github.io/2022/02/20/Maven-Basics/"/>
    <id>http://hellcy.github.io/2022/02/20/Maven-Basics/</id>
    <published>2022-02-20T02:34:49.000Z</published>
    <updated>2022-02-20T04:51:47.144Z</updated>
    
    <content type="html"><![CDATA[<h1 id="maven介绍"><a class="markdownIt-Anchor" href="#maven介绍"></a> Maven介绍</h1><ul><li>项目管理工具，对软件项目提供构建与依赖管理</li><li>Apache下的Java开源项目</li><li>为Java项目提供了统一的管理方式</li></ul><h1 id="maven核心特性"><a class="markdownIt-Anchor" href="#maven核心特性"></a> Maven核心特性</h1><ul><li>项目设置遵循统一的规则，保证不同开发环境的兼容性 （Eclipse， IDEA， NetBeans）</li><li>强大的依赖管理，项目依赖组件自动下载，自动更新（JARs，dependencies）</li><li>Maven中央仓库</li><li>可扩展的插件机制，使用简单，功能丰富 （自定义的插件可以加入到Maven中）</li></ul><h1 id="maven的坐标"><a class="markdownIt-Anchor" href="#maven的坐标"></a> Maven的坐标</h1><ul><li>GroupId：机构或者团体的英文，采用逆向域名的形式书写</li><li>ArtifactId：项目名称，说明其用途，例如：cms， oa</li><li>Version：版本号，一般采用版本 + 单词 形式书写 例如： 1.0.0.RELEASE</li></ul><h1 id="maven项目标准结构"><a class="markdownIt-Anchor" href="#maven项目标准结构"></a> Maven项目标准结构</h1><table><thead><tr><th>目录</th><th>用途</th></tr></thead><tbody><tr><td>root</td><td>根目录，用于保存pom.xml</td></tr><tr><td>main/java</td><td>Java源代码目录</td></tr><tr><td>main/resources</td><td>资源目录，保存配置文件，静态图片等</td></tr><tr><td>test/java</td><td>测试类的源代码</td></tr><tr><td>test/resources</td><td>测试时需要使用的资源文件</td></tr><tr><td>target</td><td>项目输出的目录，用于储存jar，war文件</td></tr><tr><td>target/classes</td><td>字节码的编译输出目录</td></tr><tr><td>pom.xml</td><td>项目对象模型文件（Project，Object，Model）</td></tr></tbody></table><h1 id="maven依赖管理"><a class="markdownIt-Anchor" href="#maven依赖管理"></a> Maven依赖管理</h1><ul><li>利用dependency自动下载，管理第三方JAR</li><li>在pom.xml文件中配置项目依赖的第三方组件</li><li>Maven自动将依赖从远程仓库（中央仓库）下载至本地仓库，并在工程中引用</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;5.1.47&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><ul><li>Maven会自动下载JAR所依赖的其他JAR</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-webmvc&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;5.3.16&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><ul><li>例如spring-webmvc依赖很多其他JAR，但当你引用spring-webmvc时，你无需管理其他依赖，Maven会自动管理并下载其他所需的依赖</li></ul><h1 id="本地仓库与中央仓库"><a class="markdownIt-Anchor" href="#本地仓库与中央仓库"></a> 本地仓库与中央仓库</h1><p><img src="/images/Maven-Basics/1.png" alt="" /><br /><img src="/images/Maven-Basics/2.png" alt="" /></p><h1 id="项目打包"><a class="markdownIt-Anchor" href="#项目打包"></a> 项目打包</h1><ul><li>Maven可将Java项目打包为JAR，WAR</li><li>项目打包是通过Plugin实现</li><li>Maven输出JAR包插件： <code>maven-assembly-plugin</code></li><li>运行JAR in Terminal<ul><li><code>java -jar jav_name</code></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;maven介绍&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#maven介绍&quot;&gt;&lt;/a&gt; Maven介绍&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;项目管理工具，对软件项目提供构建与依赖管理&lt;/li&gt;
&lt;li&gt;Apache下的Java开源项目&lt;/li</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
    <category term="Maven" scheme="http://hellcy.github.io/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>JDBC Basics</title>
    <link href="http://hellcy.github.io/2022/02/17/JDBC-Basics/"/>
    <id>http://hellcy.github.io/2022/02/17/JDBC-Basics/</id>
    <published>2022-02-17T11:51:57.000Z</published>
    <updated>2022-02-20T02:33:42.852Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是jdbc"><a class="markdownIt-Anchor" href="#什么是jdbc"></a> 什么是JDBC</h1><ul><li><code>Java Database Connectivity</code></li><li>让Java程序操作关系型数据库</li><li>JDBC基于驱动程序实现与数据库的连接与操作</li><li>驱动程序实现了JDBC API，所以Java程序可以调用JABC API（也就是调用底层的MySQL驱动程序的实现）来对数据库进行访问</li></ul><h1 id="jdbc的优点"><a class="markdownIt-Anchor" href="#jdbc的优点"></a> JDBC的优点</h1><ul><li>统一的API，提供一指的开发过程</li><li>易于学习，容易上手，代码结构稳定</li><li>功能强大，执行效率高，可处理大量数据</li></ul><h1 id="jdbc开发流程"><a class="markdownIt-Anchor" href="#jdbc开发流程"></a> JDBC开发流程</h1><ol><li>加载并注册JDBC驱动</li><li>创建数据库链接</li><li>创建Statement对象</li><li>遍历查询结果</li><li>关闭链接，释放资源</li></ol><h2 id="classforname的作用"><a class="markdownIt-Anchor" href="#classforname的作用"></a> Class.forName的作用</h2><ul><li>用于加载指定的JDBC驱动类</li><li>本质是通知JDBC注册这个驱动类</li></ul><h2 id="drivermanager"><a class="markdownIt-Anchor" href="#drivermanager"></a> DriverManager</h2><ul><li>用于注册，管理JDBC驱动程序</li><li><code>DriverManager.getConnection()</code></li><li>返回值Connection对象，对应数据库的物理网络连接</li></ul><h2 id="connection对象"><a class="markdownIt-Anchor" href="#connection对象"></a> Connection对象</h2><ul><li>Connection对象用于JDBC与数据库的网络通信对象</li><li><code>java.sql.Connection</code>是一个接口，具体实现由驱动厂商负责</li><li>所有数据库的操作都建立在Connection基础上</li></ul><h2 id="链接字符串"><a class="markdownIt-Anchor" href="#链接字符串"></a> 链接字符串</h2><ul><li><code>jdbc:mysql://localhost:port_number/database_name</code></li><li>之后可以添加额外参数</li><li>参数列表采用URL格式：name=value&amp;name2=value2&amp;…</li></ul><h3 id="mysql连接字符串常用参数"><a class="markdownIt-Anchor" href="#mysql连接字符串常用参数"></a> MySQL连接字符串常用参数</h3><table><thead><tr><th>参数名</th><th>建议参数值</th><th>说明</th></tr></thead><tbody><tr><td>useSSL</td><td>true/false</td><td>是否禁用SSL</td></tr><tr><td>useUnicode</td><td>true</td><td>启用unicode编码传输数据</td></tr><tr><td>characterEncoding</td><td>UTF-8</td><td>使用UTF-8编码</td></tr><tr><td>serverTimezone</td><td>Australia/Sydney</td><td>timezone</td></tr><tr><td>allowPublicKeyRetrieval</td><td>true</td><td>允许从客户端获取公钥加密传输</td></tr></tbody></table><ul><li>如果数据库的设置已经是使用澳洲时区，那么连接字符串的参数可以省略</li></ul><h2 id="sql注入攻击"><a class="markdownIt-Anchor" href="#sql注入攻击"></a> SQL注入攻击</h2><ul><li><code>'</code>单引号没有被特殊处理，被当成SQL query的一部分</li><li>利用SQL漏洞越权获取数据的黑客行为</li><li>根源是未对原始SQL中的敏感字符做特殊处理 （转译escape）</li><li>解决方法：放弃Statement改用PreparedStatement处理SQL</li></ul><h3 id="preparestatement"><a class="markdownIt-Anchor" href="#preparestatement"></a> PrepareStatement</h3><ul><li>是Statement的子接口</li><li>对SQL进行参数化，预防SQL注入攻击</li><li>比Statement执行效率更好</li></ul><h3 id="错误的使用方式"><a class="markdownIt-Anchor" href="#错误的使用方式"></a> 错误的使用方式</h3><ul><li><code>select * from employee where ? = 'abc';</code></li><li><code>select * from employee where salary = ? + 100;</code></li><li><code>select * from employee where ename = ?;</code></li></ul><h2 id="封装数据库连接"><a class="markdownIt-Anchor" href="#封装数据库连接"></a> 封装数据库连接</h2><ul><li>每次对数据库操作时都需要打开和关闭，我们可以将这个重复的步骤封装到一个Class中，需要时直接调用</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * 封装数据库链接</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class DbUtils &#123;</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * 创建新的数据库链接</span><br><span class="line">   * @return 新的Connection对象</span><br><span class="line">   * @throws SQLException</span><br><span class="line">   * @throws ClassNotFoundException</span><br><span class="line">   *&#x2F;</span><br><span class="line">  public static Connection getConnection() throws SQLException, ClassNotFoundException &#123;</span><br><span class="line">    &#x2F;&#x2F;1. 加载并注册JDBC驱动</span><br><span class="line">    Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;2. 创建数据库链接</span><br><span class="line">    Connection conn &#x3D; DriverManager.getConnection(</span><br><span class="line">            &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;imooc?useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;serverTimezone&#x3D;Australia&#x2F;Sydney&quot;,</span><br><span class="line">            &quot;yuan&quot;,</span><br><span class="line">            &quot;1111&quot;</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    return conn;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * 关闭链接，释放资源</span><br><span class="line">   * @param rs 结果集对象</span><br><span class="line">   * @param statement Statement对象</span><br><span class="line">   * @param conn Connection对象</span><br><span class="line">   *&#x2F;</span><br><span class="line">  public static void closeConnection(ResultSet rs, Statement statement, Connection conn) &#123;</span><br><span class="line">    &#x2F;&#x2F; 5. 关闭链接，释放资源</span><br><span class="line">    try &#123;</span><br><span class="line">      if (rs !&#x3D; null) conn.close();</span><br><span class="line">      if (statement !&#x3D; null) statement.close();</span><br><span class="line">      if (conn !&#x3D; null &amp;&amp; !conn.isClosed()) conn.close();</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="jdbc执行insert-update-delete语句"><a class="markdownIt-Anchor" href="#jdbc执行insert-update-delete语句"></a> JDBC执行INSERT, UPDATE, DELETE语句</h2><ul><li>statement.executeUpdate()</li><li>返回受影响得行数</li></ul><h2 id="jdbc事务管理"><a class="markdownIt-Anchor" href="#jdbc事务管理"></a> JDBC事务管理</h2><ul><li>事务依赖于数据库的实现，MySQL通过事务区作为数据缓冲地带</li><li>全部成功或者全部失败</li></ul><h3 id="自动提交事务模式"><a class="markdownIt-Anchor" href="#自动提交事务模式"></a> 自动提交事务模式</h3><ul><li>没执行一次写操作SQL，自动提交事务</li><li><code>conn.setAutoCommit(true)</code></li><li>默认的事务模式</li><li>无法保证多数据一致性</li></ul><h3 id="手动提交事务模式"><a class="markdownIt-Anchor" href="#手动提交事务模式"></a> 手动提交事务模式</h3><ul><li>显示调用<code>commit()</code>与<code>rollback()</code>方法管理事务</li><li><code>conn.setAutoCommit(false)</code></li><li>可保证数据一致性，但必须手动调用提交、回滚方法</li></ul><h2 id="jdbc中date日期对象的处理"><a class="markdownIt-Anchor" href="#jdbc中date日期对象的处理"></a> JDBC中Date日期对象的处理</h2><ul><li>JDBC获取日期使用<code>java.sql.Date</code>,它继承自<code>java.util.Date</code></li><li>所以当获取MySQL中的日期时，两者互相兼容</li></ul><h3 id="date日期的提取"><a class="markdownIt-Anchor" href="#date日期的提取"></a> Date日期的提取</h3><ul><li>因为sql.Date继承自util.Date所以可以直接提取</li></ul><h3 id="date日期的插入"><a class="markdownIt-Anchor" href="#date日期的插入"></a> Date日期的插入</h3><ul><li>需要先将用户输入String转换成java.util.Date<ul><li><code>String -&gt; java.util.Date</code></li></ul></li><li>再将java.util.Date转换成java.sql.Date<ul><li><code>java.util.Date -&gt; java.sql.Date</code></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 1.string -&gt; java.util.Date</span><br><span class="line">Date udHireDate &#x3D; null;</span><br><span class="line">SimpleDateFormat sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</span><br><span class="line">try &#123;</span><br><span class="line">    udHireDate &#x3D; sdf.parse(strHireDate);</span><br><span class="line">&#125; catch (ParseException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 2. java.util.Date -&gt; java.sql.Date</span><br><span class="line">long time &#x3D; udHireDate.getTime(); &#x2F;&#x2F; 获取自1970年到现在的毫秒数</span><br><span class="line">java.sql.Date sdHireDate &#x3D; new java.sql.Date(time);</span><br></pre></td></tr></table></figure><h2 id="jdbc批处理"><a class="markdownIt-Anchor" href="#jdbc批处理"></a> JDBC批处理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String sql &#x3D; &quot;insert into employee (eno, ename, salary, dname) values (?, ?, ?, ?);&quot;;</span><br><span class="line">statement &#x3D; conn.prepareStatement(sql);</span><br><span class="line"></span><br><span class="line">for (int i &#x3D; 200000; i &lt; 300000; i++) &#123;</span><br><span class="line">statement.setInt(1, i);</span><br><span class="line">statement.setString(2, &quot;员工&quot; + i);</span><br><span class="line">statement.setFloat(3, 4000);</span><br><span class="line">statement.setString(4, &quot;市场部&quot;);</span><br><span class="line">statement.addBatch(); &#x2F;&#x2F; add params to batch job, not execute yet</span><br><span class="line">&#125;</span><br><span class="line">statement.executeBatch(); &#x2F;&#x2F; execute batch job</span><br></pre></td></tr></table></figure><h2 id="连接池与jdbc进阶"><a class="markdownIt-Anchor" href="#连接池与jdbc进阶"></a> 连接池与JDBC进阶</h2><ul><li>建立数据库链接比较费时间</li><li>当我们知道应用的大概人数后，可以在应用启动时，提前创建好大概的连接并存放到连接池中</li><li>当有用户需要连接数据库时，直接从连接池中获取一个可用的连接，用完之后再放入到连接池中以供其他用户使用</li><li>管理，分配，连接</li><li>程序只负责取用和归还</li></ul><h3 id="druid连接池"><a class="markdownIt-Anchor" href="#druid连接池"></a> Druid连接池</h3><ul><li>Druid是阿里巴巴开源连接池组件</li></ul><p><img src="/images/JDBC_Basics/1.png" alt="" /></p><ul><li>连接池配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">driverClassName&#x3D;com.mysql.cj.jdbc.Driver</span><br><span class="line">url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;imooc?useSSL&#x3D;false&amp;useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;serverTimezone&#x3D;Australia&#x2F;Sydney</span><br><span class="line">username&#x3D;yuan</span><br><span class="line">password&#x3D;1111</span><br><span class="line">initialSize&#x3D;10 &#x2F;&#x2F; 初始化时创建新链接的数量</span><br><span class="line">maxActive&#x3D;20 &#x2F;&#x2F; 连接池中最多的连接数量</span><br></pre></td></tr></table></figure><ul><li>连接的步骤</li></ul><ol><li>加载属性文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Properties properties &#x3D; new Properties();</span><br><span class="line">String propertyFilePath &#x3D; DruidSample.class.getResource(&quot;&#x2F;druid-config.properties&quot;).getPath();</span><br><span class="line">&#x2F;&#x2F; URL Space will be represented as %20</span><br><span class="line">&#x2F;&#x2F; so we need to decode it</span><br><span class="line">propertyFilePath &#x3D; new URLDecoder().decode(propertyFilePath, &quot;UTF-8&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; load propertyFile to properties object</span><br><span class="line">properties.load(new FileInputStream(propertyFilePath));</span><br></pre></td></tr></table></figure><ol start="2"><li>获取dataSource数据源对象，指代数据库</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataSource &#x3D; DruidDataSourceFactory.createDataSource(properties);</span><br><span class="line">Connection conn &#x3D; dataSource.getConnection();</span><br></pre></td></tr></table></figure><ul><li>不使用连接池： conn.close()的作用是关闭连接</li><li>使用连接池： conn.close()的作用是将连接回收到连接池</li><li>当用户连接数量超过连接池设置的最大数量时，新的用户将会等待连接被其他用户关闭</li><li>Tips：可以将初始连接数量和最大连接数保持一致</li></ul><h3 id="扩展知识c3p0连接池"><a class="markdownIt-Anchor" href="#扩展知识c3p0连接池"></a> 扩展知识：C3P0连接池</h3><ul><li><a href="https://www.mchange.com/projects/c3p0/">C3P0连接池</a></li><li>C3P0会自动找到名为c3p0-config.xml的配置文件</li><li><code>DataSource dataSource = new ComboPooledDataSource();</code></li></ul><h3 id="apache-commons-dbutils"><a class="markdownIt-Anchor" href="#apache-commons-dbutils"></a> Apache Commons DBUtils</h3><ul><li>Apache提供的JDBC工具类库</li><li>它是对JDBC的简单封装，学习成本低</li><li>简化JDBC编码工作量</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Properties properties &#x3D; new Properties();</span><br><span class="line">String propertyFilePath &#x3D; DBUtilsSample.class.getResource(&quot;&#x2F;druid-config.properties&quot;).getPath();</span><br><span class="line"></span><br><span class="line">propertyFilePath &#x3D; new URLDecoder().decode(propertyFilePath, &quot;UTF-8&quot;);</span><br><span class="line">properties.load(new FileInputStream(propertyFilePath));</span><br><span class="line">DataSource dataSource &#x3D; DruidDataSourceFactory.createDataSource(properties);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Commons DBUtils会自动帮助我们创建并关闭连接</span><br><span class="line">QueryRunner qr &#x3D; new QueryRunner(dataSource);</span><br><span class="line">&#x2F;&#x2F; convert ResultSet into list of employees</span><br><span class="line">List&lt;Employee&gt; list &#x3D; qr.query(&quot;select * from employee limit ?, 10;&quot;,</span><br><span class="line">        new BeanListHandler&lt;&gt;(Employee.class),</span><br><span class="line">        new Object[]&#123;10&#125; &#x2F;&#x2F; pass in the params in the SQL query</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">for (Employee emp : list) &#123;</span><br><span class="line">System.out.println(emp.getEname());</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是jdbc&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#什么是jdbc&quot;&gt;&lt;/a&gt; 什么是JDBC&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Java Database Connectivity&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;让J</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
    <category term="JDBC" scheme="http://hellcy.github.io/tags/JDBC/"/>
    
  </entry>
  
  <entry>
    <title>MySQL Basics</title>
    <link href="http://hellcy.github.io/2022/02/16/MySQL-Basics/"/>
    <id>http://hellcy.github.io/2022/02/16/MySQL-Basics/</id>
    <published>2022-02-16T04:37:40.000Z</published>
    <updated>2022-02-17T10:10:17.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是数据库系统"><a class="markdownIt-Anchor" href="#什么是数据库系统"></a> 什么是数据库系统</h1><ul><li>DBMS是指一个能为用户提供信息服务的系统，它实现了有组织的，动态的储存大量相关数据的功能，提供了数据处理和信息资源共享的便利手段</li></ul><h1 id="关系型数据库-rdbms"><a class="markdownIt-Anchor" href="#关系型数据库-rdbms"></a> 关系型数据库 （RDBMS）</h1><ul><li>使用了关系模型的数据库系统</li><li>关系模型中，数据是分类存放的，数据之间可以有联系</li></ul><h1 id="什么是nosql数据库"><a class="markdownIt-Anchor" href="#什么是nosql数据库"></a> 什么是NoSQL数据库</h1><ul><li>NoSQL数据库指的是数据分类存放，但是数据之间没有关联关系的数据库系统</li></ul><h1 id="mysql配置文件"><a class="markdownIt-Anchor" href="#mysql配置文件"></a> MySQL配置文件</h1><ul><li>MySQL默认的root用户拥有所有权限，并且只能通过localhost登录，所以我们可以新建一个用户</li><li>我们可以使用Terminal指令 mysql -u root -p 来登录MySQL数据库</li><li>我们可以使用第三方的MySQL图形界面登录管理MySQL database<ul><li>sequel pro</li><li>DataGrip</li><li>NaviCat</li></ul></li><li>我们可以设置各种MySQL的配置，例如字符集，端口号，目录地址等等</li></ul><h1 id="sql语言分类"><a class="markdownIt-Anchor" href="#sql语言分类"></a> SQL语言分类</h1><ul><li>DML - 对数据进行操作，层删改查</li><li>DCL - 数据库控制语言，控制用户，权限，事务</li><li>DDL - definition，对逻辑库，数据表，视图，索引进行操作</li></ul><h2 id="sql语句注意事项"><a class="markdownIt-Anchor" href="#sql语句注意事项"></a> SQL语句注意事项</h2><ul><li>SQL语句不区分大小写</li><li>必须以分号结尾</li><li>语句中的空白和换行没有限制</li></ul><h2 id="注释"><a class="markdownIt-Anchor" href="#注释"></a> 注释</h2><ul><li><code># this is single line comment</code></li><li><code>/* this is multi line comments */</code></li></ul><h2 id="创建逻辑库"><a class="markdownIt-Anchor" href="#创建逻辑库"></a> 创建逻辑库</h2><ul><li>CREATE DATABASE database_name;</li><li>SHOW DATABASES;</li><li>DROP database_name;</li></ul><h2 id="创建数据表"><a class="markdownIt-Anchor" href="#创建数据表"></a> 创建数据表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE student (</span><br><span class="line">    id INT UNSIGNED PRIMARY KEY,</span><br><span class="line">    name VARCHAR(20) NOT NULL,</span><br><span class="line">    sex CHAR(1) NOT NULL,</span><br><span class="line">    birthday DATE NOT NULL,</span><br><span class="line">    tel CHAR(11) NOT NULL,</span><br><span class="line">    remark VARCHAR(200)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="数据表的其他操作"><a class="markdownIt-Anchor" href="#数据表的其他操作"></a> 数据表的其他操作</h2><ul><li>SHOW tables;</li><li>DESC student;</li><li>SHOW CREATE TABLE student;</li><li>DROP TABLE student;</li></ul><h2 id="不精确的浮点数"><a class="markdownIt-Anchor" href="#不精确的浮点数"></a> 不精确的浮点数</h2><p><img src="/images/MySQL-Basics/1.png" alt="" /></p><ul><li>所以在保存重要数字的时候要选用DECIMAL类型，它会将数字以字符串的形式保存，不会丢失精度</li></ul><h2 id="修改表结构"><a class="markdownIt-Anchor" href="#修改表结构"></a> 修改表结构</h2><ul><li>添加新字段</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE student </span><br><span class="line">ADD address VARCHAR(200) NOT NULL,</span><br><span class="line">ADD home_tel CHAR(11) NOT NULL;</span><br></pre></td></tr></table></figure><ul><li>修改字段</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE student</span><br><span class="line">MODIFY home_tel VARCHAR(20) NOT NULL;</span><br></pre></td></tr></table></figure><ul><li>修改字段名称</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE student</span><br><span class="line">CHANGE address home_address VARCHAR(20) NOT NULL;</span><br></pre></td></tr></table></figure><ul><li>删除字段</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE student </span><br><span class="line">DROP home_address,</span><br><span class="line">DROP home_tel;</span><br></pre></td></tr></table></figure><h2 id="字段约束"><a class="markdownIt-Anchor" href="#字段约束"></a> 字段约束</h2><h3 id="数据库的范式"><a class="markdownIt-Anchor" href="#数据库的范式"></a> 数据库的范式</h3><ul><li>构造数据库必须遵循一定的规则，这种规则就是范式</li><li>目前关系数据库有6种范式，一般情况下，只满足第三范式即可</li></ul><h3 id="第一范式原子性"><a class="markdownIt-Anchor" href="#第一范式原子性"></a> 第一范式：原子性</h3><ul><li>第一范式是数据库的基本要求，不满足这一点就不是关系型数据库</li><li>数据表的每一列都是不可分割的基本数据项，同一列中不能有多个值，也不能存在重复的属性</li></ul><h3 id="第二范式唯一性"><a class="markdownIt-Anchor" href="#第二范式唯一性"></a> 第二范式：唯一性</h3><ul><li>数据表中的每条记录必须是唯一的，为了实现区分，通常要为表加上一个列用来储存唯一标识，这个唯一属性列被称作主键列</li></ul><h3 id="第三范式关联性"><a class="markdownIt-Anchor" href="#第三范式关联性"></a> 第三范式：关联性</h3><ul><li><p>每列都与主键有直接关系，不存在依赖传递</p></li><li><p>下图中爸爸是主键，但是女儿的玩具和女儿的衣服并不依赖与主键，而是依赖于女儿，所以不满足第三范式<br /><img src="/images/MySQL-Basics/2.png" alt="" /></p></li><li><p>依照第三范式，数据可以拆分保存到不同的数据表，彼此保持关联</p></li></ul><h3 id="mysql中的字段约束"><a class="markdownIt-Anchor" href="#mysql中的字段约束"></a> MySQL中的字段约束</h3><ul><li>主键约束 - PRIMARY KEY - 字段值唯一，且不能为NULL<ul><li>建议主键一定要使用数字类型，因为数字的检索速度会非常快</li><li>如果主键是数字类型，还可以设置自动增长</li></ul></li><li>非空约束 - NOT NULL - 字段值不能为NULL<ul><li>非空字段可以有默认值</li><li><code>name VARCHAR(20) NOT NULL DEFAULT &quot;default name&quot;</code></li></ul></li><li>唯一约束 - UNIQUE - 字段值唯一，且可以为NULL</li><li>外键约束 - FOREIGN KEY - 保持关联数据的逻辑性</li></ul><h2 id="外键约束的闭环问题"><a class="markdownIt-Anchor" href="#外键约束的闭环问题"></a> 外键约束的闭环问题</h2><ul><li>如果形成外键闭环，我们将无法删除任何一张表的记录</li></ul><h1 id="索引"><a class="markdownIt-Anchor" href="#索引"></a> 索引</h1><h2 id="数据排序的好处"><a class="markdownIt-Anchor" href="#数据排序的好处"></a> 数据排序的好处</h2><ul><li>一旦数据排序之后，查找的速度就会翻倍</li></ul><h2 id="如何创建索引"><a class="markdownIt-Anchor" href="#如何创建索引"></a> 如何创建索引</h2><ul><li>当数据排序后，MySQL后台会对索引对象创建二叉树，使用二分查找提升速度</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table t_message (</span><br><span class="line">    id int unsigned primary key,</span><br><span class="line">    content varchar(200) not null,</span><br><span class="line">    type enum(&quot;notification&quot;, &quot;annocement&quot;, &quot;letter&quot;) not null,</span><br><span class="line">    create_time timestamp not null,</span><br><span class="line">    INDEX idx_type (type)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h2 id="如何添加与删除索引"><a class="markdownIt-Anchor" href="#如何添加与删除索引"></a> 如何添加与删除索引</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">drop index idx_type on t_message;</span><br><span class="line"></span><br><span class="line">create index idx_type on t_message(type);</span><br><span class="line"></span><br><span class="line">show index from t_message;</span><br><span class="line"></span><br><span class="line">alter table t_message add index idx_type(type);</span><br></pre></td></tr></table></figure><h2 id="索引的使用原则"><a class="markdownIt-Anchor" href="#索引的使用原则"></a> 索引的使用原则</h2><ul><li>数据量很大，而且经常被查询到的数据表可以设置索引</li><li>索引只添加在经常被用作检索条件的字段上面</li><li>不要在大字段上创建索引</li></ul><h1 id="数据操作语言"><a class="markdownIt-Anchor" href="#数据操作语言"></a> 数据操作语言</h1><h2 id="记录查询"><a class="markdownIt-Anchor" href="#记录查询"></a> 记录查询</h2><ul><li>最基本的查询语言是由select和from关键字组成的</li><li>select语句屏蔽了物理层的操作，用户不必关心数据的真实存储，交由数据库高效的查找数据</li></ul><h2 id="使用列别名"><a class="markdownIt-Anchor" href="#使用列别名"></a> 使用列别名</h2><ul><li>通常情况下，select子句中使用了表达式，那么这列的名字就默认为表达式，因此需要一种对列名重命名的机制</li></ul><h2 id="数据分页"><a class="markdownIt-Anchor" href="#数据分页"></a> 数据分页</h2><ul><li>如果记录很多，我们可以使用LIMIT关键字来限定结果数量</li><li><code>Select ... from ... limit startPosition, offset</code></li><li>如果LIMIT子句只有一个参数，他表示的是offset，起始值默认为0</li></ul><h2 id="排序"><a class="markdownIt-Anchor" href="#排序"></a> 排序</h2><ul><li>如果没有设置，查询语句不会对结果集进行排序，如果想让结果按照某种顺序排列，就必须使用ORDER BY子句</li><li>ASC代表升序，DESC代表降序</li><li>如果排序列是数字类型，数据库就按照数字大小排列，如果是日期类型，就按照日期排序，如果是字符串就按照字符集序号排序</li></ul><h2 id="去除重复记录"><a class="markdownIt-Anchor" href="#去除重复记录"></a> 去除重复记录</h2><ul><li>DISTINCT</li><li>DISTINCT关键字只能使用一次，并且放在最前面</li></ul><h2 id="条件查询"><a class="markdownIt-Anchor" href="#条件查询"></a> 条件查询</h2><ul><li>WHERE</li></ul><h2 id="聚合函数"><a class="markdownIt-Anchor" href="#聚合函数"></a> 聚合函数</h2><ul><li>求和，最大值，最小值，平均值，COUNT。。。</li></ul><h2 id="数据分组"><a class="markdownIt-Anchor" href="#数据分组"></a> 数据分组</h2><ul><li>GROUP BY</li><li>通过一定的规则讲一个数据集划分成若干个小的区域，然后针对每个小区域分别进行数据汇总处理</li><li>数据库支持多列分组条件，执行的时候逐级分组</li><li>WITH ROLLUP 对分组结果再次做汇总计算</li></ul><h3 id="group_concat函数"><a class="markdownIt-Anchor" href="#group_concat函数"></a> GROUP_CONCAT函数</h3><ul><li><p>把分组查询中的某个字段拼接成一个字符串</p></li><li><p>查询每个部门工资大于2000 的</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select deptno, GROUP_CONCAT(ename), COUNT(*)</span><br><span class="line">from t_emp</span><br><span class="line">where sal &gt;&#x3D; 2000</span><br><span class="line">group by deptno;</span><br></pre></td></tr></table></figure><h3 id="having子句"><a class="markdownIt-Anchor" href="#having子句"></a> HAVING子句</h3><ul><li>查询每个部门中，1982年以后入职的员工超过2个人的部门编号</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select deptno</span><br><span class="line">from t_emp</span><br><span class="line">where hiredate &gt;&#x3D; &quot;1982-01-01&quot;</span><br><span class="line">group by deptno </span><br><span class="line">having COUNT(*) &gt;&#x3D; 2</span><br></pre></td></tr></table></figure><h2 id="表连接"><a class="markdownIt-Anchor" href="#表连接"></a> 表连接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select e.empno, e.ename, d.dname</span><br><span class="line">from t_emp e join t_dept d</span><br><span class="line">on e.deptno &#x3D; d.deptno;</span><br></pre></td></tr></table></figure><h3 id="表连接的分类"><a class="markdownIt-Anchor" href="#表连接的分类"></a> 表连接的分类</h3><ul><li>内连接，外连接</li><li>内连接是结果集中只保留符合连接条件的记录</li><li>外连接是不管符不符合连接条件，记录都要保留在结果集中</li></ul><h3 id="内连接"><a class="markdownIt-Anchor" href="#内连接"></a> 内连接</h3><ul><li>用于查询多张关系表符合条件的记录</li><li>INNER JOIN</li><li>相同的数据表也可以做表连接</li></ul><h3 id="外连接"><a class="markdownIt-Anchor" href="#外连接"></a> 外连接</h3><ul><li><p>LEFT JOIN</p></li><li><p>RIGHT JOIN</p></li><li><p>左外连接就是保留左表所有的记录，与右表做链接，如果右表有符合条件的记录就与左表链接，如果没有，就用NULL与左表链接</p></li><li><p>右外连接也是如此</p></li><li><p>UNION关键字可以将多个查询语句的结果集进行合并</p></li><li><p>查询每个部门的人数，没有部门的员工用NULL代表部门的名称</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    select d.dname, COUNT(e.deptno)</span><br><span class="line">    from t_dept d LEFT JOIN t_emp e</span><br><span class="line">    on d.deptno &#x3D; e.deptno</span><br><span class="line">    GROUP BY d.deptno</span><br><span class="line">) </span><br><span class="line">UNION</span><br><span class="line">(</span><br><span class="line">    select d.name, COUNT(*)</span><br><span class="line">    from t_dept d RIGHT JOIN t_emp e</span><br><span class="line">    ON d.deptno &#x3D; e.deptno</span><br><span class="line">    GROUP BY d.deptno</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li>在外链接里，条件写在WHERE子句里，不符合条件的记录会被过滤掉，不会被保留下来</li></ul><h2 id="子查询"><a class="markdownIt-Anchor" href="#子查询"></a> 子查询</h2><ul><li>嵌套在查询语句中的查询</li><li>子查询可以写在三个地方： WHERE， FROM， SELECT</li><li>只有写在FROM中的子查询是最可取的，其他地方的效率不高</li></ul><h3 id="where子句中的多行子查询"><a class="markdownIt-Anchor" href="#where子句中的多行子查询"></a> WHERE子句中的多行子查询</h3><ul><li>可以使用IN， ALL， ANY， EXISTS关键字来处理多行表达式结果集的条件判断</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select ename from t_emp</span><br><span class="line">where sal &gt; ALL (100, 200)</span><br></pre></td></tr></table></figure><h3 id="exists-关键字"><a class="markdownIt-Anchor" href="#exists-关键字"></a> EXISTS 关键字</h3><ul><li>把原来在子查询之外的条件判断写到了子查询的里面</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT SupplierName</span><br><span class="line">FROM Suppliers</span><br><span class="line">WHERE EXISTS </span><br><span class="line">(</span><br><span class="line">    SELECT ProductName </span><br><span class="line">    FROM Products </span><br><span class="line">    WHERE Products.SupplierID &#x3D; Suppliers.supplierID </span><br><span class="line">    AND Price &lt; 20</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h1 id="数据操作语言-2"><a class="markdownIt-Anchor" href="#数据操作语言-2"></a> 数据操作语言</h1><h2 id="insert语句"><a class="markdownIt-Anchor" href="#insert语句"></a> INSERT语句</h2><ul><li>向数据表中写入记录，可以使一条记录也可以是多条记录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into table_name (column_1, column_2, ...)</span><br><span class="line">values (value_1, value_2, ...);</span><br></pre></td></tr></table></figure><ul><li>写入多条记录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">insert into table_name (column_1, column_2, ...)</span><br><span class="line">values </span><br><span class="line">(value_1, value_2, ...),</span><br><span class="line">(value_1, value_2, ...),</span><br><span class="line">(value_1, value_2, ...);</span><br></pre></td></tr></table></figure><h2 id="ignore关键字"><a class="markdownIt-Anchor" href="#ignore关键字"></a> IGNORE关键字</h2><ul><li>会让INSERT只插入数据库不存在的记录,不会报错</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert ignore into table_name (...) values (...);</span><br></pre></td></tr></table></figure><h2 id="update语句"><a class="markdownIt-Anchor" href="#update语句"></a> UPDATE语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update table_name set column_name &#x3D; value_name, ....;</span><br></pre></td></tr></table></figure><h3 id="update语句的表连接"><a class="markdownIt-Anchor" href="#update语句的表连接"></a> UPDATE语句的表连接</h3><ul><li>可以修改多张表的记录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">update table_1 join table_2 on condition</span><br><span class="line">set column_1 &#x3D; value_1, column_2 &#x3D; value_2,...;</span><br></pre></td></tr></table></figure><h2 id="delete-语句"><a class="markdownIt-Anchor" href="#delete-语句"></a> DELETE 语句</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete from table_name</span><br><span class="line">where ...;</span><br></pre></td></tr></table></figure><h3 id="delete语句的表连接"><a class="markdownIt-Anchor" href="#delete语句的表连接"></a> DELETE语句的表连接</h3><ul><li>删除多张表的记录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">delete table_1, table_2</span><br><span class="line">from table_1 join table_2 on condition</span><br><span class="line">where...;</span><br></pre></td></tr></table></figure><h1 id="事务机制"><a class="markdownIt-Anchor" href="#事务机制"></a> 事务机制</h1><h2 id="避免写入直接操作数据文件"><a class="markdownIt-Anchor" href="#避免写入直接操作数据文件"></a> 避免写入直接操作数据文件</h2><ul><li>利用日志来实现间接写入</li><li>redo，undo日志</li></ul><h2 id="transaction"><a class="markdownIt-Anchor" href="#transaction"></a> Transaction</h2><ul><li>全部成功或者全部失败</li><li>默认情况下，MySQL执行每条SQL语句都会自动开启和提交事务</li><li>为了让多条SQL语句纳入到一个事务之下，可以手动管理事务</li><li><code>start transaction;</code></li></ul><h2 id="acid"><a class="markdownIt-Anchor" href="#acid"></a> ACID</h2><h3 id="原子性"><a class="markdownIt-Anchor" href="#原子性"></a> 原子性</h3><ul><li>全部成功或者全部失败， 没有中间状态</li></ul><h3 id="一致性"><a class="markdownIt-Anchor" href="#一致性"></a> 一致性</h3><ul><li>不管在任何给定时间，并发事务有多少，事务必须保证运行结果的一致性</li><li>事务的临时状态不会被读取</li></ul><h3 id="隔离性"><a class="markdownIt-Anchor" href="#隔离性"></a> 隔离性</h3><ul><li>事务不受其他并发事务的影响，如同再给定时间内，改事务是数据库唯一运行的事务</li><li>默认情况下，只能看到日志中该事物的相关数据</li></ul><h3 id="持久性"><a class="markdownIt-Anchor" href="#持久性"></a> 持久性</h3><ul><li>事务一旦提交，结果便是永久性的，即便发生宕机，仍然可以依靠事务日志完成数据的持久化</li></ul><h2 id="隔离级别"><a class="markdownIt-Anchor" href="#隔离级别"></a> 隔离级别</h2><h3 id="read-uncommitted"><a class="markdownIt-Anchor" href="#read-uncommitted"></a> READ UNCOMMITTED</h3><ul><li>代表可以读取日志中其他事物的未提交的数据，即使其他事物还没有commit</li><li><code>set session transaction isolation level read uncommitted;</code></li></ul><h3 id="read-committed"><a class="markdownIt-Anchor" href="#read-committed"></a> READ COMMITTED</h3><ul><li>代表只能读取其他事物提交的数据</li><li><code>set session transaction isolation level read committed;</code></li></ul><h3 id="repeatable-read-默认级别"><a class="markdownIt-Anchor" href="#repeatable-read-默认级别"></a> REPEATABLE READ （默认级别）</h3><ul><li>事务在执行中反复读取数据，得到的结果是一致的，不会受到其他事物的影响</li><li><code>set session transaction isolation level repeatable read;</code></li></ul><h3 id="serializable"><a class="markdownIt-Anchor" href="#serializable"></a> SERIALIZABLE</h3><ul><li>让事务逐一执行，不存在并发性</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是数据库系统&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#什么是数据库系统&quot;&gt;&lt;/a&gt; 什么是数据库系统&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DBMS是指一个能为用户提供信息服务的系统，它实现了有组织的，动态的储存大量相关数据的功能，提供</summary>
      
    
    
    
    
    <category term="MySQL" scheme="http://hellcy.github.io/tags/MySQL/"/>
    
    <category term="Database" scheme="http://hellcy.github.io/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>Frontend Basics</title>
    <link href="http://hellcy.github.io/2022/02/15/Frontend-Basics/"/>
    <id>http://hellcy.github.io/2022/02/15/Frontend-Basics/</id>
    <published>2022-02-15T06:42:21.000Z</published>
    <updated>2022-02-16T04:34:04.771Z</updated>
    
    <content type="html"><![CDATA[<h1 id="jquery"><a class="markdownIt-Anchor" href="#jquery"></a> jQuery</h1><ul><li>为了简化JS开发的第三方库 （jQuery，Vue，AngularJS，React）</li><li>jQuery的核心是选择器，用于获取页面元素</li></ul><h2 id="选择器"><a class="markdownIt-Anchor" href="#选择器"></a> 选择器</h2><h3 id="基本选择器"><a class="markdownIt-Anchor" href="#基本选择器"></a> 基本选择器</h3><table><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>$(&quot;#id&quot;)</code></td><td>ID选择器，指定ID元素的对象</td></tr><tr><td><code>$(&quot;标签&quot;)</code></td><td>元素选择器，选择指定标签名的选择器</td></tr><tr><td><code>$(&quot;.class&quot;)</code></td><td>类选择器，选中拥有指定css类的元素</td></tr><tr><td><code>$(&quot;S1, S2, S3&quot;)</code></td><td>组合选择器，对元素进行组合</td></tr></tbody></table><h3 id="层叠选择器"><a class="markdownIt-Anchor" href="#层叠选择器"></a> 层叠选择器</h3><ul><li>根据元素的位置关系来获取元素的选择器表达式</li></ul><table><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>$(&quot;ancestor descendant&quot;)</code></td><td>后代选择器，选择父节点的所有后代子节点（包括后代的后代的后代。。。）</td></tr><tr><td><code>$(&quot;ancestor&gt;descendant&quot;)</code></td><td>子选择器，只选择父节点的直接子节点</td></tr><tr><td><code>$(&quot;prev~siblings&quot;)</code></td><td>兄弟选择器，只会选择同级别的，在当前元素之后的兄弟元素</td></tr></tbody></table><h3 id="属性选择器"><a class="markdownIt-Anchor" href="#属性选择器"></a> 属性选择器</h3><ul><li>属性选择器是根据元素的属性值来选择元素的选择器表达式</li></ul><table><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>$(&quot;selector[attribute=value]&quot;)</code></td><td>选中属性值等于具体值得组件</td></tr><tr><td><code>$(&quot;selector[attribute^=value]&quot;)</code></td><td>选中属性值以某值开头得组件</td></tr><tr><td><code>$(&quot;selector[attribute$=value]&quot;)</code></td><td>选中属性值以某值结尾得组件</td></tr><tr><td><code>$(&quot;selector[attribute*=value]&quot;)</code></td><td>选中属性值包含某值得组件</td></tr></tbody></table><h3 id="位置选择器"><a class="markdownIt-Anchor" href="#位置选择器"></a> 位置选择器</h3><table><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>$(&quot;selector:first&quot;)</code></td><td>获取第一个元素</td></tr><tr><td><code>$(&quot;selector:last&quot;)</code></td><td>获取最后一个元素</td></tr><tr><td><code>$(&quot;selector:even&quot;)</code></td><td>获取偶数位置的元素（从0开始）</td></tr><tr><td><code>$(&quot;selector:odd&quot;)</code></td><td>获取奇数位置的元素（从0开始）</td></tr><tr><td><code>$(&quot;selector:eq(n)&quot;)</code></td><td>获取指定位置的元素（从0开始）</td></tr></tbody></table><h3 id="表单选择器"><a class="markdownIt-Anchor" href="#表单选择器"></a> 表单选择器</h3><ul><li>获取表单元素的简化形式</li></ul><table><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>$(&quot;selector:input&quot;)</code></td><td>所有输入元素</td></tr><tr><td><code>$(&quot;selector:text&quot;)</code></td><td>获取文本框</td></tr><tr><td><code>$(&quot;selector:password&quot;)</code></td><td>获取密码框</td></tr><tr><td><code>$(&quot;selector:submit&quot;)</code></td><td>获取调教按钮</td></tr><tr><td><code>$(&quot;selector:reset&quot;)</code></td><td>获取重置按钮</td></tr></tbody></table><h2 id="操作元素"><a class="markdownIt-Anchor" href="#操作元素"></a> 操作元素</h2><h3 id="操作元素属性"><a class="markdownIt-Anchor" href="#操作元素属性"></a> 操作元素属性</h3><ul><li>.attr()</li><li>.removeAttr()</li></ul><h3 id="操作元素css样式"><a class="markdownIt-Anchor" href="#操作元素css样式"></a> 操作元素CSS样式</h3><ul><li>css() - 获取或者设置匹配元素的样式属性</li><li>addclass() - 为每个匹配的元素添加指定的类名</li><li>removeClass() - 从所有匹配的元素中删除全部或者指定的类</li></ul><h3 id="设置元素内容"><a class="markdownIt-Anchor" href="#设置元素内容"></a> 设置元素内容</h3><ul><li>val() - 获取或设置输入项的值</li><li>text() - 获取或设置元素的纯文本</li><li>html() - 获取或设置元素内部的HTML</li></ul><h3 id="jquery事件处理方法"><a class="markdownIt-Anchor" href="#jquery事件处理方法"></a> jQuery事件处理方法</h3><ul><li><code>on(&quot;click&quot;, function)</code> - 为选中的页面元素绑定单机事件</li><li>click(function) - 是绑定事件的简写形式</li><li>处理方法中提供了event参数包含了事件的相关信息</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$(&quot;p.myClass&quot;).on(&quot;click&quot;, function() &#123;</span><br><span class="line">    &#x2F;&#x2F; $(this) 是指当前事件产生的对象，也就是p.myClass对象本身</span><br><span class="line">    $(this).css(&quot;backgroung-color&quot;, &quot;red&quot;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="ajax"><a class="markdownIt-Anchor" href="#ajax"></a> Ajax</h2><ol><li>创建XMLHttpRequest对象</li><li>向服务器发送请求</li><li>服务器返回Response到JavaScript</li></ol><h2 id="jquery对ajax的支持"><a class="markdownIt-Anchor" href="#jquery对ajax的支持"></a> jQuery对Ajax的支持</h2><ul><li>jQuery对Ajax进行了封装，提供了<code>$.ajax()</code>方法<br />| 常用设置项 | 说明 |<br />| url | 发送请求地址 |<br />| type | 请求类型 get、post |<br />| data | 向服务器传递的参数 |<br />| dataType | 服务器响应的数据类型（text，json，xml，html，jsonp，script）|<br />| success | 接受响应时的处理函数 |<br />| error | 请求失败时的处理函数 |</li></ul><h1 id="过滤器"><a class="markdownIt-Anchor" href="#过滤器"></a> 过滤器</h1><ul><li>J2EE Servlet模块下的组件</li><li>Filter的作用是对URL进行统一的拦截处理</li><li>Filter通常用于应用程序层面进行全局处理</li></ul><h2 id="过滤器开发三要素"><a class="markdownIt-Anchor" href="#过滤器开发三要素"></a> 过滤器开发三要素</h2><ul><li>任何过滤器都要实现<code>javax.servlet.Filter</code>接口</li><li>在filter接口的<code>doFilter()</code>方法中编写过滤器的功能代码</li><li>在<code>web.xml</code>中对过滤器进行配置，说明拦截URL的范围,那些请求会被拦截，那些请求不会被拦截</li></ul><h2 id="过滤器的生命周期"><a class="markdownIt-Anchor" href="#过滤器的生命周期"></a> 过滤器的生命周期</h2><ul><li>初始化，Tomcat启动时 - Filter.init()</li><li>提供服务 - Filter.doFilter()</li><li>销毁 - 应用关闭或重启时 - Filter.destroy()</li></ul><h2 id="过滤器的特性"><a class="markdownIt-Anchor" href="#过滤器的特性"></a> 过滤器的特性</h2><ul><li>过滤器对象在Web应用启动时被创建且全局唯一</li><li>唯一的过滤器对象在并发环境中采用“单例多线程”提供服务<ul><li>过滤器会为每一个请求创建一个单独的线程</li></ul></li></ul><h2 id="过滤器的配置形式"><a class="markdownIt-Anchor" href="#过滤器的配置形式"></a> 过滤器的配置形式</h2><ul><li>所有的信息存放在web.xml中</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;filter&gt;</span><br><span class="line">    &lt;filter-name&gt;MyFirstFileter&lt;&#x2F;filter-name&gt;</span><br><span class="line">    &lt;filter-class&gt;com.yuan.filter.MyFirstFileter&lt;&#x2F;filter-class&gt;</span><br><span class="line">&lt;&#x2F;filter&gt;</span><br><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;MyFirstFilter&lt;&#x2F;filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt;</span><br><span class="line">&lt;&#x2F;filter-mapping&gt;</span><br></pre></td></tr></table></figure><h2 id="过滤器的注解形式"><a class="markdownIt-Anchor" href="#过滤器的注解形式"></a> 过滤器的注解形式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@WebFilter(filterName &#x3D; &quot;MyAnnoationFilter&quot;, urlPatterns&#x3D;&quot;&#x2F;*&quot;)</span><br><span class="line">public class MyAnnotationFilter implements Filter &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="配置与注解如何选择"><a class="markdownIt-Anchor" href="#配置与注解如何选择"></a> 配置与注解如何选择</h2><ul><li>配置形式维护性更好，适合应用全局过滤</li><li>注解形式开发体验更好，适用于小型项目敏捷开发</li><li>注解形式每次过滤器的改变都需要重新编译、</li></ul><h2 id="过滤器参数化"><a class="markdownIt-Anchor" href="#过滤器参数化"></a> 过滤器参数化</h2><ul><li>过滤器为了增强灵活性，允许配置信息放在web.xml</li><li>在web.xml中配置<code>&lt;init-param&gt;</code>设置过滤器参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;init-param&gt;</span><br><span class="line">    &lt;param-name&gt;encoding&lt;&#x2F;param-name&gt;</span><br><span class="line">    &lt;param-value&gt;UTF-8&lt;&#x2F;param-value&gt;</span><br><span class="line">&lt;&#x2F;init-param&gt;</span><br></pre></td></tr></table></figure><ul><li>在Filter类中的init方法取得配置信息中的参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">private String encoding &#x3D; filterConfig.getInitParameter(&quot;encoding&quot;);</span><br></pre></td></tr></table></figure><h2 id="过滤链"><a class="markdownIt-Anchor" href="#过滤链"></a> 过滤链</h2><ul><li>每一个过滤器应具有单独的功能</li><li>过滤器执行顺序以<code>&lt;filter-mapping&gt;</code>为准</li><li>调用chain.doFiler()将请求向后传递·</li></ul><h1 id="监听器-listener"><a class="markdownIt-Anchor" href="#监听器-listener"></a> 监听器 - Listener</h1><ul><li>对Web应用对象的行为进行监控</li><li>Listener是J2EE Servlet模块下的组件</li><li>Listener的作用是对web应用对象的行为进行监控</li><li>通过监听自动触发指定的功能代码</li></ul><h2 id="监听对象"><a class="markdownIt-Anchor" href="#监听对象"></a> 监听对象</h2><ul><li>ServletContext - 对全局context及其属性监听</li><li>HttpSession - 对用户会话及其属性监听</li><li>ServletRequest - 对请求及其属性监听</li></ul><h2 id="监听器和过滤器的区别"><a class="markdownIt-Anchor" href="#监听器和过滤器的区别"></a> 监听器和过滤器的区别</h2><ul><li>过滤器的职责是对URL进行过滤拦截，是主动的进行</li><li>监听器的职责是对web对象进行监听，是被动触发</li></ul><h2 id="开发监听器三要素"><a class="markdownIt-Anchor" href="#开发监听器三要素"></a> 开发监听器三要素</h2><ol><li>实现Listener接口，不同接口对应不同的监听对象</li><li>实现每个接口中独有的方法，实现触发监听后续操作</li><li>在web.xml中配置<code>&lt;listener&gt;</code>使监听器生效</li></ol><h2 id="六种常用监听接口"><a class="markdownIt-Anchor" href="#六种常用监听接口"></a> 六种常用监听接口</h2><ol><li>ServletContextListener</li><li>HttpSessionListener</li><li>ServletRequestListener</li></ol><h3 id="属性监听接口"><a class="markdownIt-Anchor" href="#属性监听接口"></a> 属性监听接口</h3><ol start="4"><li>ServletContextAttributeListener - 监听全局属性操作</li><li>HttpSessionAttributeListener - 监听用户会话属性操作</li><li>ServletRequestAttributeListener - 监听请求属性操作</li></ol><h1 id="模板引擎"><a class="markdownIt-Anchor" href="#模板引擎"></a> 模板引擎</h1><h2 id="什么是模板引擎"><a class="markdownIt-Anchor" href="#什么是模板引擎"></a> 什么是模板引擎</h2><ul><li>数据 + 模板 = 结果</li><li>将数据与展现有效的解耦</li></ul><h2 id="freemarker"><a class="markdownIt-Anchor" href="#freemarker"></a> Freemarker</h2><ul><li>脚本为Freemarker template language</li><li>提供了大量内建函数来简化开发</li></ul><h2 id="ftl取值"><a class="markdownIt-Anchor" href="#ftl取值"></a> FTL取值</h2><ul><li>${attribute} - 取值，可对属性进行计算</li><li>${attribute!default} - 使用默认值</li><li>${attribute?string} 格式化输出</li></ul><h2 id="分支判断"><a class="markdownIt-Anchor" href="#分支判断"></a> 分支判断</h2><ul><li>if else</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;#if computer.state &#x3D;&#x3D; 1&gt;</span><br><span class="line">    State: using</span><br><span class="line">&lt;#elseif computer.state &#x3D;&#x3D; 2&gt;</span><br><span class="line">    State: idle</span><br><span class="line">&lt;#elseif computer.state &#x3D;&#x3D; 3&gt;</span><br><span class="line">    State: finished</span><br><span class="line">&lt;&#x2F;#if&gt;</span><br></pre></td></tr></table></figure><ul><li>switch</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;#switch computer.state&gt;</span><br><span class="line">    &lt;#case 1&gt;</span><br><span class="line">        State: using</span><br><span class="line">        &lt;#break&gt;</span><br><span class="line">    &lt;#case 2&gt;</span><br><span class="line">        State: idle</span><br><span class="line">        &lt;#break&gt;</span><br><span class="line">    &lt;#case 3&gt;</span><br><span class="line">        State: finished</span><br><span class="line">        &lt;#break&gt;</span><br><span class="line">    &lt;#default &gt;</span><br><span class="line">        State: not exist</span><br><span class="line">&lt;&#x2F;#switch&gt;</span><br></pre></td></tr></table></figure><ul><li>check if value is null</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;#if computer.user??&gt;</span><br><span class="line">    User: $&#123;computer.user&#125;</span><br><span class="line">&lt;&#x2F;#if&gt;</span><br></pre></td></tr></table></figure><h2 id="list迭代列表"><a class="markdownIt-Anchor" href="#list迭代列表"></a> list迭代列表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;#list students as stu&gt;</span><br><span class="line">    &lt;li&gt;$&#123;stu_index&#125;-$&#123;stu.name&#125;&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;#list&gt;</span><br></pre></td></tr></table></figure><h2 id="list迭代map"><a class="markdownIt-Anchor" href="#list迭代map"></a> list迭代Map</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;#list map?keys as key&gt;</span><br><span class="line">    $&#123;key&#125;:$&#123;map[key]&#125;</span><br><span class="line">&lt;&#x2F;#list&gt;</span><br></pre></td></tr></table></figure><h2 id="内建函数"><a class="markdownIt-Anchor" href="#内建函数"></a> 内建函数</h2><ul><li>lower_case/upper_case</li><li>cap_first</li><li>index_of</li><li>length</li><li>round/floor/ceiling</li><li>size</li><li>first/last</li><li>sort_by</li></ul><h1 id="mvc模式"><a class="markdownIt-Anchor" href="#mvc模式"></a> MVC模式</h1><h2 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h2><ul><li>负责生产业务需要的数据</li></ul><h2 id="controller"><a class="markdownIt-Anchor" href="#controller"></a> Controller</h2><ul><li>接受来自web的请求</li><li>调用service进行处理</li><li>将数据放入request</li><li>跳转界面</li></ul><h2 id="view"><a class="markdownIt-Anchor" href="#view"></a> View</h2><ul><li>用于展示最终结果</li><li>通常使用模板引擎展示</li></ul><h2 id="mvc优点"><a class="markdownIt-Anchor" href="#mvc优点"></a> MVC优点</h2><ul><li>软件团队分工合作，成员各司其职</li><li>分层开发，显示与数据解耦，便于维护</li><li>组件可灵活替代，互不影响</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;jquery&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#jquery&quot;&gt;&lt;/a&gt; jQuery&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;为了简化JS开发的第三方库 （jQuery，Vue，AngularJS，React）&lt;/li&gt;
&lt;li&gt;jQ</summary>
      
    
    
    
    
    <category term="Frontend" scheme="http://hellcy.github.io/tags/Frontend/"/>
    
  </entry>
  
  <entry>
    <title>Servlet and JSP</title>
    <link href="http://hellcy.github.io/2022/02/15/Servlet-and-JSP/"/>
    <id>http://hellcy.github.io/2022/02/15/Servlet-and-JSP/</id>
    <published>2022-02-15T03:46:18.000Z</published>
    <updated>2022-02-15T17:10:55.141Z</updated>
    
    <content type="html"><![CDATA[<h1 id="软件结构"><a class="markdownIt-Anchor" href="#软件结构"></a> 软件结构</h1><h2 id="单机时代-桌面应用"><a class="markdownIt-Anchor" href="#单机时代-桌面应用"></a> 单机时代 - 桌面应用</h2><ul><li>单机应用，软件所有数据都保存在电脑本地硬盘中</li><li>易于使用，结构简单</li><li>缺点： 数据难以共享，安全性差，更新不及时</li></ul><h2 id="client-server模式"><a class="markdownIt-Anchor" href="#client-server模式"></a> Client-Server模式</h2><ul><li>数据保存在服务器上</li><li>客户端通过与服务器进行通讯来获取数据</li><li>优点：数据方便共享，安全性高</li><li>缺点：必须安装客户端，升级与维护困难</li></ul><h2 id="browser-server模式"><a class="markdownIt-Anchor" href="#browser-server模式"></a> Browser-Server模式</h2><ul><li>编写网页，通过服务器动态生成网页</li><li>优点：开发简单，无需安装客户端，数据以与共享</li><li>缺点：执行速度与用户体验相对较弱</li></ul><h2 id="请求与响应"><a class="markdownIt-Anchor" href="#请求与响应"></a> 请求与响应</h2><ul><li>从浏览器发出送给服务器的数据包成为：请求 Request</li><li>从服务器返回给浏览器的结果成为： 响应 Response</li></ul><h1 id="j2ee是什么"><a class="markdownIt-Anchor" href="#j2ee是什么"></a> J2EE是什么</h1><ul><li>J2EE是指Java 2 企业版</li><li>开发Web应用程序就是J2EE最核心的功能</li><li>J2EE由13个功能模块组成</li></ul><h2 id="13个功能模块"><a class="markdownIt-Anchor" href="#13个功能模块"></a> 13个功能模块</h2><h3 id="重要"><a class="markdownIt-Anchor" href="#重要"></a> 重要</h3><ol><li>Servlet - web服务器小程序</li><li>JSP - 服务器页面</li><li>JDBC - 数据库交互模块</li><li>XML - XML交互模块</li></ol><h3 id="次要"><a class="markdownIt-Anchor" href="#次要"></a> 次要</h3><ol start="5"><li>EJB - 企业级Java Bean</li><li>RMI - 远程调用</li><li>JNDI - 目录服务</li><li>JMS - 消息服务</li><li>JTA - 事务管理</li><li>JavaMail - 发送接收Email</li><li>JAF - 安全框架</li><li>CORBA - CORBA集成</li><li>JTS - CORBA事务监控</li></ol><h1 id="apache-tomcat"><a class="markdownIt-Anchor" href="#apache-tomcat"></a> Apache Tomcat</h1><ul><li>Tomcat是Apache软件基金会旗下一款免费的开源Web应用服务器程序</li></ul><h2 id="j2ee与tomcat的关系"><a class="markdownIt-Anchor" href="#j2ee与tomcat的关系"></a> J2EE与Tomcat的关系</h2><ul><li>J2EE是一组技术规范与指南，具体实现由软件厂商决定</li><li>Tomcat是J2EE Web（Servlet与JSP）标准的实现者</li><li>J2SE是J2EE运行的基石，运行Tomcat离不开J2SE</li></ul><h1 id="servlet"><a class="markdownIt-Anchor" href="#servlet"></a> Servlet</h1><ul><li>Servlet(Server Applet)服务器小程序，主要功能用于生成动态Web内容</li><li>Servlet是J2EE最重要的组成部分，也是我们学习的重点</li></ul><p><img src="/images/Servlet-and-JSP/1.png" alt="" /></p><h2 id="install-tomcat-on-macos"><a class="markdownIt-Anchor" href="#install-tomcat-on-macos"></a> Install Tomcat on macOS</h2><ul><li><a href="https://jeongwhanchoi.medium.com/how-to-install-apache-tomcat-on-mac-os-x-605b1cb55252">Install Tomcat on macOS</a></li><li>after successfully installation, you can use <code>./start.sh</code> in Terminal to start Tomcat</li><li>you can go to <code>localhost:8080</code> to check if Tomcat is running</li></ul><h2 id="servlet开发步骤"><a class="markdownIt-Anchor" href="#servlet开发步骤"></a> Servlet开发步骤</h2><ul><li>创建Servlet类，继承HttpServlet （类似于之后SpringMVC的Controller）</li><li>重写service方法，编写程序代码</li><li>配置web.xml，绑定URL</li></ul><h2 id="请求参数"><a class="markdownIt-Anchor" href="#请求参数"></a> 请求参数</h2><ul><li>请求参数是指浏览器通过请求向Tomcat提交的数据</li><li>请求参数通常是用户输入的数据，待Servlet进行处理</li><li>用&amp;符号连接</li></ul><h2 id="get与post请求方法"><a class="markdownIt-Anchor" href="#get与post请求方法"></a> Get与Post请求方法</h2><ul><li>Get方式是将数据通过在URL附加数据显性向服务器发送数据<ul><li>常用于不包含敏感信息的查询功能</li></ul></li><li>Post方式会将数据存放在Form中隐性向服务器发送数据<ul><li>常用于安全性较高的功能或者服务器的写操作</li><li>用户登录</li><li>用户注册</li><li>更新公司账目</li></ul></li></ul><h2 id="servlet生命周期"><a class="markdownIt-Anchor" href="#servlet生命周期"></a> Servlet生命周期</h2><ol><li>装载 - web.xml</li><li>创建 - 构造函数</li><li>初始化 - init()</li><li>提供服务 - service()</li><li>销毁 - destroy()</li></ol><h2 id="使用注解简化配置"><a class="markdownIt-Anchor" href="#使用注解简化配置"></a> 使用注解简化配置</h2><ul><li><code>@WebServlet</code></li></ul><h2 id="启动时加载servlet"><a class="markdownIt-Anchor" href="#启动时加载servlet"></a> 启动时加载Servlet</h2><ul><li>web.xml 使用<code>&lt;load-on-startup&gt;</code>设置启动加载</li><li>在工作中常用于系统的预处理</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;软件结构&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#软件结构&quot;&gt;&lt;/a&gt; 软件结构&lt;/h1&gt;
&lt;h2 id=&quot;单机时代-桌面应用&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#单机时代-桌面应用&quot;&gt;&lt;/a</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>XML Basics</title>
    <link href="http://hellcy.github.io/2022/02/14/XML-Basics/"/>
    <id>http://hellcy.github.io/2022/02/14/XML-Basics/</id>
    <published>2022-02-14T06:47:24.000Z</published>
    <updated>2022-02-15T03:31:57.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="xml是什么"><a class="markdownIt-Anchor" href="#xml是什么"></a> XML是什么</h1><ul><li>XML的全称是Exensible Markup Language，可扩展标记语言</li><li>编写XML就是编写标签，与HTML非常相似</li><li>良好的人机可读性</li></ul><h2 id="xml与html的比较"><a class="markdownIt-Anchor" href="#xml与html的比较"></a> XML与HTML的比较</h2><ul><li>XML与HTML非常相似，都是编写标签</li><li>XML没有预定义的标签，HTML存在大量预定义的标签</li><li>XML重在保存与传输数据，HTML用于显示信息</li></ul><h1 id="xml的用途"><a class="markdownIt-Anchor" href="#xml的用途"></a> XML的用途</h1><ul><li>作为应用配置文件</li><li>用于保存程序产生的数据</li><li>网络间的数据传输，利用soap协议</li></ul><h1 id="xml文档结构"><a class="markdownIt-Anchor" href="#xml文档结构"></a> XML文档结构</h1><ul><li>第一行必须是XML声明</li><li>有且只有一个根节点</li><li>XML标签的书写规则与HTML相同</li></ul><h2 id="xml声明"><a class="markdownIt-Anchor" href="#xml声明"></a> XML声明</h2><ul><li>XML声明说明XML文档的基本信息，包括版本号与字符集，写在XML第一行</li><li><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</code></li></ul><h1 id="xml标签书写规则"><a class="markdownIt-Anchor" href="#xml标签书写规则"></a> XML标签书写规则</h1><h2 id="合法的标签名"><a class="markdownIt-Anchor" href="#合法的标签名"></a> 合法的标签名</h2><ul><li>标签名要有意义</li><li>建议使用英文小写字母，单词之间使用<code>-</code>分割</li><li>建议多级标签之间不要存在重名的情况</li></ul><h2 id="适当的注释与缩进"><a class="markdownIt-Anchor" href="#适当的注释与缩进"></a> 适当的注释与缩进</h2><ul><li>更容易阅读</li></ul><h2 id="合理使用属性"><a class="markdownIt-Anchor" href="#合理使用属性"></a> 合理使用属性</h2><ul><li>具有唯一性的值可以作为属性</li></ul><h2 id="处理特殊字符"><a class="markdownIt-Anchor" href="#处理特殊字符"></a> 处理特殊字符</h2><ul><li><code>&lt;</code>和<code>&gt;</code></li><li><code>&amp;lt;</code>和<code>&amp;gt;</code></li></ul><h2 id="cdata标签"><a class="markdownIt-Anchor" href="#cdata标签"></a> CDATA标签</h2><ul><li>CDATA指的是不应由XML解析器进行解析的文本数据</li><li><code>&lt;![CDATA[&quot;content&quot;]]&gt;</code></li></ul><h2 id="有序的子元素"><a class="markdownIt-Anchor" href="#有序的子元素"></a> 有序的子元素</h2><ul><li>在XML多层嵌套的子元素中，标签前后顺序应保持一致</li></ul><h1 id="xml语义约束"><a class="markdownIt-Anchor" href="#xml语义约束"></a> XML语义约束</h1><ul><li>XML文档结构正确，但可能不是有效的</li><li>XML语义约束有两种定义方式，DTD和XML Schema</li></ul><h2 id="document-type-definition"><a class="markdownIt-Anchor" href="#document-type-definition"></a> Document Type Definition</h2><ul><li>文件扩展名为 .dtd</li><li>在XML中使用<code>&lt;!DOCTYPE&gt;</code>标签来引用DTD文件</li></ul><h2 id="xml-schema"><a class="markdownIt-Anchor" href="#xml-schema"></a> XML Schema</h2><ul><li>文件扩展名为 .xsd</li><li>比DTD更为复杂，提供了更多功能</li><li>提供了数据类型，格式限定，数据范围等特性</li><li>是W3C标准</li></ul><h1 id="dom文档对象模型"><a class="markdownIt-Anchor" href="#dom文档对象模型"></a> DOM文档对象模型</h1><ul><li>Document Object Model 定义了访问和操作XML文档的标准方法，DOM把XML文档作为树结构来查看，能够通过DOM树来读写所有元素</li></ul><h2 id="dom4j"><a class="markdownIt-Anchor" href="#dom4j"></a> Dom4j</h2><ul><li>一个易用的，开源的库，用于解析XML，它应用于Java平台，具有性能优异，功能强大和及其易用的特点</li><li>Dom4j将XML视为document对象</li><li>XML标签被Dom4j定义为element对象</li></ul><h1 id="xpath路径表达式"><a class="markdownIt-Anchor" href="#xpath路径表达式"></a> XPath路径表达式</h1><ul><li>XML文档中查找数据的语言</li><li>掌握XPath可以极大地提高在提取数据时的开发效率</li><li>学习XPath本质就是掌握各种形式表达式的使用技巧</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;xml是什么&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#xml是什么&quot;&gt;&lt;/a&gt; XML是什么&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;XML的全称是Exensible Markup Language，可扩展标记语言&lt;/li&gt;
&lt;li&gt;编写XM</summary>
      
    
    
    
    
    <category term="XML" scheme="http://hellcy.github.io/tags/XML/"/>
    
  </entry>
  
  <entry>
    <title>Java Concurrency</title>
    <link href="http://hellcy.github.io/2022/02/12/Java-Concurrency/"/>
    <id>http://hellcy.github.io/2022/02/12/Java-Concurrency/</id>
    <published>2022-02-12T10:23:56.000Z</published>
    <updated>2022-02-12T12:11:41.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是进程"><a class="markdownIt-Anchor" href="#什么是进程"></a> 什么是进程</h1><ul><li>进程是指可执行程序并存放在计算机存储器的一个指令序列，他是一个动态执行的过程</li></ul><h1 id="什么是线程"><a class="markdownIt-Anchor" href="#什么是线程"></a> 什么是线程</h1><ul><li>线程是比进程还要小的运行单位，一个进程包含多个线程</li><li>线程可以看做一个子程序</li></ul><h1 id="线程的创建"><a class="markdownIt-Anchor" href="#线程的创建"></a> 线程的创建</h1><ul><li>创建一个Thread类，或者一个Thread子类的对象</li><li>创建一个实现了Runnable接口的类的对象</li><li>创建一个实现了Callable接口的类的对象</li></ul><h2 id="继承thread类"><a class="markdownIt-Anchor" href="#继承thread类"></a> 继承Thread类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public class CustomThread extends Thread &#123;</span><br><span class="line">  public void run() &#123;</span><br><span class="line">    System.out.println(getName() + &quot; thread is running&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="runnable-接口"><a class="markdownIt-Anchor" href="#runnable-接口"></a> Runnable 接口</h2><ul><li>只有一个方法run()</li><li>Runnable是Java中用已实现线程的接口</li><li>任何实现线程功能的类都必须实现该接口</li><li>为什么要实现runnable接口？<ul><li>Java不支持多重继承，如果一个类已经继承了一个父类，那么他只能通过实现runnable接口变成线程</li><li>可以不重写Thread类的其他方法，只需要重写Run()方法</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class PrintRunnable implements Runnable&#123;</span><br><span class="line">  int i &#x3D; 1;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  public void run() &#123;</span><br><span class="line">    while (i &lt;&#x3D; 10) &#123;</span><br><span class="line">      System.out.println(Thread.currentThread().getName() + &quot; is running &quot; + (i++));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class PrintRunnableTest &#123;</span><br><span class="line">  @Test</span><br><span class="line">  public void testRunnableThread() &#123;</span><br><span class="line">    PrintRunnable runnable1 &#x3D; new PrintRunnable();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; use runnable object to create thread</span><br><span class="line">    &#x2F;&#x2F; 2 threads are sharing the same variable</span><br><span class="line">    Thread t1 &#x3D; new Thread(runnable1);</span><br><span class="line">    t1.start();</span><br><span class="line"></span><br><span class="line">    Thread t2 &#x3D; new Thread(runnable1);</span><br><span class="line">    t2.start();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实现callable接口"><a class="markdownIt-Anchor" href="#实现callable接口"></a> 实现callable接口</h2><ul><li>重写call()方法，作为线程的主体，具有返回值，并且可以对异常进行声明和抛出， 使用start()方法来启动线程</li></ul><ol><li>创建callable接口的实现类，并实现call()方法</li><li>创建callable实现类的实例，使用FutureTask类来包装callable对象，该FutureTask对象封装了callable对象的call()方法的返回值</li><li>使用FutureTask对象作为Thread对象的target，创建并启动线程</li><li>调用FutureTask对象的get()方法来获得子线程执行结束后的返回值</li></ol><ul><li>实现callable接口，创建线程</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public class CustomThread implements Callable&lt;String&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public String call() throws Exception &#123;</span><br><span class="line">        String str &#x3D; &quot;thread message&quot;;</span><br><span class="line">        return str;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>test thread</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testCustomThread() &#123;</span><br><span class="line">    Callable&lt;String&gt; callObj &#x3D; new CustomThread();</span><br><span class="line">    FutureTask&lt;String&gt; ft &#x3D; new FutureTask&lt;&gt;(callObj);</span><br><span class="line">    Thread thread &#x3D; new Thread(ft);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; start thread</span><br><span class="line">    thread.start();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; the return value can only be got after thread has been started</span><br><span class="line">    try &#123;</span><br><span class="line">      System.out.println(ft.get());</span><br><span class="line">    &#125; catch (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="线程的状态"><a class="markdownIt-Anchor" href="#线程的状态"></a> 线程的状态</h1><ol><li>new</li><li>runnable</li><li>running</li><li>blocked</li><li>dead</li></ol><p><img src="/../images/Java-Concurrency/1.png" alt="" /></p><h2 id="sleep方法的使用"><a class="markdownIt-Anchor" href="#sleep方法的使用"></a> sleep方法的使用</h2><ul><li><code>public static void sleep(long millis)</code></li><li>作用：在指定的毫秒数内让正在执行的线程休眠（暂停执行）</li><li>参数为休眠的时间，单位是毫秒</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">while (i &lt;&#x3D; 30) &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + &quot; is running &quot; + (i++));</span><br><span class="line">    try &#123;</span><br><span class="line">    Thread.sleep(1000);</span><br><span class="line">    &#125; catch (InterruptedException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="join方法应用"><a class="markdownIt-Anchor" href="#join方法应用"></a> join方法应用</h2><ul><li><code>public final void join()</code></li><li>等待调用该方法的线程结束后才能执行 （调用者抢占CPU使用权）</li><li><code>public final void join(long millis)</code></li><li>等待该线程终止的最长时间为millis毫秒</li></ul><h2 id="线程优先级"><a class="markdownIt-Anchor" href="#线程优先级"></a> 线程优先级</h2><ul><li>Java为线程类提供了10个优先级</li><li>优先级可以用整数1-10表示，超过范围会抛出异常</li><li>主线程默认优先级为5</li><li><code>getPriority()</code></li><li><code>setPriority()</code></li></ul><h3 id="优先级常量"><a class="markdownIt-Anchor" href="#优先级常量"></a> 优先级常量</h3><ul><li>MAX_PRIORITY 10</li><li>MIN_PRIORITY 1</li><li>NORM_PRIORITY 5 (默认)</li></ul><h1 id="多线程运行问题"><a class="markdownIt-Anchor" href="#多线程运行问题"></a> 多线程运行问题</h1><ul><li>各个线程是通过竞争CPU时间获得运行机会的</li><li>各线程什么时候得到CPU时间，占用多久，是不可预测的</li><li>一个正在运行的线程在什么地方被暂停是不确定的</li></ul><h2 id="同步"><a class="markdownIt-Anchor" href="#同步"></a> 同步</h2><ul><li>synchronized 可以被用在<ul><li>成员方法</li><li>静态方法</li><li>语句块</li></ul></li></ul><h2 id="线程间通信"><a class="markdownIt-Anchor" href="#线程间通信"></a> 线程间通信</h2><ul><li>生产者 - 消费者 模型</li><li>wait()方法，终端方法的执行，是线程等待</li><li>notify()方法，唤醒处于等待的某一个线程，使其结束等待</li><li>notifyAll()方法，唤醒处于等待的所有线程，使它们结束等待</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是进程&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#什么是进程&quot;&gt;&lt;/a&gt; 什么是进程&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;进程是指可执行程序并存放在计算机存储器的一个指令序列，他是一个动态执行的过程&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java Collections</title>
    <link href="http://hellcy.github.io/2022/02/12/Java-Collections/"/>
    <id>http://hellcy.github.io/2022/02/12/Java-Collections/</id>
    <published>2022-02-11T15:17:40.000Z</published>
    <updated>2022-02-12T09:22:52.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="应用场景"><a class="markdownIt-Anchor" href="#应用场景"></a> 应用场景</h1><ul><li>无法预测存储数据的数量</li><li>同时存储具有一对一关系的数据</li><li>需要进行数据的增删</li><li>数据重复的问题</li></ul><h1 id="集合框架的体系结构"><a class="markdownIt-Anchor" href="#集合框架的体系结构"></a> 集合框架的体系结构</h1><h2 id="collection-类的对象"><a class="markdownIt-Anchor" href="#collection-类的对象"></a> Collection - 类的对象</h2><ul><li>List<ul><li>ArrayList</li></ul></li><li>Queue<ul><li>LinkedList</li></ul></li><li>Set<ul><li>HashSet</li></ul></li></ul><h2 id="map-key-value-pairs"><a class="markdownIt-Anchor" href="#map-key-value-pairs"></a> Map - key value pairs</h2><ul><li>HashMap</li></ul><h1 id="list"><a class="markdownIt-Anchor" href="#list"></a> List</h1><ul><li>List是元素有序并且可以重复的集合，成为序列</li><li>List可以精确的控制每个元素的插入位置，或删除某个位置的元素</li><li>List的两个主要实现是ArrayList和LinkedList</li></ul><h2 id="arraylist"><a class="markdownIt-Anchor" href="#arraylist"></a> ArrayList</h2><ul><li>ArrayList底层是由数组实现的</li><li>动态增长（倍增)，以满足应用程序的需求</li><li>在列表尾部插入或删除数据非常有效，增删中间部分的元素则不是非常高效</li><li>更适合查找和更新元素</li><li>ArrayList中的元素可以为null</li></ul><h1 id="set"><a class="markdownIt-Anchor" href="#set"></a> Set</h1><ul><li>Set是元素无序并且不可以重复的集合</li></ul><h2 id="hashset"><a class="markdownIt-Anchor" href="#hashset"></a> HashSet</h2><ul><li>HashSet是Set的一个重要的实现类，</li><li>HashSet中的元素无序并且不可以重复</li><li>HashSet中只允许一个null元素</li><li>具有良好的存取和查找性能</li></ul><h2 id="迭代器-iterator"><a class="markdownIt-Anchor" href="#迭代器-iterator"></a> 迭代器 Iterator</h2><ul><li>Iterator接口可以以统一的方法对各种集合元素进行遍历</li><li>hasNext()方法检测集合中是否还有下一个元素</li><li>next()方法返回集合中的下一个元素</li></ul><h1 id="map"><a class="markdownIt-Anchor" href="#map"></a> Map</h1><ul><li>Map中的数据是以key value的形式存储的</li><li>key value是以Entry类型的对象实例存在</li><li>可以通过key值快速得查找value</li><li>一个映射不能包含重复的key， value可以重复</li><li>每个key最多映射到一个value</li></ul><h2 id="hashmap"><a class="markdownIt-Anchor" href="#hashmap"></a> HashMap</h2><ul><li>给予哈希表得Map实现</li><li>允许使用null值和null key</li><li>key值不允许重复</li><li>HashMap中的entry对象是无序排列得</li></ul><h1 id="集合排序"><a class="markdownIt-Anchor" href="#集合排序"></a> 集合排序</h1><ul><li>使用Collections类的sort()方法</li><li>使用Comparator和Comparable接口进行排序</li></ul><h2 id="comparator接口"><a class="markdownIt-Anchor" href="#comparator接口"></a> Comparator接口</h2><ul><li>强行对某个对象进行整体排序的比较函数</li><li>可以将comparator传递给sort方法 （如Collections.sort或者Arrays.sort）</li><li>int compare(T o1, T o2) 比较用来排序的两个参数<ul><li>if o1 &lt; o2, return negative integer</li><li>if o1 == o2, return 0</li><li>if o1 &gt; o2, return positive integer</li></ul></li></ul><h2 id="comparable接口"><a class="markdownIt-Anchor" href="#comparable接口"></a> Comparable接口</h2><ul><li>此接口强行对实现他的每个类的对象进行整体排序</li><li>这种排序被称为类的自然排序，类的compareTo方法被称为他的自然比较方法</li><li>对于集合，通过调用Collections.sort方法进行排序</li><li>对于数组，通过调用Arrays.sort方法进行排序</li><li>int compareTo(T o)<ul><li>该对象小于，等于或大于指定对象，则分别返回负整数，零或者正整数 （与之前的<code>int compare(T o1, T o2)</code>相同）</li></ul></li></ul><h2 id="comparator-和-comparable-的区别"><a class="markdownIt-Anchor" href="#comparator-和-comparable-的区别"></a> Comparator 和 Comparable 的区别</h2><table><thead><tr><th>Comparator</th><th>Comparable</th></tr></thead><tbody><tr><td>位于java.util包</td><td>位于java.lang包</td></tr><tr><td>在要比较的类的外部实现该接口，并且可以实现多个不同的Comparator</td><td>在要比较的类上实现该接口</td></tr><tr><td>调用sort方法时，要指定comparator的实现类</td><td>调用sort方法时，只需指定集合名即可</td></tr></tbody></table><h2 id="应用场景-2"><a class="markdownIt-Anchor" href="#应用场景-2"></a> 应用场景</h2><ul><li>如果一个类实现了comparable接口，还希望通过不同的方式进行排序，我们还可以定义额外的Comparator</li><li>Comparable会作为默认的排序方式，Comparator接口则作为一个扩展的排序方式</li></ul><h2 id="treeset"><a class="markdownIt-Anchor" href="#treeset"></a> TreeSet</h2><ul><li>TreeSet 是一个有序的集合，它支持自然排序个根据实现Comparable和Comparator接口进行排序</li><li>当TreeSet用来储存String或者Integer对象时，会按照他们的升序排列</li><li>TreeSet无法排列自定义类，元素需要实现Comparator或者Comparable来排序</li></ul><h1 id="泛型"><a class="markdownIt-Anchor" href="#泛型"></a> 泛型</h1><ul><li><p>在Java中增加泛型之前，泛型程序设计使用继承来实现</p><ul><li>坏处<ul><li>需要强制转换</li><li>可向集合中添加任意类型的元素，存在风险</li></ul></li></ul></li><li><p>泛型的使用</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br></pre></td></tr></table></figure><ul><li>Java SE7及以后的版本中，构造方法中的泛型可以省略</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h2 id="多态与泛型"><a class="markdownIt-Anchor" href="#多态与泛型"></a> 多态与泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Cat extends Animal &#123;&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Animal&gt; list &#x3D; new ArrayList&lt;Cat&gt;(); &#x2F;&#x2F; not allowed</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; numbers &#x3D; new ArrayList&lt;Integer&gt;(); &#x2F;&#x2F; not allowed</span><br></pre></td></tr></table></figure><ul><li>以上代码是不允许的， 变量声明的类型必须匹配传递给实际对象的类型</li></ul><h2 id="自定义泛型类"><a class="markdownIt-Anchor" href="#自定义泛型类"></a> 自定义泛型类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public class NumGeneric&lt;T&gt; &#123;</span><br><span class="line">  private T num;</span><br><span class="line"></span><br><span class="line">  public T getNum() &#123;</span><br><span class="line">    return num;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setNum(T num) &#123;</span><br><span class="line">    this.num &#x3D; num;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; generic class with two generic types</span><br><span class="line">public class TwoNumGeneric&lt;A, B&gt; &#123;</span><br><span class="line">  private A num1;</span><br><span class="line">  private B num2;</span><br><span class="line"></span><br><span class="line">  public TwoNumGeneric(A num1, B num2) &#123;</span><br><span class="line">    this.num1 &#x3D; num1;</span><br><span class="line">    this.num2 &#x3D; num2;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public A getNum1() &#123;</span><br><span class="line">    return num1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setNum1(A num1) &#123;</span><br><span class="line">    this.num1 &#x3D; num1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public B getNum2() &#123;</span><br><span class="line">    return num2;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public void setNum2(B num2) &#123;</span><br><span class="line">    this.num2 &#x3D; num2;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自定义泛型方法"><a class="markdownIt-Anchor" href="#自定义泛型方法"></a> 自定义泛型方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class GenericMethod &#123;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; &lt;T&gt; between public and void declare that this is a generic method</span><br><span class="line">  public &lt;T&gt; void printValue(T t) &#123;</span><br><span class="line">    System.out.println(t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;应用场景&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#应用场景&quot;&gt;&lt;/a&gt; 应用场景&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;无法预测存储数据的数量&lt;/li&gt;
&lt;li&gt;同时存储具有一对一关系的数据&lt;/li&gt;
&lt;li&gt;需要进行数据的增删&lt;/li&gt;
</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java Exceptions</title>
    <link href="http://hellcy.github.io/2022/02/11/Java-Exceptions/"/>
    <id>http://hellcy.github.io/2022/02/11/Java-Exceptions/</id>
    <published>2022-02-10T23:51:36.000Z</published>
    <updated>2022-02-12T12:21:13.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是异常"><a class="markdownIt-Anchor" href="#什么是异常"></a> 什么是异常</h1><ul><li>错误在我们编写程序的过程中会经常发生，包括编译期间和运行期间的错误</li><li>在程序运行过程中，意外发生的情况，背离我们程序本身意图的表现，都可以理解为异常</li></ul><h1 id="异常分类"><a class="markdownIt-Anchor" href="#异常分类"></a> 异常分类</h1><ul><li>Throwable<ul><li>Error<ul><li>OutOfMemoryError</li><li>ThreadDeath</li><li>…</li></ul></li><li>Exception<ul><li>程序本身可以处理的异常</li><li>unchecked exception （runtime exception）<ul><li>NullPointerException</li><li>ArrayIndexOutOfBoundsException</li><li>…</li></ul></li><li>checked exception<ul><li>IOException</li><li>SQLException</li><li>…</li></ul></li></ul></li></ul></li></ul><h1 id="捕获异常"><a class="markdownIt-Anchor" href="#捕获异常"></a> 捕获异常</h1><ul><li>对于运行时异常，错误或可查异常，Java技术所要求的异常处理方式有所不同</li><li>对于可查异常必须捕捉，或者声明抛出</li><li>允许忽略不可查的RuntimeException 和 Error</li></ul><h1 id="异常处理"><a class="markdownIt-Anchor" href="#异常处理"></a> 异常处理</h1><ul><li>通过5个关键字来实现： try, catch, finally, throw, throws</li></ul><h2 id="捕获异常-2"><a class="markdownIt-Anchor" href="#捕获异常-2"></a> 捕获异常</h2><ul><li>try: 执行可能产生异常的代码</li><li>catch: 捕获异常</li><li>finally: 无论是否发生异常，总是执行的代码</li></ul><h2 id="声明异常"><a class="markdownIt-Anchor" href="#声明异常"></a> 声明异常</h2><ul><li>throws: 声明可能要抛出的异常</li></ul><h2 id="抛出异常"><a class="markdownIt-Anchor" href="#抛出异常"></a> 抛出异常</h2><ul><li>手动抛出异常</li></ul><h2 id="try-catch-finally"><a class="markdownIt-Anchor" href="#try-catch-finally"></a> try catch finally</h2><ul><li>try之后可以接零个或多个catch， 如果没有catch，则必须接一个finally</li></ul><h2 id="throw-throws"><a class="markdownIt-Anchor" href="#throw-throws"></a> throw throws</h2><ul><li>可以通过throws声明将要抛出何种类型的异常， 通过throw将产生的异常抛出</li></ul><h1 id="throws"><a class="markdownIt-Anchor" href="#throws"></a> throws</h1><ul><li>如果一个方法可能会出现异常，但没有能力处理这种异常，可以在方法声明处用throws子句来声明抛出异常</li><li>throws语句用在方法定义时，生命该方法要抛出的异常类型</li><li>当方法抛出异常列表中的异常时，方法将不对这些类型及其子类类型的异常做处理，而抛向方法的调用者，由它去处理</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public int divideIntegers(int a, int b) throws Exception &#123;</span><br><span class="line">    return a &#x2F; b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        int answer &#x3D; divideIntegers(5, 10);</span><br><span class="line">    &#125; catch(ArithmeticException e) &#123;</span><br><span class="line">        </span><br><span class="line">    &#125; catch(InputMismatchException e) &#123;</span><br><span class="line"></span><br><span class="line">    &#125; catch(Exception e) &#123;</span><br><span class="line"></span><br><span class="line">    &#125; finally &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="throw"><a class="markdownIt-Anchor" href="#throw"></a> throw</h1><ul><li>throw用来抛出一个异常</li><li>抛出的只能够是可抛出类throwable或者其子类的实例对象</li><li>自己抛出的异常，自己处理</li><li>抛出异常使用throws由调用者处理</li></ul><h1 id="自定义异常"><a class="markdownIt-Anchor" href="#自定义异常"></a> 自定义异常</h1><ul><li>使用Java内置的异常类可以描述在编程时出现的大部分异常情况</li><li>也可以通过自定义异常描述特定业务产生的异常类型</li><li>自定义异常就是定义一个类，去继承Throwable类或者它的子类</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public class CustomException extends Exception &#123;</span><br><span class="line">    public CustomExpcetion () &#123;</span><br><span class="line">        super(&quot;Custom exception message&quot;)；</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="异常链"><a class="markdownIt-Anchor" href="#异常链"></a> 异常链</h1><ul><li><p>有时候我们会捕获一个异常后再抛出另一个异常</p></li><li><p>顾名思义就是：将异常发生的原因一个穿一个穿起来，把底层的异常信息传给上层，这样逐层抛出</p></li><li><p>新的异常可以保留原有异常的信息</p></li><li><p>构造方法的定义如下</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception(String mesasge, Throwable cause)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void methodOne() throws FirstException &#123;</span><br><span class="line">    throw new FirstException(&quot;first exception&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void methodTwo() throws SecondException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        methodOne();</span><br><span class="line">    &#125; catch (FirstException e) &#123;</span><br><span class="line">        throw new SecondException(&quot;Second Exception&quot;, e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void methodThree() throws ThirdException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        methodTwo();</span><br><span class="line">    &#125; catch (SecondException e) &#123;</span><br><span class="line">        ThridException exception &#x3D; new ThirdException(&quot;third exception&quot;);</span><br><span class="line">        exception.initCause(e); &#x2F;&#x2F; initCause是Exception中的另外一个成员方法，用于添加cause，原有异常的信息</span><br><span class="line">        throw exception;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是异常&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#什么是异常&quot;&gt;&lt;/a&gt; 什么是异常&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;错误在我们编写程序的过程中会经常发生，包括编译期间和运行期间的错误&lt;/li&gt;
&lt;li&gt;在程序运行过程中，意外发生的</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Java Polymorphism</title>
    <link href="http://hellcy.github.io/2022/02/10/Java-Polymorphism/"/>
    <id>http://hellcy.github.io/2022/02/10/Java-Polymorphism/</id>
    <published>2022-02-10T05:16:18.000Z</published>
    <updated>2022-02-12T12:20:50.095Z</updated>
    
    <content type="html"><![CDATA[<h1 id="编译时多态"><a class="markdownIt-Anchor" href="#编译时多态"></a> 编译时多态</h1><ul><li>设计时多态方法重载</li></ul><h1 id="运行时多态"><a class="markdownIt-Anchor" href="#运行时多态"></a> 运行时多态</h1><ul><li>程序运行时动态决定调用那个方法</li></ul><h1 id="必要条件"><a class="markdownIt-Anchor" href="#必要条件"></a> 必要条件</h1><ol><li>满足继承关系</li><li>父类引用指向子类对象</li></ol><h1 id="向上转型"><a class="markdownIt-Anchor" href="#向上转型"></a> 向上转型</h1><ul><li>父类引用指向子类实例，可以调用子类重写父类的方法以及父类派生的方法，无法调用子类独有的方法</li><li>父类中的静态方法无法被子类重写，所以向上转型之后，只能调用到父类原有的静态方法</li></ul><h1 id="向下转型"><a class="markdownIt-Anchor" href="#向下转型"></a> 向下转型</h1><ul><li>子类引用指向父类对象，此处可以使用<code>instanceof</code>进行检查，避免类型转换时的安全性问题</li><li>可以调用子类独有的方法</li></ul><h1 id="抽象类-abstract-class"><a class="markdownIt-Anchor" href="#抽象类-abstract-class"></a> 抽象类 abstract class</h1><ul><li>限制实例化</li><li>只能被继承</li><li>应用场景： 某个父类只是知道其子类应该包含怎样的方法，但无法准确知道这些子类如何实现这些方法</li></ul><h1 id="抽象方法-abstract-method"><a class="markdownIt-Anchor" href="#抽象方法-abstract-method"></a> 抽象方法 abstract method</h1><ul><li>不能有方法体</li><li>必须由子类实现</li><li>子类如果没有重写父类的所有抽象方法，则也要定义为抽象类</li></ul><h1 id="接口-interface"><a class="markdownIt-Anchor" href="#接口-interface"></a> 接口 Interface</h1><ul><li>当多个类具有相同能力的时候，可以使用接口抽象出相同的能力</li><li>接口定义了某一批类所需要遵守的规范</li><li>接口不关心这些类的内部数据，也不关心这些类里的方法的实现细节，它只规定这些类里必须提供某些方法</li><li>接口方法可以不写<code>abstract</code>关键字，并且默认为public的访问权限</li><li>当类实现接口时，需要去实现接口中的所有抽象方法，否则需要将该类设置为抽象类</li><li>接口中可以定义常量，默认为<code>public static final</code></li></ul><h2 id="默认方法"><a class="markdownIt-Anchor" href="#默认方法"></a> 默认方法</h2><ul><li>自JDK1.8之后，接口中可以存在默认方法，使用<code>default</code>关键字定义</li><li>默认方法可以带方法体，子类实现接口时可以不用实现默认方法</li><li>子类可以重写默认方法，并可以通过接口的引用调用</li></ul><h2 id="静态方法"><a class="markdownIt-Anchor" href="#静态方法"></a> 静态方法</h2><ul><li>自JDK1.8之后，接口中可以存在静态方法，使用<code>static</code>关键字定义</li><li>静态方法可以带方法体，子类可以通过使用接口名访问接口的静态方法</li></ul><h2 id="多重实现"><a class="markdownIt-Anchor" href="#多重实现"></a> 多重实现</h2><ul><li>子类可以继承一个父类，但是可以实现多个接口</li><li>当多个接口中具有相同签名的方法时，子类需要重写方法</li><li>当父类和接口具有相同签名的方法时，父类方法具有优先权</li><li>当父类和接口具有相同名字的变量时，子类需要重新定义该变量，父类中的变量不具有优先权</li></ul><h2 id="接口的继承"><a class="markdownIt-Anchor" href="#接口的继承"></a> 接口的继承</h2><ul><li>接口也可以实现继承关系</li><li>接口可以继承多个父接口</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public interface ParentOne &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public interface ParentTwo &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public interface Child extends ParentOne, ParentTwo &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="内部类"><a class="markdownIt-Anchor" href="#内部类"></a> 内部类</h1><ul><li>内部类提供了更好的封装，不允许其他外部类访问内部类的信息</li></ul><h2 id="成员内部类"><a class="markdownIt-Anchor" href="#成员内部类"></a> 成员内部类</h2><ul><li>最常见的内部类，也称为普通内部类</li><li>内部类在外部使用时，无法直接实例化，需要借由外部类信息才能完成实例化</li><li>内部类的访问修饰符，可以是任意的，但是访问权限会受到修饰符的影响</li><li>内部类可以直接访问外部类的成员（包括成员属性和成员方法），如果出现同名属性，优先访问内部类中定义的</li><li>外部类访问内部类的信息需要通过内部类的实例，无法直接访问</li><li>内部类编译后得class文件名：外部类$内部类.class</li></ul><h3 id="获取内部类对象实例"><a class="markdownIt-Anchor" href="#获取内部类对象实例"></a> 获取内部类对象实例</h3><ol><li>new 外部类.new 内部类</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person.Heart myHeart &#x3D; new Person().new Heart();</span><br></pre></td></tr></table></figure><ol start="2"><li>外部类对象.new 内部类</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myHeart &#x3D; myPerson.new Heart();</span><br></pre></td></tr></table></figure><ol start="3"><li>外部类对象.获取方法</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myHeart &#x3D; myPerson.getHeart();</span><br></pre></td></tr></table></figure><h2 id="静态内部类"><a class="markdownIt-Anchor" href="#静态内部类"></a> 静态内部类</h2><ul><li>静态内部类中，只能直接访问外部类的静态成员</li><li>需要使用外部类的实例对象来访问非静态成员</li><li>访问静态内部类对象实例时，可以不依赖于外部类对象</li></ul><h3 id="获取静态内部类实例"><a class="markdownIt-Anchor" href="#获取静态内部类实例"></a> 获取静态内部类实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person.Heart myHeart &#x3D; new Person.Heart();</span><br></pre></td></tr></table></figure><h2 id="方法内部类"><a class="markdownIt-Anchor" href="#方法内部类"></a> 方法内部类</h2><ul><li>定义在外部类方法中的内部类，也成为局部内部类</li><li>方法内部类中无法定义静态成员</li><li>类中可以使用final，abstract成员</li><li>和方法内部成员使用规则一样，class前面不可以添加public，private，protected，static等关键字</li></ul><h2 id="匿名内部类"><a class="markdownIt-Anchor" href="#匿名内部类"></a> 匿名内部类</h2><ul><li>将类的定义和类的创建放在一起完成，程序只会用到一次类的实例，所以类名无关紧要</li><li>对于抽象类Person来说，如果我们想调用其中的抽象方法，一种做法是创建一个实现read方法的子类</li><li>但是如果这个子类只会被用到一次，那这个子类的名字就不重要，就可以使用匿名内部类来解决</li><li>无法使用private，public，protected，static修饰</li><li>无法编写构造方法，但是可以添加初始化代码块</li><li>不能出现静态成员</li><li>可以实现接口也可以继承父类，但是不能同时</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public abstract class Person &#123;</span><br><span class="line">    public abstract void read();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class PersonTest &#123;</span><br><span class="line">    public void getRead(Person person) &#123;</span><br><span class="line">        person.read();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        PersonTest personTest &#x3D; new PersonTest();</span><br><span class="line"></span><br><span class="line">        personTest.getRead(new Person() &#123;</span><br><span class="line"></span><br><span class="line">            @Override</span><br><span class="line">            public void read() &#123;</span><br><span class="line">                System.out.println(&quot;implement read method in Person parent abstract class&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="匿名类的例子"><a class="markdownIt-Anchor" href="#匿名类的例子"></a> 匿名类的例子</h3><ul><li><p>在我们使用comparator对Collections进行排序的时候可以使用匿名类来省去创建子类的过程</p></li><li><p>不使用匿名类对List排序</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; create a child class implements parent Comparator</span><br><span class="line">public class CustomComparator implements Comparator&lt;String&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int compare(String o1, String o2) &#123;</span><br><span class="line">        return o1.compareTo(o2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class testComparator() &#123;</span><br><span class="line">    List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; use CustomComparator to sort list</span><br><span class="line">    Collections.sort(list, new CustomComparator());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用匿名类对list排序</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public class testComparator() &#123;</span><br><span class="line">    List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    Collections.sort(list, new Comparator&lt;String&gt;() &#123;</span><br><span class="line">        public int compare(String o1, String o2) &#123;</span><br><span class="line">            return o1.compareTo(o2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>自Java1.8以后，可以使用lambda expression来省去方法名，匿名方法</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public class testComparator() &#123;</span><br><span class="line">    List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    Collections.sort(list, (x, y) -&gt; x.compareTo(y));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>因为上面的匿名方法和String里面定义的compareTo方法一样，我们可以使用method reference来更加简化代码</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public class testComparator() &#123;</span><br><span class="line">    List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    Collections.sort(list, String::compareTo);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;编译时多态&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#编译时多态&quot;&gt;&lt;/a&gt; 编译时多态&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;设计时多态方法重载&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;运行时多态&quot;&gt;&lt;a class=&quot;markdownIt-</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Design Pattern - Singleton in Java</title>
    <link href="http://hellcy.github.io/2022/02/09/Design-Pattern-Singleton-in-Java/"/>
    <id>http://hellcy.github.io/2022/02/09/Design-Pattern-Singleton-in-Java/</id>
    <published>2022-02-09T06:26:19.000Z</published>
    <updated>2022-02-10T14:39:24.300Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目的"><a class="markdownIt-Anchor" href="#目的"></a> 目的</h1><ul><li>使得类的一个对象成为该类系统中唯一的实例</li></ul><h1 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h1><ul><li>一个类有且仅有一个实例，并且自行实例化向整个系统提供</li></ul><h1 id="要点"><a class="markdownIt-Anchor" href="#要点"></a> 要点</h1><ol><li>某个类只能有一个实例</li><li>必须自行创建实例</li><li>必须自行向这个系统提供这个实例</li></ol><h1 id="实现"><a class="markdownIt-Anchor" href="#实现"></a> 实现</h1><ol><li>只提供私有的构造方法</li><li>含有一个该类的静态私有对象</li><li>提供一个静态的公有方法用于创建，获取静态私有对象</li></ol><ul><li>Create class instance when class is loaded</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public class SingletonOne &#123;</span><br><span class="line">  &#x2F;&#x2F; private class constructor</span><br><span class="line">  private SingletonOne () &#123;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; private static instance</span><br><span class="line">  private static SingletonOne INSTANCE &#x3D; new SingletonOne();</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; return instance in public method</span><br><span class="line">  public static SingletonOne getInstance() &#123;</span><br><span class="line">    return INSTANCE;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; SingletonOne instance &#x3D; SingletonOne.getInstance();</span><br></pre></td></tr></table></figure><ul><li>Only create instance when the method is being called</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class SingletonTwo &#123;</span><br><span class="line">  private SingletonTwo () &#123;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private static SingletonTwo INSTANCE &#x3D; null;</span><br><span class="line"></span><br><span class="line">  public static SingletonTwo getInstance() &#123;</span><br><span class="line">    if (INSTANCE &#x3D;&#x3D; null) INSTANCE &#x3D; new SingletonTwo();</span><br><span class="line">    return INSTANCE;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="注意"><a class="markdownIt-Anchor" href="#注意"></a> 注意</h1><ul><li>lazy loading 存在线程风险 1<ol><li>同步锁</li><li>双重校验锁</li><li>静态内部类</li><li>枚举</li></ol></li></ul><h1 id="优点"><a class="markdownIt-Anchor" href="#优点"></a> 优点</h1><ol><li>在内存中只有一个对象，节省内存空间</li><li>避免频繁地创建对象，提高性能</li><li>避免对共享资源的多重占用</li></ol><h1 id="缺点"><a class="markdownIt-Anchor" href="#缺点"></a> 缺点</h1><ol><li>扩展比较困难</li><li>如果实例化后的对象长期不利用，系统将默认为垃圾进行回收，造成对象状态丢失</li></ol><h1 id="场景"><a class="markdownIt-Anchor" href="#场景"></a> 场景</h1><ol><li>创建对象时占用资源过多，但同时有需要用到该类对象</li><li>对系统内资源要求统一读写，如读写配置信息</li><li>当多个实例存在可能引起程序逻辑错误，如号码生成器 （ID）</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目的&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#目的&quot;&gt;&lt;/a&gt; 目的&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;使得类的一个对象成为该类系统中唯一的实例&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;定义&quot;&gt;&lt;a class=&quot;markdownIt-A</summary>
      
    
    
    
    
    <category term="Java" scheme="http://hellcy.github.io/tags/Java/"/>
    
    <category term="Design Patterns" scheme="http://hellcy.github.io/tags/Design-Patterns/"/>
    
  </entry>
  
  <entry>
    <title>Elastic Search</title>
    <link href="http://hellcy.github.io/2022/01/02/Elastic-Search/"/>
    <id>http://hellcy.github.io/2022/01/02/Elastic-Search/</id>
    <published>2022-01-02T06:41:13.000Z</published>
    <updated>2022-02-10T14:39:24.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="basics"><a class="markdownIt-Anchor" href="#basics"></a> Basics</h1><ul><li>Nodes store the data that we add to ElasticSearch</li><li>A cluster is a collection of nodes</li><li>Data is stored as document which are JSON objects</li><li>Documents are grouped together with indices</li></ul><h2 id="the-purpose-of-sharding"><a class="markdownIt-Anchor" href="#the-purpose-of-sharding"></a> The purpose of sharding</h2><ul><li>mainly to be able to store more documents</li><li>to easier fit large indices onto nodes</li><li>improved performance<ul><li>parallelization of queries increases the throughput of an index</li></ul></li></ul><h2 id="configuring-the-number-of-shards"><a class="markdownIt-Anchor" href="#configuring-the-number-of-shards"></a> Configuring the number of shards</h2><ul><li>an index contains a single shard by default</li><li>increase the number of shards with the Split API</li><li>reduce the number of shards with the Shrink API</li><li>when changing the number of shards, new index will be created and documents in the old index will be migrated to the new index</li></ul><h2 id="replication"><a class="markdownIt-Anchor" href="#replication"></a> Replication</h2><ul><li><p>replication is configured at the index level</p></li><li><p>replication works by creating copies of shards, referred to as replica shards</p></li><li><p>a shard that has been replicated is called a primary shard</p></li><li><p>a primary shard and its replica shards are referred as a replication group</p></li><li><p>replica shards are a complete copy of a shard</p></li><li><p>a replica shard can serve search requests, exactly like its primary shard</p></li><li><p>the number of replicas can be configured at index creation</p></li><li><p>node can store multiple shards, and primary shard and replica shards will never be stored in the same node. So the data will NOT be lost if the node fails</p></li></ul><h2 id="snapshots"><a class="markdownIt-Anchor" href="#snapshots"></a> snapshots</h2><ul><li>ElasticSearch supports taking snapshots as backups</li><li>snapshots can be used to restore to a given point in time</li><li>snapshots can be taken at the index level, or for the entire cluster</li><li>use snapshots for backups, and replication for high availability and performance</li></ul><h2 id="increasing-query-throughput-with-replication"><a class="markdownIt-Anchor" href="#increasing-query-throughput-with-replication"></a> increasing query throughput with replication</h2><ul><li>replica shards of a replication group can serve different search requests simultaneously<ul><li>this increases the number of requests that can be handled at the same time</li></ul></li><li>ElasticSearch intelligently routes requests to the best shard</li><li>CPU parallelization (CPU has multiple cores now and can run queries on different threads at the same time) improves performance if multiple replica shards are stored on the same node</li></ul><h2 id="master-eligible-node"><a class="markdownIt-Anchor" href="#master-eligible-node"></a> Master-eligible node</h2><ul><li>the node may be elected as the cluster’s master node</li><li>a master node is responsible for creating and deleting indices, among others</li><li>may be used for having dedicated master nodes<ul><li>useful for large clusters</li><li>meaning that this master node will not be serving requests, only focusing on its own tasks</li></ul></li></ul><h2 id="data-node"><a class="markdownIt-Anchor" href="#data-node"></a> Data node</h2><ul><li>enables a node to store data</li><li>storing data includes performing queries related to that data, such as search queries</li><li>for relatively small clusters, this role is almost always enabled</li></ul><h1 id="managing-documents"><a class="markdownIt-Anchor" href="#managing-documents"></a> Managing Documents</h1><ul><li>Delete index</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE &#x2F;index_name</span><br></pre></td></tr></table></figure><ul><li>create index</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;index_name</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;number_of_shards&quot;: 2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="indexing-documents"><a class="markdownIt-Anchor" href="#indexing-documents"></a> indexing documents</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;products&#x2F;_doc</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Coffee Maker&quot;,</span><br><span class="line">  &quot;price&quot;: 53,</span><br><span class="line">  &quot;in_stack&quot;: 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="retrieving-document-by-id"><a class="markdownIt-Anchor" href="#retrieving-document-by-id"></a> Retrieving document by ID</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;products&#x2F;_doc&#x2F;document_ID</span><br></pre></td></tr></table></figure><h2 id="updating-document"><a class="markdownIt-Anchor" href="#updating-document"></a> Updating document</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;products&#x2F;_update&#x2F;document_ID</span><br><span class="line">&#123;</span><br><span class="line">  &quot;doc&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;new name&quot;,</span><br><span class="line">    &quot;new field&quot;: &quot;this is a new field&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="upserts"><a class="markdownIt-Anchor" href="#upserts"></a> Upserts</h2><ul><li>insert the new document if not exists, and run the script if the document exists</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;products&#x2F;_update&#x2F;101</span><br><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;source&quot;: &quot;ctx._source.in_stock++</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;upsert&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;name&quot;,</span><br><span class="line">    &quot;price&quot;: 399,</span><br><span class="line">    &quot;in_stock&quot;: 5</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="delete-document"><a class="markdownIt-Anchor" href="#delete-document"></a> Delete document</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE &#x2F;products&#x2F;_doc&#x2F;101</span><br></pre></td></tr></table></figure><h2 id="routing"><a class="markdownIt-Anchor" href="#routing"></a> Routing</h2><ul><li>routing is the process of resolving a shard for a document</li><li>the default routing strategy ensures that documents are distributed evenly</li></ul><h2 id="optimistic-concurrency-control"><a class="markdownIt-Anchor" href="#optimistic-concurrency-control"></a> Optimistic concurrency control</h2><ul><li>prevent overwriting documents inadvertently dur to concurrent operations</li><li>primary terms<ul><li>a way to distinguish between old and new primary shards</li><li>essentially a counter for how many times the primary shard has changed</li><li>the primary term is appended to write operations</li></ul></li><li>sequence numbers<ul><li>appended to write operations together with the primary term</li><li>essentially a counter that is incremented for each write operation</li><li>the primary shard increases the sequence number</li><li>enables ElasticSearch to order write operations</li></ul></li><li>sending write requests to ElasticSearch concurrently may overwrite changes made by other concurrent processes</li><li>we use the primary terms and sequence number fields</li><li>ElasticSearch will reject a write operation if it contains the wrong primary term or sequence number</li></ul><h2 id="updating-multiple-document"><a class="markdownIt-Anchor" href="#updating-multiple-document"></a> Updating multiple document</h2><ul><li>the query creates a snapshot to do optimistic concurrency control</li><li>search queries and bulk requests are sent to replication groups sequentially<ul><li>ElasticSearch retries these queries up to ten times</li><li>if the queries still fail, the whole query is aborted</li><li>any changes already made to documents, are NOT rolled back</li></ul></li><li>the API returns information about failures</li></ul><h1 id="mapping-and-analysis"><a class="markdownIt-Anchor" href="#mapping-and-analysis"></a> Mapping and Analysis</h1><ul><li>a field’s values are stored in one of several data structures<ul><li>the data structure depends on the field’s data type</li></ul></li><li>ensures efficient data access</li></ul><h2 id="inverted-indices"><a class="markdownIt-Anchor" href="#inverted-indices"></a> Inverted indices</h2><ul><li><p>mapping between terms (tags) and which documents contain them</p></li><li><p>outside the context of analyzers, we use the terminology ‘terms’</p></li><li><p>an inverted index is created for EACH text field</p></li><li><p>values for a text field are analyzed and the results are stored within an inverted index</p></li><li><p>each field has a dedicated inverted index</p></li><li><p>an inverted index is a mapping between terms and which documents contain them</p></li><li><p>terms are sorted alphabetically for performance reasons</p></li><li><p>created and maintained by Apache Lucene</p></li><li><p>inverted indices enable fast searches</p></li></ul><p>Note: for array of strings</p><ul><li>In ElasticSearch, there is no dedicated array data type, any field can contain zero or more values by default, however, all values in the array must be of the same data type</li><li>when adding a field dynamically, the first value in the array determines the field type</li><li>meaning for array of strings, ElasticSearch would have created a inverted index mapping table for it</li></ul><h2 id="keyword-data-type"><a class="markdownIt-Anchor" href="#keyword-data-type"></a> keyword data type</h2><ul><li><p>keyword fields are analyzed with the keyword analyzer</p></li><li><p>the keyword analyzer is an no-op analyzer</p><ul><li>it outputs the unmodified string as a single token</li><li>this token is then placed into the inverted index</li></ul></li><li><p>used for exact matching of values</p></li><li><p>typically used for filtering, aggregations, and sorting</p></li><li><p>for full-text searches, use the text data type instead</p></li></ul><h2 id="arrays"><a class="markdownIt-Anchor" href="#arrays"></a> Arrays</h2><ul><li>there is no such thing as an array data type</li><li>any field may contain zero or more values<ul><li>no configuration or mapping needed</li><li>simply supply an array when indexing a document</li></ul></li><li>how is the array stored in the ElasticSearch internally?<ul><li>e.g. if it is an array of strings</li><li>the strings are simply concatenated before being analyzed</li><li>and the resulting tokens are stored within an inverted index as normal String data type</li></ul></li></ul><h2 id="dates"><a class="markdownIt-Anchor" href="#dates"></a> Dates</h2><ul><li>specified in one of the three ways<ul><li>specially formatted strings</li><li>milliseconds since the epoch</li><li>seconds since the epoch</li></ul></li><li>epoch refers to the 1st of Jan 1970</li><li>custom formats are supported</li></ul><h3 id="how-date-fields-are-stored"><a class="markdownIt-Anchor" href="#how-date-fields-are-stored"></a> How date fields are stored</h3><ul><li>stored internally as milliseconds since the epoch</li><li>any valid value that you supply at index time is converted to a long value internally</li><li>dates are converted to the UTC timezone</li><li>the same date conversion happens for search queries too</li></ul><h2 id="missing-fields"><a class="markdownIt-Anchor" href="#missing-fields"></a> Missing fields</h2><ul><li>all fields are ElasticSearch are optional</li><li>you can leave out a field when indexing documents</li><li>unlike relational databases when you need to allow NULL values</li><li>some integrity checks need to be done at the application level<ul><li>e.g. having required fields</li></ul></li><li>adding a field mapping does not make a field required</li><li>searches automatically handle missing fields</li></ul><h2 id="stemming-and-stop-words"><a class="markdownIt-Anchor" href="#stemming-and-stop-words"></a> Stemming and stop words</h2><h3 id="stemming"><a class="markdownIt-Anchor" href="#stemming"></a> Stemming</h3><ul><li>reduces words to their root form<ul><li>e.g. loved -&gt; love</li><li>drinking -&gt; drink</li></ul></li></ul><h3 id="stop-words"><a class="markdownIt-Anchor" href="#stop-words"></a> stop words</h3><ul><li>words that are filtered out during the text analysis<ul><li>common words such as ‘a’, ‘the’, ‘at’, ‘of’, ‘on’ etc…</li></ul></li><li>they provide little to no value to the relevance scoring</li><li>fairly common to remove such words<ul><li>less common in ElasticSearch today than in the past</li><li>the relevance algorithm has been improved significantly</li></ul></li><li>not removed by default</li></ul><h2 id="analyzer"><a class="markdownIt-Anchor" href="#analyzer"></a> Analyzer</h2><h3 id="standard-analyzer"><a class="markdownIt-Anchor" href="#standard-analyzer"></a> Standard analyzer</h3><ul><li>splits text at word boundaries and removes punctuation<ul><li>done by the standard tokenizer</li></ul></li><li>lowercase letter with the lowercase token filter</li><li>contains the stop token filter (for removing stop words, disabled by default)</li></ul><h3 id="simple-analyzer"><a class="markdownIt-Anchor" href="#simple-analyzer"></a> Simple analyzer</h3><h3 id="whitespace-analyzer"><a class="markdownIt-Anchor" href="#whitespace-analyzer"></a> whitespace analyzer</h3><h3 id="keyword-analyzer"><a class="markdownIt-Anchor" href="#keyword-analyzer"></a> keyword analyzer</h3><h3 id="pattern-analyzer"><a class="markdownIt-Anchor" href="#pattern-analyzer"></a> pattern analyzer</h3><ul><li>a regular expression is used to match token separators<ul><li>it should match whatever should split the text into tokens</li></ul></li><li>this analyzer is very flexible</li><li>the default pattern matches all non-word characters</li><li>lowercase letters by default</li></ul><h1 id="introduction-to-searching"><a class="markdownIt-Anchor" href="#introduction-to-searching"></a> Introduction to Searching</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;product&#x2F;default&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>search queries will hit the coordinating node first, and this node will broadcast the query to all the other nodes, they will fetch the result and combine them together and return it.</li></ul><h2 id="term-level-queries"><a class="markdownIt-Anchor" href="#term-level-queries"></a> term level queries</h2><ul><li>search for exact matches, case sensitive, searching the inverted index, not the original document</li><li>term level queries are more suited for searching static values, like enums</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;basics&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#basics&quot;&gt;&lt;/a&gt; Basics&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Nodes store the data that we add to ElasticSearch&lt;/li&gt;</summary>
      
    
    
    
    
    <category term="Elastic Search" scheme="http://hellcy.github.io/tags/Elastic-Search/"/>
    
  </entry>
  
  <entry>
    <title>Dot net + React</title>
    <link href="http://hellcy.github.io/2021/12/09/Dot-net-React/"/>
    <id>http://hellcy.github.io/2021/12/09/Dot-net-React/</id>
    <published>2021-12-09T12:17:52.000Z</published>
    <updated>2022-02-10T14:39:24.300Z</updated>
    
    
    
    
    
    <category term=".NET" scheme="http://hellcy.github.io/tags/NET/"/>
    
    <category term="React" scheme="http://hellcy.github.io/tags/React/"/>
    
  </entry>
  
  <entry>
    <title>Spring Framework</title>
    <link href="http://hellcy.github.io/2021/11/05/Spring-Framework/"/>
    <id>http://hellcy.github.io/2021/11/05/Spring-Framework/</id>
    <published>2021-11-04T13:06:37.000Z</published>
    <updated>2022-02-10T14:39:24.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spring-framework-stereotypes"><a class="markdownIt-Anchor" href="#spring-framework-stereotypes"></a> Spring Framework Stereotypes</h1><ul><li>Stereotype - a fixed general image or ser of characteristics which represent a particular type of person or thing</li><li>Spring Stereotypes are <code>class</code> level annotations used to define Spring Beans<ul><li><strong>when classes annotated with Spring Stereotypes are detected via the component scan, an instance of the class will be added to the Spring context</strong></li></ul></li><li>Available Spring Stereotypes<ul><li><code>@Component</code></li><li><code>@Controller</code></li><li><code>@RestController</code></li><li><code>@Repository</code></li><li><code>@Service</code></li></ul></li></ul><table><thead><tr><th>Annotation</th><th>Description</th></tr></thead><tbody><tr><td>@Component</td><td>Indicates that an annotated class is a component and it will be created as a bean</td></tr><tr><td>@Controller</td><td>indicates that an annotated class has the role of a Spring MVC controller</td></tr><tr><td>@RestController</td><td>convenience annotation which extends @Controller and @ResponseBody</td></tr><tr><td>@Repository</td><td>indicates class is a Repository, a mechanism for encapsulating storage, retrieval, and search behavior which emulates a collection of objects</td></tr><tr><td>@Service</td><td>indicates that an annotated class is a Service, an operation offered as an interface that stands alone in the model, with no encapsulated state</td></tr></tbody></table><h1 id="spring-component-scan"><a class="markdownIt-Anchor" href="#spring-component-scan"></a> Spring Component Scan</h1><ul><li>Spring Beans defined with Spring Stereotypes are detected with Spring component scan</li><li>On startup, Spring is told to scan packages for classes with Spring Stereotype annotations</li><li>This configuration is Spring Framework specific, not Spring Boot</li><li>Spring Boot’s auto configuration will tell Spring to perform a component scan of the package of the main class<ul><li>this includes all sub packages of the main class package</li></ul></li><li>when using Spring Boot, if class is outside of the main class package tree, you must declare the package scan manually</li></ul><h1 id="spring-bean-scopes"><a class="markdownIt-Anchor" href="#spring-bean-scopes"></a> Spring Bean Scopes</h1><ul><li><code>Singleton</code> - (default) only one instance of the bean is created in the IoC container</li><li><code>Prototype</code> - A new instance is created each time the bean is requested</li><li><code>Request</code> - For web app, a single instance per http request, only valid in the context of a web aware Spring applicationContext</li><li><code>Session</code> - for web app, a single instance per http session, only valid in the context of a web aware Spring applicationContext</li><li><code>Global-session</code> - a single instance per global session, typically only used in a Portlet context, only valid in the context of a web aware Spring applicationContext</li><li><code>Application</code> - bean is scoped to the lifecycle of a ServletContext, only valid in the context of web aware</li><li><code>WebSocket</code> - scopes a single bean definition to the lifecycle of a WebSocket, only valid in the context of a web aware Spring applicationContext</li><li><code>Custom Scope</code> - Spring Scopes are extensible, and you can define your own scope by implementing Spring’s scope interface</li></ul><h2 id="declaring-bean-scope"><a class="markdownIt-Anchor" href="#declaring-bean-scope"></a> Declaring Bean Scope</h2><ul><li>No declaration needed for singleton scope</li><li>in Java configuration use <code>@Scope</code> annotation</li><li>in XML configuration file scope is an XML attribute of the bean tag</li><li>99% of the time singleton scope is fine</li></ul><h1 id="external-properties"><a class="markdownIt-Anchor" href="#external-properties"></a> External Properties</h1><ul><li>Why use External Properties?<ul><li>hard coding values which can change is considered a bad practice</li><li>makes your application rigid and hard to change</li><li>you want your application to be portable<ul><li>deployment artifact (jar, war) should be deployable to different environments</li></ul></li></ul></li><li>what can change?<ul><li>usernames, passwords, urls, API keys, paths, queue names etc…</li></ul></li></ul><h2 id="which-property-files-to-use"><a class="markdownIt-Anchor" href="#which-property-files-to-use"></a> which property files to use</h2><ul><li>using application.properties or application.yml in packaged JAR or WAR</li><li>using profile specific properties or YAML files for profile specific properties</li><li>for deployments, override properties that change with environment variables<ul><li>typically 70 - 80% of values do not change, only override what is needed</li><li>environment variables offer a secure way of seeing sensitive values such as passwords</li></ul></li></ul><h2 id="properties-setting-hierarchy-from-low-to-high"><a class="markdownIt-Anchor" href="#properties-setting-hierarchy-from-low-to-high"></a> Properties Setting Hierarchy (from low to high)</h2><ol><li>application.properties</li><li>environment variables</li><li>command line variables</li></ol><h1 id="thymeleaf"><a class="markdownIt-Anchor" href="#thymeleaf"></a> Thymeleaf</h1><ul><li>Thymeleaf is a Java template engine producing XML, XHTML and HTML5</li><li>Thymeleaf is a replacement of JSPs</li><li>Thymeleaf is a natural template engine (the templates are viewable in a web browser)</li><li>is not tied to web environment (can be used for producing HTML for emails)</li><li>Thymeleaf is not a web framework</li></ul><h1 id="request-methods"><a class="markdownIt-Anchor" href="#request-methods"></a> Request Methods</h1><ul><li>GET<ul><li>is a request for a resource (html file, javascript file, image, etc…)</li><li>is used when you visit a website</li></ul></li><li>HEAD<ul><li>is like GET, but only asks for meta information without the body</li></ul></li><li>POST<ul><li>is used to post data to the server</li><li>typical use case for POST is to post form data to the server (like a checkout form)</li><li>POST is a create request</li></ul></li><li>PUT<ul><li>is a request for the enclosed entity be stored at the supplied URI, if the entity exists, it is expected to be updated</li><li>PUT is a create OR update request</li></ul></li><li>DELETE<ul><li>is a request to delete the specified resource</li></ul></li><li>TRACE<ul><li>will echo the received request, can be used to see if request was altered by intermediate servers</li></ul></li><li>OPTIONS<ul><li>returns the HTTP methods supported by the server for the specified URL</li></ul></li></ul><h2 id="safe-methods"><a class="markdownIt-Anchor" href="#safe-methods"></a> Safe methods</h2><ul><li>safe methods are considered safe to use because they only fetch information and do not cause changes on the server</li><li>the safe methods are: GET, HEAD, OPTIONS, and TRACE</li></ul><h2 id="idempotent-methods"><a class="markdownIt-Anchor" href="#idempotent-methods"></a> Idempotent methods</h2><ul><li>a quality of an action such that repetitions of the action have no further effect on the outcome</li><li>PUT and DELETE are Idempotent methods</li><li>safe methods (GET, HEAD, OPTIONS and TRACE) are also Idempotent</li><li>being truly idempotent is not enforced by the protocol</li></ul><h2 id="non-idempotent-methods"><a class="markdownIt-Anchor" href="#non-idempotent-methods"></a> Non-Idempotent methods</h2><ul><li>POST is NOT idempotent</li><li>multiple POSTs are likely to create multiple resources</li><li>Ever seen websites asking you to click submit only once?</li></ul><h2 id="http-status-codes"><a class="markdownIt-Anchor" href="#http-status-codes"></a> HTTP Status Codes</h2><ul><li>100 series are information in nature</li><li>200 series indicate successful request<ul><li>200 Okay, 201 Created, 204 Accepted</li></ul></li><li>300 series are redirections<ul><li>301 Moved Permanently</li></ul></li><li>400 series are client errors<ul><li>400 Bad Request, 401 Not Authorized, 404 Not Found</li></ul></li><li>500 series are server side errors<ul><li>500 Internel Server Error, 503 Service Unavailable</li></ul></li></ul><h1 id="developer-tools"><a class="markdownIt-Anchor" href="#developer-tools"></a> Developer Tools</h1><ul><li>added to project via artifact ‘spring-boot-devtools’</li><li>developer tools are automatically disabled when running a packaged application</li><li>by default, not included in repackaged archives</li></ul><h2 id="features"><a class="markdownIt-Anchor" href="#features"></a> Features</h2><ul><li>by default you need to select Build -&gt; Make project</li><li>there is an advanced setting you can change to make this more seamless</li></ul><h2 id="template-caching"><a class="markdownIt-Anchor" href="#template-caching"></a> Template caching</h2><ul><li>by default templates are cached for performance</li><li>but caching will require a container restart to refresh the cache</li><li>developer tools will disable template caching so the restart is not required to see changes</li></ul><h2 id="livereload"><a class="markdownIt-Anchor" href="#livereload"></a> LiveReload</h2><ul><li>LiveReload is a technology to automatically trigger a browser refresh when resources are changed</li><li>Spring boot developer tools includes a LiveReload server</li></ul><h1 id="entity-relationships"><a class="markdownIt-Anchor" href="#entity-relationships"></a> Entity Relationships</h1><ul><li>One to One<ul><li>One entity is related to one other entity</li></ul></li><li>One to Many<ul><li>One entity is related to many entities (List, Set, Map, SortedSet, SortedMap)</li></ul></li><li>Many to One<ul><li>the inverse relationship of one to many</li></ul></li><li>Many to Many<ul><li>many entities are related to many entities</li><li>each has a list or set reference to the other</li><li>a join table us used to define the relationships (mapping table)</li></ul></li></ul><h2 id="unidirectional-vs-bidirectional"><a class="markdownIt-Anchor" href="#unidirectional-vs-bidirectional"></a> Unidirectional vs Bidirectional</h2><ul><li>Unidirectional is one way<ul><li>mapping is only done one way, one side of the relationship will not know about the other</li></ul></li><li>Bidirectional is two way<ul><li>both sides know about each other</li><li>generally recommended to use bidirectional, since you can navigate the object graph in either direction</li></ul></li></ul><h2 id="owning-side"><a class="markdownIt-Anchor" href="#owning-side"></a> Owning side</h2><ul><li>the Owning side in the relationship will hold the foreign key in the database</li><li>one to one is the side where the foreign key is specified</li><li>one to many and many to one is the <code>Many</code> side</li><li>mappedBy is used to define the field with <code>owns</code> the reference of the relationship</li></ul><h2 id="fetch-type"><a class="markdownIt-Anchor" href="#fetch-type"></a> Fetch type</h2><ul><li>Lazy - data is not required until referenced</li><li>Eager - data is queried up front</li></ul><h2 id="jpa-cascade-types"><a class="markdownIt-Anchor" href="#jpa-cascade-types"></a> JPA Cascade Types</h2><ul><li>JPA Cascade types control how state changes are cascaded from parent objects to child objects</li><li>Types<ul><li>PERSIS</li><li>MERGE</li><li>REFRESH</li><li>REMOVE</li><li>DETACH</li><li>ALL</li></ul></li><li>by default, no operations are cascaded</li></ul><h2 id="embeddable-types"><a class="markdownIt-Anchor" href="#embeddable-types"></a> Embeddable Types</h2><ul><li>JPA / Hibernate support embeddable types</li><li>these are used to define a common set of properties</li><li>for example, an order might have a billing address and a shipping address</li><li>an embeddable type could be used for the address properties for reuse</li></ul><h2 id="inheritance"><a class="markdownIt-Anchor" href="#inheritance"></a> Inheritance</h2><ul><li>mappedSuperclass - entities inherit from a super class, a database table IS NOT created for the super class</li><li>Single Table - default - one table is used for all subclasses</li><li>Joined Table - base class and subclasses have their own tables, fetching subclass entities require a join to the parent table (subclasses will not have the common properties from the parent class)</li><li>Table Per Class - each subclass has its own table (no table for parent class. common properties will exist in all subclasses)</li></ul><h2 id="create-and-update-timestamps"><a class="markdownIt-Anchor" href="#create-and-update-timestamps"></a> Create and Update Timestamps</h2><ul><li>often a best practice to use create and update timestamps on your entities for audit purposes</li><li>JPA supports <code>@PrePersist</code> and <code>@PreUpdate</code> which can be used for support audit timestamps via JPA lifecycle callbacks</li><li>hibernate provides <code>@CreationTimestamp</code> and <code>@UpdateTimestamp</code></li></ul><h1 id="hibernate-ddl-auto"><a class="markdownIt-Anchor" href="#hibernate-ddl-auto"></a> Hibernate DDL Auto</h1><ul><li>DDL - Data Definition Language</li><li>DML - Data Manipulation Language</li><li>Hibernate property is set by the Spring property</li><li>options are<ul><li>none</li><li>validate - check if there is any table or columns that are missing</li><li>update - find missing table and columns and update the existing DB, not good for PROD environment</li><li>create - create the DB</li><li>create-drop - create the DB and drop the DB when application instance stops running.</li></ul></li><li>Spring boot will use create-drop for embedded databases (HSQL, H2, Derby) or none</li></ul><h2 id="ddl-vs-dml"><a class="markdownIt-Anchor" href="#ddl-vs-dml"></a> DDL vs DML</h2><ul><li>DDL is used to define database structures such as tables and indexes, while DML is used with data operations such as inserts and updates</li></ul><h2 id="initialize-with-hibernate"><a class="markdownIt-Anchor" href="#initialize-with-hibernate"></a> Initialize with Hibernate</h2><ul><li>data can be loaded from <code>import.sql</code>, this is a file that can be created in the root path</li><li>Hibernate feature, not Spring specific</li><li>must be on root of class path</li><li>only executed if Hinernate’s DDL auto property is set to <code>create</code> or <code>create-drop</code></li></ul><h2 id="spring-jdbc"><a class="markdownIt-Anchor" href="#spring-jdbc"></a> Spring JDBC</h2><ul><li>Spring’s datasource initializer via Spring Boot will by default load <code>schema.sql</code> and <code>data.sql</code> from the root of the class path</li><li>Spring Boot will also load from <code>schema-$&#123;platform&#125;.sql</code> and <code>data-$&#123;platform&#125;.sql</code></li><li>must set <code>spring.datasource.platform</code> property</li><li>may conflict with Hinernate DDL auto property<ul><li>if using Spring datasource initializer, should set DDL auto property to <code>none</code> or <code>validate</code></li></ul></li></ul><h2 id="spring-data-jpa-query-method"><a class="markdownIt-Anchor" href="#spring-data-jpa-query-method"></a> Spring Data JPA Query Method</h2><ul><li>define method in interface</li><li>method name rules: findBy + <code>&lt;PROPERTY_NAME&gt;</code></li><li>it will create the query based on method name and query the database to find the data and return to us.</li><li>no manual implementation needed.</li></ul><h2 id="repository-layer-and-service-layer"><a class="markdownIt-Anchor" href="#repository-layer-and-service-layer"></a> Repository layer and Service layer</h2><ul><li>All your business logic should be in the Service Layer.</li><li>Any access to the Database (any storage) should go to the Repository Layer.</li><li>Lets take an Example. You have to save an entity(Person). But before saving the Person you want to make sure that the Person’s FirstName does not exist already.<br />So the validation part should go to the business layer.<br />In the service Layer</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PersonRepository repository; </span><br><span class="line">public Person save(Person p)&#123;</span><br><span class="line">   Person p &#x3D; findByName(p.getName();</span><br><span class="line">   if (p !&#x3D; null)&#123;</span><br><span class="line">          return some customException();</span><br><span class="line">   &#125;</span><br><span class="line">   return repository.save(p); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public Person findByName(String name)&#123;</span><br><span class="line">     return repository.findByName(name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>And in your Repository Layer just concentrate on DB Operation.</p></li><li><p><code>View&lt;--&gt;Controller( create instance of Model class)&lt;--&gt;Service&lt;---&gt;Repository</code></p></li></ul><h1 id="lombok"><a class="markdownIt-Anchor" href="#lombok"></a> Lombok</h1><ul><li>hooks in via the annotation processor API</li><li>the raw source code is passed to Lombok for code generation before the Java complier continues</li><li>thus, produces properly compiled Java code in conjunction with the Java compiler</li><li>under the classes you can view the compiled class files</li><li>IntelliJ will decompile to show you the source code</li></ul><h2 id="ides"><a class="markdownIt-Anchor" href="#ides"></a> IDEs</h2><ul><li>since compiled code is change, and source files are not, IDE’s can get confused by this</li><li>more of an issue for IDEs several years old</li><li>modern IDEs such as IntelliJ, Eclipse support Project Lombok</li><li>Plug in Installation may be necessary</li><li>IntelliJ<ul><li>verify you have enabled annotation processing under compiler settings</li></ul></li></ul><h2 id="features-2"><a class="markdownIt-Anchor" href="#features-2"></a> features</h2><ul><li><code>val</code> - local variables declared final</li><li><code>var</code> - mutable local variables</li><li><code>@NonNull</code> - Null check, will throw NPE if parameter is null</li><li><code>@Cleanup</code> - will call close() on resource/connection in finally block</li><li><code>@Getter</code> - creates getter methods for all properties</li><li><code>@Setter</code> - creates setter for all non-final properties</li><li><code>@ToString</code><ul><li>generates String of classname, and each field separated by commas</li><li>optional parameter to include field names</li><li>optional parameter to include call to the super toString method</li></ul></li><li><code>@EqualsAndHashCode</code><ul><li>generates implementations of <code>equals(Object other)</code> and <code>hashCode()</code></li><li>by default will use all non-static, non-transient properties</li><li>can optionally exclude specific properties</li></ul></li><li><code>@NoArgsConstructor</code><ul><li>generates no args constructor</li><li>will cause compiler error if there are final fields</li><li>can optionally force, which will initialize final fields with 0 / false / null</li></ul></li><li><code>@RequiredArgsConstructor</code><ul><li>generates a constructor for all fields that are final or marked @NonNull</li><li>constructor will throw a NullPointerException if any <code>@NonNull</code> fields are null</li></ul></li><li><code>@AllArgsConstructor</code><ul><li>generates a constructor for all properties of class</li><li>any @NotNull properties will have null check</li></ul></li><li><code>@Data</code><ul><li>generates typical boilerplate code for POJOs</li><li>combines - <code>@Getter</code>, <code>@Setter</code>, <code>@ToString</code>, <code>@EqualsAndHashCode</code>, <code>@RequiredArgsConstructor</code></li><li>no constructor is generated if constructors have been explicitly declared</li></ul></li><li><code>@Value</code><ul><li>the immutable variant of <code>@Data</code></li><li>all fields are made private and final by default</li></ul></li><li><code>@NonNull</code><ul><li>set on parameter of method or constructor and a NullPointerException will be thrown if parameter is null</li></ul></li><li><code>@Builder</code><ul><li>implements the builder pattern for object creation</li></ul></li><li><code>@SneakyThrows</code><ul><li>throw checked exceptions without declaring in calling method’s throws clause</li></ul></li><li><code>@Synchronized</code><ul><li>a safer implementation of Java’s synchronized</li></ul></li><li><code>@Getter(lazy = true)</code><ul><li>for expensive getters</li><li>will calculate value first time and cache</li><li>additional gets will read from cache</li></ul></li><li><code>@Log</code><ul><li>creates a Java util logger</li></ul></li><li><code>@Slf4j</code><ul><li>creates a SLF4J logger</li><li>recommended - SLF4J is a generic logging facade</li><li>spring boot’s default logger is LogBack</li></ul></li></ul><h1 id="testing-terminology"><a class="markdownIt-Anchor" href="#testing-terminology"></a> Testing Terminology</h1><ul><li>Code under test<ul><li>this is the code you are testing</li></ul></li><li>Test fixture<ul><li>a test fixture is a fixed state of a set of objects used as a baseline for running tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable</li><li>includes: input data, mock objects, loading database with known data etc…</li></ul></li></ul><h2 id="unit-tests"><a class="markdownIt-Anchor" href="#unit-tests"></a> Unit Tests</h2><ul><li>code written to test code under test</li><li>designed to test specific sections of code</li><li>percentage of lines of code tested is code coverage<ul><li>ideal coverage is in the 70-80% range</li></ul></li><li>should be unity and execute very fast</li><li>should have no external dependencies<ul><li>no databases, no Spring Context etc…</li></ul></li></ul><h2 id="integration-tests"><a class="markdownIt-Anchor" href="#integration-tests"></a> Integration tests</h2><ul><li>designed to test behaviors between objects and parts of the overall system</li><li>much larger scope</li><li>can include Spring Context, database, and message brokers</li><li>will run much slower than unit tests</li></ul><h2 id="functional-tests"><a class="markdownIt-Anchor" href="#functional-tests"></a> Functional tests</h2><ul><li>typically means you are testing the running application</li><li>application is live, likely deployed in a known environment</li><li>functional touch points are tested</li><li>i.e. using a web driver, calling web services, sending / receiving messages etc…(Selenium)</li></ul><h2 id="tdd"><a class="markdownIt-Anchor" href="#tdd"></a> TDD</h2><ul><li>test driven development</li><li>write tests first, which will fail, then code to fix the tests</li></ul><h2 id="bdd"><a class="markdownIt-Anchor" href="#bdd"></a> BDD</h2><ul><li>behavior driven development</li><li>builds on TDD and specifies that tests of any unit of software should be specified in terms of desired behavior of the unit<ul><li>often implemented with DSLs to create natural language tests</li><li>JBehave, Cucumber, Spock</li></ul></li></ul><h2 id="mock"><a class="markdownIt-Anchor" href="#mock"></a> Mock</h2><ul><li>a fake implementation of a class used for testing, like a test double</li></ul><h2 id="spy"><a class="markdownIt-Anchor" href="#spy"></a> Spy</h2><ul><li>a partial mock, allowing you to override select methods of a real class</li></ul><h2 id="testing-goals"><a class="markdownIt-Anchor" href="#testing-goals"></a> Testing goals</h2><ul><li>generally, you will want the majority of your tests to be unit tests</li><li>bringing up the Spring Context makes your tests exponentially slower</li><li>try to test specific business logic in unit tests</li><li>use Integration tests to test interactions</li><li>think of a pyramid, base is unit tests, middle is integration tests, top is functional tests</li></ul><h2 id="test-scope-dependencies"><a class="markdownIt-Anchor" href="#test-scope-dependencies"></a> Test scope dependencies</h2><ul><li>using spring-boot-starter-test<ul><li>JUnit - the default standard for unit testing Java applications</li><li>Spring test and Spring boot Test - utilities and integration test support for Spring Boot applications</li><li>AssertJ - a fluent assertion library</li><li>Hamcrest - a library of matcher objects</li><li>Mockito - a Java mocking framework</li><li>JSONassert - an assertion library for JSON</li><li>JSONPath - XPath for JSON</li></ul></li></ul><h2 id="junit-annotations"><a class="markdownIt-Anchor" href="#junit-annotations"></a> JUnit Annotations</h2><table><thead><tr><th>Annotation</th><th>Description</th></tr></thead><tbody><tr><td>@Test</td><td>identifies a method as a test method</td></tr><tr><td>@Before</td><td>executed before each test, it is used to prepare the test environment (e.g. read input data, initialize the class)</td></tr><tr><td>@After</td><td>executed after each test, it is used to cleanup the test environment, it can also save memory by cleaning up expensive memory structures</td></tr><tr><td>@BeforeClass</td><td>executed once, before the start of all tests, methods marked with this annotation need to be defined as static to work with JUnit</td></tr><tr><td>@AfterClass</td><td>executed once, after all tests have been finished, methods annotated with this annotation need to be defined as static to work with JUnit</td></tr><tr><td>@Ignore</td><td>marks that the test should be ignored</td></tr><tr><td>@Test(expected = Exception.class)</td><td>fails if the method does not throw the named exception</td></tr><tr><td>@Test(timeout = 10)</td><td>fails if the method takes longer than 100 milliseconds</td></tr></tbody></table><h2 id="spring-boot-annotations"><a class="markdownIt-Anchor" href="#spring-boot-annotations"></a> Spring Boot Annotations</h2><table><thead><tr><th>Annotation</th><th>Description</th></tr></thead><tbody><tr><td>@RunWith(SpringRunner.class)</td><td>run test with Spring Context</td></tr><tr><td>@SpringBootTest</td><td>search for Spring boot application for configuration</td></tr><tr><td>@TestConfiguration</td><td>specify a Spring configuration for you test</td></tr><tr><td>@MockBean</td><td>injects Mockito mock</td></tr><tr><td>@SpyBean</td><td>injects Mockito Spy</td></tr><tr><td>@JsonTest</td><td>creates a Jackson or Gson object mapper via Spring boot</td></tr><tr><td>@WebMvcTest</td><td>used to test web context without a full http server</td></tr><tr><td>@DataJpaTest</td><td>used to test data layer with embedded database</td></tr></tbody></table><p>…</p><h2 id="argumentcaptor"><a class="markdownIt-Anchor" href="#argumentcaptor"></a> ArgumentCaptor</h2><ul><li>ArgumentCaptor allows us to capture an argument passed to a method in order to inspect it. This is especially useful when we can’t access the argument outside of the method we’d like to test.</li><li>For example, consider an EmailService class with a send method that we’d like to test:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class EmailService &#123;</span><br><span class="line"></span><br><span class="line">    private DeliveryPlatform platform;</span><br><span class="line"></span><br><span class="line">    public EmailService(DeliveryPlatform platform) &#123;</span><br><span class="line">        this.platform &#x3D; platform;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void send(String to, String subject, String body, boolean html) &#123;</span><br><span class="line">        Format format &#x3D; Format.TEXT_ONLY;</span><br><span class="line">        if (html) &#123;</span><br><span class="line">            format &#x3D; Format.HTML;</span><br><span class="line">        &#125;</span><br><span class="line">        Email email &#x3D; new Email(to, subject, body);</span><br><span class="line">        email.setFormat(format);</span><br><span class="line">        platform.deliver(email);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>In EmailService.send, notice how platform.deliver takes a new Email as an argument. As part of our test, we’d like to check that the format field of the new Email is set to Format.HTML. In order to do this, we need to capture and inspect the argument that is passed to platform.deliver.</li></ul><h3 id="set-up-the-unit-test"><a class="markdownIt-Anchor" href="#set-up-the-unit-test"></a> Set up the unit test</h3><ul><li>First, let’s create our unit test class:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">RunWith(MockitoJUnitRunner.class)</span><br><span class="line">public class EmailServiceUnitTest &#123;</span><br><span class="line"></span><br><span class="line">    @Mock</span><br><span class="line">    DeliveryPlatform platform;</span><br><span class="line"></span><br><span class="line">    @InjectMocks</span><br><span class="line">    EmailService emailService;</span><br><span class="line">  </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>We’re using the @Mock annotation to mock DeliveryPlatform, which is automatically injected into our EmailService with the @InjectMocks annotation.</li></ul><h3 id="add-an-argumentcaptor-field"><a class="markdownIt-Anchor" href="#add-an-argumentcaptor-field"></a> Add an ArgumentCaptor field</h3><ul><li>Second, let’s add a new ArgumentCaptor field of type Email to store our captured argument:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">@Captor</span><br><span class="line">ArgumentCaptor&lt;Email&gt; emailCaptor;</span><br></pre></td></tr></table></figure><h3 id="capture-the-argument"><a class="markdownIt-Anchor" href="#capture-the-argument"></a> Capture the argument</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mockito.verify(platform).deliver(emailCaptor.capture());</span><br></pre></td></tr></table></figure><ul><li>We can then get the captured value and store it as a new Email object:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Email emailCaptorValue &#x3D; emailCaptor.getValue();</span><br></pre></td></tr></table></figure><h3 id="inspect-the-captured-value"><a class="markdownIt-Anchor" href="#inspect-the-captured-value"></a> Inspect the captured value</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void whenDoesSupportHtml_expectHTMLEmailFormat() &#123;</span><br><span class="line">    String to &#x3D; &quot;info@baeldung.com&quot;;</span><br><span class="line">    String subject &#x3D; &quot;Using ArgumentCaptor&quot;;</span><br><span class="line">    String body &#x3D; &quot;Hey, let&#39;use ArgumentCaptor&quot;;</span><br><span class="line"></span><br><span class="line">    emailService.send(to, subject, body, true);</span><br><span class="line"></span><br><span class="line">    Mockito.verify(platform).deliver(emailCaptor.capture());</span><br><span class="line">    Email value &#x3D; emailCaptor.getValue();</span><br><span class="line">    assertEquals(Format.HTML, value.getFormat());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="exception-handling-in-spring-mvc"><a class="markdownIt-Anchor" href="#exception-handling-in-spring-mvc"></a> Exception Handling in Spring MVC</h1><h2 id="http-status-code"><a class="markdownIt-Anchor" href="#http-status-code"></a> HTTP status code</h2><ul><li>HTTP 5XX server error<ul><li>HTTP 500 - internal server error</li><li>generally, any unhandled exception</li><li>other 500 errors are generally not used with Spring MVC</li></ul></li><li>HTTP 4XX client errors - generally checked exceptions<ul><li>400 bad request - cannot process due to client error</li><li>401 unauthorized - authentication required</li><li>404 not found - resource not found</li><li>405 method not allowed, HTTP method not allowed</li></ul></li></ul><h2 id="responsestatus"><a class="markdownIt-Anchor" href="#responsestatus"></a> @ResponseStatus</h2><ul><li>allows you to annotate custom exception classes to indicate to the framework the HTTP status you want returned when that exception is thrown</li><li>global to the application</li></ul><h2 id="exceptionhandler"><a class="markdownIt-Anchor" href="#exceptionhandler"></a> @ExceptionHandler</h2><ul><li>works at the controller level</li><li>allows you to define custom exception handling</li><li>can be used with <code>@ResponseStatus</code> for just printing a http status</li><li>can be used to return a specific view</li><li>also can take total control and work with the Model and View</li><li>Model cannot be a parameter of an ExceptionHandler method</li></ul><h2 id="handlerexceptionresolver"><a class="markdownIt-Anchor" href="#handlerexceptionresolver"></a> @HandlerExceptionResolver</h2><ul><li>is an interface you can implement for custom exception handling</li><li>used internally by Spring MVC</li><li>note Model is not passed</li></ul><h1 id="docker"><a class="markdownIt-Anchor" href="#docker"></a> Docker</h1><h2 id="what-is-docker"><a class="markdownIt-Anchor" href="#what-is-docker"></a> What is Docker?</h2><ul><li>Docker is a standard for Linux containers</li><li>a Container is an isolated runtime inside of Linux</li><li>a Container provides a private machine like space under Linux</li><li>Containers will run under any modern Linux Kernel</li></ul><h2 id="containers-can"><a class="markdownIt-Anchor" href="#containers-can"></a> Containers can</h2><ul><li>Have their own process space</li><li>their own network interface</li><li>run processes as Root (inside the container)</li><li>have their own disk space<ul><li>can share with host too</li></ul></li><li>Container is not a VM</li></ul><h2 id="docker-terminology"><a class="markdownIt-Anchor" href="#docker-terminology"></a> Docker Terminology</h2><ul><li>Docker Image - the representation of a Docker container, kind of like a JAR or WAR file in Java</li><li>Docker Container - the standard runtime of Docker, effectively a deployed and running Docker image, like a Spring Boot Executable JAR</li><li>Docker Engine - the code which manages Docker stuff, creates and runs Docker containers</li></ul><h2 id="notes-about-docker-images-and-containers"><a class="markdownIt-Anchor" href="#notes-about-docker-images-and-containers"></a> Notes about Docker Images and Containers</h2><ul><li>Containers are like snapshots of Docker images</li><li>Docker images are built by Docker files which contains multiple layers, each layer is a Command and will generate a file for the image. The layers will be re-created everytime when we run a new container for an image.</li><li>e.g. when you <code>docker run -d mongo</code>, a new container for the image <code>mongo</code> will be created, and there is a layer for this container where you can store data in it. But when you <code>docker stop &lt;Container_ID&gt;</code> and re-start using <code>docker run -d mongo</code>, a new container with different Container ID will be created, so your old data in the previous container will not show in your new container. You can map a storage path to the container, so the current container and all future containers will save data into the directory you setup. Which will make the data persistence.</li></ul><h2 id="docker-housekeeping"><a class="markdownIt-Anchor" href="#docker-housekeeping"></a> Docker housekeeping</h2><h3 id="cleaning-up-after-docker"><a class="markdownIt-Anchor" href="#cleaning-up-after-docker"></a> Cleaning up after Docker</h3><ul><li>with development use docker can leave behind a lot of files</li><li>these files will grow and consume a lot of disk space</li><li>this is less of an issue on production systems where containers aren’t being built and restarted all the time</li><li>there are 3 key areas of house keeping<ul><li>containers</li><li>images</li><li>volumes</li></ul></li></ul><h1 id="mysql"><a class="markdownIt-Anchor" href="#mysql"></a> MySQL</h1><h2 id="mysql-features"><a class="markdownIt-Anchor" href="#mysql-features"></a> MySQL features</h2><ul><li>Stored procedures<ul><li>a piece of code inside of the database that execute against the database, runs locally on the database</li></ul></li><li>Triggers<ul><li>when something happens in the database e.g. insert a record, the trigger will run before or after that transaction</li></ul></li><li>Cursors<ul><li>point a place in a large set of data so you can scroll through it and look into the next record or previous record.</li></ul></li><li>Updated views<ul><li>a virtual table, stored inside the database</li></ul></li><li>Query cache<ul><li>database is going to remember in memory the result of your query, when you ask for that data again, it doesn’t have to go back to the file system to get the data.</li></ul></li><li>Subselects<ul><li>nested queries.</li></ul></li></ul><h2 id="acid-compliance"><a class="markdownIt-Anchor" href="#acid-compliance"></a> ACID compliance</h2><ul><li>atomicity - all or nothing</li><li>consistency - transactions are valid to rules of the DB</li><li>isolation - results of transactions are as if they are done end to end</li><li>durability - once a transaction is committed, it remains so (DB will not losing data)</li></ul><h2 id="rdbms-deployment-architectures"><a class="markdownIt-Anchor" href="#rdbms-deployment-architectures"></a> RDBMS deployment architectures</h2><ul><li>typically is driven by needs of scalability and availability</li><li>can be done on a single non-dedicated server or many dedicated servers</li><li>communication is typically over a network socket (MySQL: 3306)</li><li>the client will need software called a <code>driver</code> to talk to the database over the network socket.</li></ul><h2 id="mysql-data-types"><a class="markdownIt-Anchor" href="#mysql-data-types"></a> MySQL data types</h2><ul><li>a data type defines the data type of a column</li><li>MySQL does support the standard ANSI SQL data types</li><li>data types are broken down into the following categories<ul><li>numeric</li><li>date and time</li><li>string</li><li>spatial (location, places)</li><li>JSON</li></ul></li></ul><h2 id="mysql-installation-options"><a class="markdownIt-Anchor" href="#mysql-installation-options"></a> MySQL installation options</h2><h3 id="native-installation"><a class="markdownIt-Anchor" href="#native-installation"></a> Native installation</h3><ul><li>meaning install on your opearting system</li></ul><h3 id="running-mysql-in-a-container"><a class="markdownIt-Anchor" href="#running-mysql-in-a-container"></a> Running MySQL in a Container</h3><ul><li>MySQL can also be run inside a technology called containers</li><li>Docker is a highly popular container technology</li><li>through Docker, you can run MySQL locally using pre built image from Docker hub</li></ul><h2 id="connecting-to-mysql"><a class="markdownIt-Anchor" href="#connecting-to-mysql"></a> Connecting to MySQL</h2><ul><li>Local connection<ul><li>connecting to MySQL from the command line on the machine running MySQL</li><li>to login to docker: <code>docker exec -it yuan-mysql bash</code></li><li>to connect to mysql server in Docker: <code>mysql --user=root -p</code></li></ul></li><li>remote / client connection<ul><li>using some type of client software on the same machine running MySQL</li><li>or connecting to the MySQL server from a different machine over the network</li></ul></li></ul><h1 id="mongo-db"><a class="markdownIt-Anchor" href="#mongo-db"></a> Mongo DB</h1><h2 id="about-mongo-db"><a class="markdownIt-Anchor" href="#about-mongo-db"></a> About Mongo DB</h2><ul><li>Mongo DB is a document oriented database</li><li>developed in C++</li><li>MongoDB is a NoSQL database</li><li>MongoDB documents are stored in BSON<ul><li>binary JSON</li></ul></li></ul><h2 id="why-use-mongo-db"><a class="markdownIt-Anchor" href="#why-use-mongo-db"></a> Why use Mongo DB?</h2><ul><li>MongoDB is great for high insert systems<ul><li>such as sensor readings, social media systems, advertising systems</li></ul></li><li>good when you need schema flexibility</li><li>can also support a high number of reads per second</li></ul><h2 id="why-avoid-mongodb"><a class="markdownIt-Anchor" href="#why-avoid-mongodb"></a> Why avoid MongoDB?</h2><ul><li>MongoDB has no concept of transactions<ul><li>No ACID</li><li>no locking for transactional support, hence faster inserts</li></ul></li><li>not good for concurrent updates</li></ul><h1 id="reactive-manifesto"><a class="markdownIt-Anchor" href="#reactive-manifesto"></a> Reactive Manifesto</h1><h2 id="responsive"><a class="markdownIt-Anchor" href="#responsive"></a> Responsive</h2><ul><li>the system responds in a timely manner</li><li>responsiveness is the cornerstone of useability and utility</li><li>responsiveness also means problems may be detected quickly and dealt with effectively</li><li>responsive systems provide rapid and consistent response times</li><li>consistent behavior simplifies error handling, builds end user confidence, and encourages further interaction</li></ul><h2 id="resilient"><a class="markdownIt-Anchor" href="#resilient"></a> Resilient</h2><ul><li>system stays responsive in the face of failure</li><li>resilience is achieved by replication, containment, isolation and delegation</li><li>failures are contained within each component</li><li>parts of the system can fail, without compromising the system as a whole</li><li>recovery of each component is delegated to another</li><li>high availability is ensured by replication where necessary</li></ul><h2 id="elastic"><a class="markdownIt-Anchor" href="#elastic"></a> Elastic</h2><ul><li>the system stays responsive under varying workload</li><li>reactive systems can react to changes in the input rate by increasing or decreasing resources allocated to service inputs</li><li>reactive systems achieve elasticity ina cost effective way on commodity hardware and software platforms</li></ul><h2 id="message-driven"><a class="markdownIt-Anchor" href="#message-driven"></a> Message Driven</h2><ul><li>reactive systems rely on asynchronous message passing to establish a boundary between components<ul><li>this ensures loose coupling, isolation, and location transparency</li></ul></li><li>message passing enables load management, elasticity, and flow control</li><li>location transparent messaging makes management of failure possible</li><li>non blocking communication allows recipients to only consume resources while active, leading to less system overhead.</li></ul><h2 id="reactive-programming-with-reactive-systems"><a class="markdownIt-Anchor" href="#reactive-programming-with-reactive-systems"></a> reactive programming with reactive systems</h2><ul><li>reactive programming is a useful implementation technique</li><li>reactive programming focuses on non-blocking, asynchronous execution - a key characteristic of reactive systems</li><li>reactive programming is just one tool in building reactive systems</li></ul><h1 id="reactive-programming"><a class="markdownIt-Anchor" href="#reactive-programming"></a> Reactive Programming</h1><ul><li>reactive programming is an asynchronous programming paradigm focused on streams of data</li><li>reactive programs also maintain a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself. Interactive programs work at their own pace and mostly deal with communication, while reactive programs only work in response to external demands and mostly deal with accurate interrupt handling, real time programs are usually reactive.</li></ul><h2 id="common-use-cases"><a class="markdownIt-Anchor" href="#common-use-cases"></a> Common use cases</h2><ul><li>external service calls</li><li>highly concurrent message consumers</li><li>spreadsheets</li><li>abstraction over asynchronous processing<ul><li>abstract whether or not your program is synchronous or asynchronous</li></ul></li></ul><h2 id="features-opf-reactive-programming"><a class="markdownIt-Anchor" href="#features-opf-reactive-programming"></a> features opf reactive programming</h2><ul><li>data streams</li><li>asynchronous</li><li>non-blocking</li><li>backpressure</li><li>failures as messages</li></ul><h2 id="data-streams"><a class="markdownIt-Anchor" href="#data-streams"></a> data streams</h2><ul><li>data streams can be just about anything</li><li>mouse clicks, or other user interactions</li><li>JMS messages, RESTful service calls, Twitter feed, Stock trades, list of data from a database</li><li>a stream is a sequence of events ordered in time</li><li>events you want to listen to</li></ul><h2 id="asynchronous"><a class="markdownIt-Anchor" href="#asynchronous"></a> Asynchronous</h2><ul><li>events are captured asynchronously</li><li>a function is defined to execute when an event is emitted</li><li>another function is defined if an error is emitted</li><li>another function is defined when complete is emitted</li></ul><h2 id="observer-pattern"><a class="markdownIt-Anchor" href="#observer-pattern"></a> Observer pattern</h2><ul><li>you have a subject and an observer</li><li>when the subject is going to change, it will notify the observer</li></ul><h2 id="non-blocking"><a class="markdownIt-Anchor" href="#non-blocking"></a> Non-blocking</h2><ul><li>the concept of using non blocking is important</li><li>in blocking, the code will stop and wait for more dta (reading from disk, network etc…)</li><li>non blocking in contrast, will process available data, ask to be notified when more is available, then continue</li></ul><h2 id="back-pressure"><a class="markdownIt-Anchor" href="#back-pressure"></a> back pressure</h2><ul><li>the ability of the subscriber to throttle data</li></ul><h2 id="failures-as-messages"><a class="markdownIt-Anchor" href="#failures-as-messages"></a> failures as messages</h2><ul><li>exceptions are not thrown in a traditional sense<ul><li>would break processing of stream</li></ul></li><li>exceptions are processed by a handler function</li></ul><h1 id="reactive-streams"><a class="markdownIt-Anchor" href="#reactive-streams"></a> Reactive Streams</h1><h2 id="spring-reactive-types"><a class="markdownIt-Anchor" href="#spring-reactive-types"></a> Spring Reactive Types</h2><ul><li>two new reactive types are introduced with Spring framework 5</li><li><code>Mono</code> is a publisher with zero or one elements in data stream</li><li><code>Flux</code> is a publisher with zero or MANY elements in the data stream</li><li>both types implement the reactive streams publisher interface</li></ul><h1 id="webflux"><a class="markdownIt-Anchor" href="#webflux"></a> WebFlux</h1><table><thead><tr><th>web MVC</th><th>webFlux</th></tr></thead><tbody><tr><td>@Controller, @RequestMapping</td><td>Router functions</td></tr><tr><td>spring-webmvc</td><td>spring-webflux</td></tr><tr><td>Servlet API</td><td>HTTP / Reactive Streams</td></tr><tr><td>Servlet Container</td><td>Tomcat, Jetty, Netty, Undertow</td></tr></tbody></table><h1 id="restful-web-services"><a class="markdownIt-Anchor" href="#restful-web-services"></a> RESTful web services</h1><ul><li>because of their simplicity and versatility, RESTful web services have become the de facto standard for web services</li><li>REST - representational state transfer<ul><li>representational - typically JSON or XML</li><li>state transfer - typically via HTTP</li></ul></li></ul><h2 id="terminology"><a class="markdownIt-Anchor" href="#terminology"></a> terminology</h2><ul><li>verbs - HTTP methods: GET, PUT, POST, DELETE</li><li>messages - the payload of the action(JSON / XML)</li><li>URI - uniform resource identifier<ul><li>a unique string identifying a resource</li></ul></li><li>URL - uniform resource locator<ul><li>a URI with network information</li></ul></li><li>Idempotence<ul><li>the property of certain operations in mathematics and computer science that they can be applied multiple times without changing the result beyond the initial application</li><li>in other words, you can exercise the operation multiple times, without changing the result</li><li>example: refreshing a web page (HTTP GET operation)</li></ul></li><li>Stateless - service does not maintain any client state</li><li>HATEOAS - hypermedia as the engine of application state<ul><li>a REST client should then be able to use server-provided links dynamically to discover all the available actions and resources it needs, as access proceeds, the server responds with text that includes hyperlinks to other actions that are currently available</li></ul></li></ul><h2 id="get"><a class="markdownIt-Anchor" href="#get"></a> GET</h2><ul><li>use: to read data from resource</li><li>read only</li><li>idempotent</li><li>safe operation - does not change state of resource</li></ul><h2 id="put"><a class="markdownIt-Anchor" href="#put"></a> PUT</h2><ul><li>use: to insert or update</li><li>idempotent - multiple PUT will not change result</li><li>like saving a file multiple times</li><li>not safe operation - does change state of resource</li></ul><h2 id="post"><a class="markdownIt-Anchor" href="#post"></a> POST</h2><ul><li>use: to create new object</li><li>non-idempotent - multiple POSTs is expected to create multiple objects</li><li>not safe operation - does change state of resource</li><li>only non-idempotent, non-safe HTTP verb</li></ul><h2 id="delete"><a class="markdownIt-Anchor" href="#delete"></a> DELETE</h2><ul><li>use: to delete an object</li><li>idempotent - multiple DELETEs will have same effect / behavior</li><li>not safe operation, does change the state of resource</li></ul><h1 id="mapstruct"><a class="markdownIt-Anchor" href="#mapstruct"></a> MapStruct</h1><ul><li>MapStruct is a code generator for Java bean mapping<ul><li>helps reduce coding for type conversions</li><li>when dealing with Rest services, a common use case is to expose API data using DTOs (Data Transfer Object)</li><li>as project grows bigger, it is not good to expose POJO directly to Rest services. MapStruct is helping us to convert POJO to DTOs, then export DTOs to Rest service to be consumed by public</li></ul></li></ul><h1 id="content-negotiation"><a class="markdownIt-Anchor" href="#content-negotiation"></a> Content Negotiation</h1><h2 id="content-negotiating-view-resolver"><a class="markdownIt-Anchor" href="#content-negotiating-view-resolver"></a> Content Negotiating view resolver</h2><ul><li>used by Spring MVC to determine view handler to use</li><li>auto configured by Spring boot</li><li>the content negotiating view resolver will determine the view to use to render the data of the model to the client</li></ul><h2 id="content-type"><a class="markdownIt-Anchor" href="#content-type"></a> Content type</h2><ul><li>view to use is determined by Content Type in HTTP header<ul><li>application/json, application/xml, text/html</li></ul></li><li>if view for requested Content type is not found, HTTP status 406 not acceptable is returned.</li></ul><h1 id="helper-library-and-classes"><a class="markdownIt-Anchor" href="#helper-library-and-classes"></a> Helper Library and Classes</h1><h2 id="structure"><a class="markdownIt-Anchor" href="#structure"></a> Structure</h2><ul><li>the project should follow a structure from database to frontend: Database -&gt; Repository -&gt; Service -&gt; Controller -&gt; View</li></ul><h2 id="jpa"><a class="markdownIt-Anchor" href="#jpa"></a> JPA</h2><ul><li>H2 in memory database</li></ul><h2 id="thymeleaf-2"><a class="markdownIt-Anchor" href="#thymeleaf-2"></a> Thymeleaf</h2><ul><li>frontend template engine, model can be added to view dynamically</li></ul><h2 id="dependency-injection"><a class="markdownIt-Anchor" href="#dependency-injection"></a> Dependency Injection</h2><ul><li>Repositories and Services can be injected when needed, use Annotations @Service, @Component</li><li>classes marked as @Component, @Service and @Controller will be managed by Spring Application Context, it will inject necessary class to the right place when needed. (Beans)</li></ul><h2 id="configuration"><a class="markdownIt-Anchor" href="#configuration"></a> Configuration</h2><ul><li>config files can be Java, YAML, or XML</li><li>application.properties can define project profiles</li></ul><h2 id="jpa-entity-relationships"><a class="markdownIt-Anchor" href="#jpa-entity-relationships"></a> JPA entity relationships</h2><ul><li>One to Many</li><li>Many to One</li><li>Many to Many (needs mapping table)</li></ul><h2 id="jpa-query-methods"><a class="markdownIt-Anchor" href="#jpa-query-methods"></a> JPA Query methods</h2><ul><li>e.g. findByDescription</li><li>this is done by the library, findBy + property name</li><li>library will do the implementations for you</li></ul><h2 id="project-lombok"><a class="markdownIt-Anchor" href="#project-lombok"></a> Project Lombok</h2><ul><li>will do the constructors, getters, setters and even builder patterns for you</li></ul><h2 id="junit"><a class="markdownIt-Anchor" href="#junit"></a> JUnit</h2><ul><li>Unit Test framework</li></ul><h2 id="mockito"><a class="markdownIt-Anchor" href="#mockito"></a> Mockito</h2><ul><li>for unit tests, we do not want to bring in Spring Application Context</li><li>so to manage Repositories and Services, we need Mockito to create some Mock component for us</li><li>when(), thenReturn(), verify(), ArgumentCaptor, MockMvc…</li></ul><h2 id="webjars"><a class="markdownIt-Anchor" href="#webjars"></a> WebJars</h2><ul><li>bring in bootstrap library</li></ul><h2 id="web-data-binder"><a class="markdownIt-Anchor" href="#web-data-binder"></a> Web Data Binder</h2><ul><li>binds HTTP variables to Java object</li><li>specifically handling form posts and binding form variables to Java data objects</li><li>whereas Rest service we could use RestTemplate</li></ul><h2 id="validations"><a class="markdownIt-Anchor" href="#validations"></a> Validations</h2><ul><li>@NotBlank, @NotNull, @Max, @Min…</li><li>annotations to data models, for form validations</li></ul><h2 id="exceptions"><a class="markdownIt-Anchor" href="#exceptions"></a> Exceptions</h2><ul><li>@ResponseStatus</li><li>@ControllerAdvice</li><li>dealing with exceptions and custom error messages</li></ul><h2 id="reactive-programming-2"><a class="markdownIt-Anchor" href="#reactive-programming-2"></a> Reactive Programming</h2><ul><li>Mono, Flux</li><li>WebFlux</li><li>Reactive vs Servlet</li></ul><h2 id="resttemplate"><a class="markdownIt-Anchor" href="#resttemplate"></a> RestTemplate</h2><ul><li>bind Rest Response to Java objects</li></ul><h2 id="webclient"><a class="markdownIt-Anchor" href="#webclient"></a> WebClient</h2><ul><li>performing web request</li></ul><h2 id="mapstruct-2"><a class="markdownIt-Anchor" href="#mapstruct-2"></a> MapStruct</h2><ul><li>auto mapper</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spring-framework-stereotypes&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#spring-framework-stereotypes&quot;&gt;&lt;/a&gt; Spring Framework Stereotypes&lt;/h</summary>
      
    
    
    
    
    <category term="Spring" scheme="http://hellcy.github.io/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>git advanced</title>
    <link href="http://hellcy.github.io/2021/10/14/git-advanced/"/>
    <id>http://hellcy.github.io/2021/10/14/git-advanced/</id>
    <published>2021-10-14T12:04:12.000Z</published>
    <updated>2022-02-10T14:39:24.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="head-and-detached-head"><a class="markdownIt-Anchor" href="#head-and-detached-head"></a> HEAD and detached HEAD</h1><h2 id="head"><a class="markdownIt-Anchor" href="#head"></a> HEAD</h2><ul><li>when you checkout to a branch, git is pointing to the latest commit in that branch, which is the HEAD of that branch</li></ul><h2 id="detached-head"><a class="markdownIt-Anchor" href="#detached-head"></a> Detached HEAD</h2><ul><li>you can also checkout to a specific commit in any branch, when you do that, that commit becomes a detached HEAD, it doesn’t belong to any branch anymore.</li><li>if you made some changes in the detached HEAD and commit it, it is better to create a new branch (which contains all the changes you made), before switching back to other branches, then you could merge the new branch into the master branch. Otherwise if you don’t create a new branch, the changes you made in detached mode will be lost.</li></ul><h1 id="undo-changes"><a class="markdownIt-Anchor" href="#undo-changes"></a> Undo changes</h1><h2 id="git-restore"><a class="markdownIt-Anchor" href="#git-restore"></a> git restore</h2><ul><li>you could use <code>git restore &lt;file-name&gt;</code> or <code>git restore .</code> to revert all unstaged changes</li></ul><h2 id="git-clean"><a class="markdownIt-Anchor" href="#git-clean"></a> git clean</h2><ul><li>you could use <code>git clean -d</code> to delete new files (unstaged)</li></ul><h2 id="undo-staged-changes"><a class="markdownIt-Anchor" href="#undo-staged-changes"></a> Undo staged changes</h2><ul><li><code>git restore --staged &lt;file-name&gt;</code></li><li>this will copy the latest commited file to the staging area, which basically means it will revert all changes that currently are in the staging area now</li></ul><h1 id="deleting-commits"><a class="markdownIt-Anchor" href="#deleting-commits"></a> Deleting commits</h1><h2 id="soft"><a class="markdownIt-Anchor" href="#soft"></a> soft</h2><ul><li><code>git reset --soft HEAD~1</code></li><li>commit will be deleted, changes will still be in the staging area</li></ul><h2 id="mixed-default"><a class="markdownIt-Anchor" href="#mixed-default"></a> mixed (default)</h2><ul><li><code>git reset HEAD~1</code></li><li>commit will be deleted, changes will be removed from the staging area, but the changes will still be in the working directory</li></ul><h2 id="hard"><a class="markdownIt-Anchor" href="#hard"></a> hard</h2><ul><li><code>git reset --hard HEAD~1</code></li><li>commit will be deleted, all changes will be removed from the staging area, the all changes will be removed from the working directory too</li></ul><h1 id="stash"><a class="markdownIt-Anchor" href="#stash"></a> Stash</h1><h2 id="add-current-changes-to-stash-with-a-comment"><a class="markdownIt-Anchor" href="#add-current-changes-to-stash-with-a-comment"></a> add current changes to Stash with a comment</h2><ul><li><code>git stash push -m &quot;message&quot;</code></li></ul><h2 id="see-the-list-of-stashed-changes"><a class="markdownIt-Anchor" href="#see-the-list-of-stashed-changes"></a> see the list of stashed changes</h2><ul><li><code>git stash list</code></li></ul><h2 id="add-changes-back-to-unstaged-state-and-remove-from-stash"><a class="markdownIt-Anchor" href="#add-changes-back-to-unstaged-state-and-remove-from-stash"></a> add changes back to unstaged state and remove from Stash</h2><ul><li><code>git stash pop &lt;index&gt;</code></li></ul><h2 id="add-changes-back-to-unstaged-state-but-also-keep-it-at-stash"><a class="markdownIt-Anchor" href="#add-changes-back-to-unstaged-state-but-also-keep-it-at-stash"></a> add changes back to unstaged state but also keep it at Stash</h2><ul><li><code>git stash apply &lt;index&gt;</code></li></ul><h2 id="remove-a-particular-stash-from-the-list"><a class="markdownIt-Anchor" href="#remove-a-particular-stash-from-the-list"></a> remove a particular stash from the list</h2><ul><li><code>git stash drop &lt;index&gt;</code></li></ul><h2 id="clear-the-entire-stash-list"><a class="markdownIt-Anchor" href="#clear-the-entire-stash-list"></a> clear the entire stash list</h2><ul><li><code>git stash clear</code></li></ul><h1 id="reflog"><a class="markdownIt-Anchor" href="#reflog"></a> Reflog</h1><ul><li>allows us to bring back lost information, it could be commits or branches</li><li>use <code>git reflog</code> you will see a list of commits for the last 30 days, even the commits you deleted, which can’t be found in <code>git log</code>. Then you could use <code>git reset --hard &lt;commit hash code&gt;</code> to retrieve lost commits</li></ul><h1 id="merge"><a class="markdownIt-Anchor" href="#merge"></a> Merge</h1><h2 id="fast-forward"><a class="markdownIt-Anchor" href="#fast-forward"></a> fast forward</h2><p><img src="/../images/git-advanced/merge-fast-forward.png" alt="" /></p><ul><li>can only be used when no additional commit in master (after feature branch was created)</li><li>MERGE moves HEAD forward to the latest commit but does not create new commit</li></ul><h2 id="fast-forward-with-squash"><a class="markdownIt-Anchor" href="#fast-forward-with-squash"></a> fast forward with squash</h2><ul><li><code>git merge --squash &lt;branch name&gt;</code></li><li>all commits you made in the feature branch will be combined into one new commit when you MERGE it to the master branch</li><li>more specifically, when you use <code>--squash</code>, all the changes you made across all commits will be move to the staging area in the master branch.</li></ul><h2 id="recursive"><a class="markdownIt-Anchor" href="#recursive"></a> recursive</h2><p><img src="/../images/git-advanced/merge-recursive.png" alt="" /></p><ul><li>additional commits in both master and feature branch after feature branch was created</li><li>additional commit is created in master branch when MERGE</li><li>NOTE: when you use recursive MERGE, all commits either originated from the feature branch or the master branch, will be showing in the log history. But to revert the commit, you only need to revert for 1 step: <code>git reset --hard HEAD~1</code></li><li>if you don’t want to see the commits from the feature branch to be display in your master git log history, you could use <code>--squash</code> with recursive MERGE</li></ul><h2 id="rebase"><a class="markdownIt-Anchor" href="#rebase"></a> rebase</h2><p><img src="/../images/git-advanced/merge-rebase.png" alt="" /></p><ul><li>when you have new commits for both master branch and feature branch (after you created the feature branch), you could use <code>rebase</code> to let the latest commit in master branch becomes the new base commit for commits created in feature branch</li><li>after <code>rebase</code> in the feature branch, all commits you made in the feature branch will have new commit code, even though the changes for these commits are the same. These might raise issues when working with others outside the repo because you will have different commit history</li></ul><h3 id="when-to-use-rebase"><a class="markdownIt-Anchor" href="#when-to-use-rebase"></a> when to use rebase?</h3><ul><li>you could use <code>rebase</code> when there is new commits in the master branch</li></ul><h3 id="why-use-rebase"><a class="markdownIt-Anchor" href="#why-use-rebase"></a> why use rebase?</h3><ol><li>feature branch relies on additional commits in the master branch, then you could rebase the master into feature branch</li><li>feature branch is finished, you want to merge it into master without creating new merge commit</li></ol><ul><li>you could first rebase master into feature branch</li><li>then you could use fast-forward merge because now your master branch don’t have any new commits to your feature branch (because you rebased)</li></ul><h2 id="cherry-pick"><a class="markdownIt-Anchor" href="#cherry-pick"></a> cherry-pick</h2><ul><li>add a specific commit from feature branch to your master branch</li><li>a new commit ID will be created, even though the commit content is the same</li><li>useful when you want just a commit to be in your master branch, but you don’t want to merge the entire feature branch</li></ul><h1 id="github"><a class="markdownIt-Anchor" href="#github"></a> Github</h1><h2 id="deleteing-commits-in-github"><a class="markdownIt-Anchor" href="#deleteing-commits-in-github"></a> deleteing commits in github</h2><ul><li>we could use <code>git reset --hard HEAD~1</code> to delete local commits</li><li>but when we use <code>git push</code> to push to remote git repo, it will fail because our local branch is behind the remote branch</li><li>we could use <code>git push --force origin master</code> to force push changes to remote repo, in this case the remote commits will be deleted.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;head-and-detached-head&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#head-and-detached-head&quot;&gt;&lt;/a&gt; HEAD and detached HEAD&lt;/h1&gt;
&lt;h2 id=&quot;head&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="git" scheme="http://hellcy.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Wishlist 2022</title>
    <link href="http://hellcy.github.io/2021/10/14/Wishlist-2022/"/>
    <id>http://hellcy.github.io/2021/10/14/Wishlist-2022/</id>
    <published>2021-10-14T03:22:02.000Z</published>
    <updated>2022-02-10T14:39:24.301Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Get AWS SAP Certificate</li><li>Learn Spring Boot</li><li>Join a Hackathon project</li><li>Create a personal project using AWS</li><li>Find a new job by the end of 2022 - Done</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;Get AWS SAP Certificate&lt;/li&gt;
&lt;li&gt;Learn Spring Boot&lt;/li&gt;
&lt;li&gt;Join a Hackathon project&lt;/li&gt;
&lt;li&gt;Create a personal project using AWS&lt;/</summary>
      
    
    
    
    
    <category term="Wishlist" scheme="http://hellcy.github.io/tags/Wishlist/"/>
    
  </entry>
  
  <entry>
    <title>AWS DVA review</title>
    <link href="http://hellcy.github.io/2021/09/12/AWS-DVA-review/"/>
    <id>http://hellcy.github.io/2021/09/12/AWS-DVA-review/</id>
    <published>2021-09-11T16:06:00.000Z</published>
    <updated>2022-02-10T14:39:24.299Z</updated>
    
    <content type="html"><![CDATA[<h1 id="i-passed"><a class="markdownIt-Anchor" href="#i-passed"></a> I PASSED!</h1><p><img src="/../images/AWS-DVA-Review/AWSCertifiedDeveloperAssociatecertificate.png" alt="" /><br /><a href="https://www.credly.com/badges/1ccb74e0-f7a1-4e64-9ca2-c1c5b1ece31a?source=linked_in_profile">View My Certificate</a></p><h1 id="elb-asg"><a class="markdownIt-Anchor" href="#elb-asg"></a> ELB + ASG</h1><h2 id="why-use-a-load-balancer"><a class="markdownIt-Anchor" href="#why-use-a-load-balancer"></a> Why use a load balancer</h2><ul><li>spread load across multiple downstream instances</li><li>expose a single point of access (DNS) to your application</li><li>seamlessly handle faliures of downstream instances</li><li>do regular health checks to your instances</li><li>provide SSL termination (HTTPS) for your websites</li><li>enforce stickness with cookies</li><li>high availability across zones</li><li>separate public traffic from private traffic</li></ul><h3 id="elb-integrated-with-many-aws-services"><a class="markdownIt-Anchor" href="#elb-integrated-with-many-aws-services"></a> ELB integrated with many AWS services</h3><ul><li>EC2, ASG, ECS</li><li>ACM, CloudWatch</li><li>Route 53, WAF, Global Accelerator</li></ul><h2 id="alb"><a class="markdownIt-Anchor" href="#alb"></a> ALB</h2><ul><li>Layer 7 (HTTP)</li><li>load balancing to multiple HTTP applications across machines (target groups)</li><li>load balancing to multiple applications on the same machine (containers)</li><li>support for HTTP and WebSockets</li><li>support redirects (from HTTP to HTTPS)</li><li>routing tables to different target groups<ul><li>based on path in URL</li><li>based on hostname in URL</li><li>routing based on query string, headers</li></ul></li><li>ALB are a great fit for micro services and container based application</li><li>has a port mapping feature to redirect to a dynamic port in ECS</li><li>in comparison, we need multiple CLB per application</li><li>the application doesn’t see the client IP directly</li></ul><h3 id="target-groups"><a class="markdownIt-Anchor" href="#target-groups"></a> Target groups</h3><ul><li>EC2 instances</li><li>ECS tasks</li><li>lambda functions (HTTP request is translated into a JSON event)</li><li>IP addresses - must be private IPs</li><li>ALB can route to multiple target groups, each target group could have multiple instances</li><li>health checks are at the target group level</li><li>you can set rules to decide which target groups to redirect the traffic</li></ul><h2 id="nlb"><a class="markdownIt-Anchor" href="#nlb"></a> NLB</h2><ul><li>layer 4 (TCP and UDP)</li><li>handle millions of request per second, lower latency</li><li>NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP)</li><li>NLB are used for extreme performance, TCP or UDP traffic</li></ul><h2 id="sticky-sessions"><a class="markdownIt-Anchor" href="#sticky-sessions"></a> Sticky Sessions</h2><ul><li>it is possible to implement stickness so that the same client is always redirected to the same instance behind a load balancer</li><li>this works for CLB and ALB</li><li>the cookie used for stickness has an expiration date you control</li><li>use case: make sure the user doesn’t lose his session data</li><li>enabling stickness may bring imbalance to the load over the backend EC2 instnaces</li></ul><h2 id="ssl-server-name-indication"><a class="markdownIt-Anchor" href="#ssl-server-name-indication"></a> SSL - server name indication</h2><ul><li>SNL solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)</li><li>it is a newer protocol, and requires the client to indicate the hostname and the target server in the initial SSL handshake</li><li>the server will then find the correct certificate, or return the default one</li><li>only works for ALB and NLB, CloudFront</li></ul><h2 id="connection-draining-de-registration-delay"><a class="markdownIt-Anchor" href="#connection-draining-de-registration-delay"></a> Connection Draining (De-registration delay)</h2><ul><li>time to complete the in-flight requests while the instance is de-registering or unhealthy</li><li>stops sending new requests to the EC2 instances which is de-registering</li><li>between 1 to 3600 seconds (default 300 seconds)</li><li>can be disabled (set value to 0)</li><li>set to a low value if your requests are short</li><li>instances will be terminated after the draining time is over</li></ul><h2 id="x-forwarded-for-and-x-forwarded-proto"><a class="markdownIt-Anchor" href="#x-forwarded-for-and-x-forwarded-proto"></a> X-Forwarded-For and X-Forwarded-Proto</h2><ul><li>X-Forwarded-For<ul><li>The X-Forwarded-For (XFF) header is a de-facto standard header for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or a load balancer. When traffic is intercepted between clients and servers, server access logs contain the IP address of the proxy or load balancer only. To see the original IP address of the client, the X-Forwarded-For request header is used.</li></ul></li><li>X-Forwarded-Proto<ul><li>The X-Forwarded-Proto (XFP) header is a de-facto standard header for identifying the protocol (HTTP or HTTPS) that a client used to connect to your proxy or load balancer. Your server access logs contain the protocol used between the server and the load balancer, but not the protocol used between the client and the load balancer. To determine the protocol used between the client and the load balancer, the X-Forwarded-Proto request header can be used.</li></ul></li></ul><h2 id="asg"><a class="markdownIt-Anchor" href="#asg"></a> ASG</h2><ul><li>A launch configuration (launch template is the newer version)<ul><li>AMI + instance type</li><li>EC2 user data</li><li>EBS volume</li><li>security groups</li><li>SSH key pair</li></ul></li><li>min size, max size, initial capacity</li><li>network + subnets information</li><li>load balancer information (so ASG knows which target group to launch the instnace), ASG and ELB can be linked</li><li>scaling policies</li><li>it is possible to scale an ASG based on CloudWatch alarms</li><li>an alarm monitors a metric (such as average CPU)</li><li>metrics are computed for the overall ASG instnaces</li><li>to update an ASG, you must provide a new launch configuration or new launch template</li><li>IAM roles attached to an ASG will get assigned to EC2 instances launched</li><li>ASG is free, you need to pay for the underlying resources launched</li></ul><h3 id="scaling-policies"><a class="markdownIt-Anchor" href="#scaling-policies"></a> Scaling Policies</h3><ul><li>Target tracking scaling<ul><li>most simple and easy to setup</li><li>example: I want the average ASG CPU to stay at around 40%</li></ul></li><li>Simple / Step scaling<ul><li>when a CloudWatch alarm is triggered, then add 2 units</li><li>when a CloudWatch alarm is triggered, then remove 1 unit</li><li>the difference between simple and step scaling policies are: for step policy, you can create step adjustments, and ASG will change the number of instances based on the size of the alarm breach.</li></ul></li><li>scheduled actions<ul><li>anticipate a scaling based on known usage patterns</li><li>example: increase the min capacity to 10 at 5pm on Fridays</li></ul></li><li>predictive scaling<ul><li>continuously forecast load and schedule scaling ahead</li></ul></li></ul><h3 id="good-metrics-to-scale-on"><a class="markdownIt-Anchor" href="#good-metrics-to-scale-on"></a> Good metrics to scale on</h3><ul><li>CPU utilization</li><li>request count per target</li><li>average network in / out</li></ul><h3 id="scaling-cooldowns"><a class="markdownIt-Anchor" href="#scaling-cooldowns"></a> Scaling cooldowns</h3><ul><li>after a scaling policy happens, you are in the cooldown period (default is 300 seconds)</li><li>during the cooldown period, the ASG will not launch or terminate additional instance (to allow for metrics to stablize)</li><li>advice: use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period</li></ul><h1 id="rds"><a class="markdownIt-Anchor" href="#rds"></a> RDS</h1><ul><li><p>managed DB service for DB use SQL as a language</p></li><li><p>it allows you to create databases in the cloud that are managed by AWS</p><ul><li>Postgres</li><li>MySQL</li><li>MariaDB</li><li>Oracle</li><li>SQL server</li><li>Aurora</li></ul></li><li><p>RDS is a managed service</p><ul><li>automated provisioning, OS patching</li><li>continuous backups and restore to specific timestamp (point in time restore)</li><li>monitoring dashboards</li><li>read replicas for improved read performance</li><li>Multi AZ setup for DR</li><li>maintenance windows for upgrades</li><li>scaling capability</li><li>storage backup by EBS</li><li>RDS DB will be launched in a VPC in an AZ</li></ul></li><li><p>you can’t SSH into your instances</p></li></ul><h2 id="rds-backups"><a class="markdownIt-Anchor" href="#rds-backups"></a> RDS backups</h2><ul><li>Automated backups<ul><li>daily full backup of the database</li><li>transaction logs are backuped by RDS every 5 mins</li><li>ability to restore to any point in time (from oldest to 5 mins ago)</li><li>7 days retention (can be increased to 35 days)</li></ul></li><li>DB snapshots<ul><li>manually triggered by the user</li><li>retention of backup for as long as you want</li></ul></li></ul><h2 id="rds-storage-auto-scaling"><a class="markdownIt-Anchor" href="#rds-storage-auto-scaling"></a> RDS storage auto scaling</h2><ul><li>helps you increase storage on your RDS DB instnace dynamically</li><li>when RDS detects you are running out of free database storage, it scales automatically</li><li>avoid manually scaling your database storage</li><li>you have to set Maximum storage threshold</li><li>automatically modify storage if<ul><li>free storage is less than 10% of allocated storage</li><li>low storage lasts at least 5 mins</li><li>6 hours have passed since last modification</li></ul></li><li>useful for applications with unpredictable workloads</li></ul><h2 id="rds-read-replicas-for-read-scalability"><a class="markdownIt-Anchor" href="#rds-read-replicas-for-read-scalability"></a> RDS read replicas for read scalability</h2><ul><li>up to 5 read replicas</li><li>within AZ, cross AZ or cross region</li><li>replication is async, so reads are eventually consistent</li><li>replicas can be promoted to their own DB</li><li>applications must update the connection string to leverage read replicas</li></ul><h2 id="rds-read-replicas-use-case"><a class="markdownIt-Anchor" href="#rds-read-replicas-use-case"></a> RDS read replicas - use case</h2><ul><li>you have a production database, that is taking on normal load</li><li>you want to run a reporting application to run some analytics</li><li>you create a read replica to run the new workload there</li><li>the production application is unaffected</li><li>read replicas are used for SELECT only kind of statements</li></ul><h2 id="rds-read-replicas-network-cost"><a class="markdownIt-Anchor" href="#rds-read-replicas-network-cost"></a> RDS read replicas - network cost</h2><ul><li>in AWS there is a network cost when data goes from one AZ to another</li><li>for RDS read replicas, within the same region, you don’t pay that fee, but you do need to pay if data goes to another region</li></ul><h2 id="rds-multi-az-dr"><a class="markdownIt-Anchor" href="#rds-multi-az-dr"></a> RDS multi AZ (DR)</h2><ul><li>SYNC replication</li><li>one DNS name - automatic app failover to standby</li><li>increase availability</li><li>failover in case of loss of AZ, loss of network, instance or storage failure</li><li>no manual intervention in apps</li><li>not used for scaling (standby instance can’t be read or write)</li><li>NOTE: the read replicas can be setup as Multi AZ for DR</li></ul><h2 id="rds-from-single-az-to-multi-az"><a class="markdownIt-Anchor" href="#rds-from-single-az-to-multi-az"></a> RDS from single AZ to Multi AZ</h2><ul><li>zero downtime operation (no need to stop the DB)</li><li>just click on modify for the database</li><li>the following happens internally<ul><li>a snapshot is taken</li><li>a new DB is restored from the snapshot in a new AZ</li><li>synchronization is established between the two databases</li></ul></li></ul><h2 id="rds-security-encryption"><a class="markdownIt-Anchor" href="#rds-security-encryption"></a> RDS security - encryption</h2><ul><li>at rest encryption<ul><li>possibility to encrypt the master and read replicas with AWS KMS - AES-256 encryption</li><li>encryption has to be defined at launch time</li><li>if the master is not encrypted, the read replicas cannot be encrypted</li></ul></li><li>in flight encryption<ul><li>SSL certificates to encrypt data to RDS in flight</li><li>provide SSL options with trust certificate when connecting to database</li></ul></li></ul><h2 id="rds-encryption-operations"><a class="markdownIt-Anchor" href="#rds-encryption-operations"></a> RDS encryption operations</h2><ul><li>encrypting RDS backups<ul><li>snapshots of unencrypted RDS databases are un-encrypted</li><li>snapshots of encrypted RDS databases are encrypted</li><li>can copy a snapshot into an encrypted one</li></ul></li><li>to encrypt an un-encrypted RDS database<ul><li>create a snapshot of the un-encrypted database</li><li>copy the snapshot and enable encryption for the snapshot</li><li>restore the database from the encrypted snapshot</li><li>migrate applications to the new database, and delete the old database</li></ul></li></ul><h2 id="rds-security-network-and-iam"><a class="markdownIt-Anchor" href="#rds-security-network-and-iam"></a> RDS security - Network and IAM</h2><ul><li>network security<ul><li>RDS databases are usually deployed within a private subnet, not in a public one</li><li>RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS</li></ul></li><li>Access management<ul><li>IAM policies help control who can manage AWS RDS</li><li>traditional username and password can be used to login into the database</li><li>IAM based authentication can be used to login into RDS MySQL and postgreSQL</li></ul></li></ul><h2 id="aurora"><a class="markdownIt-Anchor" href="#aurora"></a> Aurora</h2><ul><li>Aurora is a proprietary technology from AWS</li><li>postgres and MySQL are both supported as Aurora DB</li><li>Aurora is AWS cloud optimized and claims 5x performance improvement over MySQL on RDS, over 3x performance of Postgres on RDS</li><li>Aurora storage automatically grows in increments of 10GB, up to 64TB</li><li>Aurora can have 15 replicas while MySQL has 5, and the replication process is faster</li><li>failover in Aurora is instantaneous, it is HA native</li><li>user to connect to write endpoint or read endpoint, these endpoints will redirect traffic to the correct instances.</li><li>Security is the same as RDS</li><li>Aurora has 4 features<ul><li>one writer, multiple reader</li><li>one writer, multiple readers - parallel query</li><li>multiple writers</li><li>severless</li></ul></li></ul><h1 id="elasticache"><a class="markdownIt-Anchor" href="#elasticache"></a> ElastiCache</h1><ul><li>the same way RDS is to get managed relational databases</li><li>ElatiCache is to get managed Redis and Memcachced</li><li>Caches are in memory databases with really high performance, low latency</li><li>helps reduce load off of databases for read intensive workloads</li><li>helps make your application stateless</li><li>AWS takes care of OS maintenance and patching, optimizations, setup, configuration, monitoring, failure recovery and backups</li><li>using ElastiCache involves heavy application code changes</li></ul><h2 id="db-cache"><a class="markdownIt-Anchor" href="#db-cache"></a> DB cache</h2><ul><li>Application queries ElastiCache, if not avaialble, get from RDS and store in ElastiCache</li><li>helps relieve load in RDS</li><li>cache must have an invalidation strategy to make sure only the most current data is used in there</li></ul><h2 id="user-session-store"><a class="markdownIt-Anchor" href="#user-session-store"></a> User session store</h2><ul><li>user logs into any of the application</li><li>the application writes the session data into ElastiCache</li><li>the user hits another instance of our application</li><li>the instance retrieves the data and the user is already logged in</li></ul><h2 id="redis-vs-memcached"><a class="markdownIt-Anchor" href="#redis-vs-memcached"></a> Redis vs Memcached</h2><table><thead><tr><th>Redis</th><th>Memcached</th></tr></thead><tbody><tr><td>Multi AZ with auto failover</td><td>multi node for partitioning of data (sharding)</td></tr><tr><td>read replicas to scale reads and have HA</td><td>no HA</td></tr><tr><td>data durability using AOF persistence</td><td>non persistent</td></tr><tr><td>backup and restore features</td><td>no back and restore</td></tr><tr><td>-</td><td>Multi threaded architecture</td></tr></tbody></table><ul><li>Redis Auth<ul><li>if you enable encryption in transit, you can enable Redis Auth, you need to setup a token for your application to connect to Redis.</li></ul></li></ul><h2 id="caching-implementation-considerations"><a class="markdownIt-Anchor" href="#caching-implementation-considerations"></a> Caching implementation considerations</h2><ul><li>is it safe to cache data<ul><li>data may be out of data, eventaully consistent</li></ul></li><li>is caching effective for that data<ul><li>pattern: data changing slowly, few keys are frequently needed, good to use caching</li><li>anti pattern: data changing rapidly, all large keys space frequently needed, not good to use caching</li></ul></li><li>is data structured well for caching?<ul><li>key value caching, or caching of aggregations results?</li><li>caching is good for well structured data</li></ul></li></ul><h2 id="lazy-loading-cache-aside-lazy-population"><a class="markdownIt-Anchor" href="#lazy-loading-cache-aside-lazy-population"></a> lazy loading / cache aside / lazy population</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># python</span><br><span class="line"></span><br><span class="line">def get_user(user_id):</span><br><span class="line">  &#x2F;&#x2F; check the cache</span><br><span class="line">  record &#x3D; cache.get(user_id)</span><br><span class="line"></span><br><span class="line">  if record is None:</span><br><span class="line">    &#x2F;&#x2F; run a DB query</span><br><span class="line">    record &#x3D; db.query(&quot;select * from users where id &#x3D; ?&quot;, user_id)</span><br><span class="line">    &#x2F;&#x2F; populate the cache</span><br><span class="line">    cache.set(user_id, record)</span><br><span class="line">    return record</span><br><span class="line">  else:</span><br><span class="line">    return record</span><br></pre></td></tr></table></figure><h2 id="write-through-add-or-update-cache-when-database-is-updated"><a class="markdownIt-Anchor" href="#write-through-add-or-update-cache-when-database-is-updated"></a> write through - add or update cache when database is updated</h2><ul><li>when there is a write call</li></ul><ol><li>write to DB</li><li>write to cache</li></ol><ul><li>pros<ul><li>data in cache is never stale, reads are quick</li><li>wirte penalty vs read penalty (each write requires 2 calls)</li></ul></li><li>cons<ul><li>missing data until it is added/ updated in the DB, mitigation is to implement lazy loading strategy as well, combine 2 strategies together</li><li>cache churn - a lot of the data will never be read</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># python</span><br><span class="line"></span><br><span class="line">def save_user(user_id, values):</span><br><span class="line"></span><br><span class="line">  # save to DB</span><br><span class="line">  record &#x3D; db.query(&quot;update users ... where id &#x3D; ?&quot;, user_id, values)</span><br><span class="line"></span><br><span class="line">  # push into cache</span><br><span class="line">  cache.set(user_id, record)</span><br><span class="line"></span><br><span class="line">  return record</span><br></pre></td></tr></table></figure><h2 id="cache-evictions-and-ttl-time-to-live"><a class="markdownIt-Anchor" href="#cache-evictions-and-ttl-time-to-live"></a> cache evictions and TTL (time to live)</h2><ul><li>cache eviction can occur in 3 ways<ul><li>you delete item explicity in the cache</li><li>item is evicted because the memory is full and it is not recently used (LRU)</li><li>you set an item TTL</li></ul></li><li>TTL are helpful for any kind of data<ul><li>leaderboard</li><li>comments</li><li>activity streams</li></ul></li><li>TTL can range from few seconds to hours or days</li><li>if too many evictions happen due to memory, you should scale up or out</li></ul><h2 id="final-words"><a class="markdownIt-Anchor" href="#final-words"></a> Final words</h2><ul><li>lazy loading is easy to implement and works for many situations as a foundation, especially on the read side</li><li>write through is usually combined with lazy loading as targeted for the queries or workloads that benefit from this optimization</li><li>setting a TTL is usually not a bad idea, except when you are using write through, set it to a sensible value for your application</li><li>only cache data that makes sense</li></ul><h2 id="elasticache-replication-cluster-mode-disabled"><a class="markdownIt-Anchor" href="#elasticache-replication-cluster-mode-disabled"></a> ElastiCache replication - cluster mode disabled</h2><ul><li>one primary node, up to 5 replicas</li><li>asynchronous replication</li><li>the primary node is used for read and write</li><li>the other nodes are read only</li><li>we have only one shard, and all nodes are in the shard, each node has all the data</li><li>guard against data loss if node failure</li><li>multi AZ enabled by default for failover</li><li>helpful to scale read performance</li></ul><h2 id="elasticache-replication-cluster-mode-enabled"><a class="markdownIt-Anchor" href="#elasticache-replication-cluster-mode-enabled"></a> ElastiCache replication - cluster mode enabled</h2><ul><li>data is partitioned across shards (helpful to scale writes)</li><li>each shard has a primary and up to 5 replica nodes, each shard has part of the data</li><li>multi AZ capability</li><li>up to 500 nodes per cluster</li></ul><h1 id="route-53"><a class="markdownIt-Anchor" href="#route-53"></a> Route 53</h1><ul><li><p>DNS</p><ul><li>Domain Name System which translates the human friendly hostnames into the machine IP addresses</li></ul></li><li><p>Route 53</p><ul><li>A highly available, scalable, fully managed and authoritive DNS</li><li>route 53 is also a Domain Registrar</li><li>ability to check the health of your resources</li><li>the only AWS service which provides 100% availability SLA</li></ul></li></ul><h2 id="hosted-zones"><a class="markdownIt-Anchor" href="#hosted-zones"></a> Hosted zones</h2><ul><li>a container for records that define how to route traffic to a domain and its subdomains</li><li>public hosted zones<ul><li>contains records that specify how to route traffic on the internet (public domain names)</li></ul></li><li>private hosted zones<ul><li>contain records that specify how you route traffic within one or more VPCs (private domain names)</li></ul></li></ul><h2 id="cname-vs-alias"><a class="markdownIt-Anchor" href="#cname-vs-alias"></a> CNAME vs alias</h2><ul><li><p>CNAME</p><ul><li>points a hostname to any other hostname</li><li>only for non root domain</li></ul></li><li><p>Alias</p><ul><li>points a hostname to an AWS resource</li><li>works for root domain and non root domain</li><li>free of charge</li><li>native health check</li><li>automatically recognizes changes in the resource’s IP addresses</li></ul></li><li><p>Alias targets</p><ul><li>ELB</li><li>CloudFront distributions</li><li>API gateway</li><li>Elastic Beanstalk environments</li><li>S3 websites</li><li>VPC interface endpoints</li><li>Global accelerator</li><li>route 53 record in the same hosted zone</li></ul></li><li><p>You cannot set an Alias record for an EC2 DNS name</p></li></ul><h2 id="route-53-routing-policies"><a class="markdownIt-Anchor" href="#route-53-routing-policies"></a> Route 53 - routing policies</h2><ul><li>define how route 53 responds to DNS queries</li><li>route 53 supports the following routing policies<ul><li>simple</li><li>weighted</li><li>failover</li><li>latency based</li><li>geolocation</li><li>multi value answer</li><li>geoproximity (using route 53 traffic flow feature)</li></ul></li></ul><h2 id="simple"><a class="markdownIt-Anchor" href="#simple"></a> Simple</h2><ul><li>typically, route traffic to a single resource</li><li>can specify multiple values in the same record</li><li>if multiple values are returned, a random one is chosen by the client</li><li>when Alias enabled, sepecify only one AWS resource</li><li>can’t be associated with health checks</li></ul><h2 id="weighted"><a class="markdownIt-Anchor" href="#weighted"></a> Weighted</h2><ul><li>control the percentage of the requests that go to each specific resource</li><li>assign each record a relative weight</li><li>DNS records must have the same name and type</li><li>use cases: load balancing between regions, testing new application versions…</li><li>assign a weight of 0 to a record to stop sending traffic to a resource</li><li>if all records have weight of 0, then all records will be returned equally</li></ul><h2 id="latency"><a class="markdownIt-Anchor" href="#latency"></a> latency</h2><ul><li>redirect to the resource that has the least latency close to us</li><li>super helpful when latency for users is a priority</li><li>latency is based on traffic between users and AWS regions</li><li>Germany users may be directed to the US</li><li>can be associated with health checks (has a failover capability)</li></ul><h2 id="health-checks"><a class="markdownIt-Anchor" href="#health-checks"></a> health checks</h2><ul><li>HTTP health checks are only for public resources</li><li>health check =&gt; automated DNS failover</li></ul><ol><li>health checks that monitor an endpoint</li><li>health checks that monitor other health checks (calculated health checks)</li><li>health checks that monitor cloudwatch alarms</li></ol><ul><li>health checks are integrated with CW metrics</li></ul><h3 id="monitor-an-endpoint"><a class="markdownIt-Anchor" href="#monitor-an-endpoint"></a> monitor an endpoint</h3><ul><li>about 15 global health checks will check the endpoint health<ul><li>healthy / unhealthy threshold = 3</li><li>interval - 30 seconds</li><li>supported protocol: HTTP, HTTPS, TCP</li><li>if &gt; 18% of health checks report the endpoint is healthy, route 53 consider it is healthy, otherwise, it is unhealthy</li><li>ability to choose which locations you want route 53 to use</li></ul></li><li>health checks pass only when the endpoint responds with the 2xx and 3xx status codes</li><li>health checks can be setup to pass or fail based on the text in the first 5120 bytes of the response</li><li>your ELB must allow the incoming requests from the route 53 health checkers IP address range</li></ul><h3 id="calculated-health-checks"><a class="markdownIt-Anchor" href="#calculated-health-checks"></a> calculated health checks</h3><ul><li>combine the results of multiple health checks into a single health check</li><li>you can use OR, AND, or NOT</li><li>can monitor up to 256 child health checks</li><li>specify how many of the health checks need to pass to make the parent pass</li><li>usage: perform maintenance to your website without causing all health checks to fail</li></ul><h3 id="private-hosted-zones"><a class="markdownIt-Anchor" href="#private-hosted-zones"></a> private hosted zones</h3><ul><li>route 53 health checks are outside the VPC</li><li>they can’t access private endpoint</li><li>you can create a CloudWatch metric and associate a CloudWatch alarm, then create a health check that checks the alarm itself, if the CloudWatch alarm status becomes ALARM, the health checker will become to unhealthy</li></ul><h2 id="failover"><a class="markdownIt-Anchor" href="#failover"></a> failover</h2><ul><li>create two records associate with 2 resources</li><li>primary and secondary records</li><li>primary record must associated with a health checker</li><li>if the primary record is unhealthy, DNS will return IP address of the secondary resource</li></ul><h2 id="geolocation"><a class="markdownIt-Anchor" href="#geolocation"></a> Geolocation</h2><ul><li>different from latency based</li><li>this routing is based on user location</li><li>specify location by Continent, Country, or by US state</li><li>should create a default record (in case there is no match on location)</li><li>use cases: website localization, restrict content distribution, load balancing…</li><li>can be associated with health checks</li></ul><h2 id="geoproximity"><a class="markdownIt-Anchor" href="#geoproximity"></a> Geoproximity</h2><ul><li>route traffic to your resources based on the geographic location of users and resources</li><li>ability to shift more to resources based on the defined bias</li><li>to change the size of the geographic region, specify bias values<ul><li>to expand (1 to 99), more traffic to the resource</li><li>to shrink (-1 to -99), less traffic to the resource</li></ul></li><li>resource can be<ul><li>AWS resources (AWS region)</li><li>non AWS resources (latitude and longitude)</li></ul></li><li>you must use route 53 traffic flow (advanced) to use this feature</li></ul><h3 id="traffic-flow"><a class="markdownIt-Anchor" href="#traffic-flow"></a> Traffic flow</h3><ul><li>simplify the process of creating and maintaing records in large and complex configurations</li><li>visual editor to manage complex routing decision trees</li><li>configurations can be saved as traffic flow policy<ul><li>can be applied to different route 53 hosted zones</li><li>supports versioning</li></ul></li></ul><h2 id="multi-value"><a class="markdownIt-Anchor" href="#multi-value"></a> multi value</h2><ul><li>use when routing traffic to multiple resources</li><li>route 53 return multiple values / resources</li><li>can be associated with health checks (return only values for healthy resources)</li><li>up to 8 healthy records are returned for each multi value query</li><li>multi value is not a substitude for having an ELB (it is more like a client side load balancing)</li></ul><h1 id="vpc"><a class="markdownIt-Anchor" href="#vpc"></a> VPC</h1><ul><li>VPC: private network to deploy your resource</li><li>subnet: allow you to partition your network inside your VPC (AZ resource)</li><li>a public subnet is a subnet that is accessible from the internet</li><li>a private subnet is a subnet that is not accessible from the internet</li><li>to define access to the internet and between subnets, we use route tables</li></ul><h2 id="internet-gateway-and-nat-gateways"><a class="markdownIt-Anchor" href="#internet-gateway-and-nat-gateways"></a> Internet gateway and NAT gateways</h2><ul><li>internet gateways helps our VPC instances connect with the internet</li><li>public subnets have a route to the internet gateway</li><li>NAT gateways (AWS managed) and NAT instances (self managed) allow your instances in your private subnets to access the internet while remaining private</li></ul><h2 id="nacl-and-security-groups"><a class="markdownIt-Anchor" href="#nacl-and-security-groups"></a> NACL and security groups</h2><ul><li>NACL (network ACL)<ul><li>a firewall which controls traffic from and to subnet</li><li>can have ALLOW and DENY rules</li><li>are attached at the subnet level</li><li>rules only include IP addresses</li></ul></li><li>security groups<ul><li>a firewall that controls traffic to and from an ENI / an EC2 instance</li><li>can have only ALLOW rules</li><li>rules include IP addresses and other security groups</li></ul></li></ul><table><thead><tr><th>Security group</th><th>Network ACL</th></tr></thead><tbody><tr><td>operates at the instance level</td><td>operates at the subnet level</td></tr><tr><td>supports allow rules only</td><td>supports allow rules and deny rules</td></tr><tr><td>is stateful: return traffic is automatically allowed, regardless of any rules</td><td>is stateless: return traffic must be explicitly allowed by rules</td></tr><tr><td>we evaluate all rules before deciding whether to allow traffic</td><td>we process rules in number order when deciding whether to allow traffic</td></tr><tr><td>applies to an instance only if someone specifies the security group when launching the instance, or associate the security group with the instance later on</td><td>automatically applies to all instances in the subnets it’s associated with (therefore, you don’t have to rely on users to specify the security group)</td></tr></tbody></table><h2 id="vpc-flow-logs"><a class="markdownIt-Anchor" href="#vpc-flow-logs"></a> VPC Flow logs</h2><ul><li>capture information about IP traffic going into your interfaces<ul><li>VPC flow logs</li><li>subnet flow logs</li><li>ENI (elastic network interface) flow logs</li></ul></li><li>helps to monitor and troubleshoot connectivity issues<ul><li>subnets to internet</li><li>subnets to subnets</li><li>internet to subnets</li></ul></li><li>captures network information from AWS managed interfaces too: Elastic load balancers, ElastiCache, RDS, Aurora, etc…</li><li>VPC flow logs data can go to S3 / CloudWatch logs</li></ul><h2 id="vpc-peering"><a class="markdownIt-Anchor" href="#vpc-peering"></a> VPC Peering</h2><ul><li>connect two VPCs, privately using AWS network</li><li>make them behave as if they were in the same network</li><li>must not have overlapping CIDR</li><li>VPC peering connection is not transitive (must be established for each VPC that need to communicate with one another)</li></ul><h2 id="vpc-endpoints"><a class="markdownIt-Anchor" href="#vpc-endpoints"></a> VPC endpoints</h2><ul><li>endpoints allow you to connect to AWS services using a private network instead of the public www network</li><li>this gives you enhanced security and lower latency to access AWS services</li><li>VPC endpoint gateway: S3 and DynamoDB</li><li>VPC endpoint interface: the rest AWS rervices</li><li>only used within your VPC</li></ul><h2 id="site-to-site-vpn-and-direct-connect"><a class="markdownIt-Anchor" href="#site-to-site-vpn-and-direct-connect"></a> Site to site VPN and Direct connect</h2><ul><li>site to site VPN<ul><li>connect an on premises VPN to AWS</li><li>the connection is automatically encrypted</li><li>goes over the public internet</li></ul></li><li>direct connect<ul><li>establish a physical connection between on premises and AWS</li><li>the connection is private, secure, and fast</li><li>goes over a private network</li><li>takes at least a month to establish</li></ul></li><li>NOTE: site to site VPN and direct connect cannot access VPC endpoints</li></ul><h1 id="s3"><a class="markdownIt-Anchor" href="#s3"></a> S3</h1><h2 id="buckets"><a class="markdownIt-Anchor" href="#buckets"></a> buckets</h2><ul><li>S3 allows people to store objects in buckets</li><li>buckets must have a globally unique name</li><li>buckets are defined at the region level</li></ul><h2 id="objects"><a class="markdownIt-Anchor" href="#objects"></a> objects</h2><ul><li>objects have a key</li><li>the key is the FULL path</li><li>the key is composed of prefix + object name</li><li>there is no concept of directories within buckets</li><li>just keys with very long names that contain slashes</li><li>object values are the content of the body<ul><li>max object size is 5TB</li><li>if uploading more than 5GB, must use multi-part upload</li></ul></li><li>metadata (list of text key / value pairs - system or user metadata)</li><li>tags (unicode key / value pair, up to 10) - useful for security / lifecycle</li><li>version ID (if versioning is enabled)</li></ul><h2 id="versioning"><a class="markdownIt-Anchor" href="#versioning"></a> versioning</h2><ul><li>you can version your files in S3</li><li>it is enabled at the bucket level</li><li>same key overwrite will increment the version: 1,2,3…</li><li>it is best practice to version your buckets<ul><li>protect against unintended deletes</li><li>easy roll back to previous version</li></ul></li><li>note:<ul><li>any file that is not versioned prior to enabling versioning will have version null</li><li>suspending versioning does not delete the previous versions</li></ul></li></ul><h2 id="encryption-for-objects"><a class="markdownIt-Anchor" href="#encryption-for-objects"></a> Encryption for objects</h2><h3 id="sse-s3"><a class="markdownIt-Anchor" href="#sse-s3"></a> SSE-S3</h3><ul><li>encryption using keys handled and managed by S3</li><li>object is encrypted server side</li><li>AES-256 encryption type</li></ul><h3 id="sse-kms"><a class="markdownIt-Anchor" href="#sse-kms"></a> SSE-KMS</h3><ul><li>encryption using keys handled and managed by KMS</li><li>KMS advantages: user control + audit trail</li><li>object is encrypted server side</li></ul><h3 id="sse-c"><a class="markdownIt-Anchor" href="#sse-c"></a> SSE-C</h3><ul><li>server side encryption using data keys fully managed by the customer outside of AWS</li><li>S3 does not store the encryption key you provide</li><li>HTTPS must be used, because you need to send the encryption key in the header</li><li>encryption key must be provided in HTTP headers, for every HTTP request made</li></ul><h3 id="client-side-encryption"><a class="markdownIt-Anchor" href="#client-side-encryption"></a> client side encryption</h3><ul><li>client library such as the Amazon S3 encryption client</li><li>clients must encrypt data themselves before sending to S3</li><li>clients must decrypt the data themselves when retrieving from S3</li><li>customer fully manages the keys and encryption cycle</li></ul><h3 id="encryption-in-transit-ssltls"><a class="markdownIt-Anchor" href="#encryption-in-transit-ssltls"></a> Encryption in transit (SSL/TLS)</h3><ul><li>Amazon S3 exposes<ul><li>HTTP endpoint: non encrypted</li><li>HTTPS endpoint: encryption in flight</li></ul></li><li>you are free to use the endpoint you want, but HTTPS is recommended</li><li>most clients would use the HTTPS endpoint by default</li><li>HTTPS is mandatory for SSE-C</li></ul><h2 id="security"><a class="markdownIt-Anchor" href="#security"></a> security</h2><ul><li>user based<ul><li>IAM policies - which API calls should be allowed for a specific user from IAM console</li></ul></li><li>resource based<ul><li>bucket policies - bucket wide rules from the S3 console - allows cross account</li><li>object ACL - finer grain</li><li>bucket ACL - less common</li></ul></li><li>NOTE: an IAM principal can access an S3 object if<ul><li>the user IAM permissions allow it OR the resource policy alloow it</li><li>AND there is no explicit DENY</li></ul></li></ul><h3 id="bucket-settings-for-block-public-access"><a class="markdownIt-Anchor" href="#bucket-settings-for-block-public-access"></a> bucket settings for block public access</h3><ul><li>block public access to buckets and objects granted through<ul><li>new access control lists</li><li>any access control lists</li><li>new public bucket or access point policies</li><li>block public and cross account access to buckets and objects through any public bucket or access point policies</li></ul></li><li>these settings were created to prevent company data leaks</li><li>if you know your bucket should never be public, leave these on</li><li>can be set at the account level</li></ul><h3 id="others"><a class="markdownIt-Anchor" href="#others"></a> others</h3><ul><li>networking<ul><li>supports VPC endpoints (for instances in VPC without www internet)</li></ul></li><li>logging and audit<ul><li>S3 access logs can be stored in other S3 buckets</li><li>API calls can be logged in AWS cloudtrail</li></ul></li><li>user security<ul><li>MFA delete: MFA can be required in versioned buckets to delete objects</li><li>pre-signed URLs: URLs that are valid for a limited time (premium videos service for logged in users)</li></ul></li></ul><h2 id="cors"><a class="markdownIt-Anchor" href="#cors"></a> CORS</h2><ul><li>an origin is a scheme, host, and port</li><li>CORS means cross origin resource sharing</li><li>web browser based mechanism to allow requests to other origins while visiting the main origin</li><li>the requests won’t be fulfilled unless the other origin allows for the requests, using CORS headers</li><li>if a client does a cross origin request on our S3 bucket, we need to enable the correct CORS headers</li><li>you can allow for a specific origin or for * (for all origins)</li></ul><h2 id="consistency-model"><a class="markdownIt-Anchor" href="#consistency-model"></a> consistency model</h2><ul><li>strong consistency as of Dec 2020</li></ul><h1 id="aws-cli-sdk-iam-roles-and-policies"><a class="markdownIt-Anchor" href="#aws-cli-sdk-iam-roles-and-policies"></a> AWS CLI, SDK, IAM Roles and policies</h1><ul><li>AWS CLI Dry run<ul><li>tells you if your command would have succeed or not without actually executing it</li></ul></li><li>AWS CLI STS decode erros<ul><li>decode API error messgaes using the STS command line</li></ul></li></ul><h2 id="aws-ec2-instance-metadata"><a class="markdownIt-Anchor" href="#aws-ec2-instance-metadata"></a> AWS EC2 instance metadata</h2><ul><li>it allows AWS EC2 instance to learn about themselves without using an IAM role for that purpose</li><li>the URL is <a href="http://169.254.169.254/latest/meta-data/">http://169.254.169.254/latest/meta-data/</a></li><li>you can retrieve the IAM role name from the metadata, but you cannot retrieve the IAM policy</li><li>metadata = info about the EC2 instance</li><li>user data = launch script of the EC2 instance</li></ul><h2 id="mfa-with-cli"><a class="markdownIt-Anchor" href="#mfa-with-cli"></a> MFA with CLI</h2><ul><li>to use MFA with the CLI, you must create a temporary session</li><li>to do so, you must run the STS GetSessionToken API call</li></ul><h2 id="aws-sdk"><a class="markdownIt-Anchor" href="#aws-sdk"></a> AWS SDK</h2><ul><li>what if you want to perform actions on AWS directly from your applications code?</li><li>you can use an SDK</li><li>we have to use the AWS SDK when coding against AWS services such as DynamoDB</li><li>if you don’t specify or configure a default region, then us-east-1 will be chosen by default</li></ul><h2 id="aws-limit"><a class="markdownIt-Anchor" href="#aws-limit"></a> AWS limit</h2><ul><li>API rate limits<ul><li>DescribeInstances API for EC2 has a limit of 100 calls per seconds</li><li>GetObject on S3 has a limit of 5500 GET per second per prefix</li><li>for intermittent errors: implement <code>exponential backoff</code></li><li>for consistent errors: request an API throttling limit increase</li></ul></li><li>service quotas<ul><li>running on-demand standard instances: 1152 vCPU</li><li>you can request a service limit increase by opening a ticket</li><li>you can request a service quota increase by using the service quotas API</li></ul></li></ul><h3 id="exponential-backoff"><a class="markdownIt-Anchor" href="#exponential-backoff"></a> Exponential Backoff</h3><ul><li>if you get ThrottlingException intermittently, use exponential backoff</li><li>retry mechanism already included in AWS SDK API calls</li><li>must implement yourself if using the AWS API as-is or in specific cases<ul><li>must only implement the retries on 5xx server errors and throttling</li><li>do not implement on the 4xx client errors</li></ul></li></ul><h2 id="aws-cli-credentials-provider-chain"><a class="markdownIt-Anchor" href="#aws-cli-credentials-provider-chain"></a> AWS CLI credentials provider chain</h2><ul><li>the CLI will look for credentials in this order</li></ul><ol><li>command line options</li><li>environment variables</li><li>CLI credentials file</li><li>CLI configuration file</li><li>container credentials</li><li>instance profile credentials</li></ol><h2 id="aws-sdk-default-credentials-provider-chain"><a class="markdownIt-Anchor" href="#aws-sdk-default-credentials-provider-chain"></a> AWS SDK default credentials provider chain</h2><ul><li>the java SDK will look for credentials in this order</li></ul><ol><li>java system properties</li><li>environment variables</li><li>the default credential profiles file</li><li>Amazon ECS container credentials</li><li>instance profile credentials</li></ol><h3 id="credentials-scenario"><a class="markdownIt-Anchor" href="#credentials-scenario"></a> Credentials Scenario</h3><ul><li><p>an application deployed on an EC2 instance is using environment variables with credentials from an IAM user to call the Amazon S3 API</p></li><li><p>The IAM user has S3FullAccess permissions</p></li><li><p>the application only uses one S3 bucket, so according to best practices</p><ul><li>an IAM role and EC2 instance profile was created for the EC2 instance</li><li>the role was assigned the minimum permissions to access that one S3 bucket</li></ul></li><li><p>the IAM instance profile was assigned to the EC2 instance, but it still had access to all S3 buckets, why?</p></li><li><p>the credentials provider chain is still giving priorities to the environment variables</p></li></ul><h3 id="credentials-best-practice"><a class="markdownIt-Anchor" href="#credentials-best-practice"></a> credentials best practice</h3><ul><li>never store AWS credentials in your code</li><li>best practice is for credentials to be inherited from the credentials chain</li><li>if working within AWS, use IAM roles<ul><li>EC2 instance roles for EC2 instances</li><li>ECS roles for ECS tasks</li><li>lambda roles for lambda functions</li></ul></li><li>if working outside AWS, use environment variables / named profiles</li></ul><h2 id="signing-aws-api-requests"><a class="markdownIt-Anchor" href="#signing-aws-api-requests"></a> signing AWS API requests</h2><ul><li>when you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key and secret key)</li><li>note: some requests to Amazon S3 don’t need to be signed</li><li>if you use the SDK or CLI, the HTTP requests are signed for you</li><li>you should sign an AWS HTTP request using Signature v4 (SigV4)</li></ul><h3 id="sigv4-options"><a class="markdownIt-Anchor" href="#sigv4-options"></a> sigV4 options</h3><ul><li>HTTP header</li><li>query string in URL</li></ul><h1 id="s3-and-athena-advanced"><a class="markdownIt-Anchor" href="#s3-and-athena-advanced"></a> S3 and Athena Advanced</h1><h2 id="s3-mfa-delete"><a class="markdownIt-Anchor" href="#s3-mfa-delete"></a> S3 MFA delete</h2><ul><li>MFA forces user to generate a code on a device before doing important operations on S3</li><li>to use MFA delete, we need to enable versioning on the S3 bucket</li><li>you will need MFA to<ul><li>permanently delete an object version</li><li>suspend versioning on the bucket</li></ul></li><li>you won’t need MFA for<ul><li>enabling versioning</li><li>listing deleted versions</li></ul></li><li>only the bucket owner can enable / disable MFA delete</li><li>MFA delete can only be enabled using the CLI</li></ul><h2 id="s3-default-encryption-vs-bucket-policies"><a class="markdownIt-Anchor" href="#s3-default-encryption-vs-bucket-policies"></a> S3 default encryption vs bucket policies</h2><ul><li>one way to force encryption is to use a bucket policy and refuse any API call to PUT an S3 object without encryption headers</li><li>another way is to use the default encryption option in S3</li><li>note: bucket policies are evaluated before default encryption</li></ul><h2 id="s3-access-logs"><a class="markdownIt-Anchor" href="#s3-access-logs"></a> S3 access logs</h2><ul><li>for audit purpose, you may want to log all access to S3 buckets</li><li>any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket</li><li>that data can be analyzed using data analysis tools…</li><li>or Amazon Athena</li><li>do not set your logging bucket to be the monitored bucket</li><li>it will create a logging loop, and your bucket will grow in size exponentially</li></ul><h2 id="s3-replication-crr-or-srr"><a class="markdownIt-Anchor" href="#s3-replication-crr-or-srr"></a> S3 replication (CRR or SRR)</h2><ul><li><p>must enable versioning in source and destination</p></li><li><p>cross region replication - CRR</p></li><li><p>same region replication - SRR</p></li><li><p>buckets can be in different accounts</p></li><li><p>copying is asynchronous</p></li><li><p>must give proper IAM permissions to S3</p></li><li><p>CRR use cases: compliance, lower latency access for users in another region, replicatioin across accounts</p></li><li><p>SRR use cases: log aggregation, live replication between production and test accounts</p></li><li><p>after activating, only new objects are replicated (not retroactive), existing objects will not be replicated</p></li><li><p>for DELETE operations</p><ul><li>can replicate delete markers from source to target (optional setting)</li><li>deletions with a version ID are not replicated (to avoid malicious deletes), it means if you delete an object using its version ID, this operation will not be replicated</li></ul></li><li><p>there is no chaining of replication</p><ul><li>if bucket 1 has replication into bucket 2, which has replication into bucket 3</li><li>then objects created in bucket 1 are not replicated to bucket 3</li></ul></li></ul><h2 id="s3-pre-signed-urls"><a class="markdownIt-Anchor" href="#s3-pre-signed-urls"></a> S3 pre signed URLs</h2><ul><li>can generate per signed URLs using SDK or CLI<ul><li>for downloads (easy, can use the CLI)</li><li>for uploads (harder, must use the SDK)</li></ul></li><li>valid for a default of 3600 seconds, can change timeout with <code>--expires-in [TIME_BY_SECONDS]</code> argument</li><li>users given a pre signed URL inherit the permissions of the person who generated the URL for GET / PUT</li><li>examples<ul><li>allow only logged in users to download a permium video on your S3 buckets</li><li>allow an ever changing list of users to download files by generating URLs dynamically</li><li>allow temporarily a user to upload a file to a precise location in our bucket</li></ul></li></ul><h2 id="amazon-glacier-and-glacier-deep-archive"><a class="markdownIt-Anchor" href="#amazon-glacier-and-glacier-deep-archive"></a> Amazon Glacier and Glacier Deep Archive</h2><ul><li>Amazon Glacier - 3 retrival options<ul><li>expedited - 1 to 5 mins</li><li>standard - 3 to 5 hours</li><li>bulk - 5 to 12 hours</li><li>minimum storage duration of 90 days</li></ul></li><li>Amazon Glacier Deep Archive - for long term storage - cheaper<ul><li>standard - 12 hours</li><li>bulk - 48 hours</li><li>minimum storage duration of 180 days</li></ul></li></ul><h2 id="s3-lifecycle-rules"><a class="markdownIt-Anchor" href="#s3-lifecycle-rules"></a> S3 lifecycle rules</h2><ul><li>transition actions: it defines when objects are transitioned to another storage class<ul><li>move objects to standard IA class 60 days after creation</li><li>move to Glacier for archiving after 6 months</li></ul></li><li>expiration actions: configure objects to expire (delete) after some time<ul><li>access log files can be set to delete after a year</li><li>can be used to delete old versions of files (if versioning is enabled)</li><li>can be used to delete incomplete multi part uploads</li></ul></li><li>rules can be created for a certain prefix</li><li>rules can be created for certain object tags</li></ul><h2 id="s3-performance"><a class="markdownIt-Anchor" href="#s3-performance"></a> S3 performance</h2><ul><li>Amazon S3 automatically scales to high request rates, latency 100-200ms</li><li>your application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD requests per second per prefix in a bucket</li><li>there are no limits to the number of prefixes (prefix is a folder in S3) in a bucket</li></ul><h3 id="kms-limitation"><a class="markdownIt-Anchor" href="#kms-limitation"></a> KMS limitation</h3><ul><li>if you SSE-KMS, S3 performance may be impacted by the KMS limits</li><li>when you upload, it calls the GenerateDataKey KMS API</li><li>when you download it, it calls the Decrypt KMS API</li><li>count towards the KMS quota per second (5500, 10000, 30000 based on region)</li><li>you can request a quota increase using the service quotas console</li></ul><h3 id="multi-part-upload"><a class="markdownIt-Anchor" href="#multi-part-upload"></a> multi part upload</h3><ul><li>recommended for files &gt; 100MB, must be used for files &gt; 5GB</li><li>can help parallelize uploads (speed up transfers)</li></ul><h3 id="s3-transfer-acceleration"><a class="markdownIt-Anchor" href="#s3-transfer-acceleration"></a> S3 transfer acceleration</h3><ul><li>increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region</li><li>compatible with multi part upload</li></ul><h3 id="s3-byte-range-fetches"><a class="markdownIt-Anchor" href="#s3-byte-range-fetches"></a> S3 byte range fetches</h3><ul><li>parallelize GETs by requesting specific byte ranges</li><li>better resilience in case of failures</li><li>can be used to speed up downloads</li><li>can be used to retrieve only partial data (for example the head of a file)</li></ul><h2 id="s3-select-and-glacier-select"><a class="markdownIt-Anchor" href="#s3-select-and-glacier-select"></a> S3 select and Glacier select</h2><ul><li>retrieve less data using SQL by performing servide side filtering</li><li>can filter by rows and columns (complex query not supported)</li><li>less network transfer, less CPU cost client side</li></ul><h2 id="s3-event-notifications"><a class="markdownIt-Anchor" href="#s3-event-notifications"></a> S3 event notifications</h2><ul><li>use case: generate thumbnails of images uploaded to S3</li><li>can create as many S3 events as desired</li><li>S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer</li><li>it two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</li><li>if you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket</li><li>compared to CloudWatch event or EventBridge, S3 event notifications have lower latency and lower costs, it works better with S3</li></ul><h1 id="aws-athena"><a class="markdownIt-Anchor" href="#aws-athena"></a> AWS Athena</h1><ul><li>serverless service to perform analytics directly against S3 files</li><li>uses SQL language to query the files</li><li>has a JDBC / ODBC driver</li><li>charged per query and amount of data scanned</li><li>supports CSV, JSON, ORC, Avro and Parquet (built on Presto)</li><li>use cases: business intelligence, analytics, reporting, analyze and query, VPC flow logs, ELB logs, CloudTrail trails, etc…</li><li>exam tip: analyze data directly on S3 =&gt; use Athena</li></ul><h1 id="aws-cloudfront"><a class="markdownIt-Anchor" href="#aws-cloudfront"></a> AWS CloudFront</h1><ul><li>CDN</li><li>improves read performance, content is cached at the edge</li><li>216 point of presence globally (edge locations)</li><li>DDoS protection, integration, with Shield, AWS WAF</li><li>can expose external HTTPS and can talk to internal HTTPS backends</li></ul><h2 id="origins"><a class="markdownIt-Anchor" href="#origins"></a> Origins</h2><ul><li>S3 bucket<ul><li>for distributing files and caching them at the edge</li><li>enhanced security with CloudFront OAI</li><li>CloudFront can be used as ingress (to upload files to S3)</li></ul></li><li>Custom Origin (HTTP)<ul><li>ALB</li><li>EC2 instance</li><li>S3 website (must first enable the bucket as a static S3 website)</li><li>any HTTP backend you want</li></ul></li></ul><h2 id="cloudfront-geo-restriction"><a class="markdownIt-Anchor" href="#cloudfront-geo-restriction"></a> CloudFront Geo Restriction</h2><ul><li>you can restrict who can access your distribution<ul><li>whitelist: allow your users to access your content only if they are in one of the countries on a list of approved countries</li><li>blacklist: prevent your users from accessing your content if they are in one of the countries on a blacklist of banned countries</li></ul></li><li>the country is determined using a third party geo IP database</li><li>use case: copyright laws to control access to content</li></ul><h2 id="cloudfront-vs-s3-crr"><a class="markdownIt-Anchor" href="#cloudfront-vs-s3-crr"></a> CloudFront vs S3 CRR</h2><ul><li>CloudFront<ul><li>global edge network</li><li>files are cached for a TTL</li><li>great for static content that must be available everywhere</li></ul></li><li>S3 CRR<ul><li>must be setup for each region you want to replication to happen</li><li>files are updated in near real time</li><li>read only</li><li>great for dynamic content that needs to be available at low latency in few regions</li></ul></li></ul><h2 id="cloudfront-caching"><a class="markdownIt-Anchor" href="#cloudfront-caching"></a> CloudFront caching</h2><ul><li>cache based on<ul><li>headers</li><li>session cookies</li><li>query string parameters</li></ul></li><li>the cache lives at each CloudFront edge location</li><li>you want to maximize the cache hit rate to minimize request on the origin</li><li>control the TTL, can be set by the origin usign the cache-control header, expires header…</li><li>you can invalidate part of the cache using the CreateInvalidation API</li></ul><h2 id="cloudfront-signed-url-signed-cookies"><a class="markdownIt-Anchor" href="#cloudfront-signed-url-signed-cookies"></a> CloudFront signed URL / signed cookies</h2><ul><li>you want to distrbute paid shared content to premium users over the world</li><li>we can use CloudFront signed URL / signed cookie, we attach a policy with<ul><li>includes URL expiration</li><li>includes IP ranges to access the data from</li><li>trusted signers (which AWS accounts can create signed URLs)</li></ul></li><li>how long should the URL be valid for<ul><li>shared content (movie, music): make it short (a few minutes)</li><li>private content (private to the user): you can make it for years</li></ul></li><li>signed URL: access to individual files (one signed URL per file)</li><li>signed cookies: access to multiple files (one signed cookie for many files)</li></ul><h2 id="cloudfront-signed-url-vs-s3-pre-signed-url"><a class="markdownIt-Anchor" href="#cloudfront-signed-url-vs-s3-pre-signed-url"></a> CloudFront signed URL vs S3 pre signed URL</h2><h3 id="signed-url"><a class="markdownIt-Anchor" href="#signed-url"></a> Signed URL</h3><ul><li>allow access to a path, no matter the origin</li><li>account wide key pair, only the root can manage it</li><li>can filter by IP, path, date, expiration</li><li>can leverage caching features</li></ul><h3 id="s3-pre-signed-url"><a class="markdownIt-Anchor" href="#s3-pre-signed-url"></a> S3 pre signed URL</h3><ul><li>issue a request as the person who pre signed the URL</li><li>uses the IAM key of the signing IAM principal (has the same access as the IAM user who create the URL)</li><li>limited lifetime</li></ul><h2 id="cloudfront-signed-url-process"><a class="markdownIt-Anchor" href="#cloudfront-signed-url-process"></a> CloudFront signed URL process</h2><ul><li>two types of signers<ul><li>either a trusted key group (recommended)<ul><li>can leverage APIs to create and rotate keys (and IAM for API security)</li></ul></li><li>an AWS account that contains a CloudFront key pair<ul><li>need to manage keys using the root account and the AWS console</li><li>not recommended because you shouldn’t use the root account for this</li></ul></li></ul></li><li>in your CloudFront distribution, create one or more trusted key groups</li><li>you generate your own public / private key<ul><li>the private key is used by your applications to sign URLs</li><li>the public key is used by cloudfront to verify URLs</li></ul></li></ul><h2 id="price-classes"><a class="markdownIt-Anchor" href="#price-classes"></a> Price classes</h2><ul><li>you can reduce the number of edge locations for cost reduction</li><li>three price classes<ul><li>price class All: all regions - best performance</li><li>price class 200: most regions, but excludes the most expensive regions</li><li>price class 100: only the least expensive regions</li></ul></li></ul><h2 id="multiple-origin"><a class="markdownIt-Anchor" href="#multiple-origin"></a> Multiple origin</h2><ul><li>to route to different kind of origins based on the content type</li><li>based on path pattern<ul><li>/images/*</li><li>/api/*</li><li>/*</li></ul></li></ul><h2 id="origin-groups"><a class="markdownIt-Anchor" href="#origin-groups"></a> origin groups</h2><ul><li>to increase high availability and do failover</li><li>origin group: one primary and one secondary origin</li><li>if the primary origin fails, the second one is used (works for both EC2 instance and S3 buckets)</li></ul><h2 id="field-level-encryption"><a class="markdownIt-Anchor" href="#field-level-encryption"></a> field level encryption</h2><ul><li>protect user sensitive information throught application stack</li><li>adds an additional layer of security along with HTTPS</li><li>sensitive information encrypted at the edge close to user</li><li>uses asymmetric encryption</li><li>usage:<ul><li>specify set of fields in POST request that you want to be encrypted (up to 10 fields)</li><li>specify the public key to encrypt them</li><li>fields will be encrypted using the public key at edge locations and will be decrypted when the request reached the web servers</li></ul></li></ul><h1 id="ecs"><a class="markdownIt-Anchor" href="#ecs"></a> ECS</h1><h2 id="docker"><a class="markdownIt-Anchor" href="#docker"></a> Docker</h2><ul><li><p>Docker is a software development platform to deploy apps</p></li><li><p>apps are packaged in containers that can be run on any OS</p></li><li><p>apps run the same, regardless of where they are run</p><ul><li>any machine</li><li>no compatibility issues</li><li>predictable behavior</li><li>less work</li><li>easier to maintain and deploy</li><li>works with any language, any OS, any technology</li></ul></li><li><p>Docker images are stored in Docker repositories</p></li><li><p>public: Docker hub</p></li><li><p>private: Amazon ECR</p></li><li><p>Docker vs VM</p><ul><li>docker is sort of a virtualization technology, but not exactly</li><li>resources are shared with the host =&gt; many containers on one server</li></ul></li><li><p>Docker containers management</p><ul><li>to manage containers, we need a container management platform</li><li>3 choices</li><li>ECS: Amazon’s own platform</li><li>Fargate: Amazon’s own serverless platform</li><li>EKS: Amazon’s managed Kubernates (open source)</li></ul></li></ul><h2 id="ecs-clusters-overview"><a class="markdownIt-Anchor" href="#ecs-clusters-overview"></a> ECS clusters overview</h2><ul><li>ECS clusters are logical grouping of EC2 instances</li><li>EC2 instances run the ECS agent (Docker container)</li><li>the ECS agents registers the instance to the ECS cluster</li><li>the EC2 instances run a special AMI, made specifically for ECS</li></ul><h2 id="ecs-task-definitions"><a class="markdownIt-Anchor" href="#ecs-task-definitions"></a> ECS task definitions</h2><ul><li>tasks definintions are metadata in JSON form to tell ECS how to run a Docker Container</li><li>it contains crucial information around<ul><li>Image name</li><li>port binding for container and host (80 -&gt; 8080)</li><li>memory and CPU required</li><li>environment variables</li><li>networking information</li></ul></li></ul><h2 id="ecs-service"><a class="markdownIt-Anchor" href="#ecs-service"></a> ECS service</h2><ul><li>ECS service help define how many tasks should run and how they should be run</li><li>they ensure that the number of tasks desired is running across our fleet of EC2 instances</li><li>they can be linked to ELB / NLB / ALB if needed</li></ul><h2 id="ecs-service-with-load-balancer"><a class="markdownIt-Anchor" href="#ecs-service-with-load-balancer"></a> ECS service with load balancer</h2><ul><li>ALB has the dynamic port forwarding feature</li><li>when you create ECS tasks it assign random port numbers to tasks</li><li>multiple ECS tasks can be run on a single EC2 instances with different port numbers</li><li>ALB can use the dynamic port forwarding feature to route traffic to these tasks based on their port number</li></ul><h2 id="ecr"><a class="markdownIt-Anchor" href="#ecr"></a> ECR</h2><ul><li>ECR is a private Docker image reporsitory</li><li>access is controlled through IAM (if you have permission errors, check the policy)</li></ul><h3 id="how-to-login-to-ecr-using-aws-cli"><a class="markdownIt-Anchor" href="#how-to-login-to-ecr-using-aws-cli"></a> how to login to ECR using AWS CLI</h3><ul><li>if you have AWS CLI version 1<ul><li><code>$(aws ecr get-login --no-include-email --region eu-west-1)</code></li><li>then you need to execute the output of the above command</li></ul></li><li>if you have AWS CLI version 2<ul><li><code>aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 12334556790.dkr.ecr.eu-west-1.amazonaws.com</code></li><li>you could just execute the above command which is using the pipe feature</li></ul></li><li>Docker push and pull</li></ul><h2 id="fargate"><a class="markdownIt-Anchor" href="#fargate"></a> Fargate</h2><ul><li>when launching an ECS cluster, we have to create our EC2 instances</li><li>if we need to scale, we need to add EC2 instances</li><li>so we need to manage infrastructure…</li><li>with Fargate, it is all serverless</li><li>we don’t provision EC2 instances</li><li>we just create task definitions, and AWS will run our containers for us</li></ul><h2 id="ecs-iam-roles-deep-dive"><a class="markdownIt-Anchor" href="#ecs-iam-roles-deep-dive"></a> ECS IAM roles deep dive</h2><ul><li>EC2 instance profile<ul><li>for EC2 instance to run ECS task, we need to install ECS agent on the EC2 instance</li><li>ECS will do these things</li><li>make API calls to ECS service</li><li>send container logs to CloudWatch logs</li><li>pull docker image from ECR</li><li>so ECS agent will use the EC2 instance profile role to do these things</li></ul></li><li>ECS task role<ul><li>when we run ECS tasks on EC2 instance, each task will have its own role</li><li>we use different roles for the different ECS services run</li><li>task role in defined in the task definition</li></ul></li></ul><h2 id="ecs-tasks-placement"><a class="markdownIt-Anchor" href="#ecs-tasks-placement"></a> ECS tasks placement</h2><ul><li>when a task of type EC2 is launched, ECS must determine where to place it, with the constraints of CPU, memory, and available port</li><li>similarly, when a service scales in, ECS needs to determine which task to terminate</li><li>to assist with this, you can define a task placement strategy and task placement constraints</li><li>NOTE: this is only for ECS with EC2, not for Fargate</li></ul><h3 id="ecs-task-placement-process"><a class="markdownIt-Anchor" href="#ecs-task-placement-process"></a> ECS task placement process</h3><ul><li>task placement strategies are a best effort</li><li>when Amazon ECS places tasks, it uses the following process to select container instances</li></ul><ol><li>identify the instances that satisfy the CPU, memory, and port requirements in the task definition</li><li>identify the instances that satisfy the task placement constraints</li><li>identify the instances that satisfy the placement strategies</li></ol><h3 id="ecs-task-placement-strategies"><a class="markdownIt-Anchor" href="#ecs-task-placement-strategies"></a> ECS task placement strategies</h3><ul><li>Binpack<ul><li>place tasks based on the least available amount of CPU or memory</li><li>this minimize the number of instances in use (cost savings)</li></ul></li><li>random<ul><li>place the task randomly</li></ul></li><li>spread<ul><li>place the task evenly based on the specified value</li><li>example: instanceID, availability zone</li></ul></li><li>you can also mix the placement strategies together, e.g. use Spread for the AZ and Binpack for memory</li></ul><h3 id="ecs-task-placement-constraints"><a class="markdownIt-Anchor" href="#ecs-task-placement-constraints"></a> ECS task placement constraints</h3><ul><li>distinctInstance: place each task on a different container instance</li><li>memberOf: places task on instances that satisfy an expression<ul><li>uses the Cluster Query language</li><li>e.g. place tasks only on t2 instances</li></ul></li></ul><h2 id="ecs-service-auto-scaling"><a class="markdownIt-Anchor" href="#ecs-service-auto-scaling"></a> ECS service auto scaling</h2><ul><li>CPU and RAM is tracked in CloudWatch at the ECS service level</li><li>target tracking: target a specific average CloudWatch metric</li><li>step scaling: scale based on CloudWatch alarms</li><li>scheduled scaling: based on predictable changes</li><li>ECS service scaling (task level) != EC2 auto scaling (instance level)</li><li>Fargate auto scaling is much easier to setup (because of serverless)</li></ul><h2 id="ecs-cluster-capacity-provider"><a class="markdownIt-Anchor" href="#ecs-cluster-capacity-provider"></a> ECS cluster capacity provider</h2><ul><li>a capacity provider is used in association with a cluster to determine the infrastructure that a task runs on<ul><li>for ECS and Fargate users, the FARGATE and FARGATE_SPOT capacity providers are added automatically</li><li>for Amazon ECS on EC2, you need to associate the capacity provider with an auto scaling group</li></ul></li><li>when you run a task or a service, you define a capacity provider strategy, to prioritize in which provider to run</li><li>this allows the capacity provider to automatically provision infrastructure for you</li><li>if you set the average CPU to be at most 70%, then cluster capacity provider will create a new EC2 instance for you when you create a new task to run</li></ul><h2 id="ecs-data-volumes"><a class="markdownIt-Anchor" href="#ecs-data-volumes"></a> ECS data volumes</h2><h3 id="ec2-task-strategies"><a class="markdownIt-Anchor" href="#ec2-task-strategies"></a> EC2 task strategies</h3><ul><li>the EBS volume is already mounted onto the EC2 instances</li><li>this allows your Docker containers to mount the EBS volume and extend the storage capacity of your task</li><li>Problem: if your task moves from one EC2 instance to another one, it won’t be the same EBS volume and data, because EBS volume is mounted to the old EC2 instance</li><li>use cases:<ul><li>mount a data volume between different containers on the same instance</li><li>extend the temporary storage of a task</li></ul></li></ul><h3 id="efs-file-systems"><a class="markdownIt-Anchor" href="#efs-file-systems"></a> EFS file systems</h3><ul><li>works for both EC2 tasks and Fargate tasks</li><li>ability to mount EFS volumes onto tasks</li><li>tasks launched in any AZ will be able to share the same data in the EFS volume</li><li>Fargate + EFS = serverless + data storage without managing servers</li><li>use case: persistent multi AZ shared storage for your containers</li></ul><h3 id="bind-mounts-sharing-data-between-containers"><a class="markdownIt-Anchor" href="#bind-mounts-sharing-data-between-containers"></a> Bind Mounts sharing data between containers</h3><ul><li>works for both EC2 tasks (using local EC2 instance storage) and Fargate tasks (get 4GB for volume mounts)</li><li>useful to share an ephemeral storage between multiple containers part of the same ECS task</li><li>great for sidecar container pattern where the sidecar can be used to send metrics / logs to other destinations</li></ul><h1 id="beanstalk"><a class="markdownIt-Anchor" href="#beanstalk"></a> Beanstalk</h1><h2 id="developer-problems-on-aws"><a class="markdownIt-Anchor" href="#developer-problems-on-aws"></a> developer problems on AWS</h2><ul><li><p>managing infrastructure</p></li><li><p>deploying code</p></li><li><p>configuring all the databases, load balancers, etc…</p></li><li><p>scaling concerns</p></li><li><p>most web apps have the same architecture (ALB + ASG)</p></li><li><p>all the developers want is for their code to run</p></li><li><p>possibly, consistently across different applications and environments</p></li></ul><h2 id="elastic-beanstalk-overview"><a class="markdownIt-Anchor" href="#elastic-beanstalk-overview"></a> Elastic Beanstalk overview</h2><ul><li>a developer centric view of deploying an application on AWS</li><li>it uses all components’ we have seen before: EC2, ASG, ELB, RDS…</li><li>managed services<ul><li>automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration…</li><li>just the application code is the responsiblity of the developer</li></ul></li><li>we still have full control over the configuration</li><li>Beanstalk is free but you pay for the underlying instances</li></ul><h2 id="components"><a class="markdownIt-Anchor" href="#components"></a> Components</h2><ul><li><p>application: collection of Elastic Beanstalk components (environments, versions, configurations)</p></li><li><p>application version: an iteration of your application code</p></li><li><p>environment</p><ul><li>collection of AWS resources running an application version (only one application version at a time)</li><li>tiers: web server environment tier and worker environment tier</li><li>you can create multiple environmnets (dev, test, prod…)</li></ul></li><li><p>web environment</p><ul><li>use ELB with multiple EC2 instances running on different AZs and ASG to scale</li></ul></li><li><p>worker environment</p><ul><li>use SQS queue with multiple EC2 instances running on different AZs and ASG will scale based on SQS’s length</li></ul></li></ul><h2 id="beanstalk-deployment-options"><a class="markdownIt-Anchor" href="#beanstalk-deployment-options"></a> Beanstalk deployment options</h2><h3 id="all-at-once"><a class="markdownIt-Anchor" href="#all-at-once"></a> All at once</h3><ul><li>fastest deployment</li><li>application has downtime</li><li>great for quick iterations in development environment</li><li>no additional cost</li></ul><h3 id="rolling"><a class="markdownIt-Anchor" href="#rolling"></a> Rolling</h3><ul><li>application is running below capacity</li><li>can set the bucket size, bucket size is the number of new instances we launched each time</li><li>application is running both versions simultaneously</li><li>no additional cost</li><li>long deployment</li></ul><h3 id="rolling-with-additional-batches"><a class="markdownIt-Anchor" href="#rolling-with-additional-batches"></a> Rolling with additional batches</h3><ul><li>application is running at capacity</li><li>can set the bucket size</li><li>application is running both versions simultaneously</li><li>small additional cost</li><li>additional batch is removed at the end of the deployment</li><li>longer deployment</li><li>good for production environment</li></ul><h3 id="immutable"><a class="markdownIt-Anchor" href="#immutable"></a> Immutable</h3><ul><li>zero downtime</li><li>new code is deployed to new instances on a temporary ASG</li><li>high cost, double capacity</li><li>longest deployment</li><li>quick rollback in case of failures (just terminate new ASG)</li><li>great for production</li></ul><h3 id="blue-green"><a class="markdownIt-Anchor" href="#blue-green"></a> Blue / Green</h3><ul><li>not a direct feature of Elastic Beanstalk</li><li>zero downtime and release facility</li><li>create a new stage environment and deploy version 2 there</li><li>the new environment (green) can be validated independently and roll back if issue happens</li><li>route 53 can be setup using weighted policies to redirect a little bit of traffic to the stage environment</li><li>using Beanstalk, swap URLs when done with the environment test</li></ul><h3 id="traffic-splitting-canary-testing"><a class="markdownIt-Anchor" href="#traffic-splitting-canary-testing"></a> Traffic splitting (Canary Testing)</h3><ul><li>new application version is deployed to a temporary ASG with the same capacity</li><li>a small percetage of traffic is sent to the temporary ASG for a configuraion amount of time</li><li>deployment health is monitored</li><li>if there is a deployment failure, this triggers an automated roll back (very quick)</li><li>no application downtime</li><li>new instances are migrated from the temporary to the original ASG</li><li>old application version is then terminated</li></ul><h3 id="deploy-using-cli"><a class="markdownIt-Anchor" href="#deploy-using-cli"></a> Deploy using CLI</h3><ul><li>describe dependencies</li><li>package code as zip, and describe dependencies</li><li>console: upload zip file  (creates new app version), and then deploy</li><li>CLI: create new app version using CLI (uploads zip), and then deploy</li><li>Elastic Beanstalk will deploy the zip on each EC2 instance, resolve dependencies and start the application</li></ul><h2 id="beanstalk-lifecycle-policy"><a class="markdownIt-Anchor" href="#beanstalk-lifecycle-policy"></a> Beanstalk lifecycle policy</h2><ul><li>Elastic Beanstalk can store at most 1000 application versions</li><li>if you don’t remove old versions, you won’t be able to deploy anymore</li><li>to phase out old application versions, use a lifecycle policy<ul><li>based on time (old versions are removed)</li><li>based on space (when you have too many versions)</li></ul></li><li>versions that are currently used won’t be deleted</li><li>option not to delete the source bundle in S3 to prevent data loss</li></ul><h2 id="beanstalk-extensions"><a class="markdownIt-Anchor" href="#beanstalk-extensions"></a> Beanstalk extensions</h2><ul><li>a zip file containing our code must be deployed to Elastic Beanstalk</li><li>all the parameters set in the UI can be configured with code using files</li><li>requriements<ul><li>in the <code>.ebextensions/</code> directory in the root of source code</li><li>YAML / JSON format</li><li><code>.config</code> extensions (example: <code>logging.config</code>)</li><li>able to modify some default settings using: <code>option_settings</code></li><li>ability to add resources such as RDS, ElastiCache, DynamoDB, etc…</li></ul></li><li>resources managed by <code>.ebextensions</code> get deleted if the environment goes away</li></ul><h2 id="beanstalk-vs-cloudformation"><a class="markdownIt-Anchor" href="#beanstalk-vs-cloudformation"></a> Beanstalk vs CloudFormation</h2><ul><li>under the hood, Elastic Beanstalk relies on CloudFormation</li><li>CloudFormation is used to provision other AWS services</li><li>use case: you can define CloudFormation resources in your <code>.ebextensions</code> to provision ElastiCache, S3 bucket, or anything you want.</li></ul><h2 id="elastic-beanstalk-cloning"><a class="markdownIt-Anchor" href="#elastic-beanstalk-cloning"></a> Elastic Beanstalk cloning</h2><ul><li>clone an environment with the exact same configuration</li><li>useful for deploying a test version of your application</li><li>all resources and configuration are preserved<ul><li>load balancer type and configuration</li><li>RDS database type (but data is not preserved)</li><li>environment variables</li></ul></li><li>after cloning an environment, you can change settings</li></ul><h2 id="beanstalk-migration"><a class="markdownIt-Anchor" href="#beanstalk-migration"></a> Beanstalk migration</h2><h3 id="load-balancer"><a class="markdownIt-Anchor" href="#load-balancer"></a> load balancer</h3><ul><li>after creating an Elastic Beanstalk environmnet, you cannot change the ELB type</li><li>to migrate to a different ELB<ol><li>create a new env with the same configuration except LB, create your new LB here</li><li>deploy your application onto the new env</li><li>perform a CNAME swap or Route 53 update so all your traffic can be direct to the new env</li></ol></li></ul><h3 id="rds-2"><a class="markdownIt-Anchor" href="#rds-2"></a> RDS</h3><ul><li><p>RDS can be provisioned with Beanstalk, which is greate for dev / test</p></li><li><p>this is not great for production as database lifecycle is tied to the Beanstalk environment lifecycle</p></li><li><p>the best for prod is to separately create an RDS database and provide our Beanstalk application with the connection string</p></li><li><p>but what if you have already created Beanstalk application with the RDS in production? How to migrate it to a new environment without RDS?</p></li></ul><ol><li>create a snapshot of RDS DB (as a safeguard)</li><li>go to the RDS console and protect the RDS database from deletion</li><li>create a new environment, without RDS, point your application to the existing RDS in the old env</li><li>perform a CNAME swap or Route 53 update, confirm it is working</li><li>terminate the old env (RDS will not be deleted because you prevent it in the console)</li><li>delete the CloudFormation stack manually (it will be in DELETE_FAILED state because it can’t delete RDS)</li></ol><h2 id="single-docker"><a class="markdownIt-Anchor" href="#single-docker"></a> single Docker</h2><ul><li>run your application as a single Docker container</li><li>either provide<ul><li>Dockerfile: Elastic Beanstalk will build and run the Docker container</li><li>Dockerrun.aws.json (v1): describe where the Docker image is (already built)</li></ul></li><li>Beanstalk in single Docker container does not use ECS</li></ul><h2 id="multi-docker-containers"><a class="markdownIt-Anchor" href="#multi-docker-containers"></a> Multi Docker containers</h2><ul><li>multi docker helps run multiple containers per EC2 instance in EB</li><li>this will create for you<ul><li>ECS cluster</li><li>EC2 instances, configured to use the ECS cluster</li><li>load balancer (in HA mode)</li><li>task definitions and execution</li></ul></li><li>requries a config Dockerrun.aws.json (v2) at the root of the source code</li><li>Dockerrun.aws.json is used to generate the ECS task definition</li></ul><h2 id="elastic-beanstalk-and-https"><a class="markdownIt-Anchor" href="#elastic-beanstalk-and-https"></a> Elastic Beanstalk and HTTPS</h2><ul><li>Beanstalk with HTTPS<ul><li>idea: load the SSL certificate onto the load balancer</li><li>can be done from the console (EB console, load balancer configuration)</li><li>can be done from the code: <code>.ebextensions/securelistener-alb.config</code></li><li>SSL certificate can be provisioned using ACM or CLI</li><li>must configure a security group rule to allow incoming port 443 (HTTPS port)</li></ul></li><li>Beanstalk redirect HTTP to HTTPS<ul><li>configure your instances to redirect HTTP to HTTPS</li><li>configure the application load balancer with a rule</li><li>make sure health checks are not redirected (so they keep giving 200 OK, otherwise they will receive 301 and 302…)</li></ul></li></ul><h2 id="web-server-vs-worker-environment"><a class="markdownIt-Anchor" href="#web-server-vs-worker-environment"></a> Web server vs worker environment</h2><ul><li>if your application performs tasks that are long to complete, offload these tasks to a dedicated worker environment</li><li>decoupling your application into two tiers is common</li><li>example: processing a video, generating a zip file, etc…</li><li>you can define periodic tasks in a file <code>cron.yaml</code></li></ul><h2 id="custom-platform-advanced"><a class="markdownIt-Anchor" href="#custom-platform-advanced"></a> custom platform (advanced)</h2><ul><li>custom platforms are very advanced, they allow to define from scratch<ul><li>the OS</li><li>additional software</li><li>scripts that Beanstalk runs on these platforms</li></ul></li><li>use case: app language is incompatible with Beanstalk and doesn’t use Docker</li><li>to create your own platform<ul><li>define an AMI using Platform.yaml file</li><li>build that platform using the Packer software (open source tool to create AMIs)</li></ul></li><li>custom platform vs Custom image<ul><li>custom image is to tweak an existing Beanstalk platform</li><li>custom platform is to create an entirely new Beanstalk platform</li></ul></li></ul><h1 id="cicd"><a class="markdownIt-Anchor" href="#cicd"></a> CICD</h1><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><ul><li>we now know how to create resources in AWS manually</li><li>we know how to interact with AWS CLI</li><li>we have seen how to deploy code to AWS using Elastic Beanstalk</li><li>all these manual steps make it very likely for us to do mistakes</li><li>what we would like is to push our code in a repository and have it deployed onto the AWS<ul><li>automatically</li><li>the right way</li><li>making sure it is tested before deploying</li><li>with possibility to go into different stages</li><li>with manual approval where needed</li></ul></li><li>to be a proper AWS developer, we need to learn AWS CICD</li></ul><h2 id="continuous-integration"><a class="markdownIt-Anchor" href="#continuous-integration"></a> Continuous integration</h2><ul><li>developers push the code to a code repository often (github, codecommit, bitbucket, etc…)</li><li>a testing / build server checks the code as soon as it is pushed (codebuild, Jenkins CI, etc…)</li><li>the developer gets feedback about the tests and checks that have passed / failed</li><li>find bugs early, fix bugs</li><li>deliver faster as the code is tested</li><li>deploy often</li></ul><h2 id="continuous-delivery"><a class="markdownIt-Anchor" href="#continuous-delivery"></a> Continuous delivery</h2><ul><li>ensure that the software can be released reliably whenever needed</li><li>ensures deployments happen often and are quick</li><li>automated deployment</li></ul><h2 id="codecommit"><a class="markdownIt-Anchor" href="#codecommit"></a> CodeCommit</h2><ul><li><p>version control is the ability to understand the various changes that happened to the code over time</p></li><li><p>all these are enabled by using a version control system such as git</p></li><li><p>a git repository can live on one’s machine, but it usually lives on a central online repository</p></li><li><p>benefits are</p><ul><li>collaborate with other developers</li><li>make sure the code is backed up somewhere</li><li>make sure it is fully viewable and auditable</li></ul></li><li><p>git repositories can be expensive</p></li><li><p>the industry incldues</p><ul><li>github</li><li>bitbucket</li></ul></li><li><p>AWS CodeCommit</p><ul><li>private git repositories</li><li>no size limit on repositories</li><li>fully managed, HA</li><li>code only in AWS, increased security and compliance</li><li>secure</li><li>integrated with Jenkins / CodeBuild / other CI tools</li></ul></li></ul><h3 id="security-2"><a class="markdownIt-Anchor" href="#security-2"></a> security</h3><ul><li>interactions are done using git</li><li>authentication in git<ul><li>SSH keys: AWS users can configure SSH keys in their IAM console</li><li>HTTPS: done through the AWS CLI Authentication helper ot generating HTTPS credentials</li><li>MFA can be enabled for extra security</li></ul></li><li>Authorization in git<ul><li>IAM policies manage user / roles rights to repositories</li></ul></li><li>encryption<ul><li>repositories are automatically encrypted at rest using KMS</li><li>encrypted in transit (can only use HTTPS and SSH - both secure)</li></ul></li><li>cross account access<ul><li>do not share your SSH keys</li><li>do not share your AWS credentials</li><li>use IAM role in your AWS account and use AWS STS (with AssumeRole API)</li></ul></li></ul><h3 id="codecommit-vs-github"><a class="markdownIt-Anchor" href="#codecommit-vs-github"></a> CodeCommit vs Github</h3><ul><li>Similarities<ul><li>both are git repositories</li><li>both support code review</li><li>github and CodeCommit can be integrated with AWS CodeBuild</li><li>both support HTTPS and SSH method of authentication</li></ul></li><li>differences<ul><li>security<ul><li>github: github users</li><li>codecommit: AWS IAM users / roles</li></ul></li><li>hosted:<ul><li>github: hosted by github</li><li>github enterprise: self hosted on your servers</li><li>codecommit: managed and hosted by AWS</li></ul></li><li>UI<ul><li>github UI is fully featured</li></ul></li></ul></li></ul><h3 id="notifications"><a class="markdownIt-Anchor" href="#notifications"></a> notifications</h3><ul><li>you can trigger notifications in CodeCommit using AWS SNS or AWS lambda or CloudWatch event rules</li><li>use cases for notifications SNS / lambda<ul><li>deletion of branches</li><li>trigger for pushes that happens in master branch</li><li>notify external build system</li><li>trigger AWS lambda function to perform codebase analysis</li></ul></li><li>use cases for CloudWatch event rules<ul><li>trigger for pull request updates</li><li>commit comment event</li><li>CloudWatch event rules goes into an SNS topic</li></ul></li></ul><h2 id="codepipeline"><a class="markdownIt-Anchor" href="#codepipeline"></a> CodePipeline</h2><ul><li>Continuous delivery</li><li>visual workflow</li><li>source: github / codecommit / S3</li><li>build: codebuild / Jenkins</li><li>load testing: third party tools</li><li>deploy: AWS code deploy / Beanstalk / CloudFormation / ECS</li><li>made of stages<ul><li>each stage can have sequential actions or parallel actions</li><li>stages examples: build / test / deploy / load test</li><li>manual approval can be defined at any stage</li></ul></li></ul><h3 id="artifacts"><a class="markdownIt-Anchor" href="#artifacts"></a> artifacts</h3><ul><li>each pipeline stage can create artifacts</li><li>artifacts are passed stored in S3 and passed on to the next stage</li></ul><h3 id="troubleshooting"><a class="markdownIt-Anchor" href="#troubleshooting"></a> troubleshooting</h3><ul><li>codepipeline state changes happen in CloudWatch events, which can in return create SNS notifications<ul><li>you can create events for failed pipelines</li><li>you can create events for cancelled stages</li></ul></li><li>if codepipeline fails a stage, your pipeline stops and you can get information in the console</li><li>CloudTrail can be used to audit AWS API calls</li><li>if pipeline can’t perform an action, make sure the IAM service role attached does have enough permissions</li></ul><h2 id="codebuild"><a class="markdownIt-Anchor" href="#codebuild"></a> CodeBuild</h2><ul><li><p>fully managed build service</p></li><li><p>alternative to other build tools such as Jenkins</p></li><li><p>continuous scaling (no servers to manage or provision - no build queue)</p></li><li><p>pay for usage: the time it takes to complete the builds</p></li><li><p>leverages Docker under the hood for reproducible builds</p></li><li><p>possibility to extend capabilities leveraging our own base Docker images</p></li><li><p>secure: integration with KMS for encryption of build artifacts, IAM for build permissions, and VPC for network security, CloudTrail for API calls logging</p></li><li><p>source code from github / codecommit / codepipeline / S3</p></li><li><p>build instructions can be defined in code (buildspec.yml file)</p></li><li><p>output logs to S3 and AWS cloudwatch logs</p></li><li><p>metrics to monitor codebuild statistics</p></li><li><p>use cloudwatch alarms to detect failed builds and trigger notifications</p></li><li><p>cloudwatch events / lambda as a Glue</p></li><li><p>SNS notifications</p></li><li><p>ability to reproduce codebuild locally to troubleshoot in case of errors</p></li><li><p>builds can be defined within CodePipeline or Codebuild itself</p></li></ul><h3 id="buildspec"><a class="markdownIt-Anchor" href="#buildspec"></a> BuildSpec</h3><ul><li>buildspec.yml file must be at the root of your code</li><li>define environment variables<ul><li>plaintext variables</li><li>secure secrets: use SSM parameter store</li></ul></li><li>phases<ul><li>install: install dependencies you may need for your build</li><li>pre build: final commands to execute before build</li><li>build: actual build commands</li><li>post build: finishing touches (zip output)</li></ul></li><li>artifacts: what to upload to S3</li><li>cache: files to cache to S3 for future build speedup</li></ul><h3 id="local-build"><a class="markdownIt-Anchor" href="#local-build"></a> local build</h3><ul><li>in case of need of deep troubleshooting beyond logs</li><li>you can run CodeBuild on your laptop (after installing Docker)</li><li>for this, leverage CodeBuild agent</li></ul><h3 id="codebuild-in-vpc"><a class="markdownIt-Anchor" href="#codebuild-in-vpc"></a> CodeBuild in VPC</h3><ul><li>by default, your Codebuild containers are launched outside your VPC</li><li>therefore, by default, it cannot access resources in a VPC</li><li>you can specify a VPC configuration<ul><li>VPC ID</li><li>subnet ID</li><li>security group ID</li></ul></li><li>they your build can access resources in your VPC</li><li>use case: integration tests, data query, internal load balancers</li></ul><h2 id="codedeploy"><a class="markdownIt-Anchor" href="#codedeploy"></a> CodeDeploy</h2><ul><li>we want to deploy our application automatically to many EC2 instances</li><li>these instances are not managed by Elastic Beanstalk</li><li>there are several ways to handle deployments using open source tools (Ansible, Terraform, Chef, Pupper, etc…)</li></ul><h3 id="steps"><a class="markdownIt-Anchor" href="#steps"></a> Steps</h3><ul><li>Each EC2 machine (or on premises machine) must be running the CodeDeploy agent</li><li>the agent is continuously polling AWS codeDeploy for work to do</li><li>CodeDeploy sends appspec.yml file</li><li>application is pulled from github or S3</li><li>EC2 will run the deployment instructions</li><li>CodeDeploy agent will report of success / faliure of deployment on the instance</li></ul><h3 id="other-information"><a class="markdownIt-Anchor" href="#other-information"></a> other information</h3><ul><li>EC2 instances are grouped by deployment group (dev / test / prod)</li><li>lots of flexibility to define any kind of deployments</li><li>CodeDeploy can be chained into CodePipeline and use artifacts from there</li><li>CodeDeploy can reuse existing setup tools, works with any application, auto scaling integraion</li><li>Note: Blue / Green only works with EC2 instances (not on premises)</li><li>support for AWS lambda deployments</li><li>CodeDeploy does not provision resources</li></ul><h3 id="primary-components"><a class="markdownIt-Anchor" href="#primary-components"></a> primary components</h3><ul><li>application: unique name</li><li>compute platform: EC2 or on premises or lambda</li><li>deployment configuration: deployment rules for success / failures<ul><li>EC2 or on premises: you can specify the minimum number of healthy instances for the deployment</li><li>lambda: specify how traffic ;is routed to your updated lambda function versions</li></ul></li><li>deployment group: group of tagged instances (allows to deploy gradually)</li><li>deployment type: in place deployment or Blue/Green deployment</li><li>IAM instance profile: need to give EC2 the permissions to pull from S3 / github</li><li>application revision: application code + appspec.yml file</li><li>service role: role for CodeDeploy to perform what it needs</li><li>target revision: target deployment application version</li></ul><h3 id="appspec"><a class="markdownIt-Anchor" href="#appspec"></a> Appspec</h3><ul><li>file section: how to source and copy from S3 / github to filesystem</li><li>hooks: set of instructions to do to deploy the new version (hooks can have timeouts)<ul><li>applicationStop</li><li>DownloadBundle</li><li>BeforeInstall</li><li>AfterInstall</li><li>ApplicationStart</li><li>ValidateService: really important</li></ul></li></ul><h3 id="deployment-config"><a class="markdownIt-Anchor" href="#deployment-config"></a> Deployment config</h3><ul><li>Configs:<ul><li>one a time: one instance at a time, one instance fails =&gt; deployment stops</li><li>half at a time: 50%</li><li>all at once: quick but no healthy host, downtime, good for dev</li><li>custom: min healthy host = 75%</li></ul></li><li>failures:<ul><li>instances stay in failed state</li><li>new deployments will first be deployed to failed state instances</li><li>to rollback: re-deploy old deployment or enable automated rollback for failures</li></ul></li><li>deployment targets<ul><li>set of EC2 instances with tags</li><li>directly to an ASG</li><li>mix of ASG / tags so you can build deployment segments</li><li>customization in scripts with DEPLOYMENT_GROUP_NAME environment variables</li></ul></li></ul><h3 id="codedeploy-for-ec2-and-asg"><a class="markdownIt-Anchor" href="#codedeploy-for-ec2-and-asg"></a> CodeDeploy for EC2 and ASG</h3><ul><li><p>code deploy to EC2</p><ul><li>define how to deploy the application using appspec.yml + deployment strategy</li><li>will do in place update to your fleet of EC2 instances</li><li>can use hooks to verify the deployment after each deployment phase</li></ul></li><li><p>code deploy to ASG</p><ul><li>in place updates<ul><li>updates current existing EC2 instances</li><li>instances newly created by an ASG will also get automated deployments</li></ul></li><li>Blue / Green deployment<ul><li>a new auto scaling group is created (settings are copied)</li><li>choose how long to keep the old instances</li><li>must be using an ELB (for directing traffic to new ASG group)</li></ul></li></ul></li></ul><h2 id="codestar"><a class="markdownIt-Anchor" href="#codestar"></a> CodeStar</h2><ul><li>CodeStar is an integrated solution that regroups: github, codecommit, codebuild, codeDeploy, CloudFormation, codepipeline, cloudwatch</li><li>helps quickly create CICD ready projects for EC2, lambda, Beanstalk</li><li>supported language: C#, Go, HTML5, Java, Node.js, PHP, Python, Ruby</li><li>issue tracking integration with JIRA, Github issues</li><li>ability to integrate with Cloud9 to obtain a web IDE</li><li>one dashboard to view all your components</li><li>free services, pay only for the underlying usage of other services</li><li>limited customization</li></ul><h1 id="cloudformation"><a class="markdownIt-Anchor" href="#cloudformation"></a> CloudFormation</h1><h2 id="infrastructure-as-code"><a class="markdownIt-Anchor" href="#infrastructure-as-code"></a> infrastructure as code</h2><ul><li><p>manual work will be very tough to reproduce</p><ul><li>in another region</li><li>in another AWS account</li><li>within the same region if everything was deleted</li></ul></li><li><p>CloudFormation would be the code to create / update / delete our infrastructure</p></li><li><p>CloudFormation is a declarative way of outlining your AWS infrastructure, for any resources</p></li><li><p>CloudFormation creates the resources for you in the right order, with the exact configuration that you sepcify</p></li></ul><h2 id="benefits"><a class="markdownIt-Anchor" href="#benefits"></a> benefits</h2><ul><li><p>infrastructure as code</p><ul><li>no resources are manually created, which is excellent for control</li><li>the code can be version controlled for example using git</li><li>changes to the infrastructure are reviewed through code</li></ul></li><li><p>cost</p><ul><li>each resources within the stack is stagged with an identifier so you can easily see how much a stack costs you</li><li>you can estimate the costs of your resources using the CloudFormation template</li><li>savings strategy: in dev, you could automation deletion of templates at 5pm and recreate anything at 8am safely</li></ul></li><li><p>productivity</p><ul><li>ability to destroy and recreate an infrastructure on the cloud on the fly</li><li>automated generation of diagram for your templates</li><li>declarative programming (no need to figure out ordering and orchestration)</li></ul></li><li><p>separation of concern: create many stacks for many apps, and many layers</p></li><li><p>don’t reinvent the wheel</p><ul><li>leverage existing templates on the web</li><li>leverage the documentation</li></ul></li></ul><h2 id="how-cloudformation-works"><a class="markdownIt-Anchor" href="#how-cloudformation-works"></a> how cloudformation works</h2><ul><li>templates have to be uplaoded in S3 and then referenced in cloudformation</li><li>to update a template, we can’t edit previous ones, we have to reupload a new version of the template to AWS</li><li>stacks are identified by a name</li><li>deleting a stack deletes every single artifact that was created by CloudFormation</li></ul><h2 id="deploying-cloudformation-template"><a class="markdownIt-Anchor" href="#deploying-cloudformation-template"></a> deploying cloudformation template</h2><ul><li>manual way<ul><li>editing templates in the CloudFormation designer</li><li>using the console to input parameters</li></ul></li><li>automated way<ul><li>editing templates in a YAML file</li><li>using the AWS CLI to deploy the templates</li><li>recommended way when you fully want to automate your flow</li></ul></li></ul><h2 id="building-blocks"><a class="markdownIt-Anchor" href="#building-blocks"></a> building blocks</h2><ul><li>templates components<ul><li>resources: your AWS resources declared in the template (mandatory)</li><li>parameters: the dynamic inputs for your template</li><li>mappings: the static variables for your templates</li><li>outputs: references to what has been created</li><li>conditionals: list of conditions to perform resource creation</li><li>metadata</li></ul></li><li>template helpers<ul><li>references</li><li>functions</li></ul></li></ul><h2 id="resources"><a class="markdownIt-Anchor" href="#resources"></a> resources</h2><ul><li><p>resources are the core of your CloudFormation template</p></li><li><p>they repreesent the different AWS components that will be created and configured</p></li><li><p>resources are declared and can reference each other</p></li><li><p>AWS figures out creation, updates and deletes of resources for us</p></li><li><p>can I create a dynamic amount of resources</p><ul><li>no you can’t</li></ul></li><li><p>is every AWS services suported</p><ul><li>almost, only a few are not</li></ul></li></ul><h2 id="parameters"><a class="markdownIt-Anchor" href="#parameters"></a> parameters</h2><ul><li>parameters are a way to provide inputs to your AWS CloudFormation template</li><li>they are important to know about if<ul><li>you want to resue your templates across the company</li><li>some inputs can not be determined ahead of time</li></ul></li><li>parameters are extremely powerful, controlled, and can precent errors from happening in your templates thanks you types</li></ul><h3 id="how-to-reference-a-parameter"><a class="markdownIt-Anchor" href="#how-to-reference-a-parameter"></a> how to reference a parameter</h3><ul><li>the <code>Fn::Ref</code> function can be leveraged to reference parameters</li><li>parameters can be used anywhere in a template</li><li>the shorthand for this in YAML is <code>!Ref</code></li><li>the function can also reference other elements within the template</li></ul><h3 id="pseudo-parameters"><a class="markdownIt-Anchor" href="#pseudo-parameters"></a> Pseudo parameters</h3><ul><li>AWS offers us pseudo parameters in any CloudFormation template</li><li>these can be used at any time and are enabled by default</li></ul><h2 id="mappings"><a class="markdownIt-Anchor" href="#mappings"></a> mappings</h2><ul><li>mappings are fixed variables within your CloudFormation template</li><li>they are very handy to differentiate between different environments (dev vs prod), regions, AMI types, etc…</li><li>all the values are hardcoded within the template</li></ul><h3 id="when-would-you-use-mappings-vs-parameters"><a class="markdownIt-Anchor" href="#when-would-you-use-mappings-vs-parameters"></a> when would you use mappings vs parameters</h3><ul><li>mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as<ul><li>region</li><li>AZ</li><li>AWS account</li><li>environment</li></ul></li><li>they allow safer control over the template</li><li>use parameters when the values are really user specific</li></ul><h3 id="accessing-mapping-values"><a class="markdownIt-Anchor" href="#accessing-mapping-values"></a> accessing mapping values</h3><ul><li>we use <code>Fn::FindInMap</code> to return a named value from a specific key</li><li><code>!FindInMap [MapName, TopLevelKey, SecondLevelKey]</code></li></ul><h2 id="outputs"><a class="markdownIt-Anchor" href="#outputs"></a> outputs</h2><ul><li>the outputs section declares optional outputs values that we can import into other stacks (if you export them first)</li><li>you can also view the outputs in the AWS console or using the AWS CLI</li><li>they are very useful for example if you define a network CloudFormation, and output the variables such as VPC ID, and your subnet IDs</li><li>it is the best way to perform some collaboration cross stack, as you let export handle their own part of the stack</li><li>you can’t delete a CloudFormation stack if its outputs are being referenced by another CloudFormation stack</li></ul><h3 id="outputs-example"><a class="markdownIt-Anchor" href="#outputs-example"></a> outputs example</h3><ul><li>create a SSH security group as part of one template</li><li>we create an output that references that security group</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Outputs:</span><br><span class="line">  StackSSHSecurityGroup:</span><br><span class="line">    Description: The SSH security group for our company</span><br><span class="line">    Value: !Ref MyCompanyWideSSHSecurityGroup</span><br><span class="line">    Export:</span><br><span class="line">      Name: SSHSecurityGroup</span><br></pre></td></tr></table></figure><h3 id="cross-stack-reference"><a class="markdownIt-Anchor" href="#cross-stack-reference"></a> Cross stack reference</h3><ul><li>we then create a second template that leverages that security group</li><li>for this, we use the <code>Fn::ImportValue</code> function</li><li>you can’t delete the underlying stack until all the references are deleted too</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Resources:</span><br><span class="line">  MySecureInstance:</span><br><span class="line">    Type: AWS::EC2::Instance</span><br><span class="line">    Properties:</span><br><span class="line">      AvailabilityZone: us-east-1a</span><br><span class="line">      ImageId: ami-xxxxxxxx</span><br><span class="line">      InstanceType: t2.micro</span><br><span class="line">      SecurityGroups:</span><br><span class="line">        - !ImportValue SSHSecurityGroup</span><br></pre></td></tr></table></figure><h2 id="conditions"><a class="markdownIt-Anchor" href="#conditions"></a> conditions</h2><ul><li>conditions are used to control the creation of resources or outputs based on a condition</li><li>conditions can be whatever you want them to be, but common ones are<ul><li>environment</li><li>region</li><li>parameter value</li></ul></li><li>each condition can reference another condition, parameter value or mapping</li></ul><h3 id="define-a-conditon"><a class="markdownIt-Anchor" href="#define-a-conditon"></a> define a conditon</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Conditions:</span><br><span class="line">  CreateProdResources: !Equals [ !Ref EnvType, prod]</span><br></pre></td></tr></table></figure><ul><li>the logical ID is for you to choose, it is how you name condition</li><li>the intrinsic function can by any of the following<ul><li><code>Fn::And</code></li><li><code>Fn::Equals</code></li><li><code>Fn::If</code></li><li><code>Fn::Not</code></li><li><code>Fn::Or</code></li></ul></li></ul><h3 id="using-a-condition"><a class="markdownIt-Anchor" href="#using-a-condition"></a> using a condition</h3><ul><li>conditions can be applied to resources / outputs</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Resources:</span><br><span class="line">  Mountpoint:</span><br><span class="line">    Type: &quot;AWS::EC2::VolumeAttachment&quot;</span><br><span class="line">    Condition: CreateProdResources</span><br></pre></td></tr></table></figure><h2 id="intrinsic-functions"><a class="markdownIt-Anchor" href="#intrinsic-functions"></a> Intrinsic functions</h2><h3 id="fnref"><a class="markdownIt-Anchor" href="#fnref"></a> Fn::Ref</h3><ul><li>can be leveraged to reference<ul><li>parameters</li><li>resources</li></ul></li></ul><h3 id="fngetatt"><a class="markdownIt-Anchor" href="#fngetatt"></a> Fn::GetAtt</h3><ul><li>attributes are attached to any resources you create</li><li>to know the attributes of your resources, the best place to look at is the documentation</li><li>example: AZ of an EC2 machine</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Resources:</span><br><span class="line">  EC2Instance:</span><br><span class="line">    Type: &quot;AWS::EC2::Instance&quot;</span><br><span class="line">    Properties:</span><br><span class="line">      ImageId: ami-xxxxx</span><br><span class="line">      InstanceType: t2.micro</span><br><span class="line"></span><br><span class="line">NewVolume:</span><br><span class="line">  Type: &quot;AWS::EC2::Instance&quot;</span><br><span class="line">  Condition: CreateProdResources</span><br><span class="line">  Properties:</span><br><span class="line">    Size: 100</span><br><span class="line">    AvailabilityZone:</span><br><span class="line">      !GetAtt EC2Instance.AvailabilityZone</span><br></pre></td></tr></table></figure><h3 id="fnfindinmap"><a class="markdownIt-Anchor" href="#fnfindinmap"></a> Fn::FindInMap</h3><ul><li>we use Fn::FindInMap to return a named value from a specific key</li><li>!FindInMap [MapName, TopLevelKey, SecondLevelKey]</li></ul><h3 id="fnimportvalue"><a class="markdownIt-Anchor" href="#fnimportvalue"></a> Fn::ImportValue</h3><ul><li>import values that are exported in other templates</li></ul><h3 id="fnjoin"><a class="markdownIt-Anchor" href="#fnjoin"></a> Fn::Join</h3><ul><li>join values with a delimiter</li><li>!Join [delimiter, [a, b, c]]</li></ul><h3 id="fnsub"><a class="markdownIt-Anchor" href="#fnsub"></a> Fn::Sub</h3><ul><li>used to substitute variables from a text, it is a very handy function that will allow you to fully customize your templates</li><li>for example: you can combine Fn::Sub with references or AWS Pseudo variables</li><li>String must contain ${VariableName} and will substitute them</li></ul><h3 id="condition-functions"><a class="markdownIt-Anchor" href="#condition-functions"></a> Condition Functions</h3><ul><li>the logical ID is for you to choose, it is how you name condition</li></ul><h2 id="cloudformation-rollbacks"><a class="markdownIt-Anchor" href="#cloudformation-rollbacks"></a> CloudFormation rollbacks</h2><ul><li>stack creation fails<ul><li>default: everything rolls back, we can look at the log</li><li>option to disable rollback and troubleshoot what happened</li></ul></li><li>stack update fails:<ul><li>the stack automatically rolls back to the previous known working state</li><li>ability to see in the log     what happened and error messages</li></ul></li></ul><h2 id="changesets"><a class="markdownIt-Anchor" href="#changesets"></a> ChangeSets</h2><ul><li>when you update a stack, you need to know what changes before it happens for greater confidence</li><li>ChangeSets won’t say if the update will be successful</li></ul><h2 id="nested-stacks"><a class="markdownIt-Anchor" href="#nested-stacks"></a> Nested Stacks</h2><ul><li>stacks as part of other stacks</li><li>they allow you to isolate repeated patterns / common components in separate stacks and call them from other stacks</li><li>Example:<ul><li>load balancer configuration that is re used</li><li>security group that is re used</li></ul></li><li>nested stacks are considered best practice</li><li>to update a nested stack, always update the parent (root stack)</li></ul><h3 id="cross-stack-vs-nested-stack"><a class="markdownIt-Anchor" href="#cross-stack-vs-nested-stack"></a> Cross stack vs nested stack</h3><ul><li><p>Cross stacks</p><ul><li>helpful when stacks have different lifecycles</li><li>use outputs export and Fn::ImportValue</li><li>when you need to pass export values to many stacks</li></ul></li><li><p>nested stacks</p><ul><li>helpful when components must be re used</li><li>example: re use how to properly configure an application load balancer</li><li>the nested stack only is important to the higher level stack</li></ul></li></ul><h2 id="stacksets"><a class="markdownIt-Anchor" href="#stacksets"></a> StackSets</h2><ul><li>create, update, or delete stacks across multiple accounts and regions with a single operation</li><li>administrator account to create StackSets</li><li>trusted accounts to create, update, delete stack instances from StackSets</li><li>when you update a stack set, all associated stack instances are updated throughout all accounts and regions</li></ul><h2 id="cloudformation-drift"><a class="markdownIt-Anchor" href="#cloudformation-drift"></a> CloudFormation drift</h2><ul><li><p>CloudFormation allows you to create infrastructure</p></li><li><p>but it doesn’t protect you against manual configuration changes</p></li><li><p>how do we know if our resources have drifted?</p></li><li><p>we can use CloudFormation drift</p></li></ul><h1 id="monitoring"><a class="markdownIt-Anchor" href="#monitoring"></a> Monitoring</h1><ul><li>AWS CloudWatch<ul><li>metrics: collect and track key metrics</li><li>log: collect, monitor, analyze, and store log files</li><li>events: send notifications when certain events happen in your AWS</li><li>alarms: react in real-time to metrics / events</li></ul></li><li>X-Ray<ul><li>troubleshooting application performance and errors</li><li>distrbuted tracing of microservices</li></ul></li><li>CloudTrail<ul><li>internal monitoring of API calls being made</li><li>audit changes to AWS resources by your users</li></ul></li></ul><h2 id="cloudwatch-metrics"><a class="markdownIt-Anchor" href="#cloudwatch-metrics"></a> CLoudWatch metrics</h2><ul><li>CloudWatch provides metrics for every services in AWS</li><li>metric is a variable to monitor</li><li>metric belong to namespaces</li><li>dimension is an attribute of a metric</li><li>up to 10 dimensions per metric</li><li>metrics have timestamps</li><li>can create CloudWatch dashboards of metrics</li></ul><h3 id="detailed-monitoring"><a class="markdownIt-Anchor" href="#detailed-monitoring"></a> Detailed monitoring</h3><ul><li>EC2 instance metrics have metrics every 5 minutes</li><li>with detailed monitoring, you get data every 1 minute</li><li>use detailed monitoring if you want to scale faster for your ASG</li><li>the AWS free tier allows us to have 10 detailed monitoring metrics</li><li>NOTE: EC2 memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)</li></ul><h2 id="cloudwatch-custom-metrics"><a class="markdownIt-Anchor" href="#cloudwatch-custom-metrics"></a> CloudWatch Custom metrics</h2><ul><li>possibility to define and send your own custom metrics to CloudWatch</li><li>example: RAM usage, disk space, number of logged in users</li><li>use API call PutMetricData</li><li>ability to use dimensions (attribute) to segment metrics<ul><li>instance id</li><li>environment name</li></ul></li><li>matric resolution<ul><li>standard: 60 seconds</li><li>high resolution: 1 / 5 / 10 / 30 seconds - higher cost</li></ul></li><li>important: the API accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)</li></ul><h2 id="cloudwatch-logs"><a class="markdownIt-Anchor" href="#cloudwatch-logs"></a> CloudWatch logs</h2><ul><li><p>applicatoins can send logs to CloudWatch using the SDK</p></li><li><p>CLoudWatch can collect log from</p><ul><li>Elastic Beanstalk: collection of logs from application</li><li>ECS: colletion from containers</li><li>AWS lambda: collection from function logs</li><li>VPC flow logs: VPC specific logs</li><li>API gateway</li><li>CLoudTrail based on filter</li><li>CloudWatch log agents: for example on EC2 machines</li><li>route 53: log DNS queries</li></ul></li><li><p>cloudWatch logs can go to</p><ul><li>batch exporter to S3 for archival</li><li>stream to elasticSearch cluster for further analysis</li></ul></li><li><p>ClouWatch logs can use filter expressions</p></li><li><p>logs storage architecture</p><ul><li>log groups: arbitrary name, usually representing an application</li><li>log stream: instances within application / log files / containers</li></ul></li><li><p>can define log expiration policies (never, 30 days, etc…)</p></li><li><p>using the AWS CLI we can tail CloudWatch logs</p></li><li><p>to send logs to CloudWatch, make sure IAM permissions are correct</p></li><li><p>security: encryption of logs using KMS at the group level</p></li></ul><h2 id="cloudwatch-logs-for-ec2"><a class="markdownIt-Anchor" href="#cloudwatch-logs-for-ec2"></a> CloudWatch logs for EC2</h2><ul><li>by default, no logs from EC2 machine will go to CloudWatch</li><li>you need to run a CloudWatch agent on EC2 to push the log files you want</li><li>make sure IAM permissions are correct</li><li>the CLoudWatch log agent can be setup on premises too</li></ul><h3 id="cloudwatch-logs-agent-vs-unified-agent"><a class="markdownIt-Anchor" href="#cloudwatch-logs-agent-vs-unified-agent"></a> CloudWatch logs agent vs Unified agent</h3><ul><li>CloudWatch logs agent<ul><li>old version of the agent</li><li>can only send to CloudWatch logs</li></ul></li><li>CloudWatch Unified agent<ul><li>collect additional system level metrics such as RAM, processes, etc…</li><li>collect logs to send to CloudWatch logs</li><li>centralized configuration using SSM parameters store</li></ul></li></ul><h2 id="cloudwatch-logs-metric-filter"><a class="markdownIt-Anchor" href="#cloudwatch-logs-metric-filter"></a> CloudWatch logs metric filter</h2><ul><li>CloudWatch logs can use filter expressions<ul><li>for exampe: find a specific IP inside of a log</li><li>or count occurrences of ERROR in your logs</li><li>metric filters can be used to trigger alarms</li></ul></li><li>filter do not retroactively filter data. (it doesn’t not filter historical data, only data since filter is created is counted).</li><li>filters only publish the metric data points for events that happen after the filter was created</li><li>can be integrated with CloudWatch alarms, SNS, etc…</li></ul><h2 id="cloudwatch-alarms"><a class="markdownIt-Anchor" href="#cloudwatch-alarms"></a> CloudWatch alarms</h2><ul><li>alarms are used to trigger notifications for any metric</li><li>various options</li><li>alarm states<ul><li>OK</li><li>INSUFFICIETN_DATA</li><li>ALARM</li></ul></li><li>period<ul><li>length of time in seconds to evaluate the metric</li><li>high resolution custom metrics: 10 sec, 30 sec or multiple of 60 sec</li></ul></li></ul><h3 id="cloudwatch-alarm-targets"><a class="markdownIt-Anchor" href="#cloudwatch-alarm-targets"></a> CloudWatch alarm targets</h3><ul><li>stop, terminate, reboot or recover EC2 instances</li><li>trigger auto scaling action</li><li>send notification to SNS</li></ul><h3 id="good-to-know"><a class="markdownIt-Anchor" href="#good-to-know"></a> good to know</h3><ul><li>alarms can be created based on CloudWatch logs metrics filters</li><li>to test alarms and notifications, you could set the alarm state using CLI</li></ul><h2 id="cloudwatch-events"><a class="markdownIt-Anchor" href="#cloudwatch-events"></a> CloudWatch Events</h2><ul><li>event pattern: intercept events from AWS service<ul><li>EC2 instance state change, code build failure, S3…</li><li>can intercept any API call with CloudTrail integration</li></ul></li><li>schedule or Cron</li><li>A json payload is created from the event and passed to a target</li></ul><h2 id="cloudwatch-event-bridge"><a class="markdownIt-Anchor" href="#cloudwatch-event-bridge"></a> CloudWatch event bridge</h2><ul><li>EventBridge is the next evolution of CloudWatch events</li><li>default event bus: generated by AWS service</li><li>partner event bus: receive events from SaaS service or applications</li><li>custom event bus: for you own application</li><li>event buses can be accessed by other AWS accounts</li><li>rules: how to process the events (similar to CloudWatch event rules)</li></ul><h3 id="schema-registry"><a class="markdownIt-Anchor" href="#schema-registry"></a> Schema registry</h3><ul><li>eventBridge can analyze the events in your bus and infer the schema</li><li>the schema registry allows you to generate code for your application that will know in advance how data is structured in the event bus</li><li>schema can be versioned</li></ul><h2 id="amazon-eventbridge-vs-cloudwatch-events"><a class="markdownIt-Anchor" href="#amazon-eventbridge-vs-cloudwatch-events"></a> Amazon EventBridge vs CloudWatch events</h2><ul><li>EventBridge builds upon and extends CloudWatch events</li><li>it uses the same service API and endpoint, and the same underlying service infrastructure</li><li>EventBridge allows extension to add event buses for your custom applications and third party SaaS apps</li><li>EventBridge has the schema registry capability</li><li>EventBridge has a different name to mark the new capabilities</li><li>over time, the CloudWatch events name will be replaced with EventBridge</li></ul><h2 id="x-ray"><a class="markdownIt-Anchor" href="#x-ray"></a> X-Ray</h2><ul><li>debugging in Production, the old way<ul><li>test locally</li><li>add log statements everywhere</li><li>re deploy in production</li></ul></li><li>log formats differ across applications using CLoudWatch and analytics is hard</li></ul><h3 id="x-ray-advantages"><a class="markdownIt-Anchor" href="#x-ray-advantages"></a> X-ray advantages</h3><ul><li>troubleshooting performance</li><li>understand dependencies in a microservices architecture</li><li>pinpoint service issues</li><li>review request behavior</li><li>find errors and exceptions</li></ul><h3 id="tracing"><a class="markdownIt-Anchor" href="#tracing"></a> tracing</h3><ul><li>tracing is an end to end way to following a request</li><li>each component dealing with the request adds its own trace</li><li>tracing is made of segments</li><li>annotations can be added to traces to provide extra information</li><li>ability to trace<ul><li>every request</li><li>sample request (as a percentage for example or a rate per minute)</li></ul></li><li>X-Ray security<ul><li>IAM for authorization</li></ul></li></ul><h3 id="how-to-enable"><a class="markdownIt-Anchor" href="#how-to-enable"></a> How to enable?</h3><ol><li>Your code must import the AWS X-Ray SDK</li></ol><ul><li>very little code modification needed</li><li>the application SDK will then capture<ul><li>calls to AWS service</li><li>HTTP / HTTPS requests</li><li>database calls</li><li>queue calls</li></ul></li></ul><ol start="2"><li>install X-Ray daemon or enable X-Ray AWS integration</li></ol><ul><li>X-Ray daemon works as a low level UDP packet interceptor</li><li>lambda / other AWS services already run the X-Ray daemon for you</li><li>each application must have the IAM rights to write data to X-Ray</li></ul><h3 id="x-ray-magic"><a class="markdownIt-Anchor" href="#x-ray-magic"></a> X-Ray magic</h3><ul><li>X-Ray service collects data from all the different services</li><li>service map is computed from all the segments and traces</li><li>X-Ray is graphical, so even non technical people can help troubleshoot</li></ul><h3 id="x-ray-troubleshooting"><a class="markdownIt-Anchor" href="#x-ray-troubleshooting"></a> X-Ray troubleshooting</h3><ul><li>if X-Ray is not working on EC2<ul><li>ensure the EC2 IAM role has the proper permissions</li><li>ensure the EC2 instance is running the X-Ray daemon</li></ul></li><li>to enable on AWS lambda<ul><li>ensure it has an IAM execution role with proper policy</li><li>ensure that X-Ray is imported in the code</li></ul></li></ul><h2 id="x-ray-instrumentation-and-concepts"><a class="markdownIt-Anchor" href="#x-ray-instrumentation-and-concepts"></a> X-Ray instrumentation and concepts</h2><ul><li>instrumentation means the measure of product’s performance, diagnose errors and to write trace information</li><li>to instrument your application code, you use the X-Ray SDK</li><li>many SDK require only configuration changes</li><li>you can modify your application code to customize and annotation the data that the SDK sends to X-Ray, using interceptors, filters, handlers, middleware…</li></ul><h3 id="x-ray-concepts"><a class="markdownIt-Anchor" href="#x-ray-concepts"></a> X-Ray concepts</h3><ul><li>segments: each application / service will send them</li><li>subsegments: if you need more details in your segment</li><li>trace: segments collected together to form an end to end trace</li><li>sampling: decrease the amount of requests sent to X-Ray, reduce cost</li><li>annotations: key value pairs used to index traces and use with filters</li><li>metadata: key value pairs, not indexed, not used for searching</li><li>the X-Ray daemon / agent has a config to send traces cross account<ul><li>make sure the IAM permissions are correct - the agent will assume the role</li><li>this allows to have a central account for all your application tracing</li></ul></li></ul><h3 id="x-ray-sampling-rules"><a class="markdownIt-Anchor" href="#x-ray-sampling-rules"></a> X-Ray sampling rules</h3><ul><li>with sampling rules, you control the amount of data that you record</li><li>you can modify sampling rules without changing your code</li><li>by default, the X-Ray SDK records the first request each second, and five percent of any additional requests</li><li>one request per  second is the reservior, which requests that at least one trace is recorded each second as long the service is serving requests</li><li>Five percent is the rate, at which additional requests beyond the reservior size are sampled</li></ul><h2 id="x-ray-with-beanstalk"><a class="markdownIt-Anchor" href="#x-ray-with-beanstalk"></a> X-Ray with Beanstalk</h2><ul><li>Elastic Beanstalk platforms include the X-Ray daemon</li><li>you can run the daemon by setting an option in the Elastic Beanstalk console or with a configuration file (in .ebextension/xray-daemon.config)</li><li>make sure to give your instance profile the correct IAM permissions so that the X-Ray daemon can function correctly</li><li>then make sure your application code is intrumentated with the X-Ray SDK</li><li>note: the X-Ray daemon is not provided for multicontainer Docker</li></ul><h2 id="cloudtrail"><a class="markdownIt-Anchor" href="#cloudtrail"></a> CloudTrail</h2><ul><li>provides governance, compliance, and audit for your AWS account</li><li>CloudTrail is enabled by default</li><li>get an history of events / API calls made within your AWS accounts by<ul><li>console</li><li>SDK</li><li>CLI</li><li>AWS services</li></ul></li><li>can put logs from CloudTrail into CLoudWatch logs or S3</li><li>a trail can be applied to All regions (default), or a single region</li><li>if a resource is deleted in AWS, investigate CloudTrail first</li></ul><h3 id="cloudtrail-events"><a class="markdownIt-Anchor" href="#cloudtrail-events"></a> CloudTrail events</h3><ul><li>management events<ul><li>operations that are performed on resources in you account</li><li>examples<ul><li>configuring security</li><li>configuring rules for routing data</li><li>setting up logging</li></ul></li><li>by default, trails are configured to log management events</li><li>can separate read events (that don’t modify resources) and write events (that may modify resources)</li></ul></li><li>data events<ul><li>by default, data events are not logged (because high volume operations)</li><li>S3 object-level activity</li><li>can separate read and write events</li><li>lambda function execution activity</li></ul></li></ul><h3 id="cloudtrail-insights"><a class="markdownIt-Anchor" href="#cloudtrail-insights"></a> CloudTrail insights</h3><ul><li>enable CloudTrail insights to detect unusual activity in your account<ul><li>inaccurate resource provisioning</li><li>hitting service limits</li><li>bursts of IAM actions</li><li>gaps in periodic maintenance activity</li></ul></li><li>CloudTrail insights analyzes normal management events to create a baseline</li><li>and then continuously analyzes write events to detect usuaual patterns<ul><li>anomalies appear in the CloudTrail console</li><li>event is sent to S3</li><li>eventBridge is generated</li></ul></li></ul><h3 id="cloudtrail-events-retention"><a class="markdownIt-Anchor" href="#cloudtrail-events-retention"></a> CloudTrail events retention</h3><ul><li>events are stored for 90 days in CloudTrail</li><li>to keep events beyond this period, log them to S3 and use Athena</li></ul><h2 id="cloudtrail-vs-cloudwatch-vs-x-ray"><a class="markdownIt-Anchor" href="#cloudtrail-vs-cloudwatch-vs-x-ray"></a> CloudTrail vs CloudWatch vs X-Ray</h2><ul><li>CloudTrail<ul><li>audit API calls made by users / services / AWS console</li><li>useful to detect unauthorized calls or root cause of changes</li></ul></li><li>CloudWatch<ul><li>metrics over time for monitoring</li><li>logs for storing application log</li><li>alarms to send notifications in case of unexpected metrics</li></ul></li><li>X-Ray<ul><li>automated trace analysis and central service map visualization</li><li>latency, errors and fault analysis</li><li>request tracking across distributed systems</li></ul></li></ul><h1 id="sqs"><a class="markdownIt-Anchor" href="#sqs"></a> SQS</h1><h2 id="communications-between-applications"><a class="markdownIt-Anchor" href="#communications-between-applications"></a> Communications between applications</h2><ul><li>Synchronous<ul><li>synchronous between applications can be problematic if there are sudden spikes of traffic</li><li>what if you need to suddenly encode 1000 videos but usually it is 10?</li></ul></li><li>asynchronous<ul><li>it is better to decouple your applications</li><li>SQS: queue model</li><li>SNS: pub/sub model</li><li>Kinesis: real time streaming model</li><li>these services can scale independently from our application</li></ul></li></ul><h2 id="sqs-standard-queue"><a class="markdownIt-Anchor" href="#sqs-standard-queue"></a> SQS - standard queue</h2><ul><li>fully managed service, used to decouple applications</li><li>attributes<ul><li>unlimited throughput, unlimited number of messages in queue</li><li>default retention of messages: 4 to 14 days</li><li>law latency</li><li>limitation of 256 KB per message sent</li></ul></li><li>can have duplicate messages (at least once delivery, occasionally)</li><li>can have out of order messages (best effort ordering)</li></ul><h2 id="producing-messages"><a class="markdownIt-Anchor" href="#producing-messages"></a> Producing messages</h2><ul><li>produced to SQS using the SDK (SendMessage API)</li><li>the message is persisted in SQS until a consumer deletes it</li></ul><h2 id="comsuming-messages"><a class="markdownIt-Anchor" href="#comsuming-messages"></a> comsuming messages</h2><ul><li>consumers (running on EC2 instances, servers, or lambda)</li><li>poll SQS for messages (receive up to 10 messages at a time)</li><li>process the messages (example: insert the message into an RDS database)</li><li>delete the messages using the DeleteMessage API</li></ul><h2 id="multiple-ec2-instances-consumers"><a class="markdownIt-Anchor" href="#multiple-ec2-instances-consumers"></a> multiple EC2 instances consumers</h2><ul><li>consumers receive and process messages in parallel</li><li>at least once delivery</li><li>best effort message ordering</li><li>consumers delete messages after processing them</li><li>we can scale consumers horizontally to improve throughput of processing (using ASG)</li></ul><h2 id="security-3"><a class="markdownIt-Anchor" href="#security-3"></a> security</h2><ul><li>encryption<ul><li>in flight encryption using HTTPS API</li><li>at rest encryption using KMS keys</li><li>client side encryption if the client wants to perform encryption / decryption itself</li></ul></li><li>access controls: IAM policies to regulate access to SQS API</li><li>SQS access policies: (similar to S3 bucket policies)<ul><li>useful for cross account access to SQS queues</li><li>useful for allowing other services (SNS, S3…) to write to an SQS queue</li></ul></li></ul><h2 id="message-visibility-timeout"><a class="markdownIt-Anchor" href="#message-visibility-timeout"></a> message visibility timeout</h2><ul><li><p>after a message is polled by a consumer, it becomes invisible to other consumers</p></li><li><p>by default, the message visibility timeout is 30 seconds</p></li><li><p>that means the message has 30 seconds to be processed</p></li><li><p>after the message visibility timeout is over, the message is visible again in SQS</p></li><li><p>if a message is not processed within the visibility timeout, it will be received by consumer again so it will be processed twice</p></li><li><p>a consumer could call the ChangeMessageVisibility API to get more time</p></li><li><p>if visibility timeout is high, and consumer crashes, it will take longer time for the message to become visible in the queue and being consumed by others</p></li><li><p>if the visibility timeout is low, we may get duplicates</p></li></ul><h2 id="dead-letter-queue"><a class="markdownIt-Anchor" href="#dead-letter-queue"></a> Dead letter queue</h2><ul><li>if a consumer fails to process a message within the visibility timeout, the message goes back to the queue</li><li>we can set a threshold of how many times a message can go back to the queue</li><li>after the MaximumRecevies threshold is exceeded, the message goes into a dead letter queue</li><li>useful for debugging</li><li>make sure to process the messages in the DLQ before they expire<ul><li>good to set a retention of 14 days in the DLQ</li></ul></li></ul><h2 id="delay-queue"><a class="markdownIt-Anchor" href="#delay-queue"></a> delay queue</h2><ul><li>delay a message (consumers don’t see it immediately) up to 15 minutes</li><li>default is 0 seconds (message is available right away)</li><li>can set a default at queue level</li><li>can override the default on send using the DelaySeconds parameter</li></ul><h2 id="long-polling"><a class="markdownIt-Anchor" href="#long-polling"></a> long polling</h2><ul><li>when a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue</li><li>this is called long polling</li><li>long polling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application</li><li>the wait time can be between 1 to 20 seconds</li><li>long polling is preferable to short polling</li><li>long polling can be enabled at the queue level or at the API level using WaitTimeSeconds</li></ul><h2 id="sqs-extended-client"><a class="markdownIt-Anchor" href="#sqs-extended-client"></a> SQS extended client</h2><ul><li>message size limit is 256 KB, how to send large messages?</li><li>using the SQS extended client (Java Library)</li><li>it can be implemented using any language, it first uploads the large object to S3</li><li>then send the metadata of that object to SQS, once the consumer received the metadata, it will fetch the real object from S3.</li></ul><h2 id="fifo-queue"><a class="markdownIt-Anchor" href="#fifo-queue"></a> FIFO queue</h2><ul><li>First in first out</li><li>limited throughput: 300 messages / second, without batching, 3000 m/s with batching</li><li>exactly once send capability (by removing duplicates)</li><li>messages are processed in order by the consumer</li></ul><h3 id="fifo-deduplication"><a class="markdownIt-Anchor" href="#fifo-deduplication"></a> FIFO deduplication</h3><ul><li>deduplication interval is 5 minutes</li><li>two deduplication methods<ul><li>content based deduplication: will do a SHA-256 hash of the message body</li><li>explicitly provide a message deduplication ID</li></ul></li><li>if the queue receives messages with the same hash key or the same deduplication ID, it will refuse to receive the message</li></ul><h3 id="message-grouping"><a class="markdownIt-Anchor" href="#message-grouping"></a> message grouping</h3><ul><li>if you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order</li><li>to get ordering at the level of a subset of messages, specify different values for MessageGroupID<ul><li>messages that share a common message group ID will be in order within the group</li><li>each group ID can have a different consumer (parallel processing)</li><li>ordering across groups is not guaranteed</li></ul></li></ul><h1 id="sns"><a class="markdownIt-Anchor" href="#sns"></a> SNS</h1><ul><li><p>what if you want to send one message to many receivers?</p></li><li><p>the event producer only sends message to one SNS topic</p></li><li><p>as many event receivers as we want to listen to the SNS topic notifications</p></li><li><p>each subscriber to the topic will get all the messages (note: new feature to filter messages)</p></li><li><p>up to 10 million subscriptions per topic</p></li><li><p>100k topics limit</p></li><li><p>subscribers can be</p><ul><li>SQS</li><li>HTTP / HTTPS</li><li>lambda</li><li>emails</li><li>SMS messages</li><li>mobile notifications</li></ul></li><li><p>SNS integrates with a lot of AWS services</p><ul><li>many AWS services can send data directly to SNS for notifications</li><li>CloudWatch alarms</li><li>ASG notifications</li><li>S3</li><li>CloudFormation (upon state changes =&gt; failed to build etc…)</li></ul></li></ul><h2 id="how-to-publish"><a class="markdownIt-Anchor" href="#how-to-publish"></a> How to publish</h2><ul><li>topic publish (using the SDK)<ul><li>create a topic</li><li>create a subscription</li><li>publish to the topic</li></ul></li><li>direct publish (for mobile apps SDK)<ul><li>create a platform application</li><li>create a platform endpoint</li><li>publish to the platform endpoint</li><li>works with Google GCM, Apple APNS, Amazon ADM…</li></ul></li></ul><h2 id="security-4"><a class="markdownIt-Anchor" href="#security-4"></a> security</h2><ul><li>encryption<ul><li>in flight encryption using HTTPS API</li><li>at rest encryption using KMS keys</li><li>client side encryption if the client wants to perform encryption / decryption itself</li></ul></li><li>access controls: IAM policies to regulate access to the SNS API</li><li>SNS access policies (similar to S3 bucket policies)<ul><li>useful for cross account to SNS topic</li><li>useful for allowing other services to write to an SNS topic</li></ul></li></ul><h2 id="sns-sqs-fan-out"><a class="markdownIt-Anchor" href="#sns-sqs-fan-out"></a> SNS + SQS: Fan out</h2><ul><li>push once in SNS, receive in all SQS queues that are subscribers</li><li>fully decoupled, no data loss</li><li>SQS allows for: data persistence, delayed processing and retries of work</li><li>ability to add more SQS subscribers over time</li><li>make sure your SQS queue access policy allows for SNS to write</li></ul><h2 id="s3-events-to-multiple-queues"><a class="markdownIt-Anchor" href="#s3-events-to-multiple-queues"></a> S3 events to multiple queues</h2><ul><li>for the same combination of: event type and prefix, you can only have one S3 event rule</li><li>if you want to send the same S3 event to many SQS queues, use fanout (SNS + SQS)</li></ul><h2 id="sns-fifo"><a class="markdownIt-Anchor" href="#sns-fifo"></a> SNS - FIFO</h2><ul><li>similar features as SQS FIFO<ul><li>ordering by message group ID</li><li>deduplication using a deduplication ID or Content based deduplication</li></ul></li><li>can only have SQS FIFO queues as subscribers</li><li>limited throughput (same throughput as SQS FIFO)</li></ul><h2 id="message-filtering"><a class="markdownIt-Anchor" href="#message-filtering"></a> message filtering</h2><ul><li>JSON policy used to filter messages sent to SNS topic’s subscriptions</li><li>if a subscription doesn’t have a filter policy, it receives every message</li></ul><h1 id="kinesis"><a class="markdownIt-Anchor" href="#kinesis"></a> Kinesis</h1><h2 id="kinesis-data-streams"><a class="markdownIt-Anchor" href="#kinesis-data-streams"></a> Kinesis data streams</h2><ul><li>billing is per shard provisioned, can have as many shards as you want</li><li>retention between 1 to 365 days</li><li>ability to reprocess data (because data will not be deleted by consumer, it stays in Kinesis data streams until retention period is over)</li><li>once data is inserted in Kinesis, it can’t be deleted (immutability)</li><li>data that shares the same partition goes to the same shard (shard level ordering)</li><li>producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis agent</li><li>consumers<ul><li>write your own: Kinesis Client Library (KCL), AWS SDK</li><li>managed: AWS lambda, Kinesis data firehose, Kinesis data analytics</li></ul></li></ul><h3 id="kinesis-data-streams-security"><a class="markdownIt-Anchor" href="#kinesis-data-streams-security"></a> Kinesis data streams security</h3><ul><li>control access / authorization using IAM policies</li><li>encryption in flight using HTTPS</li><li>encryption at rest using KMS</li><li>you can implement encryption / decryption of data on client side</li><li>VPC endpoints available for Kinesis to access within VPC (e.g. EC2 instance in private subnet access Kinesis data stream using VPC endpoint)</li><li>monitor API calls using CloudTrail</li></ul><h2 id="kinesis-consumers"><a class="markdownIt-Anchor" href="#kinesis-consumers"></a> Kinesis consumers</h2><h3 id="kinesis-consumer-types"><a class="markdownIt-Anchor" href="#kinesis-consumer-types"></a> Kinesis consumer types</h3><table><thead><tr><th>Shared fanout consumer - pull</th><th>enhanced fanout consumer - push</th></tr></thead><tbody><tr><td>low number of consuming applications</td><td>multiple consuming applications for the same stream</td></tr><tr><td>read throughput 2MB/ second per shard across all consumers</td><td>2 MB / second per consumer per shard</td></tr><tr><td>max 5 GetRecords API calls / sec</td><td>-</td></tr><tr><td>latency ~200ms</td><td>latency ~ 70ms</td></tr><tr><td>minimize cost</td><td>higher cost</td></tr><tr><td>consumers poll data from Kinesis using GetRecords API call</td><td>Kinesis push data to consumers over HTTP</td></tr><tr><td>returns up to 10MB or up to 10000 records</td><td>soft limit of 5 consumer applications per data stream</td></tr></tbody></table><h2 id="kinesis-client-library-kcl"><a class="markdownIt-Anchor" href="#kinesis-client-library-kcl"></a> Kinesis Client library (KCL)</h2><ul><li>a Java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload</li><li>each shard is to be read by only one KCL instance<ul><li>e.g. 4 shards =&gt; max 4 KCL instances</li></ul></li><li>progress is checkpointed into DynamoDB (needs IAM access from KCL instance to DynamoDB), this means if one KCL instance is down, DynamoDB will save the checkpoint and knows where to resume when KCL instance goes backup</li><li>track other workers and share the work amongst shards using DynamoDB</li><li>KCL can run on EC2, elastic Beanstalk and on premises</li><li>records are read in order at the shard level</li><li>versions<ul><li>KCL 1.x (supports shared consumer)</li><li>KCL 2.x (supports shared and enhanced fanout consumer)</li></ul></li></ul><h2 id="kinesis-operations"><a class="markdownIt-Anchor" href="#kinesis-operations"></a> Kinesis operations</h2><h3 id="shard-splitting"><a class="markdownIt-Anchor" href="#shard-splitting"></a> Shard splitting</h3><ul><li>used to increase the Stream capacity</li><li>used to divide a hot shard</li><li>the old shard is closed and will be deleted once the data is expired (until the retention period is over)</li><li>no automatic scaling (manually increase / decrease capacity)</li><li>can’t split into more than two shards in a single operation</li></ul><h3 id="merging-shards"><a class="markdownIt-Anchor" href="#merging-shards"></a> merging shards</h3><ul><li>decrease the Stream capacity and save costs</li><li>can be used to group two shards with low traffic</li><li>old shards are closed and will be deleted once the data is expired</li><li>can’t merge more than two shards in a single operation</li></ul><h2 id="kinesis-data-firehose"><a class="markdownIt-Anchor" href="#kinesis-data-firehose"></a> Kinesis data firehose</h2><ul><li>fully managed service, no administration, automatic scaling, serverless<ul><li>target: redshift, S3, ElasticSearch</li><li>third party</li><li>custom HTTP endpoint</li></ul></li><li>pay for data going through firehose</li><li>near real time<ul><li>60 seconds latency minimum for non full batches</li><li>or minimum 32 MB of data at a time</li><li>it is not real time because it will batch the data into 60 seconds of data or 32MB of data</li></ul></li><li>supports many data formats, conversions, transformations, compression</li><li>supports custom data transformations using AWS lambda</li><li>can send failed or all data to a backup S3 bucket</li></ul><h3 id="kinesis-data-streams-vs-firehose"><a class="markdownIt-Anchor" href="#kinesis-data-streams-vs-firehose"></a> Kinesis data streams vs Firehose</h3><table><thead><tr><th>Kinesis data streams</th><th>Kinesis data firehose</th></tr></thead><tbody><tr><td>streaming service for ingest at scale</td><td>load streaming data into S3 / redshift / ElasticSearch / Thrid party / custom HTTP</td></tr><tr><td>write custom code (producer / consumer)</td><td>fully managed</td></tr><tr><td>real time (~200ms)</td><td>near real time (60 seconds or 32MB)</td></tr><tr><td>manage scaling (shard spliting / shard merging)</td><td>automatic scaling</td></tr><tr><td>data storage for 1 to 365 days</td><td>no data storage</td></tr><tr><td>supports replay capability</td><td>doesn’t support replay capability</td></tr></tbody></table><h2 id="kinesis-data-analytics-sql-application"><a class="markdownIt-Anchor" href="#kinesis-data-analytics-sql-application"></a> Kinesis data analytics (SQL application)</h2><ul><li>perform real time analytics on Kinesis streams using SQL</li><li>fully managed, no server to provision</li><li>automatic scaling</li><li>real time analytics</li><li>pay for actual consumption rate</li><li>can create streams out of the real time queries</li><li>use cases<ul><li>time series analytics</li><li>real time dashboards</li><li>real time metrics</li></ul></li></ul><h2 id="sqs-vs-sns-vs-kinesis"><a class="markdownIt-Anchor" href="#sqs-vs-sns-vs-kinesis"></a> SQS vs SNS vs Kinesis</h2><h3 id="sqs-2"><a class="markdownIt-Anchor" href="#sqs-2"></a> SQS</h3><ul><li>consumer pull data</li><li>data is deleted after being consumed</li><li>can have as many as workers as we want</li><li>no need to provision throughput</li><li>ordering guarantees only on FIFO queues</li><li>individual message delay capability</li></ul><h3 id="sns-2"><a class="markdownIt-Anchor" href="#sns-2"></a> SNS</h3><ul><li>push data to many subscribers</li><li>data is not persisted (lost if not delivered)</li><li>pub/sub</li><li>no need to provision throughput</li><li>integrates with SQS for fanout architecture pattern</li><li>FIFO capability for SQS FIFO</li></ul><h3 id="kinesis-2"><a class="markdownIt-Anchor" href="#kinesis-2"></a> Kinesis</h3><ul><li>standard: pull data, 2 MB per shard</li><li>enhanced fanout: push data, 2 MB per shard per consumer</li><li>possibility to replay data</li><li>meant for real time big data, analytics and ETL</li><li>ordering at the shard level</li><li>data expires after X days</li><li>must provision throughput</li></ul><h2 id="kinesis-vs-sqs-ordering"><a class="markdownIt-Anchor" href="#kinesis-vs-sqs-ordering"></a> Kinesis vs SQS ordering</h2><ul><li>let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO</li><li>Kinesis data streams<ul><li>on average you will have 20 trucks per shard</li><li>trucks will have their data ordered within each shard</li><li>the maximum amount of consumer in parallel we can have is 5</li></ul></li><li>SQS FIFO<ul><li>you only have one SQS FIFO queue</li><li>you will have 100 group ID</li><li>you can have up to 100 consumers (due to the 100 group ID)</li><li>you have up to 300 message per second (or 3000 if using batching, because one GetRecords API call can receive up to 10 messages)</li></ul></li></ul><h1 id="lambda"><a class="markdownIt-Anchor" href="#lambda"></a> Lambda</h1><h2 id="what-is-serverless"><a class="markdownIt-Anchor" href="#what-is-serverless"></a> what is serverless</h2><ul><li>serverless is a new paradigm in which the developers don’t have to manage servers anymore</li><li>they just deploy code</li><li>serverless was pioneered by AWS lambda but now also includes anything that is managed: databases, messaging, storage, etc…</li><li>serverless does not mean there are no servers, it means you just don’t manage / provision / see them</li></ul><h3 id="serverless-in-aws"><a class="markdownIt-Anchor" href="#serverless-in-aws"></a> serverless in AWS</h3><ul><li>lambda</li><li>DynamoDB</li><li>Cognito</li><li>API Gateway</li><li>S3</li><li>SNS and SQS</li><li>Kinesis data firehose</li><li>Aurora serverless</li><li>Step functions</li><li>Fargate</li></ul><h2 id="lambda-synchronous-invocations"><a class="markdownIt-Anchor" href="#lambda-synchronous-invocations"></a> Lambda synchronous invocations</h2><ul><li>synchronous: CLI, SDK, API Gateway, ALB<ul><li>results is returned right away</li><li>error handling must happen client side</li></ul></li></ul><h2 id="lambda-integration-with-alb"><a class="markdownIt-Anchor" href="#lambda-integration-with-alb"></a> lambda integration with ALB</h2><ul><li>to expose a lambda function as an HTTP endpoint</li><li>you can use the ALB or an API gateway</li><li>the lambda function must be registered in a target group</li><li>ALB will convert the request HTTP to JSON and convert the response JSON to HTTP</li></ul><h3 id="alb-multi-healer-values"><a class="markdownIt-Anchor" href="#alb-multi-healer-values"></a> ALB multi healer values</h3><ul><li>ALB can support multi header values</li><li>when you enable multi value headers, HTTP headers and query string parametersthat are sent with multiple values are shown as arrays within the AWS lambda event and response objects</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HTTP</span><br><span class="line">http:&#x2F;&#x2F;example.com&#x2F;path?name&#x3D;foo&amp;name&#x3D;bar</span><br><span class="line"></span><br><span class="line">JSON</span><br><span class="line">&quot;queryStringParameters&quot;:&#123;&quot;name&quot;:[&quot;foo&quot;, &quot;bar&quot;]&#125;</span><br></pre></td></tr></table></figure><h2 id="lambdaedge"><a class="markdownIt-Anchor" href="#lambdaedge"></a> lambda@Edge</h2><ul><li><p>you have deployed a CDN using CloudFront</p></li><li><p>what if you wanted to run a global lambda alongside?</p></li><li><p>or how to implement request filtering before reaching your application?</p></li><li><p>for this, you can use Lambda@edge, deploy lambda functions alongside your CloudFront CDN</p><ul><li>build more responsive applications</li><li>you don’t manage servers, lambda is deployed globally</li><li>customize the CDN content</li><li>pay only for what you use</li></ul></li><li><p>you can use lambda to change CloudFront requests and responses</p><ul><li>after CloudFront receives a request from a viewer</li><li>before CloudFront forwards the request to the origin</li><li>after CloudFront receives the response from the origin</li><li>before CloudFront forwards the response to the viewer</li></ul></li><li><p>you can also generate responses to viewers without ever sending the request to the origin</p></li></ul><h2 id="lambda-asynchronous-invocations"><a class="markdownIt-Anchor" href="#lambda-asynchronous-invocations"></a> lambda - asynchronous invocations</h2><ul><li>S3, SNS, CloudWatch events</li><li>the events are placed in an event queue</li><li>lambda attempts to retry on errors<ul><li>3 tries total</li><li>1 minute after first, then 2 minutes wait</li></ul></li><li>make sure the processing is idempotent (result is the same after retry)</li><li>if the function is retried, you will see duplicate logs entries in CloudWatch logs</li><li>can define a DLQ - SNS or SQS - for failed processing (need correct IAM permissions for lambda to write to SQS)</li><li>asynchronous invocations allow you to speed up the processing if you don’t need to wait for the result</li></ul><h2 id="lambda-event-source-mapping"><a class="markdownIt-Anchor" href="#lambda-event-source-mapping"></a> lambda event source mapping</h2><ul><li>Kinesis data Streams and DynamoDB Streams</li><li>SQS and SQS FIFO queue</li><li>common denominator: records need to be pulled from the source</li><li>your lambda function is invoked synchronously</li></ul><h3 id="streams-and-lambda-kinesis-and-dynamodb"><a class="markdownIt-Anchor" href="#streams-and-lambda-kinesis-and-dynamodb"></a> Streams and lambda (Kinesis and DynamoDB)</h3><ul><li>an event source mapping creates an iterator for each shard, processes items in order</li><li>start with new items, from the beginning or from timestamp</li><li>processed items aren’t removed from the stream (other consumers can read them again)</li><li>if traffic is low, we can use batch window to accumulate records before processing</li><li>you can process multiple batches in parallel<ul><li>up to 10 batches per shard</li><li>in order processing is still guaranteed for each partition key</li></ul></li></ul><h3 id="streams-and-lambda-error-handling"><a class="markdownIt-Anchor" href="#streams-and-lambda-error-handling"></a> Streams and lambda - error handling</h3><ul><li>by default, if your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire</li><li>to ensure in order processing, processing for the affected shard is paused until the error is resolved</li><li>you can configure the event source mapping to<ul><li>discard old events</li><li>restrict the number of retries</li><li>split the batch on error (to work around lambda timeout issue, maybe there is not enough time to process the whole batch, so we split the batch to make it small and faster to process)</li></ul></li><li>discarded events can go to a Destination</li></ul><h3 id="sqs-and-sqs-fifo-with-lambda"><a class="markdownIt-Anchor" href="#sqs-and-sqs-fifo-with-lambda"></a> SQS and SQS FIFO with lambda</h3><ul><li><p>event source mapping will pull SQS (long polling)</p></li><li><p>specify batch size (1 to 10 messages)</p></li><li><p>recommended: set the queue visibility timeout to 6x the timeout of your lambda function</p></li><li><p>to use a DLQ:</p><ul><li>setup on the SQS queue, not lambda (DLQ for lambda is only for async invocations)</li><li>or use a lambda Destination for failures</li></ul></li><li><p>lambda also supports in order processing for FIFO queues, scaling up to the number of active message groups</p></li><li><p>for standard queues, items aren’t necessarily processed in order</p></li><li><p>lambda scales up to process a standard queue as quickly as possible</p></li><li><p>when an error occurs, batches are returned to the queue as individual items and might be processed in a different grouping than the original batch</p></li><li><p>occasionally, the event source mapping receive the same item from the queue twice, even if no function error occurred</p></li><li><p>lambda deletes items from the queue after they are processed successfully</p></li><li><p>you can configure the source queue to send items to a DLQ if they can’t be processed</p></li></ul><h3 id="lambda-event-mapper-scaling"><a class="markdownIt-Anchor" href="#lambda-event-mapper-scaling"></a> lambda event mapper scaling</h3><ul><li>Kinesis data streams and DynamoDB streams<ul><li>one lambda invocation per stream shard</li><li>if you use parallelization, up to 10 batches processed per shard simultaneously</li></ul></li><li>SQS standard<ul><li>lambda adds 60 more instances per minute to scale up</li><li>up to 1000 batches of messages processed simultaneously</li></ul></li><li>SQS FIFO<ul><li>messages with the same group ID will be processed in order</li><li>the lambda function scales to the number of active message groups</li></ul></li></ul><h2 id="lambda-destinations"><a class="markdownIt-Anchor" href="#lambda-destinations"></a> lambda - Destinations</h2><ul><li>for asynchronous invocations, we can define destinations for successful and failed event<ul><li>SQS</li><li>SNS</li><li>lambda</li><li>EventBridge bus</li></ul></li><li>note: AWS recommends you use Destinations instead of DLQ now (but both can be used at the same time)</li></ul><h2 id="lambda-permissions-iam-roles-and-resource-policies"><a class="markdownIt-Anchor" href="#lambda-permissions-iam-roles-and-resource-policies"></a> lambda permissions - IAM roles and resource policies</h2><ul><li>lambda execution role<ul><li>grants the lambda function permissions to AWS services / resources</li><li>when you use an event source mapping to invoke your function, lambda uses the execution role to read event data (e.g. lambda need permission to pull messages from SQS)</li></ul></li><li>lambda resource based policies<ul><li>use resource based policies to give other accounts and AWS services permission to use your lambda resources</li><li>similar to S3 bucket policies for S3 bucket</li><li>an IAM principal can access lambda<ul><li>if the IAM policy attached to the principal authorizes it (user access)</li><li>or if the resource based policy authorizes (service access)</li></ul></li><li>when an AWS service like S3 calls your lambda function, the resource based policy gives it access</li></ul></li></ul><h2 id="lambda-environment-variables"><a class="markdownIt-Anchor" href="#lambda-environment-variables"></a> lambda environment variables</h2><ul><li>environment variable = key / value pair in string form</li><li>adjust the function behavior without updating code</li><li>the environment variable are available to your code</li><li>lambda service adds its own system environment variables as well</li><li>helpful to store secrets (encrypted by KMS)</li><li>secrets can be encrypted by the lambda service key, or your own CMK</li></ul><h2 id="lambda-logging-and-monitoring"><a class="markdownIt-Anchor" href="#lambda-logging-and-monitoring"></a> lambda logging and monitoring</h2><ul><li>CLoudWatch logs<ul><li>lambda execution logs are stored in AWS CloudWatch logs</li><li>make sure your AWS lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch logs</li></ul></li><li>CLoudWatch metrics<ul><li>lambda metrics are displayed in AWS CloudWatch metrics</li><li>invocations, Durations, concurrent executions</li><li>error count, success rates, throttles</li><li>async delivery failures</li><li>iterator age (lagging for Kinesis and DynamoDB streams)</li></ul></li></ul><h3 id="lambda-tracing-with-x-ray"><a class="markdownIt-Anchor" href="#lambda-tracing-with-x-ray"></a> lambda tracing with X-Ray</h3><ul><li>enable in lambda configuration (active tracing)</li><li>runs the X-Ray daemon for you</li><li>use AWS X-Ray SDK in code</li><li>ensure lambda function has a correct IAM execution role to write to X-Ray<ul><li>the managed policy is called: <code>AWSXRayDaemonWriteAccess</code></li></ul></li></ul><h2 id="lambda-in-vpc"><a class="markdownIt-Anchor" href="#lambda-in-vpc"></a> lambda in VPC</h2><h3 id="lambda-by-default"><a class="markdownIt-Anchor" href="#lambda-by-default"></a> lambda by default</h3><ul><li>by default, your lambda function is launched outside your own VPC (in an AWS owned VPC)</li><li>therefore it cannot access resources in your VPC</li></ul><h3 id="lambda-in-vpc-2"><a class="markdownIt-Anchor" href="#lambda-in-vpc-2"></a> lambda in VPC</h3><ul><li>you must define the VPC ID, the subnets and the security groups</li><li>lambda will create an ENI in your subnets</li><li>lambda needs <code>AWSLambdaVPCAccessExecutionRole</code></li></ul><h3 id="internet-access"><a class="markdownIt-Anchor" href="#internet-access"></a> internet access</h3><ul><li>a lambda function in your VPC does not have internet access</li><li>deploying a lambda function in a public subnet does not give it internet access or a public IP</li><li>deploying a lambda function in a private subnet gives it internet access if you have a NAT gateway / NAT instance</li><li>you can use VPC endpoints to privately access AWS services without a NAT</li></ul><h2 id="lambda-function-performance"><a class="markdownIt-Anchor" href="#lambda-function-performance"></a> lambda function performance</h2><h3 id="configuration"><a class="markdownIt-Anchor" href="#configuration"></a> configuration</h3><ul><li>RAM<ul><li>from 128MB to 3008MB in 64MB increments</li><li>the more RAM you add, the more vCPU credits you get</li><li>at 1792MB, a function has the equivalent of one full vCPU</li><li>after 1792MB, you get more than one CPU, and need to use multi threading in your code to benefit from it</li></ul></li><li>if your application is CPU-bound (computation heavy), increase RAM</li><li>timeout: default 3 seconds, maximum is 900 seconds</li></ul><h3 id="lambda-execution-context"><a class="markdownIt-Anchor" href="#lambda-execution-context"></a> lambda execution context</h3><ul><li>the execution context is a temporary runtime environment that initialize any external dependencies of your lambda code</li><li>great for database connections, HTTP clients, SDK clients…</li><li>the execution context is maintained for some time in anticipation of another lambda function invocation</li><li>the next function invocation can reuse the context to execution time and save time in initializing connections objects (e.g. establish database connection outside of function handler)</li><li>the execution context includes the <code>/tmp</code> directory</li></ul><h3 id="lambda-function-tmp-space"><a class="markdownIt-Anchor" href="#lambda-function-tmp-space"></a> lambda function <code>/tmp</code> space</h3><ul><li>if your lambda function needs to download a big file to work</li><li>if your lambda function needs disk space to perform operations</li><li>you can use the <code>/tmp</code> directory</li><li>max size is 512 MB</li><li>the directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations (helpful to checkpoint your work)</li><li>for permanent persistence of object, use S3</li></ul><h2 id="lambda-concurrency"><a class="markdownIt-Anchor" href="#lambda-concurrency"></a> lambda concurrency</h2><ul><li>concurrency limit: up to 1000 concurrent executions across entire account, so if one of your lambda function takes up all the concurrencies (if you didn’t setup reserved concurrency limit), the other lambda functions will be throttled.</li><li>can set a reserved concurrency at the function level</li><li>each invocation over the concurrency limit will trigger a throttle</li><li>throttle behavior</li><li>if synchronous invocation = return throttle error 429</li><li>if asynchronous invocation = retry automatically and then go to DLQ</li><li>if you need a higher limit, open a support ticket</li></ul><h3 id="lambda-concurrency-and-asynchronous-invocations"><a class="markdownIt-Anchor" href="#lambda-concurrency-and-asynchronous-invocations"></a> lambda concurrency and asynchronous invocations</h3><ul><li>if the function doesn’t have enough concurrency available to process all events, additional requests are throttled</li><li>for throttling errors and system errors, lambda returns the event to the queue and attempts to run the funtion again for up to 6 hours</li><li>the retry interval increases exponentially from 1 second after the first attempt to a maximum of 5 minutes</li></ul><h3 id="cold-start-and-provisioned-concurrency"><a class="markdownIt-Anchor" href="#cold-start-and-provisioned-concurrency"></a> Cold start and provisioned concurrency</h3><ul><li>cold start<ul><li>new instance =&gt; code is loaded and code outside the handler run (init)</li><li>if the init is large, this process can take some time</li><li>first request served by new instances has higher latency than the rest</li></ul></li><li>provisioned concurrency<ul><li>concurrency is allocated before the function is invoked (in advance)</li><li>so the cold start never happens and all invocations have low latency</li><li>application auto scaling can manage concurrency</li></ul></li></ul><h2 id="lambda-external-dependencies"><a class="markdownIt-Anchor" href="#lambda-external-dependencies"></a> lambda external dependencies</h2><ul><li>if your lambda function depends on external libraries<ul><li>for example AWS X-Ray SDK, database client, etc…</li></ul></li><li>you need to install the packages alongside your code and zip it together</li><li>upload the zip straight to lambda if less than 50MB, else to S3 first and reference from S3</li><li>native libraries work: they need to be complied on Amazon Linux</li><li>AWS SDK comes by default with every lambda function</li></ul><h2 id="lambda-and-cloudformation"><a class="markdownIt-Anchor" href="#lambda-and-cloudformation"></a> lambda and CloudFormation</h2><h3 id="inline"><a class="markdownIt-Anchor" href="#inline"></a> inline</h3><ul><li>inline functions are very simple</li><li>use the code.zipfile property</li><li>you cannot include function dependencies with inline functions</li></ul><h3 id="through-s3"><a class="markdownIt-Anchor" href="#through-s3"></a> through S3</h3><ul><li>you must store the lambda zip in S3</li><li>you must refer the S3 zip location in the CloudFormation code<ul><li>S3 bucket</li><li>S3 key: full path to zip</li><li>S3 object version: if versioned bucket</li></ul></li><li>if you update the code in S3, but don’t update S3 bucket, S3 key or S3 object version, CloudFormation won’t update your function because it will not detect the change</li></ul><h2 id="lambda-layers"><a class="markdownIt-Anchor" href="#lambda-layers"></a> lambda layers</h2><ul><li>externalize dependencies to re use them</li></ul><h2 id="lambda-container-images"><a class="markdownIt-Anchor" href="#lambda-container-images"></a> lambda container images</h2><ul><li>deploy lambda function as container images of up to 10GB from ECR</li><li>pack complex dependencies, large dependencies in a container</li><li>base images are available</li><li>can create your own image as long as it implements the lambda runtime API</li><li>test the containers locally using the lambda runtime interface emulator</li><li>unified workflow to build apps</li></ul><h2 id="lambda-versions-and-aliases"><a class="markdownIt-Anchor" href="#lambda-versions-and-aliases"></a> lambda versions and aliases</h2><h3 id="lambda-versions"><a class="markdownIt-Anchor" href="#lambda-versions"></a> lambda versions</h3><ul><li>when you work on a lambda function, we work on <code>$LATEST</code>, which is an unpublished mutable version</li><li>when we are ready to publish a lambda function, we create a version</li><li>versions are immutable</li><li>versions have increasing version numbers</li><li>versions get their own ARN</li><li>version = code + configuration</li><li>each version of the lambda function can be accessed</li></ul><h3 id="lambda-aliases"><a class="markdownIt-Anchor" href="#lambda-aliases"></a> lambda aliases</h3><ul><li>aliases are pointers to lambda function versions</li><li>we can define a dev, test, prod aliases and have them point at different lambda versions</li><li>aliases are mutable</li><li>aliases enable Blue / Green deployment by assigning weights to lambda functions</li><li>aliases enable stable configuration of our event triggers / destinations</li><li>aliases have their own ARNs</li><li>aliases cannot reference other aliases</li></ul><h2 id="lambda-and-codedeploy"><a class="markdownIt-Anchor" href="#lambda-and-codedeploy"></a> lambda and CodeDeploy</h2><ul><li>CodeDeploy can help you automate traffic shift for lambda aliases</li><li>feature is integrated within the SAM framework</li><li>linear<ul><li>grow traffic every N minutes until 100%</li></ul></li><li>canary<ul><li>try X percent then 100%</li></ul></li><li>AllAtOnce<ul><li>immediate</li></ul></li><li>can create pre and post traffic hooks to check the health of the lambda function</li></ul><h2 id="lambda-limits-good-to-know-per-region"><a class="markdownIt-Anchor" href="#lambda-limits-good-to-know-per-region"></a> lambda limits good to know - per region</h2><ul><li>memory allocation: 128MB - 10 GB</li><li>maximum execution time: 15 minutes</li><li>environment variables: 4KB</li><li>disk capacity in the function container in <code>/tmp</code>: 512 MB</li><li>concurrency executions: 1000</li><li>lambda function deployment size(zipped): 50MB</li><li>size of uncompressed deployment(code + dependencies): 250MB</li><li>can use the <code>/tmp</code> directory to load other files at startup</li></ul><h2 id="lambda-best-practices"><a class="markdownIt-Anchor" href="#lambda-best-practices"></a> lambda best practices</h2><ul><li>perform heavy duty work outside of your function handler<ul><li>connect to databases</li><li>initilize the SDK</li><li>pull in dependencies</li></ul></li><li>use environment variables for<ul><li>database connection sttrings, S3 buckets, etc…</li><li>passwords, sensitive values</li></ul></li><li>minimize your deployment package size to its runtime necessities<ul><li>break down the function</li><li>remember lambda limits</li><li>use Layers where necessary</li></ul></li><li>aviod using recursive code, never have a lambda function call itself</li></ul><h1 id="dynamodb"><a class="markdownIt-Anchor" href="#dynamodb"></a> DynamoDB</h1><h2 id="nosql-database"><a class="markdownIt-Anchor" href="#nosql-database"></a> NoSQL database</h2><ul><li>non-relational databases and are distributed</li><li>include MongoDB, DynamoDB…</li><li>do not support query joins (or just limited support)</li><li>all the data that is needed for a query is present in one row</li><li>don’t perform aggregations such as SUM, AVG…</li><li>scale horizontally</li><li>there is no right or wrong for NoSQL or SQL, they just require to model the data differently and think about user queries differently</li></ul><h2 id="amazon-dynamodb"><a class="markdownIt-Anchor" href="#amazon-dynamodb"></a> Amazon DynamoDB</h2><ul><li>fully managed, highly available with replication across multiple AZ</li><li>NoSQL database</li><li>scales to massive workloads, distributed database</li><li>millions of requests per second, trillions of row, 100s of TB of storage</li><li>fast and consistent in performance (low latency on retrieval)</li><li>integrated with IAM for security, authorization and administration</li><li>enables event driven programming with DynamoDB streams</li><li>low cost and auto scaling capabilities</li></ul><h2 id="basics"><a class="markdownIt-Anchor" href="#basics"></a> basics</h2><ul><li><p>DynamoDB is made of Tables</p></li><li><p>each table has a Primary Key (must be decided at creation time)</p></li><li><p>each table can have an infinite number of items</p></li><li><p>each item has attributes (can be added over time - can be null)</p></li><li><p>maximum size of an item is 400KB</p></li><li><p>data types supported are:</p><ul><li>scalar types: String, Number, Binary, Boolean, Null</li><li>Document types: List, Map</li><li>Set Types: String Set, Number Set, Binary Set</li></ul></li><li><p>Primary keys</p><ul><li>Partition Key (HASH)<ul><li>partition key must be unique for each item</li><li>partition key must be diverse so that the data is distributed</li></ul></li><li>Partition Key + Sort Key (HASH + RANGE)<ul><li>the combination must be unique for each item</li><li>data is grouped by partition key</li></ul></li></ul></li></ul><h2 id="read-write-capacity-modes"><a class="markdownIt-Anchor" href="#read-write-capacity-modes"></a> Read / Write capacity modes</h2><ul><li>control how you manage your table’s capacity</li><li>provisioned mode (default)<ul><li>you specify the number of reads/ writes per second</li><li>you need to plan capacity beforehand</li><li>pay for provisioned read / write capacity units</li></ul></li><li>on demand mode<ul><li>read / writes automatically scale up / down with your workloads</li><li>no capacity planning needed</li><li>pay for what you use, more expensive</li></ul></li><li>you can switch between different modes once every 24 hours</li></ul><h2 id="rw-capacity-modes-provisioned"><a class="markdownIt-Anchor" href="#rw-capacity-modes-provisioned"></a> R/W capacity modes - provisioned</h2><ul><li>table must have provisioned read an dwrite capacity units</li><li>read capacity units (RCU)</li><li>write capacity units</li><li>option to setup auto scaling of throughput to meet demand</li><li>throughput can be exceeded temporarily using brust capacity</li><li>if burst capacity has been consumed, you will get a <code>ProvisionedThroughpuutExceededException</code></li><li>it is then advised to do an exponential backoff retry</li></ul><h2 id="write-capacity-units-wcu"><a class="markdownIt-Anchor" href="#write-capacity-units-wcu"></a> Write Capacity units (WCU)</h2><ul><li>one WCU represents one write per second for an item up to 1KB in size</li><li>if the items are larget then 1 KB, more WCUs are consumed</li></ul><h2 id="strongly-consistent-read-vs-eventually-consistent-read"><a class="markdownIt-Anchor" href="#strongly-consistent-read-vs-eventually-consistent-read"></a> Strongly consistent read vs Eventually consistent read</h2><ul><li>Eventually consistent read (default)<ul><li>if we read just after a write, it is possible we will get some stale data because of replication</li></ul></li><li>Strongly consistent read<ul><li>if we read just after a write, we will get the correct data</li><li>set ConsistentRead parameter to True in API calls</li><li>consumes twice the RCU</li></ul></li></ul><h2 id="read-capacity-units-rcu"><a class="markdownIt-Anchor" href="#read-capacity-units-rcu"></a> Read capacity units (RCU)</h2><ul><li>one RCU represents one Strongly Consistent Read per second, or two Eventually consistent reads per second, for an item up to 4KB</li><li>if the items are larger than 4KB, more RCUs are consumed</li></ul><h2 id="paritions-internal"><a class="markdownIt-Anchor" href="#paritions-internal"></a> Paritions Internal</h2><ul><li>data is stored in partitions</li><li>partition keys go through a hashing algorithm to know to which partition they go to</li><li>WCUs and RCUs are spread evenly across partitions</li></ul><h2 id="throttling"><a class="markdownIt-Anchor" href="#throttling"></a> Throttling</h2><ul><li>if we exceed provisioned RCUs or WCUs, we get <code>ProvisionedThroughputExceededException</code></li><li>reasons<ul><li>hot keys: one partition key is being read too many times (popular item)</li><li>hot partitions</li><li>very large items, remember RCU and WCU depends on size of items</li></ul></li><li>solutions<ul><li>exponential backoff when exception is encountered</li><li>distribute partition keys as much as possible</li><li>if RCU issue, we can use DynamoDB Accelerator (DAX)</li></ul></li></ul><h2 id="on-demand"><a class="markdownIt-Anchor" href="#on-demand"></a> on demand</h2><ul><li>Read and writes automatically scale up and down with your workloads</li><li>no capacity planning needed</li><li>unlimited WCU and RCU, no throttle, more expensive</li><li>you are charged for reads and writes that you use in terms of RRU and WRU</li><li>read request units (RRU) - throughput for reads (same as RCU)</li><li>write request units (WRU) - throughput for writes (same as WCU)</li><li>2.5x more expensive than provisioned capacity</li><li>use cases: unknown workloads, unpredictable application traffic…</li></ul><h2 id="writing-data"><a class="markdownIt-Anchor" href="#writing-data"></a> writing data</h2><ul><li>PutItem<ul><li>creates a new item or fully replace an old item</li><li>consumers WCUs</li></ul></li><li>UpdateItem<ul><li>edits an existing item’s attributes or adds a new item if it doesn’t exist</li><li>can be used to implement Atomic Counters - a numeric attribute that is unconditionally incremented</li></ul></li><li>conditional writes<ul><li>accept a write / update / delete only if conditions are met, otherwise returns an error</li><li>helps with concurrent access to items</li><li>no performance impact</li></ul></li></ul><h2 id="reading-data"><a class="markdownIt-Anchor" href="#reading-data"></a> reading data</h2><ul><li>GetItem<ul><li>read based on primary key</li><li>primary key can be HASH or HASH + RANGE</li><li>eventually consistent read</li><li>option to use strongly consistent reads (more RCU - might take longer)</li><li>ProjectionExpression can be specified to retrieve only certain attributes</li></ul></li></ul><h2 id="reading-data-query"><a class="markdownIt-Anchor" href="#reading-data-query"></a> reading data - query</h2><ul><li>query returns items based on<ul><li>KeyConditionExpression<ul><li>partition key value - required</li><li>sort key value = optional</li></ul></li><li>FilterExpression<ul><li>additional filtering after the query operation (before data returned to you)</li><li>use only with non key attributes</li></ul></li></ul></li><li>returns<ul><li>the number of items specified in limit</li><li>or up to 1 MB of data</li></ul></li><li>ability to do pagination on the results</li><li>can query table, a local secondary index, or a global secondary index</li></ul><h2 id="reading-data-scan"><a class="markdownIt-Anchor" href="#reading-data-scan"></a> reading data - scan</h2><ul><li>scan the entire table and then filter out data (inefficient)</li><li>returns up to 1 MB of data - use pagination to keep on reading</li><li>consumes a lot of RCU</li><li>limit impact using Limit or reduce the size of the result and pause</li><li>for faster performance, use parallel scan<ul><li>multiple workers scan multiple data segments at the same time</li><li>increases the throughput and RCU consumed</li><li>limit the impact of parallel scans just like you would for Scans</li></ul></li><li>can use ProjectionExpression and FilterExpression</li><li>filtering will be done at the client side (e.g. in the browser)</li></ul><h2 id="deleting-data"><a class="markdownIt-Anchor" href="#deleting-data"></a> deleting data</h2><ul><li>DeleteItem<ul><li>delete an individual item</li><li>ability to perform a conditional delete</li></ul></li><li>DeleteTable<ul><li>delete a whole table and all its items</li><li>much quicker deletion than calling DeleteItem on all items</li></ul></li></ul><h2 id="batch-operations"><a class="markdownIt-Anchor" href="#batch-operations"></a> batch operations</h2><ul><li>allows you to save in latency by reducing the number of API calls</li><li>operations are done in parallel for better efficiency</li><li>part of a batch can fail, in which case we need to try again for the failed items</li><li>BatchWriteItem<ul><li>up to 25 PutItem and DeleteItem in one call</li><li>up to 16 MB of data written, up to 400KB of data per item</li><li>can’t update items</li></ul></li><li>BatchGetItem<ul><li>return items from one or more tables</li><li>up to 100 items, up to 16 MB of data</li><li>items are retrieved in parallel to minimize latency</li></ul></li></ul><h2 id="local-secondary-index-lsi"><a class="markdownIt-Anchor" href="#local-secondary-index-lsi"></a> Local Secondary Index (LSI)</h2><ul><li>alternative sort key for your table (use the same partition key)</li><li>the sort key consists of one scalar attribute</li><li>up to 5 local secondary indexes per table</li><li>must be defined at table creation time</li><li>attribute projections - can contain some or all the attributes of the base table</li></ul><h2 id="global-secondary-index-gsi"><a class="markdownIt-Anchor" href="#global-secondary-index-gsi"></a> Global secondary index (GSI)</h2><ul><li>alternative Primary key (HASH + HASH + RANGE) from the base table</li><li>speed up queries on non key attributes</li><li>the index key consists of scalar attributes</li><li>attribute projections - some or all the attributes of the base table</li><li>must provision RCUs and WCUs for the index</li><li>can be added / modified after table creation</li></ul><h2 id="indexes-and-throttling"><a class="markdownIt-Anchor" href="#indexes-and-throttling"></a> indexes and throttling</h2><ul><li>GSI<ul><li>if the writes are throttled on the GSI, then the main table will be throttled</li><li>even if the WCU on the main tables are fine</li><li>choose your GSI partition key carefully</li><li>assign your WCU capacity carefully</li></ul></li><li>LSI<ul><li>uses the WCUs and RCUs of the main table</li><li>no special throttling considerations</li></ul></li></ul><h2 id="optimistic-locking"><a class="markdownIt-Anchor" href="#optimistic-locking"></a> Optimistic locking</h2><ul><li>DynamoDB has a feature called Conditional Writes</li><li>a strategy to ensure an item hasn’t changed before you update / delete it</li><li>each item has an attribute that acts as a version number, and each update / delete request will change the value of the item, and also update the version number</li><li>if two request send at the same time, only one will succeed because the second request will not try to change the item because the version is different already.</li></ul><h2 id="dynamodb-dax"><a class="markdownIt-Anchor" href="#dynamodb-dax"></a> DynamoDB DAX</h2><ul><li>fully managed, highly available, seamless in memory cache for DynamoDB</li><li>microseconds latency for cached reads and queries</li><li>doesn’t require application logic modification</li><li>solves the hot key problem (too many reads)</li><li>5 minutes TTL for cache (default)</li><li>up to 10 nodes in the cluster</li><li>multi AZ</li><li>secure</li></ul><h3 id="dax-vs-elasticache"><a class="markdownIt-Anchor" href="#dax-vs-elasticache"></a> DAX vs ElastiCache</h3><ul><li>DAX is for individual object cache and simple query and scan</li><li>ElastiCache can store aggregation result and complex intermediate results</li></ul><h2 id="dynamodb-streams"><a class="markdownIt-Anchor" href="#dynamodb-streams"></a> DynamoDB Streams</h2><ul><li><p>ordered stream of item level modifications in a table</p></li><li><p>stream records can be</p><ul><li>sent to Kinesis Data Streams</li><li>read by AWS lambda</li><li>read by Kinesis Client Linrary applications</li></ul></li><li><p>data retention for up to 24 hours</p></li><li><p>use case</p><ul><li>react to changes in real time</li><li>analytics</li><li>insert into derivative tables</li><li>insert into ElasticSearch</li><li>implement cross region replication</li></ul></li><li><p>ability to choose the information that will be written to the stream</p><ul><li>KEYS_ONLY - only the key attributes of the modified item</li><li>NEW_IMAGE - the entire item, as it appears after it was modified</li><li>OLD_IMGAE - the entire item, as it appeared before it was modified</li><li>NEW_AND_OLD_IMAGES - both the new and old images of the item</li></ul></li><li><p>DynamoDB streams are made of shards, just like Kinesis Data Streams, so Kinesis KCL can be the consumer for DynamoDB Streams</p></li><li><p>you don’t need to provision shards, this is automated by AWS</p></li><li><p>records are not retroactively populated in a stream after enabling it</p></li></ul><h3 id="streams-and-lambda"><a class="markdownIt-Anchor" href="#streams-and-lambda"></a> Streams and lambda</h3><ul><li>you need to define an Event Source Mapping to read from DynamoDB streams</li><li>you need to ensure the lambda function has the appropriate permissions</li><li>your lambda function is invoked synchronously</li></ul><h2 id="dynamodb-ttl"><a class="markdownIt-Anchor" href="#dynamodb-ttl"></a> DynamoDB TTL</h2><ul><li>automatically delete items after an expiry timestamp</li><li>doesn’t consume any WCUs</li><li>the TTL attribute must be a number data type with Unix Epoch timestamp value</li><li>expired items deleted within 48 hours of expiration</li><li>expired items that haven’t been deleted, appears in reads/queries/scans (if you don’t want them, filter them out)</li><li>expired items are deleted from both LSIs and GSIs</li><li>a delete operation for each expired item enters the DynamoDB streams (can help recover expired items)</li><li>use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, user sessions…</li></ul><h2 id="dynamodb-cli"><a class="markdownIt-Anchor" href="#dynamodb-cli"></a> DynamoDB CLI</h2><ul><li><code>--projection-expression</code>: one or more attributes to retrieve</li><li><code>--filter-expression</code>: filter items before returned to you</li><li>general CLI pagination options<ul><li><code>--page-size</code>: specify that CLI retrieves the full list of items but with a larger number of API calls instead of one API call</li><li><code>--max-items</code>: max number of items to show in the CLI (returns NextToken)</li><li><code>--starting-token</code>: specify the last NextToken to retrieve the next set of items</li></ul></li></ul><h2 id="dynamodb-transactions"><a class="markdownIt-Anchor" href="#dynamodb-transactions"></a> DynamoDB transactions</h2><ul><li>coordinated, all or nothing opeartions to multiple items across one or more tables</li><li>provides Atomicity, Consistency, Isolation, and Durability (ACID)</li><li>read modes - Eventual consistency, strong consistency, transactional</li><li>write modes - standard, transactional</li><li>consumers 2x WCUs and 2x RCUs</li><li>two operations<ul><li>TransactGetItems - one or more GetItem operations</li><li>TransactWriteItems - one or more PutItem, UpdateItem, DeleteItem operations</li></ul></li><li>use cases: financial transactions, managing orders, multi player games…</li></ul><h2 id="dynamodb-session-state-cache"><a class="markdownIt-Anchor" href="#dynamodb-session-state-cache"></a> DynamoDB Session State Cache</h2><ul><li>it is common to use DynamoDB to store session state</li><li>vs ElastiCache<ul><li>ElastiCache is in memory, but DynamoDB is serverless with auto scaling</li><li>both are key value pairs</li></ul></li><li>vs EFS<ul><li>EFS must be attached to EC2 instances as a network drive</li></ul></li><li>vs EBS and Instance store<ul><li>EBS and Instance store can only be used for local caching, not shared caching</li></ul></li><li>vs S3<ul><li>S3 is higher latency, and not meant for small objects</li></ul></li></ul><h2 id="dynamodb-security-and-other-features"><a class="markdownIt-Anchor" href="#dynamodb-security-and-other-features"></a> DynamoDB Security and other features</h2><ul><li>security<ul><li>VPC endpoints available to access DynamoDB without using the internet</li><li>access fully controled by IAM</li><li>encryption at rest using KMS and in transit using SSL/TLS</li></ul></li><li>backup and restore feature available<ul><li>point in time recovery (PITR) like RDS</li><li>no performance impact</li></ul></li><li>global tables<ul><li>multi region, multi active, fully replicated, high performance, need to enable DynamoDB streams first</li></ul></li><li>DynamoDB local<ul><li>develop and test apps locally without accessing the DynamoDB web service (without internet)</li></ul></li><li>AWS database migration service can be used to migrate to DynamoDB</li></ul><h3 id="fine-grained-access-control"><a class="markdownIt-Anchor" href="#fine-grained-access-control"></a> Fine-Grained access control</h3><ul><li>using web identity federation or cognito identity pools, each user gets AWS credentials</li><li>you can assign an IAM role to these users with a condition to limit their API access to DynamoDB</li><li>Leading Keys - limit row level access for users on the primary key</li><li>Attributes - limit specific attributes the user can see</li></ul><h1 id="api-gateway"><a class="markdownIt-Anchor" href="#api-gateway"></a> API Gateway</h1><h2 id="integrations-high-level"><a class="markdownIt-Anchor" href="#integrations-high-level"></a> Integrations high level</h2><ul><li>lambda function<ul><li>invoke lambda function</li><li>easy way to expose REST API backed by lambda</li></ul></li><li>HTTP<ul><li>expose HTTP endpoints in the backend</li><li>why? add rate limiting, caching, user authentications, API keys, etc…</li></ul></li><li>AWS service<ul><li>expose any API through the API Gateway</li><li>example: Step function workflow, post a message to SQS</li><li>why? add authentication, deploy publicly, rate control…</li></ul></li></ul><h2 id="endpoint-types"><a class="markdownIt-Anchor" href="#endpoint-types"></a> endpoint types</h2><ul><li>Edge-Optimized (default): for global clients<ul><li>requests are routed through the CloudFront Edge locations</li><li>the API Gateway still lives in only one region</li></ul></li><li>regional<ul><li>for clients within the same region</li><li>could manually combine with CloudFront (more control over the caching strategies and the distribution)</li></ul></li><li>private<ul><li>can only be accessed from your VPC using an interface VPC endpoint (ENI)</li><li>use a resource policy to define access</li></ul></li></ul><h2 id="deployment-stages"><a class="markdownIt-Anchor" href="#deployment-stages"></a> Deployment stages</h2><ul><li>making changes in the API Gateway does not mean they are effective</li><li>you need to make a deployment for them to be in effect</li><li>changes are deployed to Stages (as many as you want)</li><li>use the naming you like for stages (dev, test, prod)</li><li>each stage has its own configuration parameters</li><li>stages can be rolled back as a history of deployments is kept</li></ul><h3 id="stage-variables"><a class="markdownIt-Anchor" href="#stage-variables"></a> stage variables</h3><ul><li>stage variables are like environment variables for API Gateway</li><li>use them to change often changing configuration values</li><li>they can be used in<ul><li>lambda function ARN</li><li>HTTP endpoint</li><li>parameter mapping templates</li></ul></li><li>use cases:<ul><li>configure HTTP endpoints your stages talk to</li><li>pass configuration parameters to lambda through mapping templates</li></ul></li><li>stage variables are passed to the context object in lambda</li></ul><h3 id="stage-variables-with-lambda-aliases"><a class="markdownIt-Anchor" href="#stage-variables-with-lambda-aliases"></a> stage variables with lambda aliases</h3><ul><li>we can create a stage variable to indicate the corresponding lambda alias</li><li>our API gateway will automatically invoke the right lambda function</li></ul><h2 id="canary-deployment"><a class="markdownIt-Anchor" href="#canary-deployment"></a> canary deployment</h2><ul><li>possibility to enable canary deployments for any stage</li><li>choose the percentage of traffic the canary channel receives</li><li>metrics and logs are separate (for better monitoring)</li><li>possiblity to override stage variables for canary</li><li>this is Blue / Green deployment with lambda and API gateway</li></ul><h2 id="api-gateway-integration-types"><a class="markdownIt-Anchor" href="#api-gateway-integration-types"></a> API Gateway - Integration types</h2><ul><li>MOCK<ul><li>API Gateway returns a response without sending the request to the backend (for testing and dev purpose)</li></ul></li><li>HTTP / AWS<ul><li>you must configure both the integration request and integration response</li><li>setup data mapping using mapping templates for the request and response</li></ul></li><li>AWS_PROXY (lambda proxy)<ul><li>incoming request from the client is the input to lambda</li><li>the function is responsible for the logic of request / response</li><li>no mapping template, headers, query string parameters</li></ul></li><li>HTTP_PROXY<ul><li>no mapping template</li><li>the HTTP request is passed to the backend</li><li>the HTTP response from the backend is forwarded by API gateway</li></ul></li></ul><h3 id="mapping-template"><a class="markdownIt-Anchor" href="#mapping-template"></a> Mapping template</h3><ul><li>mapping templates can be used to modify request / response</li><li>rename and modify query string parameters</li><li>modify body content</li><li>add headers</li><li>uses Velocity template language</li><li>filter output results</li></ul><h3 id="mapping-template-json-to-xml-with-soap"><a class="markdownIt-Anchor" href="#mapping-template-json-to-xml-with-soap"></a> Mapping template: JSON to XML with SOAP</h3><ul><li>SOAP API are XML based, whereas REST API are JSON based</li><li>in this case, API gateway should<ul><li>extract data from the request: either path, payload or header</li><li>build SOAP message based on request data (mapping template)</li><li>call SOAP service and receive XML response</li><li>transform XML response to desired format and respond to the user</li></ul></li></ul><h2 id="api-gateway-swagger-open-api-spec"><a class="markdownIt-Anchor" href="#api-gateway-swagger-open-api-spec"></a> API Gateway Swagger / Open API spec</h2><ul><li>common way of defining REST APIs, using API defintion as code</li><li>import existing Swagger / OpenAPI 3.0 spec to API Gateway<ul><li>method</li><li>method request</li><li>integration request</li><li>method response</li><li>extensions for API Gateway and setup every single option</li></ul></li><li>can export current API as Swagger / OpenAPI spec</li><li>swagger can be written in YAML or JSON</li></ul><h2 id="caching-api-response"><a class="markdownIt-Anchor" href="#caching-api-response"></a> Caching API response</h2><ul><li>caching reduces the number of calls made to the backend</li><li>default TTL is 300 seconds</li><li>caches are defined per stage</li><li>possible to override cache settings per method</li><li>cache encryption option</li><li>cache capacity between 0.5 to 237 GB</li><li>cache is expensive, makes sense in production, may not make sense in dev and test</li></ul><h3 id="api-gateway-cache-invalidation"><a class="markdownIt-Anchor" href="#api-gateway-cache-invalidation"></a> API Gateway cache invalidation</h3><ul><li>able to flush the entire cache immediately</li><li>clients can invalidate the cache with header: <code>Cache-Control: max-age=0</code> (with proper IAM authorization)</li><li>if you don’t impose an InvalidateCache policy or choose the require authorization check box in the console, any client can invalidate the API cache, which is not good.</li></ul><h2 id="usage-plan-and-api-keys"><a class="markdownIt-Anchor" href="#usage-plan-and-api-keys"></a> Usage plan and API keys</h2><ul><li>if you want to make an API available as an offering to your customers</li><li>usage plan<ul><li>who can access one or more deployed API stages and methods</li><li>how much and how fast they can access them</li><li>uses API keys to identify API clients and meter access</li><li>configure throttling limits and quota limits that are enforced on individual client</li></ul></li><li>API keys<ul><li>alphanumberic string values to distribute to your customers</li><li>can use with usage plans to control access</li><li>throttling limits are applied to API keys</li><li>quotas limits is the overall number of maximum requests</li></ul></li></ul><h2 id="logging-and-tracing"><a class="markdownIt-Anchor" href="#logging-and-tracing"></a> logging and tracing</h2><ul><li>CloudWatch logs<ul><li>enable CloudWatch logging at the stage level</li><li>can override settings on a per API basis</li><li>log contains information about request / response body</li></ul></li><li>X-Ray<ul><li>enable tracing to get extra information about requests in API gateway</li><li>X-Ray API Gateway + Lambda gives you the full picture</li></ul></li></ul><h2 id="cloudwatch-metrics-2"><a class="markdownIt-Anchor" href="#cloudwatch-metrics-2"></a> CloudWatch metrics</h2><ul><li>metrics are by stage, possiblity to enable detailed metrics</li><li>CacheHitCount and CacheMissCount: efficiency of the cache</li><li>Count: the total number of API requests in a given period</li><li>IntegrationLatency: the time between when API Gateway relays a request to the backend and when receives a response from the backend</li><li>Latency: the time between when API gateway receives a request from a client and when it returns a response to the client, the latency includes the integration latency and other API gateway overhead</li><li>4xx Error (client side) and 5xx error (server side)</li></ul><h2 id="throttling-2"><a class="markdownIt-Anchor" href="#throttling-2"></a> throttling</h2><ul><li>account limit<ul><li>API gateway throttles requests at 10000 rps across all API</li><li>soft limit that can be increased upon request</li></ul></li><li>in case of throttling = 429 too many requests</li><li>can set stage limit and method limits to improve performance</li><li>or you can define usage plans to throttle per customer</li><li>just like lambda concurrency, one API that is overloaded, if not limited, can cause the other APIs to be throttled too.</li></ul><h2 id="cors-2"><a class="markdownIt-Anchor" href="#cors-2"></a> CORS</h2><ul><li>CORS must be enabled when you receive API calls from another domain</li><li>the OPTIONS pre flight request must contain the following headers<ul><li>Access-Control-Allow-Methods</li><li>Access-Control-Allow-Headers</li><li>Access-Control-Allow-Origin</li></ul></li><li>CORS can be enabled through the console</li></ul><h2 id="authentication-and-authorization"><a class="markdownIt-Anchor" href="#authentication-and-authorization"></a> Authentication and Authorization</h2><ul><li>IAM<ul><li>great for users already within your AWS accounts + resource policy for cross account</li></ul></li><li>Custom Authorizer<ul><li>great for third party tokens</li><li>very flexible in terms of what IAM policy is returned</li></ul></li><li>Cognito User Pool<ul><li>you manage your own user pool</li><li>no need to write any custom code</li><li>must implement authorization in the backend</li></ul></li></ul><h2 id="websocket-api"><a class="markdownIt-Anchor" href="#websocket-api"></a> WebSocket API</h2><ul><li>what is WebSocket<ul><li>two way interactive communication between a user’s browser and a server</li><li>server can push information to the client</li><li>this enables stateful application use cases</li></ul></li><li>WebSocket APIs are often used in real time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms</li><li>works with AWS services (lambda, DynamoDB) or HTTP endpoints</li></ul><h3 id="routing"><a class="markdownIt-Anchor" href="#routing"></a> Routing</h3><ul><li>incoming JSON messages are routed to different backend</li><li>if no routes =&gt; send to default</li><li>you request a route selection expression to select the field on JSON to route from</li><li>the result is evaluated against the route keys available in your API gateway</li><li>the route is then connceted to the backend you have setup through API gateway</li></ul><h2 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h2><ul><li>create a single interface for all the microservices in your company</li><li>use API endpoints with various resources</li><li>apply a simple domain name and SSL certificates</li><li>can apply forwarding and transformation rules at the API gateway level</li></ul><h1 id="sam-serverless-application-model"><a class="markdownIt-Anchor" href="#sam-serverless-application-model"></a> SAM (serverless application model)</h1><ul><li>framework for developing and deploying serverless applications</li><li>all the configurations is YAML code</li><li>generate complex CloudFormation from simple SAM YAML file</li><li>supports anything from CLoudFormation</li><li>only two commmands to deploy to AWS</li><li>SAM can use CodeDeploy to deploy lambda functions</li><li>SAM can help you to run lambda, API gateway, DynamoDB locally</li></ul><h2 id="recipe"><a class="markdownIt-Anchor" href="#recipe"></a> Recipe</h2><ul><li>transform header indicates its SAM template<ul><li><code>Transform:</code></li></ul></li><li>write code<ul><li>AWS::Serverless::Function</li><li>AWS::Serverless::Api</li><li>AWS::Serverless::SimpleTable</li></ul></li><li>package and deploy<ul><li>aws cloudformation package / sam package</li><li>aws cloudformation deploy / sam deploy</li></ul></li></ul><h2 id="sam-policy-templates"><a class="markdownIt-Anchor" href="#sam-policy-templates"></a> SAM policy templates</h2><ul><li>list of templates to apply permissions to your lambda functions</li><li>important examples<ul><li>S3ReadPolicy: give read only permissions to objects in S3</li><li>SQSPollerPolicy: allows to poll an SQS queue</li><li>DynamoDBCrudPolicy: CRUD = create read update delete</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MyFunction:</span><br><span class="line">  Type: &#39;AWS::Serverless::Function&#39;</span><br><span class="line">  Properties:</span><br><span class="line">    CodeUri: xxxxx</span><br><span class="line">    Handler: xxxxxx</span><br><span class="line">    Runtime: xxxxxx</span><br><span class="line">    Policies:</span><br><span class="line">      - SQSPollerPolicy:</span><br><span class="line">        QueueName:</span><br><span class="line">          !GetAtt MyQueue.QueueName</span><br></pre></td></tr></table></figure><h2 id="sam-sumary"><a class="markdownIt-Anchor" href="#sam-sumary"></a> SAM Sumary</h2><ul><li>SAM is built on CloudFormation</li><li>SAM requires the Transform and Resources sections</li><li>commands to know<ul><li>sam build: fetch dependencies and create local deployment artifacts</li><li>sam package: package and upload to Amazon S3, generate CloudFormation template</li><li>sam deploy: deploy to CloudFormation</li></ul></li><li>SAM policy templates for easy IAM policy definition</li><li>SAM is integrated with CodeDeploy to do deploy to lambda aliases</li></ul><h2 id="serverless-application-repository-sar"><a class="markdownIt-Anchor" href="#serverless-application-repository-sar"></a> Serverless Application Repository (SAR)</h2><ul><li>managed repository for serverless applications</li><li>the applications are packaged using SAM</li><li>build and publish applications that can be re used by organizations<ul><li>can share publicly</li><li>can share with specific accounts</li></ul></li><li>this prevents duplicate work, and just go straight to publishing</li><li>application settings and behavior can be customized using Environment variables</li></ul><h1 id="cloud-development-kit-cdk"><a class="markdownIt-Anchor" href="#cloud-development-kit-cdk"></a> Cloud Development Kit (CDK)</h1><ul><li>define your cloud infrastructure using a familiar language</li><li>contains high level components called <code>constructs</code></li><li>the code is complied into a CloudFormation template (YAML / JSON)</li><li>you can therefore deploy infrastructure and application runtime code togther<ul><li>great for lambda functions</li><li>great for Docker Containers in ECS / EKS</li></ul></li></ul><h2 id="cdk-vs-sam"><a class="markdownIt-Anchor" href="#cdk-vs-sam"></a> CDK vs SAM</h2><ul><li>SAM<ul><li>serverless focused</li><li>write your template declaratively in JSON or YAML</li><li>great for quickly getting started with lambda</li><li>leverages CloudFormation</li></ul></li><li>CDK<ul><li>all aws services</li><li>write infra in a programming language</li><li>leverages CloudFormation</li></ul></li></ul><h1 id="cognito"><a class="markdownIt-Anchor" href="#cognito"></a> Cognito</h1><ul><li>we want to give our users an identity so that they can interact with our application</li><li>Cognito user pools<ul><li>sign in functionality for app users</li><li>integrate with API gateway and ALB</li></ul></li><li>Cognito Identity Pool (federated identity)<ul><li>provide AWS credentials to users so they can access AWS resources directly</li><li>integrate with Cognito user pools as an identity provider</li></ul></li><li>Cognito Sync<ul><li>Synchronize data from device to Cognito</li><li>is deprecated and replaced by AppSync</li></ul></li></ul><h2 id="cognito-user-pools"><a class="markdownIt-Anchor" href="#cognito-user-pools"></a> Cognito User Pools</h2><ul><li><p>create a serverless database of user for your web and mobile apps</p></li><li><p>simple login: username and password combination</p></li><li><p>password reset</p></li><li><p>email and phone number verification</p></li><li><p>federated identities: users from Facebook, Google, SAML…</p></li><li><p>feature: block users if their credentials are compromised elsewhere</p></li><li><p>login send back a JSON web token (JWT)</p></li><li><p>Cognito has a hosted authentication UI that you can add to your app to handle signup and signin workflows</p></li><li><p>using the hosted UI, you have a foundation for integration with social logins, OIDC or SAML</p></li><li><p>can customize with a custom logo and custom CSS</p></li></ul><h2 id="cognito-identity-pools"><a class="markdownIt-Anchor" href="#cognito-identity-pools"></a> Cognito Identity Pools</h2><ul><li>get identities for users so they obtain temporary AWS credentials</li><li>your identity pool can include<ul><li>public providers (login with Amazon, Facebook, Google, Apple)</li><li>users in an Amazon Cognito user pool</li><li>OpenID Connect Providers and SAML identity providers</li><li>developer authenticated identities</li><li>Cognito identity pools allow for unauthenticated (guest) access</li></ul></li><li>users can then access AWS service directly or through API gateway<ul><li>the IAM policies applied to the credentials are defined in Cognito</li><li>they can be customized based on the user_id for fine grained control</li></ul></li></ul><h3 id="iam-roles"><a class="markdownIt-Anchor" href="#iam-roles"></a> IAM roles</h3><ul><li>default IAM roles for authenticated and guest users</li><li>define rules to choose the role for each user based on the user’s ID</li><li>you can partition your users’ access using policy variables</li><li>IAM credentials are obtained by Cognito identity pools through STS</li><li>the roles must have a trust policy of Cognito identity pools</li></ul><h2 id="cognito-user-pools-vs-cognito-identity-pools"><a class="markdownIt-Anchor" href="#cognito-user-pools-vs-cognito-identity-pools"></a> Cognito User Pools vs Cognito Identity Pools</h2><ul><li>Cognito User Pool<ul><li>database of users for your web and mobile application</li><li>allows to federate logins through public social identity provider, OIDC, SAML…</li><li>can customize the hosted UI for authentication</li><li>has triggers with AWS lamdba during the authentication flow</li></ul></li><li>Cognito identity pools<ul><li>obtain AWS credentials for your users</li><li>users can login through public social, OIDC, SAML and Cognito User Pools</li><li>users can be unauthenticated</li><li>users are mapped to IAM roles and policies, can leverage policy variables</li></ul></li><li>CUP + CIP = manage users / password + access AWS services</li></ul><h2 id="cognito-sync"><a class="markdownIt-Anchor" href="#cognito-sync"></a> Cognito Sync</h2><ul><li>Deprecated - use AWS AppSync now</li><li>store preferences, configuration, state of app</li><li>cross device synchronization</li><li>offline capability</li><li>store data in datasets</li><li>push sync: silently notify across all devices when identity data changes</li><li>Cognito Stream: stream data from Cognito into Kinesis</li><li>Cognito Events: execute lambda functions in response to events</li></ul><h1 id="step-functions"><a class="markdownIt-Anchor" href="#step-functions"></a> Step Functions</h1><ul><li>model your workflows as state machines (one per workflow)<ul><li>order fulfillment, data processing</li><li>web applications, any workflow</li></ul></li><li>written in JSON</li><li>visualization of the workflow and the execution of the workflow, as well as history</li><li>start workflow with SDK call, API gateway, eventbridge</li></ul><h2 id="task-states"><a class="markdownIt-Anchor" href="#task-states"></a> task states</h2><ul><li>do some work in your state machine</li><li>invoke one service<ul><li>can invoke a lambda function</li><li>run an batch job</li><li>run an ECS task and wait for it to complete</li><li>insert an item from DynamoDB</li><li>publish message to SNS, SQS</li><li>launch another step function workflow</li></ul></li><li>run an activity<ul><li>EC2, Amazon ECS, on premises</li><li>activities poll the step functions for work</li><li>activities send result back to step functions</li></ul></li></ul><h2 id="states"><a class="markdownIt-Anchor" href="#states"></a> states</h2><ul><li>choice state: test for a condition to send to a branch</li><li>fail or succeed state: stop execution with failure or success</li><li>pass state: simply pass its input to its output or inject some fixed data, without performing work</li><li>wait state: provide a delay for a certain amount of time or until a specified time/date</li><li>Map state: dynamically iterate steps</li><li>parallel state: begin parallel branches of execution</li></ul><h2 id="error-handling"><a class="markdownIt-Anchor" href="#error-handling"></a> Error handling</h2><ul><li>any state can encounter runtime errors for various reasons<ul><li>state machine definition issues</li><li>task failtures</li><li>transient issues</li></ul></li><li>use <code>retry</code> and <code>catch</code> in the state machine to handle the errors instead of inside the application code</li><li>the state may report its own errors</li></ul><h3 id="retry"><a class="markdownIt-Anchor" href="#retry"></a> Retry</h3><ul><li>evaluated from top to bottom</li><li>ErrorEquals: match a specific kind of error</li><li>IntervalSeconds: initial delay before retrying</li><li>BackOffRate: multiple the delay after each retry</li><li>MaxAttempts: default to 3, set to 0 for never retried</li><li>When max attempts are reached, the <code>catch</code> kicks in</li></ul><h3 id="catch"><a class="markdownIt-Anchor" href="#catch"></a> Catch</h3><ul><li>evaluated from top to bottom</li><li>ErrorEquals: match a specific kind of error</li><li>Next: state to send to</li><li>ResultPath: a path that determines what input is sent to the state specified in the Next field</li></ul><h3 id="resultpath"><a class="markdownIt-Anchor" href="#resultpath"></a> ResultPath</h3><ul><li>include the error in the input</li></ul><h1 id="appsync"><a class="markdownIt-Anchor" href="#appsync"></a> AppSync</h1><ul><li>AppSync is a managed service that uses GraphQL</li><li>GraphQL makes it easy for applications to get exactly the data they needed</li><li>this includes combining data from one or more sources</li><li>retrieve data in real time with WebSocket or MQTT on WebSocket</li><li>for mobile apps: local data access and data Synchronization</li><li>it all starts with uploading one GraphQL schema</li></ul><h2 id="security-5"><a class="markdownIt-Anchor" href="#security-5"></a> Security</h2><ul><li>there are four ways you can authorize applications to interact with your AppSync GraphQL API<ul><li>API KEY</li><li>IAM</li><li>OPENID_CONNECT</li><li>COGNITO USER POOLS</li></ul></li><li>for custom domain and HTTPS, use CloudFront in front of AppSync</li></ul><h1 id="sts-security-token-service"><a class="markdownIt-Anchor" href="#sts-security-token-service"></a> STS (security Token service)</h1><ul><li>Allows to grant limited and temporary access to AWS resources</li><li>AssumeRole: assume roles within your account or cross account</li><li>AssumeRoleWithSAML: return credentials for users logged in with SAML</li><li>AssumeRoleWithWebIdentity<ul><li>return credentials for users logged with an IDP</li><li>AWS recommends against using this, and using Cognito User Pools instead</li></ul></li><li>GetSessionToken: For MFA, from a user or account root user</li><li>GetFederationToken: obtain temporary credentials for a federated user</li><li>GetCallerIdentity: return details about the IAM user or role used in the API call</li><li>DecodeAuthorizationMessage: decode error message when an AWS API is called</li></ul><h2 id="using-sts-to-assume-a-role"><a class="markdownIt-Anchor" href="#using-sts-to-assume-a-role"></a> using STS to assume a role</h2><ul><li>define an IAM role within your account or cross account</li><li>define which principals can access this IAM role</li><li>user STS to retrieve credentials and impersonate the IAM role you have access to</li><li>temporary credentials can be valid between 15 minutes to 1 hour</li></ul><h2 id="sts-with-mfa"><a class="markdownIt-Anchor" href="#sts-with-mfa"></a> STS with MFA</h2><ul><li>use GetSessionToken from STS</li><li>appropriate IAM policy using IAM conditions</li><li><code>aws:MultiFactorAuthPresent:true</code></li><li>GetSessionToken returns<ul><li>access ID</li><li>secret key</li><li>session token</li><li>expiration date</li></ul></li></ul><h1 id="advanced-iam"><a class="markdownIt-Anchor" href="#advanced-iam"></a> Advanced IAM</h1><h2 id="iam-policies-and-s3-bucket-policies"><a class="markdownIt-Anchor" href="#iam-policies-and-s3-bucket-policies"></a> IAM policies and S3 Bucket policies</h2><ul><li>IAM policies are attached to users, roles and groups</li><li>S3 bucket policies are attached to buckets</li><li>when evaluating if an IAM principal can perform an operation X on a bucket, the <code>union</code> of its assigned IAM policies and S3 bucket policies will be evaluated at the same time.</li></ul><h2 id="dynamic-policies-with-iam"><a class="markdownIt-Anchor" href="#dynamic-policies-with-iam"></a> Dynamic policies with IAM</h2><ul><li>how do you assign each user access to their own foler in S3 bucket?</li><li>create one dynamic policy with IAM</li><li>leverage the special policy variable <code>$&#123;aws:username&#125;</code></li></ul><h2 id="inline-vs-managed-policies"><a class="markdownIt-Anchor" href="#inline-vs-managed-policies"></a> inline vs managed policies</h2><ul><li>AWS managed policy<ul><li>maintained by AWS</li><li>good for power users and administrators</li><li>updated in case of new services and new APIs</li></ul></li><li>customer managed policy<ul><li>best practice, re usable, can be applied to many principals</li><li>version controlled + rollback, central change management</li></ul></li><li>inline<ul><li>strict one to one relationship between policy and principal</li><li>policy is deleted if you delete the IAM principal</li></ul></li></ul><h2 id="granting-a-user-permissions-to-pass-a-role-to-an-aws-service"><a class="markdownIt-Anchor" href="#granting-a-user-permissions-to-pass-a-role-to-an-aws-service"></a> granting a user permissions to pass a role to an AWS service</h2><ul><li>to configure many services, you must pass an IAM role to the service</li><li>the service will later assume the role and perform actions</li><li>for this, you need the IAM permission <code>iam:PassRole</code></li><li>it often comes with <code>iam:GetRole</code> to view the role being passed</li></ul><h3 id="can-a-role-be-passed-to-any-service"><a class="markdownIt-Anchor" href="#can-a-role-be-passed-to-any-service"></a> can a role be passed to any service?</h3><ul><li>no: roles can only be passed to what their trust allows</li><li>a trust policy for the role that allows the service to assume the role</li></ul><h2 id="directory-service-overview"><a class="markdownIt-Anchor" href="#directory-service-overview"></a> Directory service - overview</h2><ul><li>AWS managed Microsoft AD<ul><li>create your own AD in AWS, manage users locally, supports MFA</li><li>establish trust connections with your on permise AD</li></ul></li><li>AD connector<ul><li>directory gateway to redirect to on premises AD</li><li>users are managed on the on premises AD only</li></ul></li><li>Simple AD<ul><li>AD compatible managed directory on AWS</li><li>cannot be joined with on premises AD</li></ul></li></ul><h1 id="kms"><a class="markdownIt-Anchor" href="#kms"></a> KMS</h1><h2 id="encryption"><a class="markdownIt-Anchor" href="#encryption"></a> Encryption</h2><h3 id="encryption-in-flight"><a class="markdownIt-Anchor" href="#encryption-in-flight"></a> Encryption in flight</h3><ul><li>data is encrypted before sending and decrypted after receiving</li><li>SSL certificate help with encryption</li><li>encryption in flight ensures no MITM can happen</li></ul><h3 id="server-side-encryption-at-rest"><a class="markdownIt-Anchor" href="#server-side-encryption-at-rest"></a> server side encryption at rest</h3><ul><li>data is encrypted after being received by the server</li><li>data is decrypted before being sent</li><li>it is stored in an encrypted form thanks to a key</li><li>the encryption / decryption keys must be managed somewhere and the server must have access to it</li></ul><h3 id="client-side-encryption-2"><a class="markdownIt-Anchor" href="#client-side-encryption-2"></a> Client side encryption</h3><ul><li>data is encrypted by the client and never decrypted by the server</li><li>data will be decrypted by a receiving client</li><li>the server should not be able to decrypt the data</li><li>could leverage Envelop encryption</li></ul><h2 id="aws-kms"><a class="markdownIt-Anchor" href="#aws-kms"></a> AWS KMS</h2><ul><li>fully integrated with IAM for authorization</li><li>seamlessly integrated into<ul><li>EBS</li><li>S3</li><li>RedShift</li><li>RDS</li><li>SSM</li></ul></li><li>but you can also use CLI / SDK</li><li>the value in KMS is the CMK used to encrypt data can never be retrieved by user, and the CMK can be rotated for extra security</li><li>KMS can only help in encryping up to 4KB of data per call, if data &gt; 4KB, we need to use Envelope encryption</li><li>to give access to KMS to someone<ul><li>make sure the key policy allows the user</li><li>make sure the IAM policy allows the API calls</li></ul></li></ul><h2 id="cmk-types"><a class="markdownIt-Anchor" href="#cmk-types"></a> CMK Types</h2><ul><li>Symmetric<ul><li>first offering of KMS, single encryption key that is used to encrypt and decrypt</li><li>AWS services that are integrated with KMS use Symmetric CMKs</li><li>necessary for envelope encryption</li><li>you never get access to the key uncrypted (must call KMS API to use)</li></ul></li><li>Asymmetric<ul><li>public and private key pair</li><li>used for encrypt and decrypt</li><li>the public key is downloadable, but you can’t access the private key unencrypted</li><li>use case: encryption outside of AWS by users who can’t call the KMS API</li></ul></li></ul><h2 id="kms-key-policies"><a class="markdownIt-Anchor" href="#kms-key-policies"></a> KMS key policies</h2><ul><li>control access to KMS keys, similar to S3 bucket policies</li><li>difference: you cannot control access without them</li><li>default KMS key policy<ul><li>created if you don’t provide a specific key policy</li><li>compelete access to the key to the root user, which means all IAM users can access the key</li><li>gives access to the IAM policies to the KMS key</li></ul></li><li>custom KMS key policy<ul><li>define users, roles that can access the KMS key</li><li>define who can administer the key</li><li>helpful for cross account access of your KMS key</li></ul></li></ul><h3 id="copying-snapshots-across-accounts"><a class="markdownIt-Anchor" href="#copying-snapshots-across-accounts"></a> copying snapshots across accounts</h3><ul><li>create a snapshot, encrypted with your own CMK</li><li>attach a KMS key policy to authorize cross account access</li><li>share the encrypted snapshot</li><li>create a copy of the snapshot, encrypt it with a KMS key in your account</li><li>create a volume from the snapshot</li></ul><h2 id="envelope-encryption"><a class="markdownIt-Anchor" href="#envelope-encryption"></a> Envelope encryption</h2><ul><li>KMS encrypt API call has limit of 4kb</li><li>if you want to encrypt &gt; 4KB, we need to user envelope encryption</li><li>the main API that will help us is the <code>GenerateDataKey</code> API</li><li>steps<ul><li>Encryption<ol><li>call GenerateDataKey API to get the plaintext date key and encrypted data key (encrypted using your CMK)</li><li>encrypt the big file using the plaintext data key on your local machine (client side)</li><li>create an envelope includes the encrypted date key and the encrypted big file</li></ol></li><li>decryption<ol><li>call Decrypt API, send the encrypted data key to KMS to decrypt using your own CMK</li><li>plaintext data key will be returned</li><li>use the plaintext data key to decrypt your encrypted big file.</li></ol></li></ul></li></ul><h3 id="encryption-sdk"><a class="markdownIt-Anchor" href="#encryption-sdk"></a> Encryption SDK</h3><ul><li>the Encryption SDK implemented envelope encryption for us</li><li>the encryption SDK also exists as a CLI tool we can install</li><li>feature - data key caching<ul><li>re use data keys instead of creating new data keys for each encryption</li><li>helps with reducing the number of API calls to KMS with a security trade off</li></ul></li></ul><h2 id="kms-symmetric-api-summary"><a class="markdownIt-Anchor" href="#kms-symmetric-api-summary"></a> KMS symmetric - API summary</h2><ul><li>encrypt: up to 4KB</li><li>GenerateDataKey: generates a unique symmetric data key<ul><li>returns a plaintext copy of the data key</li><li>and a copy that is encrypted under the CMK that you specify</li></ul></li><li>decrypt: decrypt up to 4KB of data (including data encryption keys)</li><li>GenerateRamdom: returns a random byte string</li></ul><h2 id="quota-limits"><a class="markdownIt-Anchor" href="#quota-limits"></a> Quota limits</h2><ul><li>when you exceed a request quota, you get a <code>ThrottlingException</code></li><li>to respond, use exponential backoff</li><li>for crytographic operations, they share the same quota</li><li>this includes requests made by AWS on your behalf</li><li>for GenerateDataKey, consider using DEK caching from the encryption SDK</li><li>you can also request quotas increase through AWS support</li></ul><h2 id="sse-kms-deep-dive"><a class="markdownIt-Anchor" href="#sse-kms-deep-dive"></a> SSE-KMS deep dive</h2><ul><li>SSE-KMS leverages the GenerateDataKey and Decrypt KMS API calls</li><li>these KMS API calls will show up in CloudTrail, helpful for logging</li><li>to perform SSE-KMS, you need<ul><li>a KMS key policy that authorize the user / role (so we could use the key)</li><li>an IAM policy that authorizes access to KMS (so we could access the AWS KMS service)</li><li>otherwise you will get an access denied error</li></ul></li><li>S3 calls to KMS for SSE-KMS count against your KMS limits<ul><li>if throttling, try exponential backoff</li><li>or request an increase in KMS limits</li></ul></li></ul><h3 id="s3-bucket-policies-force-ssl"><a class="markdownIt-Anchor" href="#s3-bucket-policies-force-ssl"></a> S3 bucket policies - force SSL</h3><ul><li>to force SSL, create an S3 bucket policy with a DENY on the condition <code>aws:SecureTransport=false</code></li></ul><h3 id="s3-bucket-policy-force-encryption-of-sse-kms"><a class="markdownIt-Anchor" href="#s3-bucket-policy-force-encryption-of-sse-kms"></a> S3 bucket policy - force encryption of SSE-KMS</h3><ol><li>deny incorrect encryption header: make sure it includes <code>aws:kms</code></li><li>deny no encryption header to ensure objects are not uploaded un encrypted</li></ol><ul><li>we could also use S3 default encryption of SSE-KMS, in this case, we don’t need the second policy.</li></ul><h2 id="s3-bucket-key-for-sse-kms-encryption"><a class="markdownIt-Anchor" href="#s3-bucket-key-for-sse-kms-encryption"></a> S3 bucket key for SSE-KMS encryption</h2><ul><li>we could enable S3 bucket key to reduce the API calls to KMS directly</li><li>the key is used to encrypt kMS objects with new data keys using envelope encryption</li><li>you will see less KMS cloudtrail events</li></ul><h1 id="ssm-parameter-store"><a class="markdownIt-Anchor" href="#ssm-parameter-store"></a> SSM Parameter Store</h1><ul><li>secure storage for configuration and secrets</li><li>optional seamless encryption using KMS</li><li>serverless, scalable, durable, easy sdk</li><li>version tracking of configurations / secrets</li><li>configuration management using path and IAM</li><li>notifications with CloudWatch events</li><li>integration with CloudFormation</li></ul><h2 id="parameter-policies"><a class="markdownIt-Anchor" href="#parameter-policies"></a> Parameter policies</h2><ul><li>allow to assign a TTL to a parameter to force updating or deleting sensitive data</li><li>can assign multiple policies at a time</li></ul><h1 id="secrets-manager"><a class="markdownIt-Anchor" href="#secrets-manager"></a> Secrets Manager</h1><ul><li>Newer service, meant for storing secrets</li><li>capability to force rotation of secrets every X days</li><li>automate generation of secrets on rotation using lambda function</li><li>integration with RDS</li><li>secrets are encrypted using KMS</li><li>mostly meant for RDS integration</li></ul><h2 id="ssm-parameter-store-vs-secrets-manager"><a class="markdownIt-Anchor" href="#ssm-parameter-store-vs-secrets-manager"></a> SSM Parameter store vs secrets manager</h2><ul><li>secrets manager<ul><li>automatic rotation of secrets with lambda</li><li>lambda function is provided for RDS, Redshift…</li><li>KMS encryption is mandatory</li></ul></li><li>SSM parameter store<ul><li>simple API</li><li>no secret rotation (can be implemented using CloudWatch events and lambda)</li><li>KMS encryption is optional</li><li>can pull a secrets manager secrets using the SSM parameter Store API</li></ul></li></ul><h2 id="cloudwatch-logs-encryption"><a class="markdownIt-Anchor" href="#cloudwatch-logs-encryption"></a> CloudWatch logs - encryption</h2><ul><li>you can encrypt CloudWatch logs with KMS keys</li><li>encryption is enabled at the log group level, by associating a CMK with a log group, either when you create the log group or after it exists</li><li>you cannot associate a CMK with a log group using the CloudWatch console, have to use CLI</li><li>you must use the CloudWatch logs API<ul><li><code>associate-kms-key</code>: if the log group already exists</li><li><code>create-log-group</code>: if the log group doesn’t exist yet</li></ul></li></ul><h1 id="acm-aws-certificate-manager"><a class="markdownIt-Anchor" href="#acm-aws-certificate-manager"></a> ACM (AWS certificate manager)</h1><ul><li>provision, manage, and deploy SSL / TLS certificates</li><li>used to provide in flight encryption for websites</li><li>supports both public and private TLS certificates</li><li>free of charge for public TLS certificates</li><li>automatic TLS certificate renewal</li><li>integration with<ul><li>ELB</li><li>CloudFront</li><li>APIs on API Gateway</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;i-passed&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#i-passed&quot;&gt;&lt;/a&gt; I PASSED!&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/../images/AWS-DVA-Review/AWSCertifiedDevel</summary>
      
    
    
    
    
    <category term="AWS" scheme="http://hellcy.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>AWS SAA Review</title>
    <link href="http://hellcy.github.io/2021/08/09/AWS-SAA-Review/"/>
    <id>http://hellcy.github.io/2021/08/09/AWS-SAA-Review/</id>
    <published>2021-08-09T06:32:59.000Z</published>
    <updated>2022-02-10T14:39:24.300Z</updated>
    
    <content type="html"><![CDATA[<h1 id="i-passed"><a class="markdownIt-Anchor" href="#i-passed"></a> I PASSED!</h1><p><img src="/../images/AWS-SAA-Review/AWSCertifiedSolutionsArchitectAssociateCertificate.png" alt="" /><br /><a href="https://www.credly.com/badges/58d1f023-d657-470e-a22e-fbe8ddf4124b?source=linked_in_profile">View My Certificate</a></p><h1 id="getting-started"><a class="markdownIt-Anchor" href="#getting-started"></a> Getting Started</h1><h2 id="aws-regions"><a class="markdownIt-Anchor" href="#aws-regions"></a> AWS Regions</h2><ul><li>AWS has regions all aroung the world</li><li>Names can be us-east-1, eu-west-3…</li><li>A region is a cluster of data centers</li><li>Most AWS services are regoin-scoped</li></ul><h2 id="how-to-choose-an-aws-region"><a class="markdownIt-Anchor" href="#how-to-choose-an-aws-region"></a> How to choose an AWS Region?</h2><ul><li>Compliance: with data governance and legal requirements, data never leaves a region without your explicit permission</li><li>Proximity to customers: reduced latency</li><li>Available services within a region: new services and new features aren’t available in every region</li><li>Pricing: pricing varies region to region and is transparent in the service pricing page</li></ul><h2 id="aws-availability-zones"><a class="markdownIt-Anchor" href="#aws-availability-zones"></a> AWS Availability Zones</h2><ul><li>Each region has many AZs example:<ul><li>ap-southeast-2a</li><li>ap-southeast-2b</li><li>ap-southeast-2c</li></ul></li><li>Each AZ is one or more discrete data centers with redundant power, networking and connectivity</li><li>they are separate from each other, so that they are isolated from disasters</li><li>they are connected with high bandwidth, ultra low latency networking</li></ul><h2 id="aws-points-of-presence-edge-locations"><a class="markdownIt-Anchor" href="#aws-points-of-presence-edge-locations"></a> AWS Points of Presence (Edge locations)</h2><ul><li>Content is delivered to end users with lower latency</li></ul><h1 id="iam-and-aws-cli"><a class="markdownIt-Anchor" href="#iam-and-aws-cli"></a> IAM and AWS CLI</h1><h2 id="iam"><a class="markdownIt-Anchor" href="#iam"></a> IAM</h2><ul><li>Identity and Access Management, Global service</li><li>Root Account: created by default, shouldn’t be used or shared</li><li>Users are people within your organization, and can be grouped</li><li>Groups only contain users, not other groups</li><li>Users don’t have to belong to a group, and user can belong to multiple groups</li></ul><h3 id="iam-permissions"><a class="markdownIt-Anchor" href="#iam-permissions"></a> IAM Permissions</h3><ul><li>Users or Groups can be assigned JSON documents called policies</li><li>These policies define the permissions of the users</li><li>In AWS you apply the least privilege principle, don’t give more permissions than a user needs</li></ul><h3 id="iam-policies-structure"><a class="markdownIt-Anchor" href="#iam-policies-structure"></a> IAM Policies Structure</h3><ul><li>Consist of<ul><li>Version: policy language version, always include ‘2012-10-17’</li><li>Id: an identifier for the policy (optional)</li><li>Statement: one or more individual statements (required)</li></ul></li><li>Statements consists of<ul><li>Sid: an identifier for the statement (optional)</li><li>Effect: whether the statement allows or denies access (Allow, Deny)</li><li>Principal: account/user/role to which this policy applied to</li><li>Action: list of actions this policy allows or denies</li><li>Resource: list of resources to which the actions applied to</li><li>Condition: conditions for when this policy is in effect (optional)</li></ul></li></ul><h3 id="how-can-users-access-aws"><a class="markdownIt-Anchor" href="#how-can-users-access-aws"></a> How can users access AWS?</h3><ol><li>AWS Management Console: protected by password + MFA</li><li>AWS Command Line Interface: protected by access keys</li><li>AWS SDK: for code, protected by access keys</li></ol><ul><li>Access key ID = username</li><li>Secret access key = password</li></ul><h3 id="iam-roles-for-services"><a class="markdownIt-Anchor" href="#iam-roles-for-services"></a> IAM Roles for services</h3><ul><li>Some AWS service will need to perform actions on your behalf</li><li>we will assign permissions to AWS services with IAM Roles</li><li>Common roles:<ul><li>EC2 instance roles</li><li>lambda function roles</li><li>roles for CloudFormation</li></ul></li></ul><h3 id="iam-security-tools"><a class="markdownIt-Anchor" href="#iam-security-tools"></a> IAM Security Tools</h3><ul><li>IAM credentials report (account-level)<ul><li>a report that lists all your account’s users and the status of their various credentials</li></ul></li><li>IAM Access Advisor (user-level)<ul><li>Access Advisor shows the service permissions granted to a user and when those services were last accessed</li><li>you can use this information to revise your policies</li></ul></li></ul><h3 id="iam-guidelines-and-best-practices"><a class="markdownIt-Anchor" href="#iam-guidelines-and-best-practices"></a> IAM Guidelines and Best practices</h3><ul><li>Don’t user root account except for AWS account setup</li><li>One physical user = one AWS user</li><li>assign users to groups and assign permissions to groups</li><li>create a strong password policy</li><li>use and enforce the use of MFA</li><li>create and use roles for giving permissions to AWS services</li><li>use access keys for CLI and SDK</li><li>Audit permissions of your account with the IAM Credential Report</li><li>Never share IAM users and access keys</li></ul><h1 id="ec2"><a class="markdownIt-Anchor" href="#ec2"></a> EC2</h1><ul><li>EC2 is one of the most popular of AWS offering</li><li>EC2 = Elastic Compute Cloud = Infrastructure as a Service</li><li>It mainly consists in the capability of<ul><li>Renting virtual machines (EC2)</li><li>Storing data on virtual drives (EBS)</li><li>Distributing load across machines (ELB)</li><li>Scaling the services using an auto-scaling group (ASG)</li></ul></li><li>Knowing EC2 is fundamental to understand how to Cloud works</li></ul><h2 id="ec2-instance-types-overview"><a class="markdownIt-Anchor" href="#ec2-instance-types-overview"></a> EC2 Instance Types - Overview</h2><ul><li>you can use different types of EC2 instances that are optimised for different use cases</li><li>e.g. m5.2xlarge<ul><li>m: instance class</li><li>5: generation</li><li>2xlarge: size within the instance class</li></ul></li></ul><h3 id="general-purpose"><a class="markdownIt-Anchor" href="#general-purpose"></a> General Purpose</h3><ul><li>Great for a diversity of workloads such as web servers or code repositories</li><li>Balance between<ul><li>Compute</li><li>Memory</li><li>Networking</li></ul></li></ul><h3 id="compute-optimized"><a class="markdownIt-Anchor" href="#compute-optimized"></a> Compute Optimized</h3><ul><li>Great for compute intensive tasks that require high performance processors<ul><li>Batch processing workloads</li><li>media transcoding</li><li>high performance web servers</li><li>high performance computing</li><li>scientific modeling and machine learning</li><li>dedicated gaming servers</li></ul></li></ul><h3 id="memory-optimized"><a class="markdownIt-Anchor" href="#memory-optimized"></a> Memory Optimized</h3><ul><li>Fast performance for workloads that process large data sets in memory</li><li>high performance, relational/non-relational databases</li><li>distributed web scale cache stores</li><li>in memory databases optimized for BI</li><li>applications performing real time processing of big unstructured data</li></ul><h3 id="storage-optimized"><a class="markdownIt-Anchor" href="#storage-optimized"></a> Storage Optimized</h3><ul><li>great for storage intensive tasks that require high, sequential read and write access to large data sets on local storage</li><li>high frequency online transaction processing systems</li><li>relational and NoSQL databases</li><li>cache for in memory databases</li><li>data warehousing applications</li><li>distributed file systems</li></ul><h2 id="security-groups"><a class="markdownIt-Anchor" href="#security-groups"></a> Security Groups</h2><ul><li>the fundamental of network security in AWS</li><li>they control how traffic is allowed into or out of our EC2 instance</li><li>security groups only contain allow rules</li><li>security groups rules can reference by IP or by security group (inbound/outbound rules)</li></ul><h3 id="good-to-know"><a class="markdownIt-Anchor" href="#good-to-know"></a> Good to know</h3><ul><li>security groups can be attached to multiple instances and one instance can have multiple security groups attach to it</li><li>security group are locked down to a region and VPC</li><li>security group live outside the EC2, if traffic is blocked, the EC2 instacne won’t see it (doesn’t know it tried to get in)</li><li>if your application is not accessible (time out), then its a security group issue</li><li>if your application gives a connection failed error, then its an application error or its not launched</li><li>all inbound traffic is blocked by default</li><li>all outbound traffic is authorized by default</li></ul><h2 id="classic-ports-to-know"><a class="markdownIt-Anchor" href="#classic-ports-to-know"></a> Classic Ports to know</h2><ul><li>22 = SSH (Secure Shell) - log into a Linux instance</li><li>21 = FTP (File Transfer Protocol) - upload files into a file share</li><li>22 = SFTP (Secure File Transfer Protocol) - upload files using SSH</li><li>80 = HTTP - access unsecured websites</li><li>443 = HTTPS - access secured websites</li><li>3389 = RDP (Remote Desktop Protocol) - log into a Windows instance</li></ul><h2 id="ec2-instances-purchasing-options"><a class="markdownIt-Anchor" href="#ec2-instances-purchasing-options"></a> EC2 Instances purchasing options</h2><ul><li>on-demand instance: short workload, predictable pricing</li><li>reserved, minimum 1 year:<ul><li>reserved instances: long workloads</li><li>convertible reserved instances: long workloads with flexible instances</li><li>scheduled reserved instances: example - every Thursday between 3 and 6 pm</li></ul></li><li>Spot instance: short workloads, cheap and can lose instances (less reliable)</li><li>dedicated hosts: book an entire physical server, control instance placement</li></ul><h3 id="ec2-on-demand"><a class="markdownIt-Anchor" href="#ec2-on-demand"></a> EC2 on demand</h3><ul><li><p>pay for what you use</p><ul><li>linux - billing per second, after the first minute</li><li>all other os - billing per hour</li></ul></li><li><p>has the highest cost but no upfront payment</p></li><li><p>no long-term commitment</p></li><li><p>recommended for short term and un-interrupted workloads, where you can’t predict how the application will behave.</p></li></ul><h3 id="ec2-reserved-instances"><a class="markdownIt-Anchor" href="#ec2-reserved-instances"></a> EC2 reserved instances</h3><ul><li>up to 75% discount compared to on demand</li><li>reservation period: 1 year or 3 year2</li><li>purchasing options: no upfront / partial upfront / all upfront</li><li>reserve a specific instance type</li><li>recommended for steady state usage applications (think database)</li></ul><h4 id="convertible-reserved-instance"><a class="markdownIt-Anchor" href="#convertible-reserved-instance"></a> Convertible reserved instance</h4><ul><li>can change the EC2 instance type</li><li>up to 54% discount</li></ul><h4 id="scheduled-reserved-instances"><a class="markdownIt-Anchor" href="#scheduled-reserved-instances"></a> scheduled reserved instances</h4><ul><li>launch within time window you reserve</li><li>when you require a fraction of day / week / month</li><li>still commitment over 1 to 3 years</li></ul><h3 id="ec2-spot-instance"><a class="markdownIt-Anchor" href="#ec2-spot-instance"></a> EC2 Spot instance</h3><ul><li><p>can get a discount of up to 90% compared to on demand</p></li><li><p>instances that you can lose at any point of time if your max price is less than the current spot price</p></li><li><p>the MOST cost efficient instances in AWS</p></li><li><p>useful for workloads that are resilient to failure</p></li><li><p>not suitable for critical jobs or databases</p></li></ul><h4 id="spot-instance-requests"><a class="markdownIt-Anchor" href="#spot-instance-requests"></a> Spot instance requests</h4><ul><li>define max spot price and get the instance while current spot price &lt; max<ul><li>the hourly spot price varies based on offer and capacity</li><li>if the current spot price &gt; your max price you can choose to stop or terminate your instance with a 2 minutes grace period</li></ul></li><li>other strategy: spot block<ul><li>block spot instance during a specified time frame (1 to 6 hours) without interruptions</li><li>in rare situations, the instance may be reclaimed</li></ul></li><li>cancel the spot instance request before terminate the spot instances</li></ul><h4 id="spot-fleets"><a class="markdownIt-Anchor" href="#spot-fleets"></a> Spot fleets</h4><ul><li>spot fleets = set of spot instances + (optional) on demand instances</li><li>the spot fleet will try to meet the target capacity with price constraints<ul><li>define possible launch pools: instance type, OS, AZ</li><li>can have multiple launch pools, so that the fleet can choose</li><li>spot fleet stops launching instnaces when reaching capacity or max cost</li></ul></li><li>strategies to allocate spot instances<ul><li>lowest price: from the pool with the lowest price (cost optimization, short workload)</li><li>diversified: distributed across all pools (great for availability, long workloads)</li><li>capacity optimized: pool with the optimal capacity for the number of instances</li></ul></li><li>spot fleets allow us to automatically request spot instance with the lowest price</li></ul><h3 id="ec2-dedicated-hosts"><a class="markdownIt-Anchor" href="#ec2-dedicated-hosts"></a> EC2 dedicated hosts</h3><ul><li><p>an Amazon EC2 dedicated host is a physical server with EC2 instance capacity fully dedicated to your use, dedicated hosts can help you address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses</p></li><li><p>allocated for your account for a 3 year period reservation</p></li><li><p>more expensive</p></li><li><p>useful for software that have complicated licensing model (BYOL - bring your own license)</p></li><li><p>or for companies that have strong regulatory or compliance needs</p></li></ul><h2 id="elastic-ip"><a class="markdownIt-Anchor" href="#elastic-ip"></a> Elastic IP</h2><ul><li><p>when you stop and then start an EC2 instance, it can change its public IP</p></li><li><p>if you need to have a fixed public IP for your instance, you need an Elastic IP</p></li><li><p>an Elastic IP is a public IPv4 IP you own as long as you don’t delete it</p></li><li><p>you can attach it to one instance at a time</p></li><li><p>with an Elastic IP, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account (not common)</p></li><li><p>you can only have 5 Elastic IP in your account</p></li><li><p>Overall, try to avoid using Elastic IP</p><ul><li>they often reflect poor architectrual decisions</li><li>instead, use a random public IP and register a DNS name to it</li><li>use a load balancer and don’t use a public IP</li></ul></li></ul><h2 id="placement-groups"><a class="markdownIt-Anchor" href="#placement-groups"></a> Placement Groups</h2><ul><li>Sometimes you want control over the EC2 instance placement strategy</li><li>when you create a placement group, you specify one of the following strategies for the group<ul><li>Cluster: clusters instances into a low latency group in a single AZ</li><li>Spread: spreads instances across underlying hardware (max 7 instances per group per AZ) - critical applications</li><li>partition: spreads instances across many different partitions (which rely on different sets of racks) within an AZ, Scales to 100 of EC2 instances per group (Hadoop, Cassandra, Kafka), the instances in a partition do not share racks with the instances in the other partitions, a partition failure can affect many EC2 but won’t affect other partitions</li></ul></li></ul><h2 id="elastic-network-interfaces-eni"><a class="markdownIt-Anchor" href="#elastic-network-interfaces-eni"></a> Elastic Network Interfaces (ENI)</h2><ul><li>logical component in a VPC that represents a virtual network card</li><li>the ENI can have the following attributes<ul><li>Primary private IPv4, one or more secondary IPv4</li><li>One Elastic IP per private IPv4</li><li>one public IPv4</li><li>one or more security groups</li><li>a MAC address</li></ul></li><li>you can create ENI independently and attach them on the fly on EC2 instances for failover</li><li>bound to a specific AZ</li></ul><h2 id="ec2-hibernate"><a class="markdownIt-Anchor" href="#ec2-hibernate"></a> EC2 Hibernate</h2><ul><li><p>Stop: the data on disk (EBS) is kept intact in the next start</p></li><li><p>Terminate: any EBS volums (root) also setup to be destroyed is lost</p></li><li><p>First start: the OS boots and the EC2 user data script is run</p></li><li><p>Following starts: the OS boots up</p></li><li><p>the your application starts, caches get warmed up and that can take time</p></li><li><p>Introducing EC2 Hibernate</p><ul><li>RAM state is preserved</li><li>the instance boot is much faster (the OS is not stopped / restarted)</li><li>under the hood: the RAM state is written to a file in the root EBS volume</li><li>the root EBS volume must be encrypted</li></ul></li><li><p>Use cases</p><ul><li>long running process</li><li>saving the RAM state</li><li>servcies that take time to initialize</li></ul></li></ul><h2 id="ec2-nitro"><a class="markdownIt-Anchor" href="#ec2-nitro"></a> EC2 Nitro</h2><ul><li>underlying platform for the next generation of EC2 instances</li><li>new virtualization technology</li><li>allows for better performance<ul><li>better networking options</li><li>Higher Speed EBS</li></ul></li><li>better underlying security</li></ul><h2 id="ec2-vcpu"><a class="markdownIt-Anchor" href="#ec2-vcpu"></a> EC2 vCPU</h2><ul><li>EC2 instance comes with a combination of RAM and vCPU</li><li>in some cases, you may want to change the vCPU options<ul><li>change the number of CPU cores</li><li>chagne the number of vCPUs (threads) per core</li></ul></li><li>only specified during instance launch</li></ul><h2 id="ec2-capacity-reservations"><a class="markdownIt-Anchor" href="#ec2-capacity-reservations"></a> EC2 capacity reservations</h2><ul><li>ensure you have EC2 capacity when needed</li><li>manual or planned end date for the reservation</li><li>no need for 1 or 3 year commitment</li><li>capacity access is immediate, you get billed as soon as it starts</li><li>combine with reserved instances and savings plans to do cost saving</li></ul><h2 id="ec2-ebs"><a class="markdownIt-Anchor" href="#ec2-ebs"></a> EC2 EBS</h2><ul><li>an EBS volume is a network drive you can attach to your instances while they run</li><li>it allows your instances to persist data, even after their termination</li><li>they can only be mounted to one instance at a time (in some cases, some EBS can be attached to multiple EC2 instances at same time)</li><li>they are bound to a specific AZ</li><li>think of them as a network USB stick</li></ul><h3 id="ebs-volume"><a class="markdownIt-Anchor" href="#ebs-volume"></a> EBS volume</h3><ul><li>its a network drive<ul><li>it uses the network to communicate the instance, which means there might be a bit of latency</li><li>it can be detached from an EC2 instance and attached to another one quickly</li></ul></li><li>its locked down to an AZ<ul><li>an EBS volume in us-east-1a cannot be attached to an instance in us-east-1b</li><li>to move a volume across, you first need to snapshot it</li></ul></li><li>have a provisioned capacity<ul><li>you get billed for all the provisioned capacity</li><li>you can increase the capacity of the drive over time</li></ul></li></ul><h3 id="ebs-volume-types"><a class="markdownIt-Anchor" href="#ebs-volume-types"></a> EBS volume types</h3><ul><li>gp2 / gp3: general purpose SSD<ul><li>gp3: newer generation, IOPS and throughput are independet</li><li>gp2: IOPS and throughput are linked</li></ul></li><li>io1 / io2: highest performance SSD</li><li>st1: low cost HDD, for throughput intensive workloads</li><li>sc1: lowest cost HDD</li><li>EBS volumes are characterized in Size / Throughput / IOPS</li></ul><h3 id="ebs-multi-attach-io1io2-family"><a class="markdownIt-Anchor" href="#ebs-multi-attach-io1io2-family"></a> EBS multi-attach - io1/io2 family</h3><ul><li>attach the same EBS volume to multiple EC2 instances in the same AZ</li><li>each instance has full read and write permissions to the volume</li><li>applications must manage concurrent write operations</li><li>must use a file system thta’s cluster-aware</li></ul><h3 id="ebs-encryption"><a class="markdownIt-Anchor" href="#ebs-encryption"></a> EBS encryption</h3><ul><li>when you create an encrypted EBS volume, you get the following<ul><li>data at rest is encrypted inside the volume</li><li>all the data in flight mobing between the instance and the volume is encrypted</li><li>all snapshots are encrypted</li><li>all volumes created from the snapshot are encryped</li></ul></li><li>encryption and decryption are handled transparently</li><li>encryption has a minimal impact on latency</li><li>EBS encryption leverages keys from KMS (AES-256)</li><li>copying an unencrypted snapshot allows encryption</li></ul><h3 id="ebs-raid"><a class="markdownIt-Anchor" href="#ebs-raid"></a> EBS RAID</h3><h4 id="raid-0"><a class="markdownIt-Anchor" href="#raid-0"></a> RAID 0</h4><ul><li>increase performance</li><li>combining 2 or more volumes and getting the total disk space and I/O</li><li>but if one disk fails, all the data is failed</li><li>using this, we can have a very big disk with a lot of IOPS</li></ul><h4 id="raid-1"><a class="markdownIt-Anchor" href="#raid-1"></a> RAID 1</h4><ul><li>increase fault tolerance</li><li>mirroring a volume to another</li><li>we have to send the data to two EBS volume at the same time (2 * network)</li></ul><h3 id="ebs-delete-on-termination"><a class="markdownIt-Anchor" href="#ebs-delete-on-termination"></a> EBS delete on termination</h3><ul><li>controls the EBS behaviour when an EC2 instance terminates<ul><li>by default, the root EBS volume is delete (attribute enabled)</li><li>by default, any other attached EBS volume is not deleted (attribute disabled)</li></ul></li><li>this can be controlled by the AWS console / CLI</li><li>use case: preseve root volume when instance is terminated (disable the attribute)</li></ul><h2 id="ebs-snapshots"><a class="markdownIt-Anchor" href="#ebs-snapshots"></a> EBS Snapshots</h2><ul><li>make a backup (snapshot) of your EBS volume at a point of time</li><li>not necessary to detach volume to do snapshot, but recommended</li><li>can copy snapshots across AZ or Region</li><li>can create volume from snapshot</li></ul><h2 id="efs-elastic-file-system"><a class="markdownIt-Anchor" href="#efs-elastic-file-system"></a> EFS - Elastic File System</h2><ul><li>Managed NFS (network file system) that can be mounted on many EC2</li><li>EFS works with EC2 instances in multi-AZ</li><li>highly avialble, scalable, expensive, pay per use</li><li>uses security group to control access to EFS</li><li>compatible with Linux based AMI (not Windows)</li><li>encryption at rest using KMS</li><li>to mount EFS to EC2, you need to add EC2 security group as a inbound rule in EFS security group</li></ul><h3 id="performance-and-storage-class"><a class="markdownIt-Anchor" href="#performance-and-storage-class"></a> Performance and Storage Class</h3><ul><li>Performance mode<ul><li>general purpose</li><li>MAX I/O</li></ul></li><li>Throughput mode<ul><li>bursting</li><li>provisioned</li></ul></li><li>Storage tiers<ul><li>standard</li><li>infrequent access, cost to retrieve files, lower price to store</li></ul></li></ul><h2 id="ami-overview"><a class="markdownIt-Anchor" href="#ami-overview"></a> AMI Overview</h2><ul><li>Amazon Machien Image</li><li>a customization of an EC2 instance<ul><li>you add your own software, configuration, operating system etc…</li><li>faster boot / configuration time because all your software is pre-packaged</li></ul></li><li>AMI are built for a specific region and can be copied across regions</li><li>you can launch EC2 instance from<ul><li>public AMI: provided by AWS</li><li>your own AMI: you make and maintain them yourself</li><li>AWS marketplace AMI: an AMI someone else made</li></ul></li></ul><h2 id="ec2-instance-store"><a class="markdownIt-Anchor" href="#ec2-instance-store"></a> EC2 instance store</h2><ul><li>EBS volumes are network drives with good but limited performance</li><li>if you need a high performance hardware disk, use EC2 instance store</li><li>better I/O performance</li><li>EC2 instance store lose their storage if they are stopped (ephemeral)</li><li>good for buffer / cache / scratch data / temporary content</li><li>risk of data loss if hardware fails</li><li>backups and replication are your responsibility</li></ul><h2 id="ec2-metadata"><a class="markdownIt-Anchor" href="#ec2-metadata"></a> EC2 Metadata</h2><ul><li>AWS EC2 instance metadata is powerful but one of the least known features to developers</li><li>it allows EC2 instance to learn about themselves without using an IAM role for that purpose</li><li>the URL is <code>http://169.254.169.254/latest/meta-data</code></li><li>you can retrieve the IAM role name from the metadata, but you CANNOT retrieve the IAM policy</li></ul><h1 id="elastic-load-balancer"><a class="markdownIt-Anchor" href="#elastic-load-balancer"></a> Elastic Load Balancer</h1><h2 id="what-is-load-balancing"><a class="markdownIt-Anchor" href="#what-is-load-balancing"></a> What is load balancing?</h2><ul><li>load balancers are servers that forward internet traffic to multiple servers (EC2 instances) downstream</li></ul><h2 id="why-use-a-load-balancer"><a class="markdownIt-Anchor" href="#why-use-a-load-balancer"></a> Why use a load balancer?</h2><ul><li><p>Spread load across multiple downstream instances</p></li><li><p>expose a single point of access (DNS) to your application</p></li><li><p>seamlessly handle failures of downstream instances</p></li><li><p>do regular health checks to your instances</p></li><li><p>provide SSL termination (HTTPS) for your websites</p></li><li><p>enforce stickness with cookies</p></li><li><p>high availability across zones</p></li><li><p>separate public traffic from private traffic</p></li><li><p>An ELB is a managed load balancer</p><ul><li>AWS guarantees that it will be working</li><li>AWS takes care of upgrades, maintenance, high availability</li><li>AWS provides only a few configuration knobs</li></ul></li><li><p>it costs less to setup your own load balancer but it will be a lot more effort on your end</p></li><li><p>it is integrated with many AWS offering / services</p></li></ul><h2 id="health-checks"><a class="markdownIt-Anchor" href="#health-checks"></a> Health Checks</h2><ul><li>Health Checks are crucial for load balancers</li><li>they enable the load balancer to know if instances it forwards traffic to are available to reply to requests</li><li>the health check is done on a port and a route (/health is common)</li><li>if the response is not 200, then the instance is unhealthy</li></ul><h2 id="classic-load-balanceers-v1"><a class="markdownIt-Anchor" href="#classic-load-balanceers-v1"></a> Classic Load Balanceers (v1)</h2><ul><li>supports TCP (layer 4), HTTP and HTTPS (layer 7)</li><li>health checks are TCP or HTTP based</li></ul><h2 id="application-load-balancer-v2"><a class="markdownIt-Anchor" href="#application-load-balancer-v2"></a> Application Load Balancer (v2)</h2><ul><li>Application load balancer is layer 7 (HTTP)</li><li>load balancing to multiple HTTP applications across machines (target groups)</li><li>load balancing to multiple applications on the same machine (containers)</li><li>support for HTTP/2 and WebSocket</li><li>Support redirects (from HTTP to HTTPS)</li><li>Routing tables to differnt target groups<ul><li>routing based on path in URL (<a href="http://example.com/users">example.com/users</a> &amp; <a href="http://example.com/posts">example.com/posts</a>)</li><li>routing based on hostname in URL (<a href="http://one.example.com">one.example.com</a> &amp; <a href="http://other.example.com">other.example.com</a>)</li><li>routing based on query string, headers (<a href="http://example.com/users?id=123&amp;other=false">example.com/users?id=123&amp;other=false</a>)</li></ul></li><li>ALB are a great fit for micro services and container based application (Docker and Amazon ECS)</li><li>Has a port mapping feature to redirect to a dynamic port in ECS</li><li>in comparison, we would need multiple CLB, one for each application</li></ul><h3 id="target-groups"><a class="markdownIt-Anchor" href="#target-groups"></a> Target Groups</h3><ul><li>EC2 instances can be managed by an Auto Scaling Group - HTTP</li><li>ECS tasks (managed by ECS itself) - HTTP</li><li>Lambda function - HTTP request is translated into a JSON event</li><li>IP addresses - must be private IPs</li><li>ALB can route to multiple target groups</li><li>health checks are at the target group level</li></ul><h2 id="network-load-balancer-v2"><a class="markdownIt-Anchor" href="#network-load-balancer-v2"></a> Network Load Balancer (v2)</h2><ul><li>network load balancer (layer 4)<ul><li>forward TCP and UDP traffic to your instance</li><li>handle millions of request per second</li><li>less latency ~ 100 ms (vs 400 ms for ALB)</li></ul></li><li>NLB has one static IP per AZ, and supports assigning Elastic IP (helpful for whitelisting specific IP)</li><li>NLB are used for extreme performance, TCP or UDP traffic</li><li>Not included in AWS free tier</li></ul><h2 id="sticky-sessions-session-affinity"><a class="markdownIt-Anchor" href="#sticky-sessions-session-affinity"></a> Sticky Sessions (Session Affinity)</h2><ul><li><p>it is possible to implement stickness so that the same client is always redirected to the same instance behind a load balancer</p></li><li><p>this works for CLB and ALB</p></li><li><p>the cookie used for stickness has an expiration date you control</p></li><li><p>use case: make sure the user doesn’t lost his session data</p></li><li><p>enabling stickness may bring imbalance to the load over the backend EC2 instances</p></li><li><p>Application based cookies</p><ul><li>custom cookie<ul><li>generated by the target</li><li>can include any custom attributes required by the application</li><li>cookie name must be specified individually for each target group</li><li>don’t use AWSALB, AWSALBAPP, AWSALBTG (reserved for use by the ELB)</li></ul></li><li>application cookie<ul><li>generated by the load balancer</li><li>cookie name is AWSALBAPP</li></ul></li></ul></li><li><p>Duration based cookie</p><ul><li>cookie generated by the load balancer</li><li>cookie name is AWSALB for ALB, AWSELB for CLB</li></ul></li></ul><h2 id="cross-zone-load-balancing"><a class="markdownIt-Anchor" href="#cross-zone-load-balancing"></a> Cross Zone Load Balancing</h2><ul><li>each load balancer instance distribute evenly across all registered instances in all AZ</li><li>ALB<ul><li>always on (can’t be disabled)</li><li>no charges for inter AZ data</li></ul></li><li>NLB<ul><li>disabled by default</li><li>you pay charges for inter AZ data if enabled</li></ul></li><li>CLB<ul><li>Through console =&gt; enabled by default</li><li>through CLI / API =&gt; disabled by default</li><li>no charges</li></ul></li></ul><h2 id="ssltls"><a class="markdownIt-Anchor" href="#ssltls"></a> SSL/TLS</h2><ul><li><p>an SSL certificate allows traffic between your clients and your load balancer to be encrypted in transit</p></li><li><p>SSL refers to Secure Sockets Layer, used to encrypt connections</p></li><li><p>TLS refers to Transport Layer Security, which is a newer version</p></li><li><p>TLS certificate are mainly used, but people still refer as SSL</p></li><li><p>public SSL certificates are issued by Certificate Authorities (CA)</p></li><li><p>SSL certificates have an expiration date and must be renewed</p></li><li><p>the load balancer uses an X.509 certificate (SSL/TLS server certificate)</p></li><li><p>you can manage certificates using ACM (AWS Certificate Manager)</p></li><li><p>You can create upload your own certificate</p></li><li><p>HTTPS listner</p><ul><li>you must specify a default certificate</li><li>you can add an optional list of certs to support multiple domains</li><li>clients can use SNI (Server Name Indication) to specify the host name they reach</li><li>ability to specify a security policy to support older version of SSL/TLS</li></ul></li></ul><h3 id="ssl-server-name-indication"><a class="markdownIt-Anchor" href="#ssl-server-name-indication"></a> SSL - Server Name Indication</h3><ul><li>SNI solves the problem of loading multiple SSL certificate onto one web server (to serve multiple website)</li><li>its newer protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake</li><li>the server will then find the correct certificate, or return the default one</li><li>Only works for ALB and NLB, CloudFront</li><li>doesn’t work for CLB</li></ul><h2 id="elb-connection-draining"><a class="markdownIt-Anchor" href="#elb-connection-draining"></a> ELB Connection Draining</h2><ul><li>Time to complete in-flight requests while the instance is de-registering or unhealthy</li><li>stops sending new requests to the instance which is de-registering</li><li>between 1 to 3600 seconds, default is 300 seconds</li><li>can be disabled (set to zero)</li><li>set to a low value if your requests are short</li></ul><h1 id="auto-scaling-group"><a class="markdownIt-Anchor" href="#auto-scaling-group"></a> Auto Scaling Group</h1><ul><li><p>in real life, the load on your websites and application can change</p></li><li><p>in the cloud, you can create and get rid of servers very quickly</p></li><li><p>the goal of an Auto Scaling Group is to</p><ul><li>scale out to match an increased load</li><li>scale in to match an decreased load</li><li>ensure we have a minimum and maximum number of machines running</li><li>automatically register new instances to a load balancer</li></ul></li></ul><h2 id="asg-attributes"><a class="markdownIt-Anchor" href="#asg-attributes"></a> ASG attributes</h2><ul><li>A launch configuration<ul><li>AMI + instance type</li><li>EC2 user data</li><li>EBS volumes</li><li>Security groups</li><li>SSH key pair</li></ul></li><li>min size / max size / initial capacity</li><li>network + subnets information</li><li>load balancer information</li><li>scaling policies</li></ul><h2 id="auto-scaling-alarms"><a class="markdownIt-Anchor" href="#auto-scaling-alarms"></a> Auto Scaling Alarms</h2><ul><li>it is possible to scale an ASG based on CloudWatch alarms</li><li>an alarm monitors a metric (such as average CPU)</li><li>metrics are computed for the overall ASG instances</li></ul><h2 id="auto-scaling-new-rules"><a class="markdownIt-Anchor" href="#auto-scaling-new-rules"></a> Auto Scaling New Rules</h2><ul><li>it is now possible to define better auto scaling rules that are directly managed by EC2<ul><li>target average CPU usage</li><li>number of requests on the ELB per instance</li><li>average network in</li><li>average network out</li></ul></li><li>these rules are easier to setup and can make more sense</li></ul><h2 id="auto-scaling-custom-metric"><a class="markdownIt-Anchor" href="#auto-scaling-custom-metric"></a> Auto Scaling Custom Metric</h2><ol><li>send custom metric from application on EC2 to CloudWatch</li><li>Create CloudWatch alarm to react to low / high values</li><li>use the CloudWatch alarm as the scaling policy for ASG</li></ol><h2 id="good-to-know-2"><a class="markdownIt-Anchor" href="#good-to-know-2"></a> Good to know</h2><ul><li>scaling policies can be on CPU, network… and can even be on custom metrics or based on a schedule</li><li>ASGs use launch configurations or launch templates</li><li>to update an ASG, you must provide a new launch configuration / launch template</li><li>IAM roles attached to an ASG will get assigned to EC2 instances</li><li>ASG are free, you pay for the underlying resources being launched</li><li>having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create a new one as a replacement.</li><li>ASG can terminate instances marked as unhealthy by an ELB (and then replace them)</li></ul><h2 id="auto-scaling-groups-dynamic-scaling-policies"><a class="markdownIt-Anchor" href="#auto-scaling-groups-dynamic-scaling-policies"></a> Auto Scaling Groups - Dynamic Scaling Policies</h2><ul><li>target tracking scaling<ul><li>most simple and easy to setup</li><li>example: I want to average ASG CPU to stay at around 40%</li></ul></li><li>Simple / Step Scaling<ul><li>When a CloudWatch alarm is triggered (example: CPU &gt; 70%), then add 2 units</li><li>when a CloudWatch alarm is triggered (example: CPU &lt; 30%), then remove 1 unit</li></ul></li><li>Scheduled Actions<ul><li>anticipate a scaling based on known usage patterns</li><li>example: increase the min capacity to 10 at 5pm on Fridays</li></ul></li><li>predictive scaling<ul><li>continuously forecast load and schedule scaling ahead</li></ul></li></ul><h2 id="good-metrics-to-scale-on"><a class="markdownIt-Anchor" href="#good-metrics-to-scale-on"></a> Good metrics to scale on</h2><ul><li>CPU Utilization<ul><li>average CPU utilization across your instances</li></ul></li><li>Request Count Per Target<ul><li>to make sure the number of requests per EC2 instances is stable</li></ul></li><li>Average network in / out<ul><li>if your application is network bound (heavy downloads / uploads)</li></ul></li><li>Any custom metric that you push using CloudWatch</li></ul><h2 id="scaling-cooldowns"><a class="markdownIt-Anchor" href="#scaling-cooldowns"></a> Scaling Cooldowns</h2><ul><li>After a scaling activity happens, you are in the cooldown period (default 300 seconds)</li><li>during the cooldown period the ASG will not launch or terminate additional instances (to allow for metrics to stablize)</li><li>advice: use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period</li></ul><h2 id="asg-default-termination-policy"><a class="markdownIt-Anchor" href="#asg-default-termination-policy"></a> ASG default termination policy</h2><ol><li>Find the AZ which has the most number of instances</li><li>if there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration</li></ol><ul><li>ASG tries to balance the number of instances across AZ by default</li></ul><h2 id="asg-lifecycle-hooks"><a class="markdownIt-Anchor" href="#asg-lifecycle-hooks"></a> ASG lifecycle hooks</h2><ul><li>by default as soon as an instance is launched in an ASG its inservice</li><li>you have the ability to perform extra steps before the instance goes in service (pending state)</li><li>you have the ability to perform extra actions before the instance is terminated (terminating state), like extract logs, tools etc…</li></ul><h2 id="asg-launch-template-vs-launch-configuration"><a class="markdownIt-Anchor" href="#asg-launch-template-vs-launch-configuration"></a> ASG launch template vs Launch configuration</h2><ul><li>both<ul><li>ID of the AMI, the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances</li></ul></li><li>Launch Configuration<ul><li>must be re-created every time</li></ul></li><li>launch template<ul><li>can have multiple versions</li><li>create parameters subsets (partial configuration for re-use and inheritance)</li><li>provision using both on demand and stop instances</li><li>can use T2 unlimited burst feature</li><li>recommended by AWS going forward</li></ul></li></ul><h1 id="rds"><a class="markdownIt-Anchor" href="#rds"></a> RDS</h1><ul><li>RDS stands for relational database service</li><li>its a managed DB service for DB use SQL as a query language</li><li>it allows you to create databases in the cloud that are managed by AWS<ul><li>Postgres</li><li>MySQL</li><li>MariaDB</li><li>Oracle</li><li>Microsoft SQL Server</li><li>Aurora (AWS Proprietary database)</li></ul></li></ul><p>Advantage over using RDS vs deploying DB on EC2</p><ul><li>RDS is a managed service</li><li>automated provisioning, OS patching</li><li>continuous backups and restore to specific timestamp (point in time restore)</li><li>monitoring dashboards</li><li>read replicas for improved read performance</li><li>multi AZ setup for DR (Disaster Recovery)</li><li>maintenance windows for upgrades</li><li>scaling capability (vertical or horizontal)</li><li>storage backed by EBS</li><li>but you can’t SSH into your RDS instance (its managed by AWS)</li></ul><h2 id="rds-backups"><a class="markdownIt-Anchor" href="#rds-backups"></a> RDS backups</h2><ul><li>backups are automatically enabled in RDS</li><li>automatically backups<ul><li>daily full backup of the database (during the maintenance windows)</li><li>transaction logs are backed up by RDS every 5 minutes</li><li>ability to restore to any point in time (from oldest backup to 5 minutes ago)</li><li>7 days retention (can be increased to 35 days)</li></ul></li><li>DB snapshots<ul><li>manually triggered by the user</li><li>retention of backup for as long as you want</li></ul></li></ul><h2 id="rds-storage-auto-scaling"><a class="markdownIt-Anchor" href="#rds-storage-auto-scaling"></a> RDS storage auto scaling</h2><ul><li>helps you increase storage on your RDS DB instance dynamically</li><li>when RDS detects you are running out of free database storage, it scales automatically</li><li>avoid manually scaling your database storage</li><li>you have to set maximum storeage threshold (maximum limit for DB storage)</li><li>automatically modify storage if<ul><li>free storage is less than 10% of allocated storage</li><li>low storage lasts at least 5 minuts</li><li>6 hours have passed since last modification</li></ul></li><li>useful for applications with unpredicatable workloads</li><li>supports all RDS database engines (MariaDB, MySQL, PostgreSQL, SQL server, Oracle)</li></ul><h2 id="read-replicas-for-read-scalability"><a class="markdownIt-Anchor" href="#read-replicas-for-read-scalability"></a> Read Replicas for read scalability</h2><ul><li>up to 5 read replicas<ul><li>within AZ</li><li>cross AZ</li><li>cross region</li></ul></li><li>replication is ASYNC, so reads are eventually consisent, possible to read old data</li><li>replicas can be promoted to their own DB</li><li>applications must update the connection string to leverage read replicas</li></ul><h3 id="read-replicas-use-case"><a class="markdownIt-Anchor" href="#read-replicas-use-case"></a> Read replicas - use case</h3><ul><li>you have a production database that is taking on normal load</li><li>you want to run a reporting application to run some analytics</li><li>you create a read replica to run the new workload there</li><li>the production application is unaffected</li><li>read replicas are used for SELECT only kind of operations (not DELETE, INSERT, UPDATE)</li></ul><h3 id="network-cost"><a class="markdownIt-Anchor" href="#network-cost"></a> Network cost</h3><ul><li>in AWS there is a network cost when data goes from one AZ to another</li><li>for RDS read replicas within the same region, you don’t pay that fee</li><li>for read replicas across regions, you need to pay</li></ul><h3 id="multi-az-disaster-recovery"><a class="markdownIt-Anchor" href="#multi-az-disaster-recovery"></a> Multi AZ disaster recovery</h3><ul><li>SYNC replication</li><li>one DNS name - automatic app failaover to standby</li><li>increase availability</li><li>failover in case of loss of AZ, loss of network, instance or storage failure</li><li>no manual intervention in apps</li><li>not used for scaling (not handle traffic, only take over when master RDS fail)</li><li>NOTE: the read replicas can be setup as Multi AZ for disaster recovery</li></ul><h2 id="rds-from-single-az-to-multi-az"><a class="markdownIt-Anchor" href="#rds-from-single-az-to-multi-az"></a> RDS - from single AZ to multi AZ</h2><ul><li>zero downtime operation (no need to stop the DB)</li><li>just click on modify for the database</li><li>the following happens internally<ul><li>a snapshot is taken</li><li>a new DB is restored from the snapshot in a new AZ</li><li>synchronization is established between the two databases</li></ul></li></ul><h2 id="rds-security-encryption"><a class="markdownIt-Anchor" href="#rds-security-encryption"></a> RDS security - encryption</h2><ul><li>at rest<ul><li>possibility to encrypt the master and read replicas with AWS KMS - AES-256 encryption</li><li>encryption has to be defined at launch time</li><li>if the master is not encrypted, the read replica cannot be encrypted</li><li>Transparent Data Encryption (TDE) available for Oracle and SQL server</li></ul></li><li>in flight encryption<ul><li>SSL certificate to encrypt data to RDS in flight</li><li>provide SSL options with trust certificate when connecting to database</li></ul></li></ul><h3 id="encryption-operations"><a class="markdownIt-Anchor" href="#encryption-operations"></a> Encryption operations</h3><ul><li>Encrypting RDS backups<ul><li>snapshots of un-encrypted RDS databases are un-encrypted</li><li>snapshots of encrypted RDS database are encrypted</li><li>can copy a snapshot into an encrypted one</li></ul></li><li>to encrypt an un-encrypted RDS database<ul><li>create a snapshot of the un-encrypted database</li><li>copy the snapshot and enable encryption for the snapshot</li><li>restore the database from the encrypted snapshot</li><li>migrate applications to the new database and delete the old database</li></ul></li></ul><h2 id="rds-security-network-and-iam"><a class="markdownIt-Anchor" href="#rds-security-network-and-iam"></a> RDS security - network and IAM</h2><ul><li>network security<ul><li>RDS database are usually deployed within a private subnet, not in a public one</li><li>RDS security works by leveraging security groups (the same concept as for EC2 instances) - it controls which IP / security group can communicate with RDS</li></ul></li><li>access management<ul><li>IAM policies help control who can manage AWS RDS (through the RDS API)</li><li>traditional username and password can be used to login into the database</li><li>IAM based authentication can be used to login into RDS for MySQL and PostgreSQL</li></ul></li></ul><h3 id="rds-iam-authentication"><a class="markdownIt-Anchor" href="#rds-iam-authentication"></a> RDS - IAM authentication</h3><ul><li>IAM database authentication works with MySQL and PostgreSQL</li><li>you don’t need a password, just an authentication token obtained through IAM and RDS API calls</li><li>auth token has a lifetime of 15 minutes</li><li>benefits<ul><li>network in / out must be encrypted using SSL</li><li>IAM to centainly manage users instead of DB</li><li>can leverage IAM roles and EC2 instance profiles for easy integration</li></ul></li></ul><h1 id="amazon-aurora"><a class="markdownIt-Anchor" href="#amazon-aurora"></a> Amazon Aurora</h1><ul><li>Aurora is a proprietary technology from AWS (not open sourced)</li><li>Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MySQL database)</li><li>Aurora is AWS cloud optimized and claims 5x performance improvement over MySQL on RDS, over 3x performance of Postgres on RDS</li><li>Aurora storage automatically grows from 10 GB to 64 TB</li><li>Aurora can have 15 replicas while MySQL has up to 5, and the replication process is faster</li><li>failover in Aurora is instantaneous</li><li>Aurora costs more then RDS (20% more), but is more efficient</li></ul><h2 id="aurora-high-availability-and-read-scaling"><a class="markdownIt-Anchor" href="#aurora-high-availability-and-read-scaling"></a> Aurora High Availability and Read Scaling</h2><ul><li>6 copies of your data across 3 AZ<ul><li>4 copies out of 6 needed for writes</li><li>3 copies out of 6 needed for reads</li><li>self healing with peer to peer replication</li><li>storage is striped across 100s of volumes</li></ul></li><li>one Aurora instance takes writes (master)</li><li>automated failover for master in less than 30 seconds</li><li>master + up to 15 Aurora read replicas serve reads</li><li>support for cross region replication</li></ul><h2 id="aurora-custom-endpoints"><a class="markdownIt-Anchor" href="#aurora-custom-endpoints"></a> Aurora - Custom Endpoints</h2><ul><li>define a subset of Aurora instances as a custom endpoint</li><li>example: run analytical queries on specific replicas</li><li>the reader endpoint is generally not used after defining custom endpoints</li></ul><h2 id="aurora-serverless"><a class="markdownIt-Anchor" href="#aurora-serverless"></a> Aurora serverless</h2><ul><li>automated database instantiation and auto scaling based on actual usage</li><li>good for infrequent intermittent or unpredictable workloads</li><li>no capacity planning needed</li><li>pay per second, can be more cost effective</li></ul><h2 id="aurora-multi-master"><a class="markdownIt-Anchor" href="#aurora-multi-master"></a> Aurora Multi-Master</h2><ul><li>in case you want immediate failover for write node (HA)</li><li>every node does Read and write - vs - promoting a read replica as the new master (faster failover)</li></ul><h1 id="amazon-elasticache"><a class="markdownIt-Anchor" href="#amazon-elasticache"></a> Amazon ElastiCache</h1><ul><li>the same way RDS is to get managed relational databases</li><li>elastiCache is to get managed Redis or Memcached</li><li>caches are in memory databases with really high performance, low latency</li><li>helps reduce load off databases for read intensive workloads</li><li>helps make your application stateless</li><li>AWS takes care of OS maintenance / patching, optimization, setup, configuration, monitoring, failure recovery and backups</li><li>using ElastiCache involves heavy application code changes</li></ul><h2 id="db-cache"><a class="markdownIt-Anchor" href="#db-cache"></a> DB Cache</h2><ul><li>applications queries ElastiCache, if not available, get from RDS and store in ElastiCache</li><li>htlps relieve load in RDS</li><li>Cache must have an invalidation strategy to make sure only the most current data is used in there (LRU, LFU)</li></ul><h2 id="user-session-store"><a class="markdownIt-Anchor" href="#user-session-store"></a> User session store</h2><ul><li>user logs into any of the application</li><li>the application wrties the session data into ElastiCache</li><li>the user hits another instance of our application</li><li>the instance retrieve the session data and the user is already logged in</li></ul><h2 id="redis"><a class="markdownIt-Anchor" href="#redis"></a> Redis</h2><ul><li>Multi AZ witi auto faliover</li><li>read replicas to scale reads and have High Availability</li><li>data durability using AOF persistence</li><li>backup and restore features</li></ul><h2 id="memcached"><a class="markdownIt-Anchor" href="#memcached"></a> Memcached</h2><ul><li>multi-node for partitioning of data (sharding)</li><li>no high availability (replication)</li><li>non persistent</li><li>no backup and restore</li><li>multi-threaded architecture</li></ul><h2 id="patterns-for-elasticache"><a class="markdownIt-Anchor" href="#patterns-for-elasticache"></a> Patterns for ElastiCache</h2><ul><li>Lazy loading<ul><li>all the read data is cached, data can become stale in cache</li></ul></li><li>write through<ul><li>adds or update data in the cache when written to a DB (no stale data)</li></ul></li><li>session store:<ul><li>store temporary session data in a cache (using TTL features)</li></ul></li></ul><h2 id="reids-use-case"><a class="markdownIt-Anchor" href="#reids-use-case"></a> Reids use case</h2><ul><li>Gaming leaderboard</li><li>Redis Sorted sets guarantee both uniqueness and element ordering</li><li>each time a new element added, its ranked in real time, then added in correct order</li></ul><h1 id="route-53"><a class="markdownIt-Anchor" href="#route-53"></a> Route 53</h1><ul><li>route 53 is a managed DNS (domain name system)</li><li>DNS is a collection of rules and records which helps clients understand how to reach a server through its domain name</li><li>in AWS, the most common records are<ul><li>A: hostname =&gt; IPv4</li><li>AAAA: hostname =&gt; IPv6</li><li>CNAME: hostname =&gt; hostname</li><li>Alias: hostname =&gt; AWS resource</li></ul></li></ul><h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2><ul><li>route 53 can use<ul><li>public domain names you own</li><li>private domain names that can be resolved by your instances in your VPCs</li></ul></li><li>Route 53 has advanced features such as<ul><li>load balancing (through DNS, also called client load balancing)</li><li>health checks</li><li>routing policy: simple, failover, geolocation, latency, weighted…</li></ul></li><li>you pay $0.5 per month per hosted zone</li></ul><ol><li>user will first send DNS request (<a href="http://myapp.mydomain.com">http://myapp.mydomain.com</a>) to route 53 asking for IP address</li><li>route 53 will response will the ip address of that DNS and a TTL</li><li>user browser will then send the HTTP request to the correct IP to reach the server</li><li>next time when the user send DNS request, if the last request is not expire (check the TTL), browser will directly go to the last saved IP address, save traffic for route 53</li></ol><h2 id="cname-vs-alias"><a class="markdownIt-Anchor" href="#cname-vs-alias"></a> CNAME vs Alias</h2><ul><li>AWS resource (load balancer, cloudfront…) expose an AWS hostname and you want <a href="http://myapp.mydomain.com">myapp.mydomain.com</a></li><li>CNAME<ul><li>points a hostname to any other hostname</li><li>only for non root domain (<a href="http://something.mydomain.com">something.mydomain.com</a>)</li></ul></li><li>Alias<ul><li>points a hostname to an AWS resource</li><li>works for root domain and non root domain (<a href="http://mydomain.com">mydomain.com</a>)</li><li>free of charge</li><li>native health check</li></ul></li></ul><h2 id="simple-routing-policy"><a class="markdownIt-Anchor" href="#simple-routing-policy"></a> Simple Routing policy</h2><ul><li>use when you need to redirect to a single resource</li><li>you can’t attach health checks to simple routing policy</li><li>if multiple values (IP addresses) are returned, a random one is chosen by the client (client side load balancing)</li></ul><h2 id="weighted-routing-policy"><a class="markdownIt-Anchor" href="#weighted-routing-policy"></a> Weighted routing policy</h2><ul><li>control the percentage of the requests that go to specific endpoint</li><li>helpful to test percentage of traffic on new app version for example</li><li>helpful to split traffic between two regions</li><li>can be associated with health checks</li><li>but on the client side the browser is not aware that it has multiple weighted endpoints in the backend</li></ul><h2 id="latency-routing-policy"><a class="markdownIt-Anchor" href="#latency-routing-policy"></a> Latency routing policy</h2><ul><li>redirect to the server that has the least latency close to user</li><li>super helpful when latency of users is a priority</li><li>latency is evaluated in terms of user to designated AWS region</li><li>Germany may be redirected to the US (if that’s the lowest latency)</li></ul><h2 id="route-53-health-checks"><a class="markdownIt-Anchor" href="#route-53-health-checks"></a> Route 53 health checks</h2><ul><li>have X health checks failed =&gt; unhealthy (default 3)</li><li>have X health checks passed =&gt; healthy (default 3)</li><li>default health checks interval : 30 seconds (can set to 10 seconds with higher cost)</li><li>about 15 health checkers will the the endpoint health in the background (from different regions)<ul><li>one request every 2 seconds on average (30 / 15)</li><li>can have HTTP, TCP, and HTTPS health checks (no SSL verification)</li><li>possible to integrate health check with CloudWatch</li></ul></li><li>health checks can be linked to route 53 DNS queries</li></ul><h2 id="geolocation-routing-policy"><a class="markdownIt-Anchor" href="#geolocation-routing-policy"></a> GeoLocation routing policy</h2><ul><li>different from latency based</li><li>this is routing based on user location</li><li>here we specify traffic from the UK should go to this specific IP</li><li>should create a default policy (in case there is no match on location)</li></ul><h2 id="geoproximity-routing-policy"><a class="markdownIt-Anchor" href="#geoproximity-routing-policy"></a> Geoproximity routing policy</h2><ul><li>route traffic to your resources based on the geographic location of users and resources</li><li>ability to shift more traffic to resources based on the defined bias</li><li>to change the size of the geographic region, specify bias values<ul><li>to expand (1 to 99) - more traffic to the resources</li><li>to shrink (-1 to -99) - less traffic to the resources</li></ul></li><li>resources can be<ul><li>AWS resources (specify AWS region)</li><li>non-AWS resources (specify latitude and longitude)</li></ul></li><li>you must use route 53 traffic flow (advanced) to use this feature</li></ul><h2 id="multi-value-routing-policy"><a class="markdownIt-Anchor" href="#multi-value-routing-policy"></a> Multi value routing policy</h2><ul><li>use when routing traffic to multiple resources</li><li>want to associate a route 53 health checks with records</li><li>up to 8 healthy records are returned for each multi value query</li><li>multi value is not a substitute for having an ELB</li><li>client browser will randomly choose a healthy record from returned records (client side fault tolerance)</li></ul><h2 id="route-53-as-a-registrar"><a class="markdownIt-Anchor" href="#route-53-as-a-registrar"></a> Route 53 as a Registrar</h2><ul><li><p>a domain name registrar is an organization that manages the reservation of internet domain names</p><ul><li>GoDaddy</li><li>Google Domains</li></ul></li><li><p>Domain registrar != DNS</p></li><li><p>if you buy your domain on 3rd party website, you can still use route 53</p></li></ul><ol><li>create a hosted zone in route 53</li><li>update NS records on 3rd party website to use route 53 name servers (all 4 of them)</li></ol><h1 id="classic-solutions-architecture"><a class="markdownIt-Anchor" href="#classic-solutions-architecture"></a> Classic Solutions Architecture</h1><h2 id="stateful-app-with-shopping-cart"><a class="markdownIt-Anchor" href="#stateful-app-with-shopping-cart"></a> Stateful App with shopping cart</h2><ul><li>ELB sticky sessions</li><li>web clients for storing cookies and making our web app stateless</li><li>ElastiCache<ul><li>for storing sessions (alternative: DynamoDB)</li><li>for caching data from RDS</li><li>Multi AZ</li></ul></li><li>RDS<ul><li>for storing user data</li><li>read replicas for scaling reads</li><li>multi AZ for disaster recovery</li></ul></li><li>tight security with security groups referencing each other</li></ul><h2 id="instantiating-applications-quickly"><a class="markdownIt-Anchor" href="#instantiating-applications-quickly"></a> Instantiating Applications Quickly</h2><ul><li>EC2 instances<ul><li>use a golden AMI: install your applications, OS dependencies, beforehand and launch your EC2 instance from the golden AMI</li><li>bootstrap using user data: for dynamic configuration, use User Data scripts</li><li>Hybrid: mix golden AMI and User Data (Elastic Beanstalk)</li></ul></li><li>RDS databases<ul><li>restore from a snapshot: the database will have schemas and data ready</li></ul></li><li>EBS volume:<ul><li>restore from a snapshot, the disk will already be formatted and have data</li></ul></li></ul><h1 id="beanstalk"><a class="markdownIt-Anchor" href="#beanstalk"></a> Beanstalk</h1><h2 id="developer-problems-on-aws"><a class="markdownIt-Anchor" href="#developer-problems-on-aws"></a> Developer problems on AWS</h2><ul><li><p>managing infrastructure</p></li><li><p>deploying code</p></li><li><p>configuring all the databases, load balancers, etc…</p></li><li><p>scaling concerns</p></li><li><p>most web apps have the same architecture (ALB + ASG)</p></li><li><p>all the developers want is for their code to run</p></li><li><p>possibly, consistently across different applications and environments</p></li></ul><h2 id="elastic-beanstalk-overview"><a class="markdownIt-Anchor" href="#elastic-beanstalk-overview"></a> Elastic Beanstalk - overview</h2><ul><li>Elastic Beanstalk is a developer centric view of deploying an application on AWS</li><li>it uses all the component’s we have seen before: EC2, ASG, ELB, RDS…</li><li>managed service<ul><li>automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration…</li><li>just the application code is the responsiblity of the developer</li></ul></li><li>we still have full control over the configuration</li><li>Beanstalk is free but you pay for the underlying instances</li></ul><h2 id="elastic-beanstalk-components"><a class="markdownIt-Anchor" href="#elastic-beanstalk-components"></a> Elastic Beanstalk - components</h2><ul><li>application<ul><li>collectioin of Elastic Beanstalk components (environments, versions, configurations…)</li></ul></li><li>application version<ul><li>an iteration of your application code</li></ul></li><li>environment<ul><li>collection of AWS resources running an application version (only one application version at a time)</li><li>Tiers<ul><li>web server environment tier</li><li>worker environment tier</li></ul></li><li>you can create multiple environments (dev, test, prod…)</li></ul></li></ul><h1 id="s3"><a class="markdownIt-Anchor" href="#s3"></a> S3</h1><h2 id="buckets"><a class="markdownIt-Anchor" href="#buckets"></a> Buckets</h2><ul><li>Amazon S3 allows people to store objects in buckets</li><li>buckets must have a globally unique name</li><li>buckets are defined at the region level</li><li>naming convention<ul><li>No uppercase</li><li>no underscore</li><li>3-63 characters long</li><li>not an ip</li><li>must start with lowercase letter or number</li></ul></li></ul><h2 id="objects"><a class="markdownIt-Anchor" href="#objects"></a> Objects</h2><ul><li><p>objects (files) have a key</p></li><li><p>the key is the FULL path</p><ul><li>s3:&quot;//my-bucket/my_folder/another_folder/my_file.txt</li><li>key is: my_folder/another_folder/my_file.txt</li><li>prefix is: my_folder/another_folder/</li><li>object name is: my_file.txt</li></ul></li><li><p>there is no conecpt of directories within buckets</p></li><li><p>just keys with very long names that contain slashes</p></li><li><p>object values are the content of the body</p><ul><li>max object size is 5TB</li><li>if uploading more than 5GB, must use multi-part upload</li></ul></li><li><p>metadata (list of text key / value pairs, system or user metadata)</p></li><li><p>Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle</p></li><li><p>Version ID (if versioning is enabled)</p></li></ul><h2 id="versioning"><a class="markdownIt-Anchor" href="#versioning"></a> Versioning</h2><ul><li>you can version your files in Amazon S3</li><li>it is enabled at the bucket level</li><li>same key overwrite will increment the version</li><li>it is best pratice to version your buckets<ul><li>protect against unintended deletes (ability to restore a version)</li><li>easy roll back to previous version</li></ul></li><li>notes<ul><li>any file that is not versioned prior to enabling versioning will have version <code>null</code></li><li>suspending versioning does not delete the previous versions</li></ul></li></ul><h2 id="encryption-for-objects"><a class="markdownIt-Anchor" href="#encryption-for-objects"></a> Encryption for objects</h2><ol><li>SSE-S3: encrypts S3 object using keys handled and managed by AWS</li><li>SSE-KMS: leverage AWS KMS service to manage encryption keys</li><li>SSE-C: when you want to manage your own encryption keys</li><li>client side encryption</li></ol><ul><li>it is important to understand which ones are adapted to which situation for the exam</li></ul><h3 id="s3-default-encryption"><a class="markdownIt-Anchor" href="#s3-default-encryption"></a> S3 Default Encryption</h3><ul><li>one way to force encryption is to use a bucket policy and refuse any API call to PUT an S3 object without encryption headers</li><li>another way is to use the default encryption option in S3</li><li>note: bucket policies are evaluated before default encryption<ul><li>e.g. if you have a bucket policy to reject all un-encrypted files from being upload to S3, then you can’t upload un-encrypted file even if you have the default encryption enabled.</li></ul></li></ul><h3 id="sse-c"><a class="markdownIt-Anchor" href="#sse-c"></a> SSE-C</h3><ul><li>Amazon S3 does not store the encryption key you provide</li><li>HTTPS must be used (because you need to send the key in the request)</li><li>Encryption key must provided in HTTP headers, for every request</li></ul><h3 id="client-side-encryption"><a class="markdownIt-Anchor" href="#client-side-encryption"></a> Client side encryption</h3><ul><li>client library such as the Amazon S3 Encryption client</li><li>clients must encrypt the data themselves before sending to S3</li><li>clients must decrypt the data themselves when retrieving the data from S3</li><li>customer fully manages the keys and encryption cycle</li></ul><h3 id="encryption-in-transit-ssltls"><a class="markdownIt-Anchor" href="#encryption-in-transit-ssltls"></a> Encryption in transit (SSL/TLS)</h3><ul><li>Amazon S3 exposes<ul><li>HTTP endpoint, non encrypted</li><li>HTTPS endpoint, encryption in flight</li></ul></li><li>You are free to use the endpoint you want, but HTTPS is recommended</li><li>most clients would use the HTTPS endpoint by default</li><li>HTTPS is mandatory for SSE-C (because you need to send the key in the request)</li></ul><h2 id="security"><a class="markdownIt-Anchor" href="#security"></a> Security</h2><ul><li><p>User based</p><ul><li>IAM policies: which API calls should be allowed for a specific user from IAM console</li></ul></li><li><p>Resource based</p><ul><li>bucket policies: bucket wide rules from the S3 console - allows cross account</li><li>Object ACL: finer grain</li><li>Bucket ACL: less common</li></ul></li><li><p>Note: an IAM principal can access an S3 object if</p><ul><li>the user IAM permissions allow it OR the resource policy ALLOW it</li><li>AND there is no explicit DENY</li></ul></li><li><p>JSON based policies</p><ul><li>Resources: buckets and objects</li><li>Actions: Set of API to Allow or Deny</li><li>Effect: Allow / Deny</li><li>Principal: the account or user to apply the policy to</li></ul></li><li><p>use S3 bucket for policy to</p><ul><li>Grant public access to the bucket</li><li>Force objects to be encrypted at upload</li><li>Grant access to another account (cross account)</li></ul></li></ul><h2 id="cors"><a class="markdownIt-Anchor" href="#cors"></a> CORS</h2><ul><li>An origin is a scheme, host, and port</li><li>CORS means Cross-Origin Resource Sharing</li><li>Web browser based machanism to allow requests to other origins while visiting the main origin</li><li>the requests won’t be fulfilled unless the other origin allows for the requests, using CORS Headers (<code>Access-Control-Allow-Origin</code>)</li><li>if a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers</li><li>you can allow for a specific origin or * for all origins</li></ul><h2 id="s3-access-logs"><a class="markdownIt-Anchor" href="#s3-access-logs"></a> S3 Access logs</h2><ul><li>for audit purpose, you may want to log all access to S3 buckets</li><li>any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket</li><li>that data can be analyzed using data analysis tools later (Amazon Athena)</li></ul><h2 id="s3-replication-cross-region-or-same-region"><a class="markdownIt-Anchor" href="#s3-replication-cross-region-or-same-region"></a> S3 Replication (Cross-Region or Same Region)</h2><ul><li><p>Must enable versioning in source and destination</p></li><li><p>cross region replication</p></li><li><p>same region replication</p></li><li><p>buckets can be in different accounts</p></li><li><p>copying is asynchronous</p></li><li><p>must give proper IAM permissions to S3</p></li><li><p>After activating, only new objects are replicated (existing objects will not be replicated)</p></li><li><p>For DELETE operations</p><ul><li>can replicate delete markers from source to target (optional setting)</li><li>deletions with a version ID are not replicated (to avoid malicious deletes)</li></ul></li><li><p>there is not chaining of replication</p><ul><li>if bucket one has replicatio into bucket two, which has replicatioin into bucket three</li><li>then objects created in bucket one are not replicated to bucket three</li></ul></li></ul><h2 id="s3-pre-signed-url"><a class="markdownIt-Anchor" href="#s3-pre-signed-url"></a> S3 Pre-signed URL</h2><ul><li>can generate pre-signed URLs using SDK or CLI</li><li>valid for a default of 3600 seconds</li><li>users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT</li></ul><h2 id="s3-storage-class"><a class="markdownIt-Anchor" href="#s3-storage-class"></a> S3 Storage Class</h2><h3 id="standard-general-purpose"><a class="markdownIt-Anchor" href="#standard-general-purpose"></a> Standard - General Purpose</h3><ul><li>High durability of objects across multiple AZ</li><li>99.99% availability over a given year</li><li>sustain 2 concurrent facility failure</li><li>use case: big data analytics, mobile and gaming applications, content distribution…</li></ul><h3 id="standard-ia"><a class="markdownIt-Anchor" href="#standard-ia"></a> Standard - IA</h3><ul><li>suitable for data that is less frequently accessed, but requires rapid access when needed</li><li>99.9% availability</li><li>low cost compared to GP</li><li>use cases: as a data store for disater recovery, backups</li></ul><h3 id="one-zone-ia"><a class="markdownIt-Anchor" href="#one-zone-ia"></a> One Zone - IA</h3><ul><li>Same as IA but data is stored in a single AZ</li><li>99.5% availability</li><li>low latency and high throughput performance</li><li>supports SSL for data at transit and encryption at rest</li><li>low cost compared to IA</li><li>use cases: storing secondary backup copies of on-premise data, or storing data you can re-create</li></ul><h3 id="intelligent-tiering"><a class="markdownIt-Anchor" href="#intelligent-tiering"></a> Intelligent Tiering</h3><ul><li>same low latency and high throughput performance of S3 standard</li><li>small monthly monitoring and auto-tiering fee</li><li>automatically moves objects between two access tiers based on changing access patterns</li><li>resilient against events that impact an entire AZ</li></ul><h3 id="amazon-glacier"><a class="markdownIt-Anchor" href="#amazon-glacier"></a> Amazon Glacier</h3><ul><li>low cost object storage meant for archiving / backup</li><li>data is retained for the longer term (10+ years)</li><li>alternative to on-premise magnetic tape storage</li><li>cost per storage per month + retrieval cost</li><li>each item in glacier is called archive</li><li>archives are stored in Vaults</li></ul><h3 id="amazon-glacier-and-glacier-deep-archive"><a class="markdownIt-Anchor" href="#amazon-glacier-and-glacier-deep-archive"></a> Amazon Glacier and Glacier Deep Archive</h3><ul><li>Amazon Glacier - 3 retrieval options<ul><li>expedited (1 to 5 minues)</li><li>standard (3 to 5 hours)</li><li>bulk (5 to 12 hours)</li><li>minimum storage duration of 90 days</li></ul></li><li>Amazon Glacier Deep Archive<ul><li>Standard (12 hours)</li><li>Bulk (48 hours)</li><li>Minimum storage duration of 180 days</li></ul></li></ul><h2 id="s3-lifecycle-rules"><a class="markdownIt-Anchor" href="#s3-lifecycle-rules"></a> S3 Lifecycle Rules</h2><ul><li>Transition actions<ul><li>it defines when objects are transitioned to another storage class</li><li>e.g. move objects to Standard IA class 60 days after creation</li><li>e.g. move to Glacier for archiving after 6 months</li></ul></li><li>Expiration actions<ul><li>configure to expire (delete) after some time</li><li>e.g. access log files can be set to delete after a 365 days</li><li>e.g. can be used to delete old versions of files (if versioning is enabled)</li><li>e.g. can be used to delete incomplete multi-part uploads</li></ul></li></ul><h3 id="s3-analytics-storage-class-analysis"><a class="markdownIt-Anchor" href="#s3-analytics-storage-class-analysis"></a> S3 Analytics - Storage Class Analysis</h3><ul><li>You can setup S3 analytics to help determine when to transit objects from Standard to Standard IA</li><li>does not work for One Zone - IA or Glacier</li></ul><h2 id="s3-select-and-glacier-select"><a class="markdownIt-Anchor" href="#s3-select-and-glacier-select"></a> S3 Select and Glacier Select</h2><ul><li>retrieve less data using SQL by performing server side filtering</li><li>can filter by rows and columns</li><li>less network transfer, less CPU cost client-side</li></ul><h2 id="s3-event-notifications"><a class="markdownIt-Anchor" href="#s3-event-notifications"></a> S3 Event notifications</h2><ul><li>can create as many events as desired<ul><li>SNS</li><li>SQS</li><li>Lambda function</li></ul></li><li>S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer</li><li>if two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</li><li>if you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.</li><li>Your S3 bucket needs to have permission to send message to SQS queue</li></ul><h2 id="s3-requester-pays"><a class="markdownIt-Anchor" href="#s3-requester-pays"></a> S3 Requester Pays</h2><ul><li>in general, bucket owners pay for all S3 storage and data transfer costs associated with their buckets</li><li>with Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket</li><li>helpful when you want to share large datasets with other accounts</li><li>the requester must be authenticated in AWS (cannot be anonymous)</li></ul><h2 id="glacier-vault-lock"><a class="markdownIt-Anchor" href="#glacier-vault-lock"></a> Glacier Vault Lock</h2><ul><li>Adopt a WORM (Write Once Read Many) model</li><li>lock the policy for future edits (can no longer be changed)</li><li>helpful for compliance and data retention</li></ul><h2 id="s3-object-lock-versioning-must-be-enabled"><a class="markdownIt-Anchor" href="#s3-object-lock-versioning-must-be-enabled"></a> S3 Object Lock (versioning must be enabled)</h2><ul><li>adopt a WORM (Write Once Read Many) model</li><li>block an object version deletion for a specified amount of time</li><li>object retention<ul><li>retention period: specifies a fixed period</li><li>legal hold: same protection, no expiry date</li></ul></li><li>Modes<ul><li>Governance mode: users can’t overwrite or delete an object version or alter its lock settings unless they have special permissions</li><li>Compliance mode: a protected object version can’t be overwritten or deleted by any user, including the root user. When an object is locked in compliance mode, its retention mode can’t be changed and its retention period can’t be shortened.</li></ul></li></ul><h1 id="aws-athena"><a class="markdownIt-Anchor" href="#aws-athena"></a> AWS Athena</h1><ul><li>serverless service to perform analytics directly against S3 files</li><li>uses SQL language to query the files</li><li>has a JDBC / ODBC driver (for BI tools)</li><li>charged per query and amount of data scanned</li><li>supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)</li><li>use cases: BI / Analytics / reporting / Logs / CloudTrail trails etc…</li></ul><h1 id="aws-cloudfront"><a class="markdownIt-Anchor" href="#aws-cloudfront"></a> AWS CloudFront</h1><ul><li>CDN</li><li>improves read performance, content is cached at the edge loctions</li><li>216 point of presence globally (edge locations)</li><li>DDoS protection, integration with Shield, AWS web application firewall</li><li>can expose external HTTPS and can talk to internal HTTPS backends</li></ul><h2 id="origins"><a class="markdownIt-Anchor" href="#origins"></a> Origins</h2><ul><li>S3 bucket<ul><li>for distributing files and caching them at the edge</li><li>enahnced security with CloudFront OAI (Origin Access Identity), this can block access directly to S3</li><li>CloudFront can be used as an ingress (to upload files to S3)</li></ul></li><li>Custom Origin (HTTP)<ul><li>Application Load Balancer</li><li>EC2 instance</li><li>S3 Website (must first enable the bucket as a static S3 website)</li><li>Any HTTP backend you want</li></ul></li></ul><h3 id="cloudfront-vs-s3-cross-region-replication"><a class="markdownIt-Anchor" href="#cloudfront-vs-s3-cross-region-replication"></a> CloudFront vs S3 Cross Region Replication</h3><ul><li>CloudFront<ul><li>Global Edge Network</li><li>files are cached for a TTL</li><li>great for static content that must be available everywhere, maybe outdated for a while</li></ul></li><li>S3 Cross Region Replication<ul><li>must be setup for each region you want replication to happen</li><li>files are updated in near real time</li><li>read only</li><li>great for dynamic content that needs to be available at low latency in few regions</li></ul></li></ul><h2 id="cloudfront-signed-url-signed-cookies"><a class="markdownIt-Anchor" href="#cloudfront-signed-url-signed-cookies"></a> CloudFront Signed URL / Signed Cookies</h2><ul><li>you want to distribute paid share content to premium users over the world</li><li>we can use CloudFront Signed URL / Cookie, we attach a policy with<ul><li>includes URL expiration</li><li>includes IP ranges to access the data from</li><li>Trusted Signers (which AWS accounts can create signed URLs)</li></ul></li><li>Signed URL: access to individual files (one signed URL per file)</li><li>Signed Cookies: access to multiple files (one signed Cookie for many files)</li></ul><h3 id="process"><a class="markdownIt-Anchor" href="#process"></a> Process</h3><ol><li>user authenticate and authorized to the application</li><li>the application send request to CloudFront to generate Signed URL / Cookie</li><li>the application send the signed URL / Cookie back to user</li><li>user use the signed URL / Cookie to access the file in CloudFront</li><li>CloudFront fetch the file from S3 to the user</li></ol><h3 id="cloudfront-signed-url-vs-s3-pre-signed-url"><a class="markdownIt-Anchor" href="#cloudfront-signed-url-vs-s3-pre-signed-url"></a> CloudFront Signed URL vs S3 Pre-signed URL</h3><ul><li>CloudFront Signed URL<ul><li>Allow access to a path, no matter the origin</li><li>account wide key pair, only the root can manage it</li><li>can filter by IP, path, date, expiration</li><li>can leverage caching features</li></ul></li><li>S3 Pre-signed URL<ul><li>issue a request as the person who pre-signed the URL</li><li>uses the IAM key of the signing IAM principal</li><li>limited lifetime</li></ul></li></ul><h2 id="cloudfront-price-class"><a class="markdownIt-Anchor" href="#cloudfront-price-class"></a> CloudFront - Price Class</h2><ul><li>You can reduce the number of edge locations for cost reduction</li><li>3 price classes<ul><li>price class all: all regions</li><li>price class 200: most regions, but excludes the most expensive regions</li><li>price class 100: only the least expensive regions</li></ul></li></ul><h2 id="cloudfront-multiple-origins"><a class="markdownIt-Anchor" href="#cloudfront-multiple-origins"></a> CloudFront - Multiple Origins</h2><ul><li>to route to different kind of origins based on the content type</li><li>e.g. one origin is from ALB and another origin is from S3 bucket</li></ul><h3 id="based-on-the-path-pattern"><a class="markdownIt-Anchor" href="#based-on-the-path-pattern"></a> Based on the path pattern</h3><ul><li><code>/images/*</code></li><li><code>/api/*</code></li><li><code>/*</code></li></ul><h2 id="cloudfront-origin-groups"><a class="markdownIt-Anchor" href="#cloudfront-origin-groups"></a> CloudFront - Origin Groups</h2><ul><li>to increase high availability and do failover</li><li>Origin Groups: one primary and one secondary origin</li><li>if the primary origin fails, the second one is used (CloudFront will send the same request to the secondary origin)</li></ul><h2 id="cloudfront-field-level-encryption"><a class="markdownIt-Anchor" href="#cloudfront-field-level-encryption"></a> CloudFront - Field Level Encryption</h2><ul><li>protect user sensitive information through application stack</li><li>adds an additional layer of security along with HTTPS</li><li>sensitive information encrypted at the edge close to the user</li><li>uses asymmetric encryption (public private key pair)</li><li>usage<ol><li>client send sensitive information to the edge location</li><li>edge location use public key to encrypt the information</li><li>edge location send encrypted information to CloudFront</li><li>CloudFront send information all the way to (CloudFront =&gt; ALB =&gt; Web Servers) Web server</li><li>Web server uses the private key to decrypt the information</li></ol></li></ul><h1 id="aws-global-accelerator"><a class="markdownIt-Anchor" href="#aws-global-accelerator"></a> AWS Global Accelerator</h1><h2 id="global-users-for-our-application"><a class="markdownIt-Anchor" href="#global-users-for-our-application"></a> Global users for our application</h2><ul><li>you have deployed an application and have global users who want to access it directly</li><li>they go over the public internet, which can add a lot of latency due to many hops</li><li>we wish to go as fast as possible through AWS network to minimize latency</li></ul><h2 id="unicast-ip-vs-anycast-ip"><a class="markdownIt-Anchor" href="#unicast-ip-vs-anycast-ip"></a> Unicast IP vs Anycast IP</h2><ul><li>Unicast IP<ul><li>one server holds one IP address</li></ul></li><li>Anycast IP<ul><li>all servers hold the same IP address and the client is routed to the nearest one</li></ul></li></ul><h2 id="aws-global-accelerator-2"><a class="markdownIt-Anchor" href="#aws-global-accelerator-2"></a> AWS Global Accelerator</h2><ul><li>leverage the AWS internal network to route to your application</li><li>2 Anycast IP are created for your application</li><li>the Anycast IP send traffic directly to Edge locations</li><li>the edge locations send the traffic to your application (through AWS private network)</li></ul><h2 id="global-accelerator-vs-cloudfront"><a class="markdownIt-Anchor" href="#global-accelerator-vs-cloudfront"></a> Global Accelerator vs CloudFront</h2><ul><li><p>they both use the AWS global network and its edge locations around the world</p></li><li><p>both services integrate with AWS shield for DDoS protection</p></li><li><p>CloutFront</p><ul><li>improves performance for both cacheable content (images / videos)</li><li>Dynamic content (such as API acceleration and dynamic site delivery)</li><li>content is served at the edge location</li></ul></li><li><p>Global Accelerator</p><ul><li>improves performance for a wide range of applications over TCP or UDP</li><li>proxying packets at the edge to applications running in one or more AWS regions</li><li>good fit for non-HTTP use cases: such as gaming (UDP), IoT (MQTT) or Voice Over IP</li><li>good for HTTP use cases that require static IP (if use Route 53 Geo location, client browser will cache the IP address and redirect user to the old IP for a TTL)</li><li>good for HTTP use cases that require deterministic, fast regional failover</li></ul></li></ul><h1 id="aws-snow-family"><a class="markdownIt-Anchor" href="#aws-snow-family"></a> AWS Snow Family</h1><ul><li>Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS</li></ul><h2 id="snowball-edge-for-data-transfers"><a class="markdownIt-Anchor" href="#snowball-edge-for-data-transfers"></a> Snowball Edge (for data transfers)</h2><ul><li>physical data transport solution</li><li>alternative to moving data over the network</li><li>pay per data transfer job</li><li>provide block storage and Amazon S3 compatible object storage</li><li>Snowball Edge Storage Optimzied</li><li>Snowball Edge Compute Optimized</li></ul><h2 id="snowcore"><a class="markdownIt-Anchor" href="#snowcore"></a> Snowcore</h2><ul><li>small, portable computing, anywhere, rugged and secure, withstands harsh environements</li><li>light, 2.1 kg</li><li>device used for edge computing, storage, and data transfer</li><li>8 TB usable storage</li><li>must provide your own battery and cables</li><li>can be sent back to AWS offline, or connect it to internet and use AWS datasync to send data</li></ul><h2 id="snowmobile"><a class="markdownIt-Anchor" href="#snowmobile"></a> Snowmobile</h2><ul><li>transfer exabytes of data (1 EB = 1000 PB = 1,000,000 TB)</li><li>each snowmobile has 100 PB of capacity</li><li>high security</li><li>better than snowball if you transfer more then 10 PB</li></ul><h2 id="edge-computing"><a class="markdownIt-Anchor" href="#edge-computing"></a> Edge computing</h2><ul><li>process data while its being created on an edge location<ul><li>A truck on the road, a ship on the sea, a mining station underground (no internet access)</li></ul></li><li>these locations may have<ul><li>limited / no internet access</li><li>limited / no easy access to computing power</li></ul></li><li>we setup a snowball / snowcone device to do edge computing</li><li>eventually we can ship back the device to AWS</li></ul><h2 id="aws-opshub"><a class="markdownIt-Anchor" href="#aws-opshub"></a> AWS OpsHub</h2><ul><li>Historically, to use Snow Family devices, you need a CLI</li><li>today, you can use AWS OpsHub (a software you install on your computer / laptop) to manage your snow family devices<ul><li>unlocking and configuring single or clustered devices</li><li>transferring files</li><li>launching and managing instances running on Snow family devices</li><li>monitor device metrics (storage capacity, active instances)</li><li>launch compatible AWS services on your devices</li></ul></li></ul><h3 id="snowball-into-glacier"><a class="markdownIt-Anchor" href="#snowball-into-glacier"></a> Snowball into Glacier</h3><ul><li>Snowball cannot import to Glacier directly</li><li>you must use Amazon S3 first, in combination with an S3 lifecycle policy</li></ul><h1 id="aws-storage-gateway"><a class="markdownIt-Anchor" href="#aws-storage-gateway"></a> AWS Storage Gateway</h1><ul><li>Bridge between on-premises data and cloud data in S3</li><li>use cases: disaster recovery, backup and restore, tiered storage</li></ul><h2 id="file-gateway"><a class="markdownIt-Anchor" href="#file-gateway"></a> File Gateway</h2><ul><li>configured S3 buckets are accessible using the NFS and SMB protocol</li><li>supports S3 standard, S3 IA, S3 One Zone IA</li><li>bucket access using IAM roles for each File Gateway</li><li>most recently used ata is cached in the file Gateway</li><li>can be mounted on many servers</li><li>integrated with AD (Active Directory) for user authentication</li></ul><ol><li>On-premises server communicate with File Gateway (optionally with Authentication)</li><li>File Gateway communicate with S3 buckets</li></ol><h2 id="volume-gateway"><a class="markdownIt-Anchor" href="#volume-gateway"></a> Volume Gateway</h2><ul><li>block storage using iSCSI protocol backed by S3</li><li>backed by EBS snapshot which can help restore on-premises volumes</li><li>cached volumes: low latency access to most recent data</li><li>stored volumes: entire dataset is on premises, scheduled backups to S3</li></ul><ol><li>On-premises server communicate with Volume Gateway using iSCSI protocol</li><li>Volume Gateway communicate with S3 bucket to create EBS snapshots</li></ol><ul><li>Volume Gateway is often used as data backup</li></ul><h2 id="tape-gateway"><a class="markdownIt-Anchor" href="#tape-gateway"></a> Tape Gateway</h2><ul><li>some companies have backup processes using physical tapes</li><li>with tape gateway, companies use the same processes but, in the cloud</li><li>Virtual Tape Library (VTL) backed by Amazon S3 and Glacier</li><li>backup data using existing tape-based processes</li></ul><h2 id="storage-gateway-hardware-appliance"><a class="markdownIt-Anchor" href="#storage-gateway-hardware-appliance"></a> Storage Gateway - Hardware appliance</h2><ul><li>if you don’t have on-premises virtual server, you can buy from Amazon</li></ul><h1 id="aws-fsx"><a class="markdownIt-Anchor" href="#aws-fsx"></a> AWS FSx</h1><h2 id="aws-fsx-for-windows"><a class="markdownIt-Anchor" href="#aws-fsx-for-windows"></a> AWS FSx for Windows</h2><ul><li>EFS is a shared POSIX system for Linux system</li><li>FSx is a fully managed Windows file system share drive</li><li>supports SMB protocol and Windows NTFS</li><li>Microsoft Active Directory integration, ACLs, user quotas</li><li>can be accessed from your on-premises infrastructure</li><li>can be configured to be Multi-AZ</li><li>data is backed up daily to S3</li></ul><h2 id="amazon-fsx-for-lustre"><a class="markdownIt-Anchor" href="#amazon-fsx-for-lustre"></a> Amazon FSx for Lustre</h2><ul><li>Lustre is a type of parallel distributed file system, for large scale computing</li><li>the name Lustre is derived from Linux and Cluster</li><li>Machine Learning, High Performance Computing</li><li>Video processing, Financial Modeling and Electronic Design Automation</li><li>Seamless integration with S3</li><li>can be used from on-premises servers</li></ul><h3 id="fsx-file-system-deployment-options"><a class="markdownIt-Anchor" href="#fsx-file-system-deployment-options"></a> FSx File System Deployment Options</h3><ul><li>Scratch file system<ul><li>temporary storage</li><li>data is not replicated</li><li>high burst</li><li>usage: short-term processing, optimize costs</li></ul></li><li>Persistent File System<ul><li>long-term storage</li><li>data is replicated within same AZ</li><li>replace failed files within minutes</li><li>usage: long-term processing, sensitive data</li></ul></li></ul><h1 id="aws-transfer-family"><a class="markdownIt-Anchor" href="#aws-transfer-family"></a> AWS Transfer Family</h1><ul><li>a fully managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol</li><li>supported protocols<ul><li>AWS Transfer for FTP (File Transfer Protocol)</li><li>AWS Transfer for FTPS (File Transfer Protocol over SSL)</li><li>AWS Transfer for SFTP (Secure File Transfer Protocol)</li></ul></li><li>Managed infrastructure, scalable, reliable, highly available</li><li>pay per provisioned endpoint per hour + data transfers in GB</li><li>store and manage users’ credentials within the services</li><li>integrate with existing authentication systems (Microsoft Active Directory, LDAP, Okta…)</li></ul><h1 id="aws-storage-comparison"><a class="markdownIt-Anchor" href="#aws-storage-comparison"></a> AWS Storage Comparison</h1><ul><li>S3: Object Storage<ul><li>S3 is going to be an object storage, it’s going to be serverless, you don’t have to prove incapacity ahead of time. It has some deep integration with so many database services.</li></ul></li><li>Glacier: Object Archival<ul><li>Glacier is going to be for object archival. So this is when we want to store objects for a long period of time. Retrieve it very very rarely, and when we retrieve these objects, they’re going to be taking a lot of time to get back to us because they are archived.</li></ul></li><li>EFS: Network File System for Linux instances, POSIX file system<ul><li>EFS is Elastic File System, and this is a network file system for Linux instances. It is a POSIX file system so that means for Linux again. And it is accessible from all your EC2 instances at once. So it is something that is going to be shared and across AZ.</li></ul></li><li>FSx for Windows: Network File System for Windows servers<ul><li>FSx for Windows is the same thing as EFS, but for Windows. So it’s a network file system for your Windows servers.</li></ul></li><li>FSx for Lustre: High performance computing Linux file system<ul><li>FSx for Lustre is Linux and cluster, so it’s for High Performance Computing Linux file system. This is where you’re going to do your HPC running. You only have insanely high IOPS, insanely big capacity. And it has integration with S3 in the back end.</li></ul></li><li>EBS volume: network storage for one EC2 instance at a time<ul><li>EBS volumes is your network storage for one EC2 instance at a time only. And it is bound to a specific availability zone that you create it in. And in case you wanted to change the AZ, you will need to create a snapshot, move that snapshot over, and create a volume from it.</li></ul></li><li>Instance Storage: physical storage for your EC2 instance (high IOPS)<ul><li>Instance Storage is going to be physical storage for your EC2 instance. And so, because it’s attached from the hardware, then it’s going to have a much higher IOPS than EBS. EBS volumes, as we remember, it is up to 16,000 IOPS or 64,000 IOPS for io1. But for Instance Storage, because it is physically attached to your EC2 instance, you can get, for some, millions of IOPS. Um, it’s going to be very high. But the risk is that if your EC2 instance goes down, then you will lose that storage permanently.</li></ul></li><li>Storage Gateway: file gateway, volume gateway, tape gateway<ul><li>Storage Gateway is going to be transporting files from on premise to AWS. So we have File Gateway, Volume Gateway for cache and stored, and Tape Gateway. Each with their use cases.</li></ul></li><li>Snowball / Snowmobile: to move large amount of data to the cloud, physically<ul><li>And then finally, Snowball/Snowmobile to move large amount of data to the cloud physically into S3.</li></ul></li><li>database: for specific workloads, usually with indexing and querying</li></ul><h1 id="amazon-sqs-simple-queuing-service"><a class="markdownIt-Anchor" href="#amazon-sqs-simple-queuing-service"></a> Amazon SQS (Simple Queuing Service)</h1><ul><li>Fully managed service, used to decouple applications</li><li>attributes<ul><li>unlimited throughput, unlimited number of messages in queue</li><li>default retention of messages: 4 days, to maximum 14 days</li><li>low latency</li><li>limitation of 256KB per message sent</li></ul></li><li>can have duplicate messages (at least once delivery, occasionally)</li><li>can have out of order message (best effort ordering)</li></ul><h2 id="producing-messages"><a class="markdownIt-Anchor" href="#producing-messages"></a> Producing Messages</h2><ul><li>Produced to SQS using the SDK (SendMessage API)</li><li>the message is persisted in SQS until a concumer deletes it</li></ul><h2 id="consuming-messages"><a class="markdownIt-Anchor" href="#consuming-messages"></a> Consuming Messages</h2><ul><li>consumers (running on EC2 instances, servers, or AWS lambda)</li><li>Poll SQS for messages (receive up to 10 message at a time)</li><li>process the messages (example: insert the message into an RDS database)</li><li>delete the messages using the DeleteMessage API</li></ul><h2 id="multiple-ec2-instances-consumers"><a class="markdownIt-Anchor" href="#multiple-ec2-instances-consumers"></a> Multiple EC2 instances consumers</h2><ul><li>consumers receive and process messages in parallel</li><li>at least once delivery (another consumer will receive the message if the first consumer didn’t process it fast enough)</li><li>best-effort message ordering</li><li>consumers delete messages after processing them</li><li>we can scale consumers horizontally to improve throughput of processing</li></ul><h2 id="sqs-with-auto-scaling-group"><a class="markdownIt-Anchor" href="#sqs-with-auto-scaling-group"></a> SQS with Auto Scaling Group</h2><ol><li>CloudWatch is monitoring the SQS length</li><li>if SQS length is too long, CloudWatch will trigger an alarm</li><li>Auto Scaling group will increase the number of EC2 instances if the alarm is triggered</li></ol><h2 id="sqs-security"><a class="markdownIt-Anchor" href="#sqs-security"></a> SQS Security</h2><ul><li>encryption<ul><li>in flight encryption using HTTPS API</li><li>at rest encryption using KMS keys</li><li>client side encryption if the client wants to perform encryption / decryption itself</li></ul></li><li>Access control: IAM policies to regulate access to the SQS API</li><li>SQS access policies (similar to S3 bucket policies)</li></ul><h2 id="sqs-queue-access-policy"><a class="markdownIt-Anchor" href="#sqs-queue-access-policy"></a> SQS Queue Access Policy</h2><ul><li>Cross Account Access<ul><li>if other accounts want to poll message from the SQS queue, we could add policy to the SQS specify which account and allow it to call receiveMessage API</li></ul></li><li>publish S3 event notifications to SQS queue<ol><li>we upload an object to a S3 bucket</li><li>S3 bucket triggered an event message to be sent to SQS queue</li></ol><ul><li>we need to add a policy to SQS queue allowing the bucket to call SendMessage API to the queue</li></ul></li></ul><h2 id="sqs-message-visibility-timeout"><a class="markdownIt-Anchor" href="#sqs-message-visibility-timeout"></a> SQS - Message Visibility Timeout</h2><ul><li><p>after a message is polled by a consumer, it becomes invisible to other consumers</p></li><li><p>by default, the message visibility timeout is 30 seconds</p></li><li><p>that means the message has 30 seconds to be processed and deleted from the queue</p></li><li><p>after the message visibility timeout is over, the message is visible in SQS for other consumers to receive</p></li><li><p>if a message is not processed within the visiblity timeout, it will be processed again by other consumers</p></li><li><p>a consumer could call the ChangeMessageVisibility API to get more time</p></li><li><p>if visibility timeout is high (hours), and consumer crashes, re-processing will take time</p></li><li><p>if visibility timeout is too low (seconds), we may get duplicates</p></li></ul><h2 id="sqs-dead-letter-queue"><a class="markdownIt-Anchor" href="#sqs-dead-letter-queue"></a> SQS - Dead Letter Queue</h2><ul><li>if a consumer fails to process a message within the visibility timeout, the message goes back to the queue</li><li>we can set a threshold of how many times a message can go back to the queue</li><li>after the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ)</li><li>useful for debugging</li><li>make sure to process the messages in the DLQ before they expire</li><li>good to set a retention of 14 days in the DLQ</li></ul><h2 id="sqs-request-response-systems"><a class="markdownIt-Anchor" href="#sqs-request-response-systems"></a> SQS - Request - Response Systems</h2><ol><li>producer send request with the reply-to queue ID to the Request queue</li><li>responders receive the request from the Request queue and process it</li><li>after processing, responders send the response to the corrent Response queue using the queue ID in the request</li></ol><ul><li>to implement this pattern: use the SQS Temporary Queue Client</li><li>it leverages virtual queues instead of creating / deleting SQS queues (cost effective)</li></ul><h2 id="sqs-delay-queue"><a class="markdownIt-Anchor" href="#sqs-delay-queue"></a> SQS - Delay Queue</h2><ul><li>delay a message (consumers don’t see it immediately) up to 15 minutes</li><li>default is 0 seconds (message is avaialble right away)</li><li>can set a default at queue level</li><li>can override the default on send using the DelaySeconds parameter</li></ul><h2 id="sqs-fifo-queue"><a class="markdownIt-Anchor" href="#sqs-fifo-queue"></a> SQS - FIFO queue</h2><ul><li>Frist In First Out</li><li>limited throughput</li><li>exactly once send capability (by removing duplicates), the message that failed to be processed will be insert at the end of the queue</li><li>messages are processed in order by the consumer</li></ul><h1 id="amazon-sns"><a class="markdownIt-Anchor" href="#amazon-sns"></a> Amazon SNS</h1><ul><li>the event producer only sends message to one SNS topic</li><li>as many event receivers (subscriptions) as we want to listen to the SNS topic notifications</li><li>each subscriber to the topic will get all the messages (note: new feature to filter messages)</li><li>subscribers can be<ul><li>SQS</li><li>HTTP / HTTPS</li><li>lambda</li><li>Emails</li><li>SMS messages</li><li>Mobile notifications</li></ul></li></ul><h2 id="sns-integrates-with-a-lot-of-aws-services"><a class="markdownIt-Anchor" href="#sns-integrates-with-a-lot-of-aws-services"></a> SNS integrates with a lot of AWS services</h2><ul><li>many AWS services can send data directly to SNS for notifications</li><li>CloudWatch (for alarms)</li><li>Auto Scaling Groups notifications</li><li>Amazon S3 (on bucket events)</li><li>CloudFormation (upon state changes =&gt; failed to build etc…)</li><li>etc…</li></ul><h2 id="how-to-publish"><a class="markdownIt-Anchor" href="#how-to-publish"></a> How to publish</h2><ul><li>topic publish (using SDK)<ul><li>create a topic</li><li>create a subscription</li><li>publish to the topic</li></ul></li><li>direct publish (for mobile apps SDK)<ul><li>create a platform application</li><li>create a platform endpoint</li><li>publish to the platform endpoint</li><li>works with Google GCM, Apple APNS, Amazon ADM…</li></ul></li></ul><h2 id="security-2"><a class="markdownIt-Anchor" href="#security-2"></a> Security</h2><ul><li>Encryption<ul><li>in flight encryption using HTTPS / API</li><li>at rest encryption using KMS keys</li><li>Client side encryption if the client wants to perform encryption / decryption itself</li></ul></li><li>Access Control: IAM policies to regulate access to the SNS API</li><li>SNS access policies (similar to S3 bucket policies)<ul><li>useful for cross account access to SNS topics</li><li>useful for allowing other services (S3…) to write to an SNS topic</li></ul></li></ul><h2 id="sns-sqs-fan-out"><a class="markdownIt-Anchor" href="#sns-sqs-fan-out"></a> SNS + SQS: Fan Out</h2><ul><li>Push once in SNS, receive in all SQS queues that are subscribers</li><li>fully decoupled, no data loss</li><li>SQS allows for: data persistence, delayed processing and retries of work</li><li>ability to add more SQS subscribers over time</li><li>make sure your SQS queue access policy allows for SNS to write</li></ul><h3 id="eg-s3-events-to-multiple-queues-or-lambda-functions"><a class="markdownIt-Anchor" href="#eg-s3-events-to-multiple-queues-or-lambda-functions"></a> e.g. S3 Events to multiple queues or lambda functions</h3><ul><li>if you want to send the same S3 event to many SQS queues, or optionally lambda functions, use fan out pattern</li></ul><h2 id="sns-fifo-topic"><a class="markdownIt-Anchor" href="#sns-fifo-topic"></a> SNS FIFO topic</h2><ul><li>similar features as SQS FIFO<ul><li>ordering by message group ID (all messages in the same group are ordered)</li><li>deduplication using a deduplication ID or Content Based Deduplication</li></ul></li><li>can only have SQS FIFO queue as subscribers</li><li>limited throughput (same throughput as SQS FIFO)</li></ul><h2 id="message-filtering"><a class="markdownIt-Anchor" href="#message-filtering"></a> Message Filtering</h2><ul><li>JSON policy used to filter messages sent to SNS topic’s subscriptions</li><li>if a subscription doesn’t have a filter policy, it receives every message</li></ul><h1 id="aws-kinesis"><a class="markdownIt-Anchor" href="#aws-kinesis"></a> AWS Kinesis</h1><ul><li>make it easy to collect, process and analyze streaming data in real time</li><li>ingest real time data such as application logs, Metrics, website clickstreams, IoT telemetry data…</li></ul><h2 id="kinesis-data-streams"><a class="markdownIt-Anchor" href="#kinesis-data-streams"></a> Kinesis Data Streams</h2><ul><li>billing is per shard provisioned, can have as many shards as you want</li><li>retention between 1 day (default) to 365 days</li><li>ability to reprocess (replay) data</li><li>once data is inserted in Kinesis, it can’t be deleted (immutability)</li><li>data that shares the same partition goes to the same shard (ordering)</li><li>producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent</li><li>consumers:<ul><li>write your own: Kinesis Client Library (KCL), AWS SDK</li><li>managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics</li></ul></li></ul><h2 id="kinesis-firehose"><a class="markdownIt-Anchor" href="#kinesis-firehose"></a> Kinesis Firehose</h2><ul><li>fully managed service, no administration, automatic scaling, serverless<ul><li>AWS redshift / Amazon S3 / ElasticSearch</li></ul></li><li>pay for data going through firehose</li><li>near real time<ul><li>60 seconds latency minimum for non full batches</li><li>or minimum 32 MB of data at a time</li></ul></li><li>supports many data formats, conversions, transformations, compression</li><li>supports custom data transformations, using AWS Lambda</li><li>can send failed or all data to a backup S3 bucket</li></ul><table><thead><tr><th>Kinesis Data Streams</th><th>Kinesis Data Firehos</th></tr></thead><tbody><tr><td>Streaming service for ingest at scale</td><td>load streaming data into S3 / redshift / ES / Thrid party / custom HTTP</td></tr><tr><td>write custom code (producer, consumer)</td><td>fully managed</td></tr><tr><td>real time (~200ms)</td><td>near real time (buffer time min 60 seconds)</td></tr><tr><td>manage scaling (shard splitting / merging)</td><td>automatic scaling</td></tr><tr><td>data storage for 1 to 365 days</td><td>no data storage</td></tr><tr><td>support replay capability</td><td>doesn’t support replay capability</td></tr></tbody></table><h2 id="kinesis-data-analytics-sql-application"><a class="markdownIt-Anchor" href="#kinesis-data-analytics-sql-application"></a> Kinesis Data Analytics (SQL application)</h2><ul><li>perform real time analytics on Kinesis Streams using SQL</li><li>fully managed, no servers to provision</li><li>automatic scaling</li><li>real time analytics</li><li>pay for actual consumtion rate</li><li>can create streams out of the real time queries</li><li>use cases<ul><li>time series analytics</li><li>real time dashboards</li><li>real time metrics</li></ul></li></ul><h2 id="kinesis-vs-sqs-ordering"><a class="markdownIt-Anchor" href="#kinesis-vs-sqs-ordering"></a> Kinesis vs SQS ordering</h2><ul><li>let’s assume we have 100 trucks, 5 Kinesis shards, 1 SQS FIFO</li><li>Kinesis data streams<ul><li>on average you will have 20 trucks per shard</li><li>trucks will have their data ordered within each shard</li><li>the maximum amount of consumer in parallel we can have is 5</li><li>can receive up to 5MB/s of data</li></ul></li><li>SQS FIFO<ul><li>you only have one SQS FIFO queue</li><li>you will have 100 group ID</li><li>you can have up to 100 consumers (due to the 100 group ID)</li><li>you have up to 300 messages per second (or 3000 if using batching)</li><li>better to use if you have a dynamic number of consumers</li></ul></li></ul><table><thead><tr><th>SQS</th><th>SNS</th><th>Kinesis</th></tr></thead><tbody><tr><td>consumer pull data</td><td>push data to many subscribers</td><td>standard: pull data (2MB per shard), enhanced-fan out: push data (2MB per shard per consumer)</td></tr><tr><td>data is deleted after being consumed</td><td>data is not persisted (lost if not delivered)</td><td>possibility to replay data</td></tr><tr><td>can have as many workers as we want</td><td>up to 12,500,000 subscribers</td><td>-</td></tr><tr><td>no need to provision throughput</td><td>no need to provision throughput</td><td>must provision throughput</td></tr><tr><td>ordering guearantees only on FIFO queues</td><td>FIFO capability for SQS FIFO</td><td>ordering at shard level</td></tr><tr><td>individual message delay capability</td><td>integrates with SQS for fan out architecture pattern</td><td>data expires after X days</td></tr><tr><td>-</td><td>-</td><td>meant for real time big data analytics and ETL</td></tr></tbody></table><h2 id="amazon-mq"><a class="markdownIt-Anchor" href="#amazon-mq"></a> Amazon MQ</h2><ul><li>SQS, SNS are cloud native serviecs, and they are using proprietary protocols from AWS</li><li>traditional applications running from on-premises may use open protocols such as MQTT, AMQP, STOMP, OpenWire, WSS</li><li>when migrating to the cloud, instead of re-engineering the application to use SQS, SNS, we can use Amazon MQ</li></ul><h1 id="container"><a class="markdownIt-Anchor" href="#container"></a> Container</h1><h2 id="docker"><a class="markdownIt-Anchor" href="#docker"></a> Docker</h2><ul><li>Docker is a software development platform to deploy apps</li><li>apps are packaged in containers that can be run on any OS</li><li>apps run the same, regardless of where they are run<ul><li>any machine</li><li>no compatibility issues</li><li>predictable behavior</li><li>less work</li><li>easier to maintain and deploy</li><li>works with any language, any OS, any techonology</li></ul></li></ul><h2 id="where-are-docker-images-stored"><a class="markdownIt-Anchor" href="#where-are-docker-images-stored"></a> Where are Docker images stored</h2><ul><li>Docker images are stored in Docker repositories</li><li>public: Docker hub: <a href="https://hub.docker.com/">https://hub.docker.com/</a></li><li>private: Amazon ECR (Elastic Container Registry)</li><li>Public: Amazon ECR public</li></ul><h2 id="docker-containers-management"><a class="markdownIt-Anchor" href="#docker-containers-management"></a> Docker Containers Management</h2><ul><li>to manage containers, we need a container managemenet platform<ul><li>ECS (Amazon’s own container platform)</li><li>Fargate: Amazon’s own serverless container platform</li><li>EKS: Amazon’s managed Kubernetes (open source)</li></ul></li></ul><h2 id="ecs-elastic-container-service"><a class="markdownIt-Anchor" href="#ecs-elastic-container-service"></a> ECS (Elastic Container Service)</h2><ul><li>Launch Docker containers on AWS</li><li>you must provision and maintain the infrastructrue (the EC2 instance)</li><li>AWS takes care of starting and stopping the containers</li><li>has integrations with the Application Load Balancer</li><li>ECS agent will be installed on the EC2 instances for ECS to know who to manage</li></ul><h2 id="fargate"><a class="markdownIt-Anchor" href="#fargate"></a> Fargate</h2><ul><li>launch Docker containers on AWS</li><li>you do not provision the infrastructure (no EC2 instances to manage)</li><li>serverless offering</li><li>AWS just runs containers for you based on the CPU / RAM you need</li><li>for each container in Fargate it needs an ENI for the public to access it</li></ul><h2 id="iam-roles-for-ecs-tasks"><a class="markdownIt-Anchor" href="#iam-roles-for-ecs-tasks"></a> IAM roles for ECS tasks</h2><ul><li>EC2 instance profile<ul><li>used by the ECS agent</li><li>makes API calls to ECS service</li><li>send container logs to CloudWatch logs</li><li>pull docker image from ECR</li><li>reference sensitive data in Secret Manager or SSM Parameter store</li></ul></li><li>ECS task role<ul><li>allow each task to have a specific role</li><li>use different roles for the different ECS services you run</li><li>task role is defined in the task definition</li></ul></li></ul><h2 id="ecs-data-volumes-efs-file-systems"><a class="markdownIt-Anchor" href="#ecs-data-volumes-efs-file-systems"></a> ECS data volumes - EFS file systems</h2><ul><li>works for both EC2 tasks and Fargate tasks</li><li>ability to mount EFS volumes onto tasks</li><li>tasks launched in any AZ will be able to share the same data in the EFS volume</li><li>Fargate + EFS = serverless  + data storage without managing servers</li><li>use case: persistent multi-AZ shared storage for your containers</li></ul><h2 id="load-balancing-for-ec2-launch-type"><a class="markdownIt-Anchor" href="#load-balancing-for-ec2-launch-type"></a> Load Balancing for EC2 Launch type</h2><ul><li>we get a dynamic port mapping</li><li>the ALB supports finding the right port on your EC2 instances</li><li>you must allow on the EC2 instnace’s security group any port from the ALB security group</li></ul><h2 id="load-balancing-for-fargate"><a class="markdownIt-Anchor" href="#load-balancing-for-fargate"></a> Load Balancing for Fargate</h2><ul><li>each task has a unique IP (because of ENI)</li><li>you must allow on the ENI’s security group the task port from the ALB security group</li></ul><h2 id="event-bridge"><a class="markdownIt-Anchor" href="#event-bridge"></a> Event Bridge</h2><ol><li>user upload object to S3</li><li>S3 triggers event to Amazon Event Bridge</li><li>Event Bridge triggers the container task to run</li><li>the task will have the role to access S3 and DynamoDB</li><li>task will get the object from S3 and save it to DynamoDB</li></ol><h2 id="ecs-rolling-updates"><a class="markdownIt-Anchor" href="#ecs-rolling-updates"></a> ECS Rolling updates</h2><ul><li>when updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order, by specifying the minimum and maximum healthy percent</li></ul><h2 id="ecr-elastic-container-registry"><a class="markdownIt-Anchor" href="#ecr-elastic-container-registry"></a> ECR (Elastic Container Registry)</h2><ul><li>store, manage and deploy containers on AWS, pay for what you use</li><li>fully integrated with ECS and IAM for security, backed by Amazon S3</li><li>supports image vulnerability scanning, version, tag, image lifecycle</li><li>but if we wanted to automate the whole process, we could be using a CICD, so Continuous Integration Continuous Deployment platform and for example, CodeBuild could help us with this to automate building a Docker image and then pushing it onto Amazon ECR to finally trigger an ECS update.</li></ul><h2 id="eks-elastic-kubernetes-service"><a class="markdownIt-Anchor" href="#eks-elastic-kubernetes-service"></a> EKS (Elastic Kubernetes Service)</h2><ul><li>it is a way to launch managed Kubernetes clusters on AWS</li><li>Kubernetes is an open source system for automatic deployment, scaling and management of containerized application</li><li>it is an alternative to ECS, similar goal but different API</li><li>EKS supports<ul><li>EC2 if you want to deploy worker nodes</li><li>Fargate to deploy serverless containers</li></ul></li><li>use case: if your company is already using Kunernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes</li><li>Kubernetes is cloud agnostic (can be used in any cloud, Azure, GCP…)</li></ul><h1 id="serverless"><a class="markdownIt-Anchor" href="#serverless"></a> Serverless</h1><ul><li>serverless is a new paradigm in which the developers don’t have to manage servers anymore</li><li>Serverless was pioneered by AWS lambda but now also includes anything that’s managed: database, messaging, storage</li><li>serverless does not mean there are no servers, it means that you just don’t need to manage / provision / see them</li></ul><h2 id="lambda"><a class="markdownIt-Anchor" href="#lambda"></a> Lambda</h2><ul><li>easy pricing<ul><li>pay per request and compute time</li></ul></li><li>integrated with the whole AWS suite of services</li><li>integrated with many programming languages<ul><li>Node.js</li><li>Python</li><li>Java</li><li>C# (.NET Core)</li><li>Golang</li><li>C# / Powershell</li><li>Ruby</li><li>Lambda container image<ul><li>the container image must implement the lambda runtime API</li><li>ECS / Fargate is perferred for running arbitrary Docker images</li></ul></li></ul></li><li>easy monitoring through AWS CloudWatch</li><li>easy to get more resources per functions</li></ul><h3 id="lambda-limits"><a class="markdownIt-Anchor" href="#lambda-limits"></a> Lambda Limits</h3><ul><li>Execution<ul><li>Memory allocation: 128MB - 10GB (64 MB increments)</li><li>Maximum execution time: 15 minutes</li><li>Environment variables (4 KB)</li><li>disk capacity in the function container (in /tmp): 512 MB</li><li>concurrency executions: 1000 (can be increased)</li></ul></li><li>Deployment<ul><li>lambda function deployment size (compressed zip): 50 MB</li><li>size of uncompressed deployment (code + dependencies): 250 MB</li><li>can use the /tmp directory to load other files at startup</li><li>size of environment variables: 4KB</li></ul></li></ul><h3 id="lambdaedge"><a class="markdownIt-Anchor" href="#lambdaedge"></a> Lambda@Edge</h3><ul><li><p>you have deployed a CDN using CloudFront</p></li><li><p>what if you wanted to run a global AWS lambda alongside?</p></li><li><p>or how to implement request filtering before reaching your application?</p></li><li><p>for this, you can use Lambda@Edge</p><ul><li>build more responsive applications</li><li>you don’t manage servers, lambda is deployed globally</li><li>customize the CDN content</li><li>pay only for what you use</li></ul></li><li><p>you can use lambda to change CloudFront requests and responses</p><ul><li>after cloudfront receives a request from a viewer (viewer request)</li><li>before cloudfront forwards the request to the origin (origin request)</li><li>after cloudfront receives the response from the origin (origin response)</li><li>before cloudfront forwards the response to the viewer (viewer response)</li></ul></li><li><p>you can also generate responses to viewers without ever sending the request to the origin</p></li></ul><h2 id="dynamodb"><a class="markdownIt-Anchor" href="#dynamodb"></a> DynamoDB</h2><ul><li>fully managed, hgihly available with replication across 3 AZ</li><li>NoSQL database, not a relational database</li><li>scales to massive workloads, distributed database</li><li>millions of requests per seconds, trillions of row, 100s of TB of storage</li><li>fast and consistent in performance (low latency on retrieval)</li><li>integrated with IAM for security, authorization and administration</li><li>enables event driven programming with DynamoDB streams</li><li>low cost and auto scaling capabilities</li></ul><h3 id="dynamodb-basics"><a class="markdownIt-Anchor" href="#dynamodb-basics"></a> DynamoDB - basics</h3><ul><li>DynamoDB is made of tables</li><li>each table has a primary key (must be decided at creation time)</li><li>each table can have an infinite number of items (rows)</li><li>each item has attributes (can be added over time - can be null)</li><li>maximum size of a item is 400KB</li><li>data types supported are<ul><li>Scalar Type: string, number, binary, booelan, null</li><li>Document type: list, map</li><li>set types: string set, number set, binary set</li></ul></li></ul><h3 id="provisioned-throughput"><a class="markdownIt-Anchor" href="#provisioned-throughput"></a> Provisioned Throughput</h3><ul><li>table must have provisioned read and write capacity units</li><li>read capacity units (RCU), throughput for reads<ul><li>1 RCU = 1 strongly consistent read of 4 KB per second</li><li>1 RCU = 2 eventually consistent read of 4 KB per second</li></ul></li><li>write capacity units (WCU), throughput for writes<ul><li>1 WCU = 1 write of 1 KB per second</li></ul></li><li>option to setup auto scaling of throughput to meet demand</li><li>throughput can be exceeded temporarily using burst credit</li><li>if burst credit are empty, you will get a ProvisionedThroughputExeception</li><li>it is then advised to do an exponential back off retry</li></ul><h3 id="dynamodb-dax"><a class="markdownIt-Anchor" href="#dynamodb-dax"></a> DynamoDB - DAX</h3><ul><li>DynamoDB accelerator</li><li>seamless cache for DynamoDB, no application re-write</li><li>writes go through DAX to DynamoDB</li><li>micro second latency for cached reads and queries</li><li>solves the Hot Key problem (too many reads)</li><li>5 minutes TTL for cache by default</li><li>up to 10 nodes in the cluster</li><li>multi AZ (3 nodes minumum recommended for production)</li><li>secure (encryption at rest with KMS, VPC, IAM, CloudTrail…)</li></ul><h3 id="dynamodb-streams"><a class="markdownIt-Anchor" href="#dynamodb-streams"></a> DynamoDB Streams</h3><ul><li>changes in DynamoDB (create, update, delete) can end up in a DynamoDB stream</li><li>this stream can be read by AWS lambda and we can then do<ul><li>react to changes in real time (welcome email to new users)</li><li>analytics</li><li>create derivative tables/ views</li><li>insert into elasticSearch</li></ul></li><li>cloud implement cross region replication using Streams</li><li>Stream has 24 hours of data retention</li></ul><h3 id="transactions-new-from-nov-2018"><a class="markdownIt-Anchor" href="#transactions-new-from-nov-2018"></a> Transactions (new from Nov 2018)</h3><ul><li>all or nothing type of operations</li><li>coordinated insert, update and delete across multiple tables</li><li>include up to 10 unique items or up to 4MB of data</li></ul><h3 id="on-demand-new-from-nov-2018"><a class="markdownIt-Anchor" href="#on-demand-new-from-nov-2018"></a> On Demand (new from Nov 2018)</h3><ul><li>no capacity planning needed (WCU / RCU) - scales automatically</li><li>2.5x more expensive than provisioned capacity</li><li>helpful when spikes are un-predicatable or the application is very low throughput</li></ul><h3 id="dynamodb-security-and-other-features"><a class="markdownIt-Anchor" href="#dynamodb-security-and-other-features"></a> DynamoDB - Security and other features</h3><ul><li>Security<ul><li>VPC endpoints available to access DynamoDB without internet</li><li>access fully controlled by IAM</li><li>encryption at rest using KMS</li><li>encryption in transit using SSL / TLS</li></ul></li><li>backup and restore feature available<ul><li>point in time restore like RDS</li><li>no performance impact</li></ul></li><li>Global Tables (cross region replication)<ul><li>multi region, fully replicated, high performance</li><li>active active replication (data will be replicated to other tables no matter which table gets the data first)</li><li>must enable DynamoDB Streams</li><li>useful for low latency, Disater Recovery purposes</li></ul></li><li>Amazon DMS (Data Migration Service) can be used to migrate to DynamoDB (from Mongo, Oracle, MySQL, S3, etc…)</li><li>you can launch a local DynamoDB on your computer for development purposes</li></ul><h2 id="api-gateway"><a class="markdownIt-Anchor" href="#api-gateway"></a> API Gateway</h2><ul><li>AWS lambda + API gateway</li><li>support for the webSocket Protocol</li><li>handle API versioning</li><li>handle different environments (dev, test, prod)</li><li>handle security (authentication, authorization)</li><li>create API keys, handle request throttling</li><li>Swagger / Open API import to quickly define APIs</li><li>Transform and validate requests and responses</li><li>generate SDK and API specifications</li><li>cache API responses</li></ul><h3 id="api-gateway-endpoint-types"><a class="markdownIt-Anchor" href="#api-gateway-endpoint-types"></a> API Gateway - Endpoint Types</h3><ul><li>Edge Optimized (default): for global clients<ul><li>requests are routed through the CloudFront edge locations (improves latency)</li><li>the API gateway still lives in only one region</li></ul></li><li>Regional<ul><li>for clients within the same region</li><li>could manually combine with CloudFront (more control over the caching strategies and the distribution)</li></ul></li><li>private<ul><li>can only by accessed from your VPC using an interface VPC endpoint (ENI)</li><li>use a resource policy to define access</li></ul></li></ul><h3 id="security-3"><a class="markdownIt-Anchor" href="#security-3"></a> Security</h3><h4 id="iam-permissions-2"><a class="markdownIt-Anchor" href="#iam-permissions-2"></a> IAM Permissions</h4><ul><li>create an IAM policy authorization and attach to User / Role</li><li>API gateway verfies IAM permissions passed by the calling application</li><li>good to provide access within your own infrastructure</li><li>leverage Sig v4 capability where IAM credential are in headers</li></ul><h4 id="lambda-authorizer"><a class="markdownIt-Anchor" href="#lambda-authorizer"></a> Lambda authorizer</h4><ul><li>users AWS lambda to validate the token in header being passed</li><li>option to cache result of authentication</li><li>helps to use OAuth / SAML / third party type of authentication</li><li>lamdba (authorizer) must return an IAM policy for the user</li></ul><h4 id="cognito-user-pools"><a class="markdownIt-Anchor" href="#cognito-user-pools"></a> Cognito User Pools</h4><ul><li>cognito fully manages user lifecycle</li><li>API gateway verifies identity automatically from AWS cognito</li><li>no custom implementation required</li><li>Cognito only helps with authentication, not authorization</li></ul><h2 id="aws-cognito"><a class="markdownIt-Anchor" href="#aws-cognito"></a> AWS Cognito</h2><ul><li>we want to give our users an identity so that they can interact with our application</li></ul><h3 id="aws-cognito-user-pools"><a class="markdownIt-Anchor" href="#aws-cognito-user-pools"></a> AWS Cognito User Pools</h3><ul><li>create a serverless database of user for your mobile apps</li><li>simple login: username or email  / password combination</li><li>possibility to verify emails / phone numbers and add MFA</li><li>can enable federated identities (Facebook, Google, SAML…)</li><li>sends back a JSON web tokens (JWT)</li><li>can be integrated with API gateway for authentication</li></ul><h3 id="aws-cognito-federated-identity-pools"><a class="markdownIt-Anchor" href="#aws-cognito-federated-identity-pools"></a> AWS Cognito - Federated Identity Pools</h3><ul><li>goal<ul><li>provide direct access to AWS resources from the client side</li></ul></li><li>How<ul><li>login to federated identity provider - or remain anonymous</li><li>get temporary AWS credentials back from the federated identity pool</li><li>these credentials come with a pre-defined IAM policy stating their permissions</li></ul></li><li>example<ul><li>provide (temporary) access to write to S3 bucket using Facebook login</li></ul></li></ul><h3 id="aws-cognito-sync"><a class="markdownIt-Anchor" href="#aws-cognito-sync"></a> AWS Cognito Sync</h3><ul><li>deprecated - use AWS AppSync now</li><li>store preferneces, configuration, state of app</li><li>cross device synchronization</li><li>offline capability (synchronization when back online)</li><li>requires federated identity pool in Cognito</li><li>store data in datasets</li></ul><h2 id="aws-sam-serverless-application-model"><a class="markdownIt-Anchor" href="#aws-sam-serverless-application-model"></a> AWS SAM - Serverless Application Model</h2><ul><li>framework for developing and deploying serverless applications</li><li>all the configurations is YAML code<ul><li>lambda functions</li><li>DynamoDB tables</li><li>API Gateway</li><li>Cognito User Pool</li></ul></li><li>SAM can help you to run Lambda, API Gateway, DynamoDB locally</li><li>SAM can use CodeDeploy to deploy lambda functions</li></ul><h1 id="databases-comparison"><a class="markdownIt-Anchor" href="#databases-comparison"></a> Databases Comparison</h1><h2 id="rds-2"><a class="markdownIt-Anchor" href="#rds-2"></a> RDS</h2><ul><li>managed postgreSQL / MySQL / Oracle / SQL server</li><li>must provision an EC2 instance and EBS volume type and size</li><li>support for read replicas and multi AZ</li><li>security through IAM, security groups, KMS, SSL in transit</li><li>backup / snapshot / point in time restore</li><li>managed and scheduled maintenance</li><li>monitoring through CloudWatch</li><li>use case: store relational datasets (RDBMS / OLTP (online transactional processing)), perform SQL queries, transactional inserts / update / delete is available</li></ul><h2 id="aurora"><a class="markdownIt-Anchor" href="#aurora"></a> Aurora</h2><ul><li>compatible API for PostgreSQL / MySQL</li><li>Data is held in 6 replicas, 3 AZ</li><li>auto healing capability</li><li>multi AZ, auto scaling read replicas</li><li>read replicas can be global</li><li>Aurora database can be global for DR or latency purpose</li><li>define EC2 instance type for Aurora instances</li><li>same security / monitoring / maintenance features as RDS</li><li>Aurora Serverless - for unpredicatble / intermittent workloads</li><li>Aurora multi-master - for continuous write failover</li><li>no need to provision</li><li>use case: same as RDS, but with less maintenance / more flexibility / more performance</li></ul><h2 id="elasticcache"><a class="markdownIt-Anchor" href="#elasticcache"></a> ElasticCache</h2><ul><li>managed Redis / Memcached (similar offering as RDS, but for caches)</li><li>in memory data store, sub-millisecond latency</li><li>must provision an EC2 instance type</li><li>support for Clustering (Redis), and Multi AZ, read replicas (sharding)</li><li>security through IAM, security groups, KMS, Redis Auth</li><li>backup / snapshot / point in time restore</li><li>managed and scheduled maintenance</li><li>monitoring through CloudWatch</li><li>use case: key value store, frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL</li></ul><h2 id="dynamodb-2"><a class="markdownIt-Anchor" href="#dynamodb-2"></a> DynamoDB</h2><ul><li>AWS proprietary technology, managed NoSQL database</li><li>serverless, provisioned capacity, auto scaling, on demand capacity</li><li>can replace ElastiCache as a key value store</li><li>highly available, multi AZ by default, read and writes are decoupled, DAX for read cache</li><li>reads can be eventually consistent (occasional old data) or strongly consistent (always latest data)</li><li>security, authentication and authorization is done through IAM</li><li>DynamoDB Streams to integrate with AWS lambda</li><li>backup and restore feature, global table feature</li><li>monitoring through CloudWatch</li><li>can only query on primay key, sort key, or indexes</li><li>use case: serverless applications development, distributed serverless cache, doesn’t have SQL query language available, has transactions capability from Nov 2018</li></ul><h2 id="s3-2"><a class="markdownIt-Anchor" href="#s3-2"></a> S3</h2><ul><li>great for big objects, not so great for small objects (because of latency)</li><li>serverless, scales infinitely, max object size is 5TB</li><li>strong consistency</li><li>Tiers: S3 standard, S3 IA, S3 One Zone IA, Glacier, for backups</li><li>features: versioning, encryption, cross region replication etc…</li><li>security: IAM, bucket policy, ACL</li><li>encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit</li><li>use case: static files, key value store for big files, website hosting</li></ul><h2 id="athena"><a class="markdownIt-Anchor" href="#athena"></a> Athena</h2><ul><li>fully serverless query engine with SQL capabilities</li><li>used to query data in S3</li><li>pay per query</li><li>output results back to S3</li><li>secured through IAM</li><li>use case: one time SQL queries, serverless queries on S3, log analytics</li></ul><h2 id="redshift"><a class="markdownIt-Anchor" href="#redshift"></a> RedShift</h2><ul><li><p>Redshift is based on PostgreSQL, but it is not used for OLTP</p></li><li><p>its OLAP - online analytical processing (analytics and data warehousing)</p></li><li><p>Columnar storage of data (instead of row based)</p></li><li><p>massively parallel query execution</p></li><li><p>pay as you go based on the instances provisioned</p></li><li><p>has a SQL interface for performing the queries</p></li><li><p>BI tools such as AWS quicksight or Tableau integrate with it</p></li><li><p>data is loaded from S3 / DynamoDB, DMS, other DBs</p></li><li><p>from 1 node to 128 nodes, up to 160 GB of space per node</p></li><li><p>leader node: for query planning, results aggregation</p></li><li><p>compute node: for performing the queries, send results to leader</p></li><li><p>Redshift Spectrum: perform queries directly against S3 (no need to load)</p></li><li><p>backup and restore, security VPC / IAM / KMS, monitoring</p></li><li><p>Redshift enhanced VPC routing: COPY / UNLOAD goes through VPC</p></li></ul><h3 id="snapshots-dr"><a class="markdownIt-Anchor" href="#snapshots-dr"></a> Snapshots / DR</h3><ul><li>Redshift has no multi AZ mode</li><li>snapshots are point in time backups of a cluster, stored internally in S3</li><li>snapshots are incremental (only what has changed is saved)</li><li>you can restore a snapshot into a new cluster</li><li>automated: every 8 hours, every 5 GB, or on schedule, set retention</li><li>manual: snapshot is retained until you delete it</li><li>you can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS region</li></ul><h3 id="redshift-specturm"><a class="markdownIt-Anchor" href="#redshift-specturm"></a> Redshift Specturm</h3><ul><li>query data that is already in S3 without loading it</li><li>must have a Redshift cluster available to start the query</li><li>the query is then submitted to thousands of Redshift Spectrum nodes</li><li>data doesn’t need to be loaded into Redshift first</li></ul><h2 id="aws-glue"><a class="markdownIt-Anchor" href="#aws-glue"></a> AWS Glue</h2><ul><li>managed extract, transform, and load (ETL) service</li><li>useful to prepare and transform data for analytics</li><li>fully serverless service</li></ul><h3 id="glue-data-catalog"><a class="markdownIt-Anchor" href="#glue-data-catalog"></a> Glue Data Catalog</h3><ul><li>Glue data catalog: catalog of datasets (metadata)</li><li>S3 =&gt; AWS Glue Data Crawler =&gt; AWS Glue Data Catalog =&gt; Amazon Athena</li></ul><h2 id="neptune"><a class="markdownIt-Anchor" href="#neptune"></a> Neptune</h2><ul><li>fully managed graph database</li><li>when do we use graphs?<ul><li>high relationship data</li><li>social networking: users friends with users, replied to comment on post of user and likes other comments</li><li>knowledge graphs (Wikipedia)</li></ul></li><li>highly available across 3 AZ, with up to 15 read replicas</li><li>point in time recovery, continuous backup to Amazon S3</li><li>support for KMS encryption at rest + HTTPS</li></ul><h2 id="elasticsearch"><a class="markdownIt-Anchor" href="#elasticsearch"></a> ElasticSearch</h2><ul><li>example: in dynamoDB, you can only find by primary key or indexes</li><li>with ElasticSearch you can search any field, even partially matches</li><li>it is common to use ElasticSearch as a complement to another database</li><li>ElasticSearch also has some usage for Big Data applications</li><li>you can provision a cluster of instances</li><li>built in integrations: Amazon Kinesis data Firehose, SSL and VPC</li><li>comes with Kibana (visulization) and Logstash (log ingestion) - ELK stack</li></ul><h1 id="aws-cloudwatch"><a class="markdownIt-Anchor" href="#aws-cloudwatch"></a> AWS CloudWatch</h1><h2 id="cloudwatch-metrics"><a class="markdownIt-Anchor" href="#cloudwatch-metrics"></a> CloudWatch Metrics</h2><ul><li>CloudWatch provides metrics for every servcies in AWS</li><li>Metric is a variable to monitor (CPU Utilization, Network In…)</li><li>metrics belong to namespaces</li><li>dimension is an attribute of a metric (instance id, environment, etc…)</li><li>up to 10 dimensions per metric</li><li>metrics have timestamps</li><li>can create CloudWatch dashboards of metrics</li></ul><h2 id="ec2-detailed-monitoring"><a class="markdownIt-Anchor" href="#ec2-detailed-monitoring"></a> EC2 Detailed monitoring</h2><ul><li>EC2 instance metrics have metrics every 5 minutes</li><li>with detailed monitoring (for a cost), you get data every 1 minute</li><li>use detailed monitoring if you want to scale faster for your ASG</li><li>the AWS free tier allows us to have 10 detailed monitoring metrics</li><li>Note: EC2 memory usage is by default not pushed (must be pushed from inside the instance as a custom metric)</li></ul><h2 id="cloudwatch-custom-metrics"><a class="markdownIt-Anchor" href="#cloudwatch-custom-metrics"></a> CloudWatch Custom Metrics</h2><ul><li>possiblity to define and send your own custom metrics to CloudWatch</li><li>example: memory usage, disk space, number of logged in users…</li><li>use API call PutMetricData</li><li>ability to use dimensions (attributes) to segment metrics<ul><li><a href="http://instance.id">instance.id</a></li><li><a href="http://environment.name">environment.name</a></li></ul></li><li>metric resolution (StorageResolution API parameter - two possible values)<ul><li>Standard: 1 minute</li><li>high resolution: 1 / 5 / 10 / 30 seconds - higher cost</li></ul></li><li>important: accepts metric data points two weeks in the past and two hours in the future (make sure to configure your EC2 instance time correctly)</li></ul><h2 id="cloudwatch-dashboards"><a class="markdownIt-Anchor" href="#cloudwatch-dashboards"></a> CloudWatch Dashboards</h2><ul><li>great way to setup custom dashboards for quick access to key metrics and alarms</li><li>dashboards are global</li><li>dashboards can include graphs from different AWS accounts and regions</li><li>you can change the time zone and time range of the dashboards</li><li>you can setup automatic refresh (10s, 1m, 2m, 5m, 15m)</li><li>dashboards can be shared with people who don’t have an AWS account (public, email address…)</li><li>pricing<ul><li>3 dashboards (up to 50 metrics) for free</li><li>$3 dollar / dashboard / month after</li></ul></li></ul><h2 id="cloudwatch-logs"><a class="markdownIt-Anchor" href="#cloudwatch-logs"></a> CloudWatch Logs</h2><ul><li><p>applications can send logs to CloudWatch using the SDK</p></li><li><p>CloudWatch can collect log from</p><ul><li>elastic beanstalk: collection of log from application</li><li>ECS: collection from containers</li><li>AWS lambda: collection from function logs</li><li>VPC flow logs: VPC specific logs</li><li>API Gateway</li><li>CloudTrail based on filter</li><li>CloudWatch log agents: for example on EC2 machines</li><li>Route53: log DNS queries</li></ul></li><li><p>CloudWatch Logs can go to</p><ul><li>batch exporter to S3 for archival</li><li>Stream to ElasticSearch cluster for further analytics</li></ul></li><li><p>Logs storage architecture</p><ul><li>log groups: arbitrary name, usually representing an application</li><li>log stream: instances within application / log files / containers</li></ul></li><li><p>can define log expiration policies (never expire, 30 days, etc…)</p></li><li><p>using the AWS CLI we can tail CloudWatch logs</p></li><li><p>to send logs to CloudWatch, make sure IAM permissions are correct</p></li><li><p>security: encryption of logs using KMS at the group level</p></li></ul><h2 id="cloudwatch-logs-for-ec2"><a class="markdownIt-Anchor" href="#cloudwatch-logs-for-ec2"></a> CloudWatch Logs for EC2</h2><ul><li>by default, no logs from your EC2 machine will go to CloudWatch</li><li>you need to run a CloudWatch agent on EC2 to push the log files you want</li><li>make sure IAM permissions are correct</li><li>the CloudWatch log agent can be setup on-premises too</li></ul><h3 id="cloudwatch-logs-agent-vs-unified-agent"><a class="markdownIt-Anchor" href="#cloudwatch-logs-agent-vs-unified-agent"></a> CloudWatch Logs Agent vs Unified Agent</h3><ul><li>CloudWatch Logs Agent<ul><li>old version of the agent</li><li>can only send to CloudWatch logs</li></ul></li><li>CloudWatch unified agent<ul><li>collect additional system-level metrics such as RAM, processors,etc…</li><li>collect logs to send to CloudWatch logs</li><li>centralized configuration using SSM Parameter Store</li></ul></li></ul><h2 id="cloudwatch-alarms"><a class="markdownIt-Anchor" href="#cloudwatch-alarms"></a> CloudWatch Alarms</h2><ul><li>Alarms are used to trigger notifications for any metric</li><li>various options (sampling, percentage, max, min, etc…)</li><li>alarm status<ul><li>OK</li><li>INSUFFICIENT_DATA</li><li>ALARM</li></ul></li><li>period<ul><li>length of time in seconds to evaluate the metric</li><li>high resolution custom metrics: 10 / 30 or multiples of 60 seconds</li></ul></li></ul><h3 id="alarm-targets"><a class="markdownIt-Anchor" href="#alarm-targets"></a> Alarm Targets</h3><ul><li>Stop, terminate, reboot or recover an EC2 instance</li><li>trigger auto scaling action</li><li>send notification to SNS (from which you can do pretty much anything)</li></ul><h3 id="ec2-instance-recovery"><a class="markdownIt-Anchor" href="#ec2-instance-recovery"></a> EC2 instance recovery</h3><ul><li>status check<ul><li>instance status = check the EC2 VM</li><li>system status = check the underlying hardware</li></ul></li><li>recovery<ul><li>same private, public, elastic IP, metadata, placement group</li></ul></li></ul><h3 id="cloudwatch-alarm-good-to-know"><a class="markdownIt-Anchor" href="#cloudwatch-alarm-good-to-know"></a> CloudWatch Alarm: good to know</h3><ul><li>alarms can be created based on CloudWatch Logs Metrics Filters</li><li>to test alarms and notifications, set the alarm state to alarm using CLI</li></ul><h2 id="cloudwatch-events"><a class="markdownIt-Anchor" href="#cloudwatch-events"></a> CloudWatch Events</h2><ul><li>event pattern: intercept events from AWS services (sources)<ul><li>example soruces: EC2 instance start, codebuild failure, S3 trsuted advisor</li><li>can intercept any API call with CloudTrail integration</li></ul></li><li>schedule or Cron (example: create an event every 4 hours)</li><li>A JSON payload is created from the event and passed to a target<ul><li>compute: lambda, batch, ECS task</li><li>integration: SQS, SNS, Kinesis data streams, Kinesis data firehose</li><li>Orchestration: step functions, codepipeline, codebuild</li><li>maintenance: SSM, EC2 actions</li></ul></li></ul><h2 id="amazon-eventbridge"><a class="markdownIt-Anchor" href="#amazon-eventbridge"></a> Amazon EventBridge</h2><ul><li><p>eventbridge is the next evolution of CloudWatch Events</p></li><li><p>default event bus: generated by AWS services (CloudWatch events)</p></li><li><p>partner event bus: receive events from SaaS service or applications</p></li><li><p>custom event buses: for your own applications</p></li><li><p>event buses can be accessed by other AWS accounts</p></li><li><p>Rules: how to process the events (similar to CloudWatch Events)</p></li><li><p>Amazon EventBridge builds upon and extends CloudWatch events</p></li><li><p>it uses the same service API and endpoint, and the same underlying service infrastructure</p></li><li><p>eventbridge allows extension to add event buses for your custom applications and your thrid party SaaS apps</p></li><li><p>event bridge has the schema registry capability</p></li><li><p>eventbridge has a different name to mark the new capabilities</p></li><li><p>over time, the CloudWatch events name will be replaced with eventbridge</p></li></ul><h2 id="aws-cloudtrail"><a class="markdownIt-Anchor" href="#aws-cloudtrail"></a> AWS CloudTrail</h2><ul><li>provides governance, compliancen and audit for your AWS account</li><li>CloudTrail is enabled by default</li><li>get an history of events / API calls made within your AWS account by<ul><li>console</li><li>SDK</li><li>CLI</li><li>AWS Service</li></ul></li><li>can put logs from CloudTrail into CloudWatch logs or S3</li><li>a trail can be applied to all regions (default) or a single region</li><li>if a resource is deleted in AWS, investigate CloudTrail first</li></ul><h2 id="cloudtrail-insights"><a class="markdownIt-Anchor" href="#cloudtrail-insights"></a> CloudTrail Insights</h2><ul><li>enable CloudTrail insights to detect unusual activity in your account<ul><li>inaccurate resource provisioning</li><li>hitting service limits</li><li>bursts of AWS IAM actions</li><li>gaps in periodic maintenance activity</li></ul></li><li>CloudTrail insights analyzes normal management events to create a baseline</li><li>and then continuously analyzes write events to detect unusual patterns<ul><li>anomalies appear in the CloudTrail console</li><li>event is sent to Amazon S3</li><li>an eventbridge event is generated (for automation needs)</li></ul></li></ul><h3 id="cloudtrail-events-retention"><a class="markdownIt-Anchor" href="#cloudtrail-events-retention"></a> CloudTrail Events retention</h3><ul><li>events are stored for 90 days in CloudTrail</li><li>to keep events beyond this period, log them to S3 and use Athena</li></ul><h1 id="aws-config"><a class="markdownIt-Anchor" href="#aws-config"></a> AWS Config</h1><ul><li>helps with auditing and recording compliance of your AWS resources</li><li>helps record configurations and changes over time</li><li>questions that can be solved by AWS Config<ul><li>is there unrestricted SSH access to my security groups</li><li>do my buckets have any public access</li><li>how has my ALB configuration changed over time</li></ul></li><li>you can receive alerts (SNS notifications) for any changes</li><li>AWS Config is a per-region service</li><li>can be aggregated across regions and accounts</li></ul><h2 id="config-rules-remediations"><a class="markdownIt-Anchor" href="#config-rules-remediations"></a> Config Rules - remediations</h2><ul><li>automate remediation of non-compliant resources using SSM automation documents</li><li>use AWS-managed automation documents or create custom automation documents<ul><li>tip: you can create custom automation documents that invokes lambda function</li></ul></li><li>you can set remediation retries if the resource is still non-compliant after auto-remediation</li></ul><h2 id="config-rules-notifications"><a class="markdownIt-Anchor" href="#config-rules-notifications"></a> Config Rules - notifications</h2><ul><li>use eventbridge to trigger notifications when AWS resources are non-compliant</li><li>ability to send configuration changes and compliance state notifications to SNS (all events - use SNS filtering or filter at client-side)</li></ul><h2 id="cloudwatch-vs-cloudtrail-vs-config"><a class="markdownIt-Anchor" href="#cloudwatch-vs-cloudtrail-vs-config"></a> CloudWatch vs CloudTrail vs Config</h2><ul><li>CloudWatch<ul><li>performance monitoring (metrics, CPU, network, etc…) and dashboards</li><li>event and alerting</li><li>log aggregation and analysis</li></ul></li><li>CloudTrail<ul><li>record API calls made within your account by everyone</li><li>can define trails for specific resources</li><li>global service</li></ul></li><li>Config<ul><li>record configuration changes</li><li>evaluate resources against comliance rules</li><li>get timeline of changes and compliance</li></ul></li></ul><h3 id="example-for-an-elastic-load-balancer"><a class="markdownIt-Anchor" href="#example-for-an-elastic-load-balancer"></a> Example for an Elastic Load Balancer</h3><ul><li>CloudWatch<ul><li>monitoring incoming connections metric</li><li>visualize error codes as a percentage over time</li><li>make a dashboard to get an idea of your load balancer performance</li></ul></li><li>CloudTrail<ul><li>track who made any changes to the load balancer with API calls</li></ul></li><li>Config<ul><li>trakc security group rules for the load balancer</li><li>track configuration changes for the load balancer</li><li>ensure an SSL certificate is always assigned to the load balancer (compliance)</li></ul></li></ul><h1 id="aws-sts-security-token-service"><a class="markdownIt-Anchor" href="#aws-sts-security-token-service"></a> AWS STS (Security Token Service)</h1><ul><li>allows to grant limited and temporary access to AWS resources</li><li>token is valid for up to one hour (must be refreshed)</li><li>AssumeRole<ul><li>within your own account: for enhanced security</li><li>cross account access: assume role in target account for perform actions there</li></ul></li><li>AssumeRoleWithSAML<ul><li>return credentials for users logged with SAML</li></ul></li><li>AssumeRoleWithWebIdentity<ul><li>return credentials for users logged with an IDP (facebook, google…)</li><li>AWS recommends against using this, and using Cognito instead</li></ul></li><li>GetSessionToken<ul><li>for MFA, from a user or AWS account root user</li></ul></li></ul><ol><li>define an IAM role within your account or cross-account</li><li>define which principals can access this IAM role</li><li>use AWS STS to retrieve credentials and impersonate the IAM role you have access to (AssumeRole API)</li><li>temporary credentials can be valid between 15 minutes to 1 hour</li></ol><h2 id="identity-federation-in-aws"><a class="markdownIt-Anchor" href="#identity-federation-in-aws"></a> Identity Federation in AWS</h2><ul><li>federation lets users outside of AWS to assume temporary role for accessing AWS resources</li><li>these users assume identity provided access role</li><li>federations can have many flavors<ul><li>SAML</li><li>Custom Identity Broker</li><li>Amazon Cognito</li><li>Single Sign On</li><li>Non-SAML with AWS Microsoft AD</li></ul></li><li>using federation, you don’t need to create IAM users (user management is outside of AWS)</li></ul><h2 id="saml-20-federation"><a class="markdownIt-Anchor" href="#saml-20-federation"></a> SAML 2.0 Federation</h2><ul><li>needs to setup a trust between AWS IAM and SAML (both ways)</li><li>SAML enables web based, cross domain SSO</li><li>uses the STS API: AssumeRoleWithSAML</li><li>note federation through SAML is the old way of doing things</li><li>Amazon SSO federation is the new managed and simpler way</li></ul><h2 id="aws-directory-services"><a class="markdownIt-Anchor" href="#aws-directory-services"></a> AWS Directory Services</h2><ul><li>AWS Managed Microsoft AD<ul><li>create your own AD in AWS, manage users locally, supports MFA</li><li>establish trust connections with your on-premises AD</li></ul></li><li>AD Connector<ul><li>Directory Gateway (proxy) to redirect to on-premises AD</li><li>users are managed on the on-premises AD (not on AWS)</li></ul></li><li>Simple AD<ul><li>AD compatible managed directory on AWS</li><li>cannot be joined with on-premises AD (users managed on AWS only)</li></ul></li></ul><h2 id="aws-organizations"><a class="markdownIt-Anchor" href="#aws-organizations"></a> AWS Organizations</h2><ul><li>global service</li><li>allows to manage multiple AWS accounts</li><li>the main account is the master account - you can’t change it</li><li>other accounts are member accounts</li><li>member accounts can only be part of one organization</li><li>consolidated billing across all accounts - single payment method</li><li>pricing benefits from aggregated usage</li><li>API is available to automate AWS account creation</li></ul><h3 id="multi-account-strategies"><a class="markdownIt-Anchor" href="#multi-account-strategies"></a> Multi account strategies</h3><ul><li>create accounts per department, per cost center, per dev/test/prod, based on regulatory restrictions (using SCP), for better resource isolation, to have separate per-account service limits, isolated account for logging</li><li>multi account vs one account multi VPC</li><li>use tagging standards for billing purposes</li><li>enable CloudTrail on all accounts, send logs to central S3 account</li><li>send CloudWatch logs to central logging account</li><li>establish cross account roles for admin purpose</li></ul><h3 id="service-control-policies-scp"><a class="markdownIt-Anchor" href="#service-control-policies-scp"></a> Service Control Policies (SCP)</h3><ul><li>whitelist or blacklist IAM actions</li><li>applied at the OU or Account level</li><li>does not apply to the master account</li><li>SCP is applied to all the users and roles of the account, including Root</li><li>the SCP does not affect service linked roles<ul><li>service linked roles enable other AWS services to integrate with AWS organizations and can’t be restricted by SCPs</li></ul></li><li>SCP must have an explicit Allow (does not allow anything by default)</li><li>use cases<ul><li>restrict access to certain services (for example: can’t use EMR)</li><li>enforce PCI compliance by explicitly disabling services</li></ul></li></ul><h3 id="aws-organization-moving-accounts"><a class="markdownIt-Anchor" href="#aws-organization-moving-accounts"></a> AWS Organization - moving accounts</h3><ul><li>to migrate accounts from one organization to another<ul><li>remove the member account from the old organization</li><li>send an invite to the new organization</li><li>accept the invite to the new organization from the member account</li></ul></li><li>if you want the master account of the old organization to also join the new organization<ul><li>remove the member accounts from the organization using the procedure above</li><li>delete the old organization</li><li>repeat the process above to invite the old master account to the new org</li></ul></li></ul><h1 id="iam-advanced"><a class="markdownIt-Anchor" href="#iam-advanced"></a> IAM Advanced</h1><h2 id="iam-for-s3"><a class="markdownIt-Anchor" href="#iam-for-s3"></a> IAM for S3</h2><ul><li>ListBucket permission applies to<ul><li><code>arn:aws:s3:::test</code></li><li>bucket level permission</li></ul></li><li>GetObject, PutObject, DeleteObject applies to<ul><li><code>arn:aws:s3:::test/*</code></li><li>object level permission</li></ul></li></ul><h2 id="iam-roles-vs-resource-based-policies"><a class="markdownIt-Anchor" href="#iam-roles-vs-resource-based-policies"></a> IAM Roles vs Resource Based Policies</h2><ul><li>when you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role</li><li>when using a resource based policy, the principal doesn’t have to give up his permissions</li></ul><h2 id="iam-permission-boundaries"><a class="markdownIt-Anchor" href="#iam-permission-boundaries"></a> IAM permission boundaries</h2><ul><li>IAM permission boundaries are supported for users and roles (not groups)</li><li>advanced feature to use a managed policy to set the maximum permissions an IAM entity can get</li><li>if a user has been assigned a permission boundary so that it can only access S3, then no matter what permission policies it has, it can only access to S3, nothing else.</li></ul><h2 id="aws-resource-access-manager-ram"><a class="markdownIt-Anchor" href="#aws-resource-access-manager-ram"></a> AWS Resource Access Manager (RAM)</h2><ul><li>share AWS resources that you own with other AWS accounts</li><li>share with any account or within your organization</li><li>avoid resource duplication</li><li>VPC subnets<ul><li>allow to have all the resources launched in the same subnets</li><li>must be from the same AWS organization</li><li>cannot share security groups and default VPC</li><li>participants can manage their own resources in there</li><li>participants can’t view, modify, delete resources that belong to other participants or the owner</li></ul></li><li>AWS transit gateway</li><li>route53 resolver rules</li><li>license manager configurations</li></ul><h2 id="aws-sso"><a class="markdownIt-Anchor" href="#aws-sso"></a> AWS SSO</h2><ul><li>centrally manage single sign on to access multiple accounts and third party business applications</li><li>integrated with AWS organizations</li><li>support SAML 2.0 markup</li><li>integration with on-premises active directory</li><li>centralized permissioin management</li><li>centralized auditing with CloudTrail</li></ul><h1 id="aws-security"><a class="markdownIt-Anchor" href="#aws-security"></a> AWS Security</h1><h2 id="encryption-in-flight-ssl"><a class="markdownIt-Anchor" href="#encryption-in-flight-ssl"></a> Encryption in flight (SSL)</h2><ul><li>data is encrypted before sending and decrypted after receiving</li><li>SSL certificate help with encryption (HTTPS)</li><li>encryption in flight ensures no MITM (man in the middle) attach can happen</li></ul><h2 id="server-side-encryption-at-rest"><a class="markdownIt-Anchor" href="#server-side-encryption-at-rest"></a> Server side encryption at rest</h2><ul><li>data is encrypted after being received by the server</li><li>data is decrypted before being sent</li><li>it is stored in an encrypted form thanks to a key (usually a data key)</li><li>the encryption / decryption keys must be managed somewhere and the server must have access to it</li></ul><h2 id="client-side-encryption-2"><a class="markdownIt-Anchor" href="#client-side-encryption-2"></a> Client side encryption</h2><ul><li>data is encrypted by the client and never decrypted by the server</li><li>data will be decrypted by a receiving client</li><li>the server should not be able to decrypt the data</li></ul><h2 id="aws-kms-key-management-service"><a class="markdownIt-Anchor" href="#aws-kms-key-management-service"></a> AWS KMS (key management service)</h2><ul><li>anytime you hear encryption for an AWS service, it is most likely KMS</li><li>easy way to control access to your data, AWS manages keys for us</li><li>fully integrated with IAM authorization</li><li>seamlessly integrated into<ul><li>EBS</li><li>S3</li><li>Redshift</li><li>RDS</li><li>SSM</li></ul></li><li>but you can also use the CLI / SDK</li></ul><h3 id="kms-customer-master-key-cmk-types"><a class="markdownIt-Anchor" href="#kms-customer-master-key-cmk-types"></a> KMS - Customer Master Key (CMK) Types</h3><ul><li>Symmetric (AES-256)<ul><li>first offering of KMS, single encryption key that is used to encrypt and decrypt</li><li>AWS services that are integrated with KMS use Symmetric CMKs</li><li>you never get access to the key unencrypted (must call KMS API to use)</li></ul></li><li>Asymmetric (RSA and ECC key pairs)<ul><li>public (Encrypt) and private (decrypt) key</li><li>used for encrypt / decrypt, or sign / verify operations</li><li>the public key ios downloadable, but you can’t access the private key unencrypted</li><li>use case: encryption outside of AWS by users who can’t call the KMS API</li></ul></li></ul><h3 id="pricing"><a class="markdownIt-Anchor" href="#pricing"></a> Pricing</h3><ul><li>able to fully manage the keys and policies<ul><li>create</li><li>rotation policies</li><li>disable</li><li>enable</li></ul></li><li>able to audit key usage (using CloudTrail)</li><li>3 types of customer master keys (CMK)<ul><li>AWS managed service default CMK: free</li><li>user keys created in KMS: $1 / month</li><li>user keys imported (must be 256-bit symmetric key): $1 / month</li></ul></li><li>plus pay for API call to KMS</li></ul><h3 id="kms-101"><a class="markdownIt-Anchor" href="#kms-101"></a> KMS 101</h3><ul><li>anytime you need to share sensitive information, use KMS<ul><li>database passwords</li><li>credentials to external service</li><li>private key of SSL certificates</li></ul></li><li>the value in KMS is that the CMK used to encrypt data can never be retrieved by the user, and the CMK can be rotated for extra security</li><li>never ever store your secrets in plaintext, especially in your code</li><li>encrypted secrets can be stored in the code / environment variables</li><li>KMS can only help in encrypting up to 4 KB of data per call</li><li>if data &gt; 4KB, use envelope encryption</li></ul><h3 id="kms-key-policies"><a class="markdownIt-Anchor" href="#kms-key-policies"></a> KMS key policies</h3><ul><li>control access to KMS keys, similar to S3 bucket policies</li><li>different: you cannot control access without them</li><li>default KMS key policy<ul><li>created if you don’t provide a specific KMS key policy</li><li>complete access to the key to the root user = entire AWS account</li><li>the root user can administer the key and all IAM accounts can use the key</li><li>gives access to the IAM policies to the KMS key</li></ul></li><li>custom KMS key policy<ul><li>define users, roles that can access the KMS key</li><li>define who can administer the key</li><li>useful for cross-account access of your KMS key</li></ul></li></ul><h3 id="kms-automatic-key-rotation"><a class="markdownIt-Anchor" href="#kms-automatic-key-rotation"></a> KMS Automatic Key Rotation</h3><ul><li>for Costumer managed CMK (not AWS managed CMK)</li><li>if enabled: automatic key rotation heppens every 1 year</li><li>previous key is kept active so you can decrypt old data</li><li>new key has the same CMK ID (only the backing key is changed)</li></ul><h2 id="ssm-parameter-store"><a class="markdownIt-Anchor" href="#ssm-parameter-store"></a> SSM Parameter Store</h2><ul><li>secure storage for configuration and secrets</li><li>optional seamless encryption using KMS</li><li>serverless, scalable, durable, easy SDK</li><li>version tracking of configurations / secrets</li><li>configuration management using path and IAM</li><li>notifications with CloudWatch events</li><li>integration with CloudFormation</li></ul><h2 id="aws-secrets-manager"><a class="markdownIt-Anchor" href="#aws-secrets-manager"></a> AWS Secrets Manager</h2><ul><li>newer service, meant for storing secrets</li><li>capability to force rotation of secrets every X days</li><li>automate generation of secrets on rotation (uses lambda)</li><li>integration with Amazon RDS</li><li>secrets are encrypted using KMS</li></ul><h2 id="cloudhsm"><a class="markdownIt-Anchor" href="#cloudhsm"></a> CloudHSM</h2><ul><li>AWS provisions encryption hardware</li><li>dedicated hardware (HSM = Hardware Security Module)</li><li>you manage your own encryption keys entirely (not AWS)</li><li>HSM device is tamper resistant</li><li>supports both symmetric and asymmetric encryption (SSL/TLS keys)</li><li>no free tier available</li><li>must use the CloudHSM client software</li><li>refshift support CloudHSM for database encryption and key management</li><li>good option to use with SSE-C encryption</li></ul><h2 id="aws-shield"><a class="markdownIt-Anchor" href="#aws-shield"></a> AWS Shield</h2><ul><li>AWS shield standard<ul><li>free service that is activated for every AWS customer</li><li>provides protection from attacks such as SYN/UDP floods, reflection attacks and other layer 3 / layer 4 attacks</li></ul></li><li>AWS Shield Advanced<ul><li>optional DDoS mitigation service ($3000 per month per organization)</li><li>protect against more sophisticated attack on EC2, ELB, CloudFront, Global Accelerator, Route53</li></ul></li></ul><h2 id="aws-waf-web-application-firewall"><a class="markdownIt-Anchor" href="#aws-waf-web-application-firewall"></a> AWS WAF (Web Application Firewall)</h2><ul><li>protects your web applications from common web exploits (layer 7)</li><li>layer 7 is HTTP (vs layer 4 is TCP)</li><li>deploy on Application Load Balancer, API Gateway, CloudFront</li><li>define web ACL<ul><li>rules can include: IP addresses, HTTP headers, HTTP body, URI strings</li><li>protects from common attack - SQL injection and cross-site scripting (XSS)</li><li>size constraints, geo-match</li><li>rate based rules (to count occurrences of events) - for DDoS protection</li></ul></li></ul><h2 id="aws-guardduty"><a class="markdownIt-Anchor" href="#aws-guardduty"></a> AWS GuardDuty</h2><ul><li>intelligent threat discovery to protect AWS account</li><li>uses machine learning algorithms, anomaly detection, third party data</li><li>one click to enable (30 days trial), no need to install software</li><li>input data includes<ul><li>CloudTrail log: unusual API calls, unauthorized deployments</li><li>VPC flow logs: unusual internal traffic, unusual IP addresses</li><li>DNS logs: compromised EC2 instances sending encoded data within DNS queries</li></ul></li><li>can setup CloudWatch event rules to be notified in case of findings</li><li>CloudWatch events rules can target AWS lambda or SNS</li><li>can protect against CryptoCurrency attacks</li></ul><h2 id="aws-inspector"><a class="markdownIt-Anchor" href="#aws-inspector"></a> AWS Inspector</h2><ul><li>automated security assessments for EC2 instances</li><li>analyze the running OS against known vulnerabilities</li><li>analyze against unintended network accessibility</li><li>AWS Inspector agent must be installed on OS in EC2 instances</li><li>after the assessment, you get a report with a list of vulnerabilities</li><li>possibilitiy to send notifications to SNS</li></ul><h2 id="aws-macie"><a class="markdownIt-Anchor" href="#aws-macie"></a> AWS Macie</h2><ul><li>Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS</li><li>Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)</li></ul><h1 id="aws-vpc"><a class="markdownIt-Anchor" href="#aws-vpc"></a> AWS VPC</h1><h2 id="understanding-cidr-classless-inter-domain-routing-ipv4"><a class="markdownIt-Anchor" href="#understanding-cidr-classless-inter-domain-routing-ipv4"></a> Understanding CIDR (Classless Inter-Domain Routing) - IPv4</h2><ul><li><p>CIDR are used for security groups rules, or AWS networking in general</p></li><li><p>they help to define an IP address range</p></li><li><p>a CIDR has two components</p><ul><li>the base IP</li><li>the subnet mask</li></ul></li><li><p>the base IP represents an IP contained in the range</p></li><li><p>the subnet masks defines how many bits can change in the IP</p></li><li><p>the subnet masks basically allows part of the underlying IP to get additional next values from the base IP</p><ul><li><code>/32</code> allow for 1 IP = <code>2^0</code></li><li><code>/31</code> allow for 1 IP = <code>2^1</code></li><li><code>/30</code> allow for 1 IP = <code>2^2</code></li><li><code>/29</code> allow for 1 IP = <code>2^3</code></li><li><code>/28</code> allow for 1 IP = <code>2^4</code></li><li><code>/27</code> allow for 1 IP = <code>2^5</code></li><li><code>/26</code> allow for 1 IP = <code>2^6</code></li><li><code>/25</code> allow for 1 IP = <code>2^7</code></li><li><code>/24</code> allow for 1 IP = <code>2^8</code></li><li><code>/16</code> allow for 1 IP = <code>2^16</code></li><li><code>/0</code> allow for 1 IP = <code>2^32</code></li></ul></li><li><p>quick memo</p><ul><li><code>/32</code> - no IP number can change</li><li><code>/24</code> - last IP number can change</li><li><code>/16</code> - last IP two numbers can change</li><li><code>/8</code> - last IP 3 numbers can change</li><li><code>/0</code> - all IP numbers can change</li></ul></li></ul><h2 id="private-vs-public-ip-allowed-ranges"><a class="markdownIt-Anchor" href="#private-vs-public-ip-allowed-ranges"></a> Private vs public IP allowed ranges</h2><ul><li>the internet assigned number authority established certain blocks of IPv4 addresses for the use of private (LAN) and public addresses</li><li>private IP can only allow certain values<ul><li>10.0.0.0 - 10.255.255.255 (in big networks)</li><li>172.16.0.0 - 172.31.255.255 (default AWS one)</li><li>192.168.0.0 - 192.168.255.255 (home networks)</li></ul></li><li>all the rest of the IP on the internet are public IP</li></ul><h2 id="default-vpc-walkthrough"><a class="markdownIt-Anchor" href="#default-vpc-walkthrough"></a> Default VPC walkthrough</h2><ul><li>all new accounts have a default VPC</li><li>new instances are launched into default VPC if no subnet is specified</li><li>default VPC have internet connectivity and all instances have public IP</li><li>we also get a public and private DNS name</li></ul><h2 id="vpc-in-aws-ipv4"><a class="markdownIt-Anchor" href="#vpc-in-aws-ipv4"></a> VPC in AWS - IPv4</h2><ul><li>you can have multiple VPCs in a region (max 5 per region)</li><li>max CIDR per VPC is 5, for each CIDR<ul><li>min size is <code>/28</code> = 16 IP addresses</li><li>max size is <code>/16</code> = 65536 IP addresses</li></ul></li><li>because VPC is private, only the private IP ranges are allowed</li><li>your VPC CIDR should not overlap with your other networks</li></ul><h2 id="subnets-ipv4"><a class="markdownIt-Anchor" href="#subnets-ipv4"></a> Subnets - IPv4</h2><ul><li>AWS reserves 5 IPs addresses (first 4 and last 1 IP address) in each subnet</li><li>these 5 IPs are not available for use and cannot be assigned to an instance</li><li>if CIDR block is 10.0.0.0/24, reserved IP are<ul><li>10.0.0.1: network address</li><li>10.0.0.2: reserved by AWS for the VPC router</li><li>10.0.0.3: reserved by AWS for mapping to Amazon provided DNS</li><li>10.0.0.4: reserved by AWS for future use</li><li>10.0.0.255: network broadcast address, AWS does not support broadcast in a VPC, therefore the address is reserved</li></ul></li><li>exam tip<ul><li>if you need 29 IP addresses for EC2 instances, you can’t choose a subnet of size /27 (32 IPs)</li><li>you need at least 64 IP, subnet size /26 (<code>64 - 5 &gt; 29</code>, but <code>32 - 5 &lt;= 29</code>)</li></ul></li></ul><h2 id="internet-gateways"><a class="markdownIt-Anchor" href="#internet-gateways"></a> Internet Gateways</h2><ul><li>internet gateways helps our VPC instances connect with the internet</li><li>it scales horizontally and is HA and redundant</li><li>must be created separately from VPC</li><li>one VPC can only be attached to one IGW and vice versa</li><li>internet gateway is also a NAT for the instances that have a public IPv4</li><li>internet gateways on their own do not allow internet access</li><li>route tables must also be edited</li></ul><p>If you launch an EC2 instance, to give the instance public internet, you need to edit the route table.</p><ol><li>associate route table to the subnet</li><li>add a rule in route table, so that when instance is trying to access a public IP, it will route the traffic to the internet gateway</li></ol><p>For instances in private subnet, if they were to access it through the internet gateway, they would also be accessible from the internet (not we want). So we need NAT</p><h2 id="nat-instances-network-address-translation"><a class="markdownIt-Anchor" href="#nat-instances-network-address-translation"></a> NAT instances - network address translation</h2><ul><li>allows instances in the private subnets to connect to the internet</li><li>must be launched in a public subnet</li><li>must disable EC2 flag: source / destination check</li><li>must have elastic IP attached to it</li><li>route table must be configured to route traffic from private subnets to NAT instance</li></ul><p>NAT instance will then route traffic to the internet gateway because of the route table rules.</p><p><img src="/../images/AWS-SAA-Review/1.png" alt="" /></p><h2 id="nat-gateway"><a class="markdownIt-Anchor" href="#nat-gateway"></a> NAT Gateway</h2><ul><li>AWS managed NAT, higher bandwidth, better availability, no admin needed</li><li>pay by the hour for usage and bandwidth</li><li>NAT is created in a specified AZ, uses an EIP</li><li>cannot be used by an instance in the subnet (only from the other subnets)</li><li>requires an IGW (private subnet -&gt; NAT -&gt; IGW)</li><li>no security group to manage / required</li></ul><h3 id="nat-gateway-with-ha"><a class="markdownIt-Anchor" href="#nat-gateway-with-ha"></a> NAT Gateway with HA</h3><ul><li>NAT gateway is resilient within a single AZ</li><li>must create multiple NAT gateway in multiple AZ for fault-tolerance</li><li>there is no cross AZ failover needed because if an AZ goes down it doesn’t need NAT</li></ul><h2 id="dns-resolution-in-vpc"><a class="markdownIt-Anchor" href="#dns-resolution-in-vpc"></a> DNS Resolution in VPC</h2><ul><li>enableDnsSupport (DNS Resolution settings)<ul><li>default is True</li><li>helps decide if DNS resolution is supported for the VPC</li><li>if True, queries the AWS DNS server at 169.254.169.253</li></ul></li><li>enableDnsHostname (DNS Hostname setting)<ul><li>False by default for newly created VPC</li><li>True by default for default VPC</li><li>won’t do anything unless enableDnsSupport is True</li><li>if True, assign public hostname to EC2 instance if it has a public</li></ul></li><li>if you use custom DNS domain names in a private zone in Route 53, you must set both these attributes to true</li></ul><h2 id="network-acls-and-security-group"><a class="markdownIt-Anchor" href="#network-acls-and-security-group"></a> Network ACLs and Security Group</h2><ul><li><p>Security group is the firewall of EC2 Instances.</p></li><li><p>Network ACL is the firewall of the VPC Subnets.</p></li><li><p>Security groups are tied to an instance whereas Network ACLs are tied to the subnet.</p></li><li><p>Network ACLs are applicable at the subnet level, so any instance in the subnet with an associated NACL will follow rules of NACL. That’s not the case with security groups, security groups has to be assigned explicitly to the instance.</p></li><li><p>This means any instances within the subnet group gets the rule applied. With Security group, you have to manually assign a security group to the instances.</p></li><li><p>Security groups are stateful: This means any changes applied to an incoming rule will be automatically applied to the outgoing rule. e.g. If you allow an incoming port 80, the outgoing port 80 will be automatically opened.</p></li><li><p>Network ACLs are stateless: This means any changes applied to an incoming rule will not be applied to the outgoing rule. e.g. If you allow an incoming port 80, you would also need to apply the rule for outgoing traffic.</p></li></ul><h3 id="rules-allow-or-deny"><a class="markdownIt-Anchor" href="#rules-allow-or-deny"></a> Rules: Allow or Deny</h3><ul><li>Security group support allow rules only (by default all rules are denied). e.g. You cannot deny a certain IP address from establishing a connection.</li><li>Network ACL support allow and deny rules. By deny rules, you could explicitly deny a certain IP address to establish a connection example: Block IP address 123.201.57.39 from establishing a connection to an EC2 Instance.</li></ul><p><img src="/../images/AWS-SAA-Review/2.png" alt="" /></p><table><thead><tr><th>Security Group</th><th>Network ACL</th></tr></thead><tbody><tr><td>Operates at the instance level</td><td>Operates at the subnet level</td></tr><tr><td>supports allow rules only</td><td>supports allow rules and deny rules</td></tr><tr><td>is stateful: return traffic is automatically allowed, regardless of any rules</td><td>is stateless: return traffic must be explicity allowed by rules</td></tr><tr><td>we evaluate all rules before deciding whether to allow traffic</td><td>we proces rules in number order when deciding whether to allow traffic</td></tr><tr><td>applies to an instance only if someone specifies the security group when launching the instance, or associates the security group with the instance later on</td><td>automatically applies to all instances in the subnets it’s associated with (therefore, you don’t have to rely on users to specify the security group)</td></tr></tbody></table><h2 id="vpc-peering"><a class="markdownIt-Anchor" href="#vpc-peering"></a> VPC Peering</h2><ul><li>connect 2 VPCs, privately using AWS network</li><li>make them behave as if they were in the same network</li><li>must not have overlapping CIDR</li><li>VPC peering connection is not transitive (must be established for each VPC that need to communicate with one another)<br />e.g. if we connect VPC A to VPC B and also connect VPC B to VPC C, this doesn’t mean VPC A is connected to VPC C.</li><li>you can do VPC peering with another AWS account</li><li>you must update route tables in each VPC’s subnets to ensure instances can communicate</li></ul><h3 id="vpc-peering-good-to-know"><a class="markdownIt-Anchor" href="#vpc-peering-good-to-know"></a> VPC Peering - good to know</h3><ul><li>VPC peering can work inter region, cross account</li><li>you can reference a security group of a peered VPC (work cross account)</li></ul><h2 id="vpc-endpoint"><a class="markdownIt-Anchor" href="#vpc-endpoint"></a> VPC Endpoint</h2><ul><li>endpoints allow you to connect to AWS services using a private network instead of the public www network</li><li>they scale horizontally and are redundant</li><li>they remove the need of IGW, NAT, etc… to access AWS services</li><li>interface<ul><li>provisions an ENI (private IP address) as an entry point (must attach security group) - most AWS services</li></ul></li><li>gateway<ul><li>provisions a target and must be used in a route table - S3 and DynamoDB</li></ul></li></ul><h2 id="flow-logs"><a class="markdownIt-Anchor" href="#flow-logs"></a> Flow Logs</h2><ul><li>capture information about IP traffic going into your interfaces<ul><li>VPC flow logs (includes the other two)</li><li>subnet flow logs</li><li>elastic network interface flow logs</li></ul></li><li>helps to monitor and troubleshoot connectivity issues</li><li>flow logs data can go to S3 / CloudWatch Logs</li><li>captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache, Redshift, WorkSpaces</li></ul><h2 id="bastion-hosts"><a class="markdownIt-Anchor" href="#bastion-hosts"></a> Bastion Hosts</h2><ul><li>we can use a Bastion Host to SSH into our private instances</li><li>the bastion is in the public subnet which is then connected to all other private subnets</li><li>Bastion Host security group must be tightened</li><li>exam tip: make sure the bastion host only has port 22 traffic from the IP you need. not from the security groups of your other instances</li></ul><h2 id="site-to-site-vpn"><a class="markdownIt-Anchor" href="#site-to-site-vpn"></a> Site to Site VPN</h2><ul><li>Virtual Private Gateway<ul><li>VPN concentrator on the AWS side of the VPN connection</li><li>VGW (VPC Gateway) is created and attached to the VPC from which you want to create the Site-to-Site VPN connection</li></ul></li><li>Customer Gateway<ul><li>software application or physical device on customer side of the VPN connection</li><li>ip address<ul><li>use static, internet-routable IP address for your customer gateway device</li><li>if behind a CGW (Cloud Gateway) behind a NAT, use the public IP address of the NAT</li></ul></li></ul></li></ul><h2 id="direct-connect-dx"><a class="markdownIt-Anchor" href="#direct-connect-dx"></a> Direct Connect (DX)</h2><ul><li>provides a dedicated private connection from a remote network to your VPC</li><li>dedicated connection must be setup between your data center and AWS direct connect locations</li><li>you need to setup a Virtual Private Gateway on your VPC</li><li>access public resources (S3), and private (EC2) on same connection</li><li>use cases<ul><li>increase bandwidth throughput: working with large data sets - lower cost</li><li>more consistent network experience - applications using real time data feeds</li><li>hybrid environments (on premises + cloud)</li></ul></li><li>supports both IPv4 and IPv6</li></ul><h3 id="direct-connect-gateway"><a class="markdownIt-Anchor" href="#direct-connect-gateway"></a> Direct connect gateway</h3><ul><li>if you want to setup a direct connect to one or more VPC in many different regions (same account), you must use a direct connect gateway</li></ul><h3 id="connection-types"><a class="markdownIt-Anchor" href="#connection-types"></a> Connection types</h3><ul><li>dedicated connections<ul><li>1 Gbps and 10 Gbps capacity</li><li>physical ethernet port dedicated to a customer</li><li>request made to AWS first, then completed by AWS direct connetion partners</li></ul></li><li>hosted connections<ul><li>50 Mbps, 500 Mbps to 10 Gbps</li><li>connection requests are made via AWS direct connect partners</li><li>capacity can be added or removed on demand</li><li>1,2,5,10 Gbps available at select AWS direct connect partners</li></ul></li><li>lead times are often longer than 1 month to establish a new connection</li></ul><h3 id="encryption"><a class="markdownIt-Anchor" href="#encryption"></a> Encryption</h3><ul><li>data in transit is not encrypted but is private</li><li>AWS direct connect + VPN provides an IPsec-encrypted private connection</li><li>good for an extra level of security, but slightly more complex to put in place</li></ul><h3 id="resiliency"><a class="markdownIt-Anchor" href="#resiliency"></a> Resiliency</h3><ul><li>High resiliency for critical workloads<ul><li>one connection at multiple locations</li></ul></li><li>maximum resiliency for critical workloads<ul><li>maximum resilience is achieved by separate connections</li><li>terminating on separate devices in more than one location</li></ul></li></ul><h2 id="egress-outgoing-only-internet-gateway"><a class="markdownIt-Anchor" href="#egress-outgoing-only-internet-gateway"></a> Egress (outgoing) only internet gateway</h2><ul><li>Egress only internet gateway is for IPv6 only</li><li>similar function as a NAT, but a NAT is for IPv4</li><li>good to know: IPv6 are all public addresses</li><li>therefore all our instances with IPv6 are publicly accessible</li><li>Egress only internet gateway gives our IPv6 instances access to the internet, but they won’t be directly reachable by the internet</li><li>after creating an Egress only internet gateway, edit the route tables</li></ul><h2 id="aws-private-link-vpc-endpoint-service"><a class="markdownIt-Anchor" href="#aws-private-link-vpc-endpoint-service"></a> AWS Private Link - VPC Endpoint Service</h2><h3 id="how-to-expose-services-in-your-vpc-to-other-vpcs"><a class="markdownIt-Anchor" href="#how-to-expose-services-in-your-vpc-to-other-vpcs"></a> How to expose services in your VPC to other VPCs?</h3><ul><li>Option1: make it public<ul><li>goes through the public www</li><li>tough to manage access</li></ul></li><li>Option2: VPC peering<ul><li>must create many peering relations (peering relation is one-to-one)</li><li>opens the whole network(maybe you just want one of your services to be exposed, not the whole VPC)</li></ul></li></ul><h3 id="aws-private-link"><a class="markdownIt-Anchor" href="#aws-private-link"></a> AWS Private Link</h3><ul><li>most secure and scalable way to expose a service to 1000s of VPCs</li><li>does not require VPC peering, internet gateway, NAT, route tables</li><li>requires a network load balancer (service VPC) and ENI (customer VPC)<ul><li>customer VPC =&gt; ENI =&gt; private link =&gt; NLB =&gt; service VPC</li></ul></li><li>if the NLB is in multiple AZ, and the ENI in multiple AZ, the solution is fault tolerant</li></ul><p><img src="/../images/AWS-SAA-Review/3.png" alt="" /></p><h2 id="ec2-classic-and-aws-classiclink-deprecated"><a class="markdownIt-Anchor" href="#ec2-classic-and-aws-classiclink-deprecated"></a> EC2 Classic and AWS ClassicLink (deprecated)</h2><ul><li>EC2 classic: instances run in a single network shared with other customers</li><li>Amazon VPC: your instances run logically isolated to your AWS account</li><li>classLink: allows you to link EC2 instances to a VPC in your account<ul><li>must associate a security group</li><li>enables communication using private IPv4 addresses</li><li>removes the need to make use of public IPv4 addresses or Elastic IP addresses</li></ul></li><li>Likely to be distractors at the exam</li></ul><h2 id="aws-vpn-cloudhub"><a class="markdownIt-Anchor" href="#aws-vpn-cloudhub"></a> AWS VPN Cloudhub</h2><ul><li>provide secure communication between sites, if you have multiple VPN connections</li><li>low cost hub-and-spoke model for primary or secondary network connectivity between locations</li><li>it is a VPN connection so it goes over the public internet</li></ul><h2 id="transit-gateway"><a class="markdownIt-Anchor" href="#transit-gateway"></a> Transit Gateway</h2><ul><li>for having transitive peering between thousands of VPCs and on premises, hub-and-spoke(star) connection</li><li>regional resource, can work cross-region</li><li>share cross account using Resource Access Manager (RAM)</li><li>you can peer transit gateway across regions</li><li>route tables: limit which VPC can talk with other VPC</li><li>works with direct connect gateway, VPN connections</li><li>supports IP multicast (not supported by any other AWS service)</li><li>share direct connect between multiple accounts<ul><li>VPCs =&gt; transite gateway -&gt; direct connect gateway =&gt; AWS direct connect endpoint =&gt; customer router</li></ul></li></ul><h3 id="transit-gateway-site-to-site-vpc-ecmp"><a class="markdownIt-Anchor" href="#transit-gateway-site-to-site-vpc-ecmp"></a> Transit Gateway: site-to-site VPC ECMP</h3><ul><li>ECMP = equal cost multi path routing</li><li>routing strategy to allow to forward a packet over multiple best path</li><li>use case: create multiple site-to-site VPN connections to increase the bandwidth of your connection to AWS</li></ul><h2 id="vpc-summary"><a class="markdownIt-Anchor" href="#vpc-summary"></a> VPC Summary</h2><p><img src="/../images/AWS-SAA-Review/4.png" alt="" /></p><ul><li>CIDR: IP range</li><li>VPC: Virtual Private Cloud =&gt; we define a list of IPv4 or IPv6 CIDR</li><li>Subnets: Tied to an AZ, we define a CIDR for a subnet</li><li>Internet gateway: at the VPC level, provide internet access</li><li>Route table: must be edited to add routes from subnets to the IGW, VPC peering connections, VPC endpoints, etc…</li><li>NAT Instances: gives internet access to the instances in private subsnets, old, must be setup in a public subnet, disable source / destination check flag</li><li>NAT gateway: managed by AWS, provides scalable internet access to private instances, IPv4 only</li><li>Private DNS + route 53: enable DNS resolution + DNS hostnames (VPC)</li><li>NACL: stateless, subnet rules for inbound and outbound, don’t forget ephemeral ports</li><li>Security groups: stateful, operate at the EC2 instance level</li><li>VPC Peering: connect two VPC with non overlapping CIDR, non transitive</li><li>VPC endpoints: provide private access to AWS services (S3, DynamoDB, CloudFormation, SSM) within VPC, no need to go through internet gateway</li><li>Bastion Host: public instance to SSH into, that has SSH connectivity to instances in private subnets</li><li>Site to Site VPN: setup a customer gateway on Data center, a Virtual Private Gateway on VPC, and site to site VPN over public internet</li><li>Direct Connect: setup a virtual private gateway on VPC, and establish a direct private connection to an AWS direct connection location</li><li>Direct Connect Gateway: setup a direct connect to many VPC in different regions</li><li>internet gateway Egress: like a NAT gateway, but for IPv6</li><li>Private Link / VPC endpoint services<ul><li>connect services privately from your service VPC to customers VPC</li><li>doesn’t need VPC peering, public internet, NAT gateway, route tables</li><li>must be used with network load balancer and ENI</li></ul></li><li>ClassicLInk: connect EC2 classic instances privately to your VPC</li><li>VPC Cloudhub: hun and spoke VPN model to connect your sites</li><li>Transit gateway: transitive peering connections for VPC, VPN and DX</li></ul><h2 id="networking-cost-in-aws"><a class="markdownIt-Anchor" href="#networking-cost-in-aws"></a> Networking cost in AWS</h2><p><img src="/../images/AWS-SAA-Review/5.png" alt="" /></p><ul><li>use private IP instead of public IP for good savings and better network performance</li><li>use same AZ for maximum savings (at the cost of HA)</li></ul><p><img src="/../images/AWS-SAA-Review/6.png" alt="" /></p><p><img src="/../images/AWS-SAA-Review/7.png" alt="" /></p><p><img src="/../images/AWS-SAA-Review/8.png" alt="" /></p><h2 id="ipv6-for-vpc"><a class="markdownIt-Anchor" href="#ipv6-for-vpc"></a> IPv6 for VPC</h2><ul><li>IPv4 cannot be disabled for your VPC and subnets</li><li>you can enable IPv6 to operate in dual-stack mode</li><li>your EC2 instance would get at least a private internal IPv4 and a public IPv6</li><li>they can communicate using either IPv4 or IPv6</li></ul><h3 id="ipv6-troubleshooting"><a class="markdownIt-Anchor" href="#ipv6-troubleshooting"></a> IPv6 Troubleshooting</h3><ul><li>if you cannot launch an instance in your subnet<ul><li>it is not because it cannot acquire an IPv6 (the space is very large)</li><li>it is because there are no available IPv4 in your subnet</li></ul></li><li>solution: create a new IPv4 CIDR in your subnet</li></ul><h1 id="disaster-recovery-overview"><a class="markdownIt-Anchor" href="#disaster-recovery-overview"></a> Disaster Recovery Overview</h1><ul><li>any event that has a negative impact on a company’s business continuity or finances is a disaster</li><li>disaster recovery is about preparing for and recovering from a disaster</li><li>what kind of disaster recovery<ul><li>on premises =&gt; on premises: tranditional DR, very expensive</li><li>on premises =&gt; AWS Cloud: hybrid recovery</li><li>AWS Cloud Region A =&gt; AWS Cloud Region B</li></ul></li><li>need to define 2 terms<ul><li>RPO: recovery point objective</li><li>RTO: recovery time objective</li></ul></li></ul><p><img src="/../images/AWS-SAA-Review/9.png" alt="" /></p><h2 id="backup-and-restore-high-rpo"><a class="markdownIt-Anchor" href="#backup-and-restore-high-rpo"></a> Backup and restore (High RPO)</h2><ul><li>needs more time to recover</li></ul><h2 id="pilot-light"><a class="markdownIt-Anchor" href="#pilot-light"></a> Pilot Light</h2><ul><li>a small version of the app is always running in the cloud</li><li>useful for the critical core (pilot light)</li><li>very similar to backup and restore</li><li>faster than backup and restore as critical systems are already up</li></ul><h2 id="warm-standby"><a class="markdownIt-Anchor" href="#warm-standby"></a> Warm Standby</h2><ul><li>full system is up and running but at minimum size</li><li>upon disaster, we can scale up to production load</li></ul><h2 id="multi-site-hot-site-approach"><a class="markdownIt-Anchor" href="#multi-site-hot-site-approach"></a> Multi Site / Hot Site Approach</h2><ul><li>very low RTO (minutes or seconds) - very expensive</li><li>full production scale is running AWS and on premises</li></ul><h2 id="disaster-recovery-tips"><a class="markdownIt-Anchor" href="#disaster-recovery-tips"></a> Disaster Recovery Tips</h2><ul><li>backup<ul><li>EBS snapshots, RDS automated backups / snapshots, etc…</li><li>regular pushes to S3 / S3 IA / Glacier, Lifecycle policy, cross region replication</li><li>from on premises: snowball or storage gateway</li></ul></li><li>HA<ul><li>use route 53 to migrate DNS over from region to region</li><li>RDS multi AZ, elastiCache multi AZ, EFS, S3</li><li>site to site VPN as a recovery from direct connect</li></ul></li><li>replication<ul><li>RDS replication, AWS Aurora + global databases</li><li>database replication from on premises to RDS</li><li>storage gateway</li></ul></li><li>automation<ul><li>cloudFormation / elastic beanstalk to re-create a whole new environment</li><li>recover / reboot EC2 instance with cloudwatch if alarms fails</li><li>AWS lambda functions for customized automations</li></ul></li><li>chans<ul><li>Netflix has a simian-army randomly terminating EC2</li></ul></li></ul><h1 id="dms-database-migration-service"><a class="markdownIt-Anchor" href="#dms-database-migration-service"></a> DMS - Database Migration Service</h1><ul><li>quickly and securely migrate databases to AWS, resilient, self healing</li><li>the source database remains available during the migration</li><li>supports<ul><li>homogeneous migrations: Oracle to Oracle</li><li>Heterogeneous: Microsoft SQL server to Aurora</li></ul></li><li>continuous Data replication using CDC</li><li>you must create an EC2 instance to perform the replication tasks</li></ul><h2 id="aws-schema-conversion-tool-sct"><a class="markdownIt-Anchor" href="#aws-schema-conversion-tool-sct"></a> AWS Schema Conversion Tool (SCT)</h2><ul><li>convert your database’s schema from one engine to another</li><li>example OLTP: sql server or Oracle =&gt; MySQL, PostgreSQL, Aurora</li><li>example OLAP: Teradata or Oracle =&gt; Amazon Redshift</li><li>you do not need to use SCT if you are migrating the same DB engine<ul><li>on premise postgreSQL =&gt; RDS postgreSQL</li><li>the DB engine is still PostgreSQL (RDS is just a platform)</li></ul></li></ul><h2 id="on-premises-strategy-with-aws"><a class="markdownIt-Anchor" href="#on-premises-strategy-with-aws"></a> On Premises Strategy with AWS</h2><ul><li>ability to download Amazon Linux 2 AMI as a VM<ul><li>use VMWare, KVM, VirtualBox to run VM</li></ul></li><li>VM import / Export<ul><li>migrate existing applications into EC2</li><li>create a DR repository strategy for your on premises VMs</li><li>can export abck the VMs from EC2 to on premises</li></ul></li><li>AWS application discovery services<ul><li>gather information about your on premises server to plan a migration</li><li>server utilization and dependency mappings</li><li>track with AWS migration hub</li></ul></li><li>AWS database migration server (DMS)<ul><li>replicate on premise =&gt; AWS</li><li>AWS =&gt; AWS</li><li>AWS =&gt; on premises</li></ul></li><li>AWS server migration service (SMS)<ul><li>incremental replication of on premises live servers to AWS</li></ul></li></ul><h2 id="aws-datasync"><a class="markdownIt-Anchor" href="#aws-datasync"></a> AWS DataSync</h2><ul><li>move large amount of data from on premise to AWS</li><li>can synchronize to: Amazon S3  (any storage class, including Glacier), Amazon EFS, FSx for Windows</li><li>move data from your NAS or file system via NFS or SMB</li><li>replication tasks can be scheduled hourly, daily or weekly</li><li>leverage the DataSync agent to connect to your systems</li><li>can setup a bandwidth limit</li></ul><h2 id="aws-backup"><a class="markdownIt-Anchor" href="#aws-backup"></a> AWS Backup</h2><ul><li>fully managed service</li><li>centrally manage and automate backups across AWS services</li><li>no need to create custom scripts and manual processes</li><li>supported services<ul><li>FSx</li><li>EFS</li><li>DynamoDB</li><li>EC2</li><li>EBS</li><li>RDS</li><li>Aurora</li><li>AWS storage gateway (volume gateway)</li></ul></li><li>supports cross region backups</li><li>supports cross account backups</li><li>supports PITR (point in time recovery) for supported services</li><li>on demand and scheduled backups</li><li>tag based backup policies</li><li>you create backup policies known as Backup Plans<ul><li>backup frequency</li><li>backup window</li><li>transition to cold storage</li><li>retention period</li></ul></li></ul><h1 id="more-solution-architectures"><a class="markdownIt-Anchor" href="#more-solution-architectures"></a> More Solution Architectures</h1><h2 id="compute-and-networking"><a class="markdownIt-Anchor" href="#compute-and-networking"></a> Compute and Networking</h2><ul><li>EC2 enhanced networking (SR-IOV)<ul><li>higher bandwidth, higher PPS (packet per second), lower latency</li><li>option1: Elastic Network Adapter (ENA) up to 100 Gbps</li><li>option2: Intel, up to 10 Gbps, legacy</li></ul></li><li>Elastic Fabric Adapter (EFA)<ul><li>improved ENA for HPC, only works for Linux</li><li>great for inter node communications, tightly coupled networks</li><li>leverages message passing interface (MPI) standard</li><li>bypasses the underlying linux OS to provide low latency, reliable transport</li></ul></li></ul><h2 id="cloudformation"><a class="markdownIt-Anchor" href="#cloudformation"></a> CloudFormation</h2><ul><li>a declarative way of outlining your AWS infrastructure, for any resources</li><li>for example, within a CloudFormation template, you say<ul><li>I want a security group</li><li>2 EC2 instances using this security group</li><li>2 Elastic IPs for these EC2 machines</li><li>1 S3 bucket</li><li>a load balancer in front of these machines</li></ul></li><li>then cloudformation creates those for you, in the right order, with the exact configuration that you specify</li><li>templates have to be uploaded in S3 and then referenced in CloudFormation</li><li>to update a template, we can’t edit previous ones, we have to re-upload a new version of the template to AWS</li><li>stacks are identified by a name</li><li>deleting a stack deletes every single artifact that was created by CloudFormation</li></ul><h3 id="deploying-cloudformation-templates"><a class="markdownIt-Anchor" href="#deploying-cloudformation-templates"></a> Deploying CloudFormation templates</h3><ul><li>manual way<ul><li>editing templates in the CloudFormation designer</li><li>using the console to input parameters, etc…</li></ul></li><li>automated way<ul><li>editing templates in a YAML file</li><li>using the AWS CLI to deploy the templates</li><li>recommended way when you fully want to automate your flow</li></ul></li></ul><h3 id="cloudformation-stacksets"><a class="markdownIt-Anchor" href="#cloudformation-stacksets"></a> CloudFormation - stacksets</h3><ul><li>create, update, or delete stacks across multiple accounts and regions with a single operation</li><li>administrator account to create stacksets</li><li>trusted accounts to create, update, delete stack instances from stacksets</li><li>when you update a stackset, all associated stack instances are updated throughout all accounts and regions</li></ul><h2 id="aws-step-functions"><a class="markdownIt-Anchor" href="#aws-step-functions"></a> AWS Step Functions</h2><ul><li>build serverless visual workflow to orchestrate your lambda functions</li><li>represent flow as a JSON state machine</li><li>features: sequence, parallel, conditions, timeouts, error handling</li><li>can also integrate with EC2, ECS, on premise servers, API gateway</li><li>possiblity to implement human approval feature</li></ul><h2 id="aws-swf-simple-workflow-service"><a class="markdownIt-Anchor" href="#aws-swf-simple-workflow-service"></a> AWS SWF - simple workflow service</h2><ul><li>coordinate work amongst applications</li><li>code runs on EC2</li><li>concept of activity step and decision step</li><li>has built in human intervention step</li><li>step function is recommended to be used for new applications, except<ul><li>if you need external signals to intervene in the processes</li><li>if you need child processes that return values to parent processes</li></ul></li></ul><h2 id="amazon-emr"><a class="markdownIt-Anchor" href="#amazon-emr"></a> Amazon EMR</h2><ul><li>EMR stands for Elastic Map Reduce</li><li>EMR helps creating Hadoop clusters (big data) to analyze and process vast amount of data</li><li>the clusters can be made of hundreds of EC2 instances</li><li>also supports Apache Spark, HBase, Presto, Flink</li><li>EMR takes care of all the provisioning and configuration</li><li>auto scaling and integrated with Spot instances</li><li>use cases: data processing, machine learning, web indexing, big data</li></ul><h2 id="aws-opsworks"><a class="markdownIt-Anchor" href="#aws-opsworks"></a> AWS Opsworks</h2><ul><li><p>Chef and Puppet help you perform server configuration automatically, or repetitive actions</p></li><li><p>they work great with EC2 and on premises VM</p></li><li><p>AWS Opsworks = managed Chef and Puppet</p></li><li><p>it is an alternative to AWS SSM</p></li><li><p>they help with managing configuration as code</p></li><li><p>helps in having consistent deployments</p></li><li><p>works with Linux and Windows</p></li></ul><h2 id="aws-elastic-transcoder"><a class="markdownIt-Anchor" href="#aws-elastic-transcoder"></a> AWS Elastic Transcoder</h2><ul><li>convert media files (video + music) stored in S3 into various formats for tablets, PC, smartphone, TV etc…</li><li>features: bit rate optimization, thumbnail, watermarks, captions, DRM, progressive download, encryption</li><li>4 components<ul><li>jobs: what does the work of the transcoder</li><li>pipeline: queue that manages the transcoding job</li><li>presets: template for converting media from one format to another</li><li>notifications: SNS for exmaple</li></ul></li><li>pay for what you use, scales automatically, fully managed</li></ul><h2 id="aws-workspaces"><a class="markdownIt-Anchor" href="#aws-workspaces"></a> AWS WorkSpaces</h2><ul><li>managed, secure cloud desktop</li><li>great to eliminate management of on premise VDI (virtual desktop infrastructure)</li><li>on demand, pay per by usage</li><li>secure, encrypted, network isolation</li><li>integrated with Microsoft active directory</li></ul><h2 id="aws-appsync"><a class="markdownIt-Anchor" href="#aws-appsync"></a> AWS AppSync</h2><ul><li>store and sync data across mobile and web apps in real time</li><li>makes use of GraphQL (mobile technology from Facebook)</li><li>client code can be generated automatically</li><li>integrations with DynamoDB</li><li>real time subscriptions</li></ul><h2 id="cost-explorer"><a class="markdownIt-Anchor" href="#cost-explorer"></a> Cost Explorer</h2><ul><li>visualize, understand and manage your AWS costs and usage over time</li><li>create custom reports that analyze cost and usage data</li><li>analyze your data at a high level, total costs and usage across all accounts</li><li>choose an optimal savings plan</li><li>forecast usage up to 12 months based on previous usage</li></ul><h2 id="cheatsheet"><a class="markdownIt-Anchor" href="#cheatsheet"></a> CheatSheet</h2><ul><li><p>CodeCommit: service where you can store your code. Similar service is GitHub</p></li><li><p>CodeBuild: build and testing service in your CICD pipelines</p></li><li><p>CodeDeploy: deploy the packaged code onto EC2 and AWS Lambda</p></li><li><p>CodePipeline: orchestrate the actions of your CICD pipelines (build stages, manual approvals, many deploys, etc)</p></li><li><p>CloudFormation: Infrastructure as Code for AWS. Declarative way to manage, create and update resources.</p></li><li><p>ECS (Elastic Container Service): Docker container management system on AWS. Helps with creating micro-services.</p></li><li><p>ECR (Elastic Container Registry): Docker images repository on AWS. Docker Images can be pushed and pulled from there</p></li><li><p>Step Functions: Orchestrate / Coordinate Lambda functions and ECS containers into a workflow</p></li><li><p>SWF (Simple Workflow Service): Old way of orchestrating a big workflow.</p></li><li><p>EMR (Elastic Map Reduce): Big Data / Hadoop / Spark clusters on AWS, deployed on EC2 for you</p></li><li><p>Glue: ETL (Extract Transform Load) service on AWS</p></li><li><p>OpsWorks: managed Chef &amp; Puppet on AWS</p></li><li><p>ElasticTranscoder: managed media (video, music) converter service into various optimized formats</p></li><li><p>Organizations: hierarchy and centralized management of multiple AWS accounts</p></li><li><p>Workspaces: Virtual Desktop on Demand in the Cloud. Replaces traditional on-premise VDI infrastructure</p></li><li><p>AppSync: GraphQL as a service on AWS</p></li><li><p>SSO (Single Sign On): One login managed by AWS to log in to various business SAML 2.0-compatible applications (office 365 etc)</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;i-passed&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#i-passed&quot;&gt;&lt;/a&gt; I PASSED!&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/../images/AWS-SAA-Review/AWSCertifiedSolut</summary>
      
    
    
    
    
    <category term="AWS" scheme="http://hellcy.github.io/tags/AWS/"/>
    
  </entry>
  
</feed>
